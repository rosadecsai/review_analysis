The authors provide a compelling case that much more biologically plausible sets of initial conditions and sparse, selective supervision and synaptic updates might help substantially close the gap in viewing deep networks as plausible visuocortical models.  	3
The paper provides novel results and important evaluations. There are some methodological issues, but they are not critical to the main point. I therefore favor acceptance. 	3
This paper is well-written and addresses an important question possibly helping bridge neuroscience and machine learning. However, this work falls short on both the relevance of the experimental results and the ad-hoc nature of the proposed approach. 	3
I think the approach and the results are new and interesting, but I have reservations about the motivation for the work and its connection to biology. 	3
The paper shows that jointly doing content driven subsampling and the high level task (semantic segmentation here) can lead to better results rather than doing them independently. This idea may be propagated to other high level vision tasks which are limited by image size of training data. Thus, the reviewer is inclined favorably towards this paper.	3
Overall, this paper has good motivation and proposes an interesting method. But after reading the whole paper, I felt that it is not the proposed method, but the joint loss that contributes most to the performance improvement. Besides, this paper lacks of in-depth explanation of the performance drop when applying the single loss, and comparisons of several similar methods. So I would give score 5 for now.	2
The method mainly extends the learn to zoom approach for classification to segmentation tasks. Importantly, the edge loss is applied to guild the learning based downsample. The authors present extensive experiments and analysis to assess the components of the method. This method brings new ideas or approaches to handle high resolution images, especially in segmentation tasks.	3
The paper is well-argued, and the proposed method is effective. Novelty is somewhat limited but can meet the ICLR bar. 	3
I am slightly leaning towards rejecting this paper. While the writing is high-quality and the analysis is interesting, the practical results are underwhelming, both in sample quality (see Figure 7) and in likelihoods.	2
"- The authors propose a variational neural cellular automaton, which learns to generate images by iterating the transition rule.
- The paper is interesting, with good results, and a good fit for ICLR.
- The paper solves an interesting problem on the topic of neural cellular automata. 
- There are some doubts/limitations that I have asked the authors to address (mainly concerning the architecture of the model).
- There are some missing references and details that would help the readers to get a better sense of the subject.

-----
**After rebuttal**: The authors have addressed my comments. I have raised my score from ""6: marginally above the acceptance threshold"" to ""8: accept, good paper""."	3
Unfortunately, almost all the technical and experimental demonstrations of the paper can be thought of as rather straight-forward consequences of the fact that the decoder is in the end a deep convolutional network. With this perspective, there is not a lot of novelty in the paper and the consequences of the fact that this deep convolutional network can in fact be implemented as cellular automata are not discussed.	2
Generally, I think this model is an incremental improvement over neural cellular automata. While I do think that this approach is interesting and the results are sound, I think there could be more careful analysis of where the VNCA shines compared to existing approaches.	3
The method is novel, useful and well presented, but the paper lacks diversity in the domains evaluated. 	3
The paper tackles an important and relevant problem: solving long-horizon tasks given a few demonstrations. The experiments are quite thorough, but there are some missing details that seem critical to evaluating the performance of the method compared to prior work. Also, it would be good to include a discussion of scenarios in which this goal relabeling method might fail, i.e., when reaching the demonstration states is difficult and so relabeling still doesn’t provide additional learning signal.	3
"My recommendation currently is to reject the paper, because I feel that the experimental results seem to indicate that the engineered features are contributing to the difference in performance and thus creating an unfair comparison, and the novelty of the goal selection method is not currently apparent to me. If the authors would be able to clarify either or both of these points, it would help immensely. If the methods which are compared to are not being currently given the engineered encodings, comparisons to versions which are given that information (as well as additional baselines) would be helpful. 
"	3
Overall I enjoy reading this paper. While the novelty is not steller, the experimental results are convincing and impressive. I recommend acceptance for broader dissemination. 	2
Novelty of the paper is extension of idea of nonlinear-CCA method to sparse case where where number of sample are less than max input feature dimensions (D) N << D. Results presented are convincing and support technical details.	3
The paper is technically sound, but the authors should carefully revise their experimental section to address my concerns.	3
I like the paper overall. However, I think the model interpretation is more important than the prediction for multimodal learning/CCA, specifically, the key strength of this proposed paper is feature selection. 	3
"An interesting paper that solves an ""old"" problem of sparse-CCA using some recent advances in stochastic gating based sparsity solution. Results are comprehensive, but the focus is less on feature selection and more on predictive power. If predictive power is the primary concern, why care about sparsity?"	3
While communication efficiency is an important aspect in federated learning, this paper appears to have limited novelty and also some issues regarding practical feasibility and experiments.	2
This paper proposed a novel idea to reduce communication in FL. However, it seems non-trivial to make this algorithm to work with partial client participation. Without this extension, I feel the current algorithm is more suitable for classical distributed training than FL.	4
I think the claims of novelty in the first half of the paper are slightly inflated. The application to federated learning is definitely novel though and should become the focus of the paper. This can be done by improving the discussion of the related work, providing fair and well-rounded evidence for the low-rank hypotheses, and extending the experiments for federated learning to the biggest data-set and model possible. I am open to increasing my score if my concerns are addressed. 	3
"I appreciate the empirical investigation of low-rank gradient space and the idea behind the proposed method.
However, the current version of this paper is not proper for publication due to the mentioned concerns."	2
This paper has novelty which is introducing a generative model into multitask network, and the effectiveness was successfully supported by the comprehensive experiments.  The reviewer think this paper can be accepted.	3
"Although I think the whole idea is interesting and worthy of discussion in the research community, I still don’t fully believe most of the claims: e.g. the pilot study that tries to support the claim ""directly using images synthesized by an off-the-shelf generative model (self-attention GAN) may hurt the performance on the downstream task"" and the performance differences between using generative models and a more complex data augmentation technique. Therefore, at this stage, I will vote for borderline reject. I will re-evaluate the score based on the authors' response."	2
"I believe this paper is slightly below the the acceptance threshold.

The paper presents a novel joint frame work that includes a generative model for multi-task learning, and quantitative experiments show that the proposed method out-performs baseline methods. 
But, it is hard to interpret the results since:

1. The proposed methods include a generative modeling block, a self-supervision block and a refinement block. Through the ablation studies, it seems that the generative blocks, brings marginal benefits while other blocks such as self-supervision blocks, brings a larger margin for improvements. 

2. Authors claim that the proposed method is better at low-data regime, which is a less convincing remark to give according to the experiment results: The NYU dataset is extremely small, which makes data ablation experiments on NYU really hard to interpret. Does it really make sense to train on 25% of NYU, which is ~250 images, with a deep learning network? The performance drop of ST/MT on tiny taskonomy is less significant than the proposed method, does that mean they are better at low-data regime? 

3. It seems the author proposed a multitask learning framework that outperforms some baseline, but it hard to interpret why. 

Based on theses observation, I believe this paper is slightly below acceptance threshold."	3
The novelty and significance are OK but the writing is not satisfactory. I'll consider raising my score if the authors can address my concerns properly.	3
The paper provides a new perspective of heterophily and GCNs, but there remains some concerns both in the theoretic part and in the experiments.	3
"The authors discuss the interesting question (in the title) and their answers in theoretical and empirical ways. The paper has merits but also has the following weaknesses:

- Table 1 and the following description cannot justify how GCNs outperform other models.
- The edge addition algorithm does not control the degrees, which is the core control variate for the experiments.
- The two-class graph results seem to be insufficient for the main claim.
- Only one specific instance of the degree distribution $\mathcal{D}_c$ for edge addition is used for the experiments.
"	3
"Overall I see the paper slightly above the line, provided some of the above points are addressed (I think this is possible).

The paper should be improved with respect to the following points (see above for more detailed suggestions and comments)
a) provide a more nuanced appraisal of literature and the role of homophily/heterophiliy
b) discuss the roles of the correlation structure between node labels and features more clearly
c) discuss the role of self-loops/ego embedding in mixing information from different classes
d) provide a more rigorous discussion on the assumptions and shortcomings of the theoretical analysis and the SBM model."	3
I think the intuition of this paper is not clear and the experiments are not persuasive.	1
An overall feeling is that the paper is an ongoing work and needs to be carefully written and improved. My recommendation is to reject it in the current form.  	2
Good analytical paper on interesting subject of long-tailed classification. It would be good to see authors thoughts on impact of the amount of data in training, impact of adversarial augmentations, and proposed directions stemming from the analysis.	3
Overall, I like the goal of this paper, i.e., analyzing the bottleneck of long-tailed learning. However, I cannot champion this paper since the data number of the used balanced set is much larger than the long-tailed set, which makes the empirical comparisons unfair and the corresponding finding unpersuasive. Moreover, the arguments in this paper should be written more rigorously. I am glad to see the response of the authors.	2
I think the paper is well written, but I do not see the methodology proposed to be better than the existing ones. 	2
The idea to decompose the function f into lower-order contributions appears to be novel, and the link to GAM’s is interesting. From the empirical evaluation, I am not convinced that the APP approach is better than existing methods for d = 2, 3, while higher-dimensional applications are not motivated.	3
I found the main idea of the paper novel and interesting. I believe the paper is well written and I particularly appreciated the theoretical guarantees that come with this model and inference scheme. However there are some parts of the paper that are confusing, even in the basic formulation section. Therefore, I believe the paper is borderline and could be significantly improved by clarifying the points above. 	3
The paper is not motivated and clear enough. Furthermore, the proposed approach is empirically poorly studied while no theoretical guarantees are investigated. Several claims are not supported by references or proof.	2
The main concern of this paper is the decomposition assumption.  Because of the nonlinearity property in neural networks, how to decompose to the target portion and the background portion for the input features/representations.  It likely has potential interactions between the target portion and the background portion. 	2
"The paper innovatively proposes to decompose the information flow for explainability and provides a new algorithm to construct subgraphs to reveal more complex interactions. This novel way of explaining graph component contributions give insights into a new explainable GNN framework and is potentially inspiring for other works. The experiments also show its superior performance and time efficiency compared with other methods (although some related work is missing). Thus, I am slightly positive of this paper.

====Post Response====

After reading the author response, they have indeed addressed some of my concerns. However, after also reading through the other reviews and their respective replies, I am inclined to keep the score of ""marginally above the acceptance threshold"" because I am still more positive towards the work. I have additionally increased the novelty and significance from a 2 to a 3. Thank you for providing the reply, especially the details regarding my original concern about Graph-SST2. "	3
The paper is easy to follow and has a reasonable method. Besides, the comprehensive experiments, especially the visualization of results, verify the effectiveness of the proposed method. I surely recommend acceptance after my above doubt is solved.	3
"In summary, this paper has its merits. The paper tackles an interesting problem. The proposed framework is generally sound. The paper is organized well in general. However, given its unclear points in experiments and technical details, I do not recommend acceptance at this point. 

====Post Response====

My doubts and suggested weaknesses are addressed partly. I am willing to suggest borderline accept. "	2
An interesting proposal worth discussion.	2
The small scale of the models/data combined with the lack of evaluation of alternative approaches diminishes the potential contribution of this work.	2
The preparatory module appears to function well, and the quenching activity is reasonable well tied to biological thalamocortical loop actions. To my knowledge, the motif-transition problem is not solved in switching recurrent networks, however I am not overly familiar with that literature. Overall, my low confidence ratings come from not knowing how interesting or novel the analytical work in the appendices is.	3
"The general topic of this paper is relevant, but I found the presentation confusing and difficult to understand. Overall, given the current state of the presentation, some issues with the details of the continual learning experiments, and some concerns related to the existence of a trivial solution that would need to be addressed, the contributions of this paper remains unclear to me and I suggest a rejection.
"	2
The paper demonstrated an interesting problem addressed by a standard transformer based model. The potential of generalization to more tasks is not clear, and the improvement over single task models falls below expectation.	2
I am a researcher interested in UI and published many paper of UI on top conferences.	2
Overall, the paper showed how multi-task learning can improve performance on several downstream UI modeling tasks. These insights might be relevant to the UI modeling community. The authors use several popular ideas in the field of multi-modal, multi-task learning and showed it's effectiveness on multiple downstream tasks. The paper is well written, and the ideas are clearly presented but it lacks in technical novelty. Additionally, the paper lacks in a few other areas -- comparison to single-task models with respect to computational cost, inference time etc, the downstream tasks are similar but the performance gains aren't significant. Due to these reasons, I think the paper is marginally below the acceptance threshold. 	2
The proposed method is a combination of two previously established methods, Dreamer and SwAV. This combination and its applications in MBRL is novel, however, I don't find it to be particularly significant. Reconstruction free MBRL from pixels is not a new problem and the paper does not provide enough theoretical (nor empirical) insights as why a prototype-based method is a better fit for such setting vs other methods. 	2
The paper is well-written and the presented approach shows promising results on the selected benchmark. However, it seems that the paper lacks novelty at the moment, and I would like to see a more thorough experimental section with more visualizations. Based on these, I do not recommend acceptance at this moment.	2
Overall, I found the paper to be a novel exploration of a natural combination of two methods from MBRL and SSL.  The paper makes modest claims, and supports them with good evidence and a well-written explanation.  Although the results show empirical improvements, the paper could do more to contribute to a more general understanding of these improvements and the approach itself.	3
I think it is a very reasonable approach to combine cluster assignment to MBRL and the paper does a good job addressing the temporal consistency, a challenge that is not presented in model-free RL. However, I am not confident how the method could scale to higher complexity environments. I would be happy to increase my score if the above concern is addressed. 	3
"The research topic is interesting and it seems that the main answer points of the research questions are appropriate.
However, main claims in the paper are not so clearly validated with respect to both of theories and experiments.
I think that the authors should reconsider whether the differences on conditional utilization rate and those of conditional learning speed really affect the generalization performances, or they should try to establish other strategies to find evidences to support main claims.
From the reason, I recommend this paper as reject."	2
This paper is easy to read and addresses the shortcomings of vanilla multi-modal training scenarios. Authors have done a good job of explaining the problem followed by laying out their hypotheses clearly. They have done experiments which enforces the hypotheses empirically. Authors should explain the re-balancing step (in section A2) and how to derive uni-modal predictions from multi-modal DNNs (in section A1) more clearly with consistent notations as used throughout the paper.	4
This paper focuses on an interesting phenomenon in the multi-modal learning: the imbalance problem in the multi-modal training. A training algorithm, balanced multi-modal learning, is proposed to solve this problem. According to the provided experiment results, this method addresses the issue of greedy learning. However, the author did not provide solid theory analysis about not only the greedy learning problem but also the proposed method. Related works are also not well discussed and compared.	2
This paper focuses on the multi-modal interaction problem, that multi-modal models tend to rely on just one modality while under-utilizing the other modalities. Since conditional utilization rate cannot be computed efficiently during training, they introduce an efficient proxy based on the pace at which a DNN learns from each modality, which we refer to as conditional learning speed. So they propose a training algorithm, balanced multi-modal learning, and demonstrate that it indeed addresses the issue of greedy learning. The proposed algorithm is found to improve the model’s generalization. So I recommend weak accept.	3
"The paper is unclear in the description of the method, so much that
some trivial solutions seem even more appropriate for the setting.

Substantial clarifications are needed to properly evaluate the quality
of the contribution.

AFTER REBUTTAL:

The authors did manage to address many of my concerns in their detailed rebuttal and extended experimental evaluation. The difference between the proposed setting and the existing alternatives is now clearer and the advantage of the method better highlighted thanks to the novel experiments. Some aspects are still a bit unclear (e.g. how the method is applied at the test phase), but I am now more inclined towards acceptance. "	3
I find this paper very interesting. The proposed approach is novel, yet simple, and clearly works well for the problem of learning from open-ended data. I would recommend to accept this paper. 	3
"Interesting approach and novel concept of mapping rank, the experimental evaluation could provide more direct evidence for the value of the contribution. 

Update after authors‘ response:
The authors addressed all points and provided convincing additional experiments, I’m hence increasing my score."	3
Conceptually and theoretically strong but line of argument needs empirical evidence to work out. Though evidence is provided in form of empirical results on three data sets, the baselines remain unclear. This problem need to be solved before publication would be OK (could be straight forward to do so) 	3
I recommend reject, because several claims are wrong or not well supported, and the contribution over related work is not sufficiently discussed or demonstrated.	3
Since a bunch of significant baselines are absent and the details of the dataset are missing, the reviewer thinks the paper in the current version is not suitable for publishing on ICLR. The reviewer does think that a revised version will make the paper much more appealing.	2
"The paper gives much importance to a graph-based representation of interacting systems. However, the proposed formulation doesn't seem to leverage the graph in full. Actually, it might even be possible to describe the entire method without discussing nodes and adjacency matrices.

Some elements of the method are not convincing, e.g. prior function, graph matching, reconstruction loss. Other components, e.g. the contrastive loss, are under-specified. Some claims are not well supported: flexibility of KINet wrt the parameter K and effect of the contrastive loss.

Overall, the method of the paper sounds promising, but I don't recommend it for acceptance in its present form. I suggest polishing the method, re-evaluating the role of the graph representation, revising some vague claims, clarifying some definitions and formulas, and incorporating more experiments and ablation studies."	3
This work propose to achieve multi-object position prediction with only using visual information like images without ground truth positions as supervision. But the detailed designs are not sufficiently novel.	2
Overall, this is an impressive work both in terms of its comprehensive mathematical presentation and its high-quality experimental results. Personally, I find the experimental results to be the most interesting aspect, because it opens the possibility of accurate MI estimation in complex scenarios studied in contemporary generative modeling. I recommend this work be accepted.	3
"The reviewer is convinced by the theoretical analysis of this paper and the novel perspectives it proposed. 
It makes some valuable contribution to our community. However, I am concerned about the practicability and efficiency of this method. "	4
This is an interesting work and my major concerns from the last round of paper reviews have been adequately addressed. As such I recommend acceptance. 	3
This submission looks more like a working draft rather than a conference paper.	1
"- The premise is interesting.
- The clarity of attribution in the background section needs to be worked on.
- ""Niche"" notations used but not defined.
- Experimental results on a single dataset without important details (training set size, training times, details of training algorithm etc0."	3
Very limited theoretical and empirical results.	1
"The paper is well written, the motivation of the authors is clear, and the framework presented is easy to understand. The theory part is written in a clear mathematical manner, with a theorem-proof structure, which I really like. The theoretical results provide new insights and should give new impulses to other scientists in this field as well as in related fields, e.g.: for improved applications as well as building theories.

My main criticism is that the good theoretical results are not sufficiently well supported by numerical experiments, which could further consolidate the good theory results. I hope to provide a possible motivation for supplementing the results in my review. However, these would have to be provided in order to recommend a publication."	3
Potentially interesting given the accuracy, but unclear	3
"In summary, I find the social impact of this work, but it still needs to be refined carefully. If there were an option for 'Weak Reject', I would select that. Since there is no such option, I recommend rejection given the many suggestions.

______________
After reading the rebuttal and other reviewers' comments, I consider arising my score to 6, although there are still many inappropriate points in the current version."	3
The paper proposes a new approach for diverse sampling of clients in each update round of federated learning. This significantly improves convergence rate and model accuracy. Though the idea is not particularly novel in the broader context, it does advance state of the art in federated learning.	3
"The problem considered in the paper is interesting and well motivated. However, the formulation is
incremental, and builds quite heavily on earlier paper. The main contribution is the analysis of the
greedy algorithm, which has serious issues as mentioned above."	2
The paper is interesting and well-written. The convergence analysis seems to be correct. However, the theoretical role of submodularity is not quite clear, besides the use of the greedy algorithm for the selection of clients. Assumption 1 seems quite strong, the authors should motivate this part more in detail. The experiments clearly show the advantage of DivFL over previous approaches in the literature.	2
Overall, I think this is a nice paper that addresses an important aspect of federated learning: improving a client sampling strategy. Experimental results are promising, though I would like to see how not limiting the number of local updates will affect the performances. If this brings a considerable gap between DivFL (ideal) and DivFL (no overhead), that implies the limitation of the proposed approach for practical settings. 	3
"The novelty of this paper is high.  However, the motivation is not quite clear, and  more IW- and DA-based methods are expected to compare in the experiment.
"	3
In summary, this paper targets at an interesting problem and shows promising theoretical and empirical results, which could be helpful for the community of recommender systems. 	3
"Overall, I liked the paper and I found the approach interesting. I think the authors should link their approach a lot more to the work presented in [1] and also covered in two of their references [2,3].
I think by doing so, it clarifies that the main difference in the problem domain is that we are in control of which of the counterfactual worlds we want to compute the risk in. 
Otherwise, all of the vocabulary and the core explanations of the soundness of the approach remain the same.

[1] @inproceedings{shalit2017estimating,
  title={Estimating individual treatment effect: generalization bounds and algorithms},
  author={Shalit, Uri and Johansson, Fredrik D and Sontag, David},
  booktitle={International Conference on Machine Learning},
  pages={3076--3085},
  year={2017},
  organization={PMLR}
}

[2] F. D. Johansson, D. Sontag, and R. Ranganath. Support and invertibility in domain-invariant representations. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 527–536. PMLR, 2019. 
[3] F. D. Johansson, U. Shalit, N. Kallus, and D. Sontag. Generalization bounds and representation learning for estimation of potential outcomes and causal effects. arXiv preprint arXiv:2001.07426, 2020. "	3
 I think this paper is potential. The proposed domain transportation strategy for the recommendation is quite interesting. I suggest the authors focus on this part and give more discussions about the motivation, interpretations, and related works. 	3
While the motivation behind the learning representations of a gesture sequence instead of single frames is interesting, it is not executed very well. A lot of time has been spent explaining prior work instead of the contributions. The experiments to analyse the learnt representations is also lacking. Some of the details in the paper are confusing and hence needs significant revisions. And finally, being a generative modelling task, it would have been good to see some example videos. 	2
"The proposed method is technically sound and attempts to solve the challenging problem of co-speech gesture generation using an insightful approach, that of vector-quantizing the latent representation space of gestures and mapping from natural language sentences to it. However, a number of technical details were unclear to me, which makes the overall method and evaluations hard to follow. I invite the authors to respond to my comments under ""weaknesses"" before I can strongly recommend the paper for acceptance."	3
"- limited technical novelty
- limited experimental analysis"	1
This paper explores an interesting problem of unsupervised representation learning in videos. Although the hierarchical architecture based on latent variables is very similar to the temporal abstraction structure in VTA [Kim et al., 2019], I think the proposed criteria for event detection are novel and reasonable. However, my major concern is about the insufficient experimental results (see my comments above), and so I give this paper a Weak Reject at this moment. 	3
I believe the paper has a very nice core idea, and a well thought out 'harness' around that idea. I miss more conceptualized explanations, and would have loved more experiments (and also more comparisons to comparable models).	3
"I currently believe this paper is borderline and I am leaning to recommend to reject. 
This is because I think the way the paper frames VPR as being comparable with prior methods like CW-VAE and as a potentially-useful framework for long-term prediction is promising, yet more work is needed (described in my review) to take this work from an exploratory stage to ready for publication. I look forward to hearing from the authors during the rebuttal period on this matter.
==========================================================================
Update after author response: The authors have provided satisfactory responses to my concerns and revised the paper with extra results and discussions accordingly. I have increased my score to a 6."	4
This paper presents a novel, well-motivated method for inferring temporal hierarchy in latent representations of sequence data. I lean towards accepting this paper, and would be willing to further increase my score if some of the concerns above (especially regarding the experiments) are addressed in revision.	3
The similar idea has been conducted in the work AdamP. And the compared methods are not convincing in the experiment part.	2
The proposed training algorithm is simple, but showed to be able to improve network generalization ability apparently.  I mostly like the results, but the proposed algorithm should be more thoroughly analyzed and compared as pointed out in the above comments.	2
Overall, see some issues with this submission that could be addressed by the author(s) during the rebuttal. I am willing to improve my score when my concerns are addressed appropriately.	3
In spite of its interesting motivation, the paper does not properly handle DNNs with Batch Norm such as ResNets and many variants, and the  way the weight norm constraint is implemented is confusing.	2
The idea of the paper is novel, interesting, and clearly exposed. However, a wealth of clarity issues in the experiments section hinder the clarity of the paper overall.	4
All in all, while I like the general idea of utilizing spectral graph information of bipartite network connectivity graphs for pruning decisions, I find the paper is lacking a clear message and the required experimental evidence to back it up. There are no comparisons to previous pruning techniques, which would allow to showcase significance. Therefore, I tend to vote for rejection.	2
"As the merit of the proposed eigen-bound based pruning is unclear, I believe the work is not suitable for publication at ICLR in this form.
"	2
"In general, the paper is well written. I hope the authors should first answer the aforementioned questions cautiously and carefully.
"	2
Overall, the paper is well written with each part of the results clearly explained and well supported. This is a highly interesting piece of work for the SPS problem, but the significance of its contribution is vague to me.	2
As i mentioned in my main review i find that the paper is well written and the main contributions are clear. To the best of my knowledge this is the first paper that provides a stochastic variant of the projected splitting algorithm for solving monotone problems with convergence guarantees.	4
"The problem considered is of interest, but the proposed algorithm does not fully address the need of the applications. The manuscript presentation also has a great room for improvement. The current organization seems to be incomplete given the length of the main paper.

After reading the revision and the responses of the authors, and discussing with the authors, my recommendation moves to weak rejection. The idea is indeed interesting, but there are still unclear parts in the text and organization, and the motivation for the algorithm seems more like finding extensions for an existing work, instead of a real need for the considered problem."	3
"The paper contains interesting new methods which exhibit very good results in a limited experimental evaluation. However, the exposition is lacking in clarity to me. Therefore, I would currently classify the paper as borderline, but I am open to adjust my score based on the authors' response.

**Edit**: Due to the author's response, I raise my score from 6 to 8."	3
The idea of this paper is original, and both RieszNet and ForestRiesz show strong experimental evaluations. But the paper needs to be well-organized. Therefore, I would recommend acceptance of this paper if the paper was well-written.	3
This paper provides a solid contribution towards the automatic estimation of policy-relevant causal estimands. I think this work should be accepted. However the paper could be substantially improved by improving exposition and making the paper accessible to a wider audience.	2
I find it difficult to evaluate the contribution of this paper because it is not clear what the contribution of this paper is. The method is novel, but the theory relies on existing different work, so I think the evaluation of the method highly relies on the novelty of experiments. However, the experiments are very simple and not enough to claim the usefulness of the method. If the methodological and theoretical contributions are significant, the paper should be highly evaluated even if the experiments are simple. However, in this case, because this paper has less novelties in  the theoretical part, the experiments are important. In summary, the method in this paper is very novel and interesting, but the experiments to support it are lacking. Besides, there are several issues in writing even though we can guess what the authors insist. This is the reason why I vote for weak rejection.	2
The paper is providing good insight about model explainability and proposes interesting concepts. Reading the paper, it seems the presentation, storyline and experiments could be improved. I find it a bit too much scattered between the supplement and main paper.  	2
"The contributions of the paper (the proposed saliency method and the new saliency metrics) are modest and their significance is not clearly explained in the paper or supported by experiments. The paper should explain the concepts of completeness, soundness, and consistency and justify why they are desirable. The experiments should support the main contributions: they should show that the proposed metrics are useful and that the proposed saliency method outperforms baselines on these metrics. 
"	2
The concept of the proposed method is innovative, but many details of the method have not been explained clearly. In addition, more persuasiveness is needed in the experimental performance.	2
While I appreciate the motivation and the overall idea of measuring soundness of saliency, the definition of correctness and soundness is a bit unclear, and the effectiveness of the proposed method is not sufficiently demonstrated by the experiments. Considering the strengths and weaknesses of this paper, I don't think it is ready for publication at this moment. 	2
"* The methods of searching the network structure consider each component to achieve both performance and latency. 
* Comparison with other SOTA network search methods is inadequate but necessary.
* Although the method is straightforward and not very interesting, it indeed gets great improvements than the baseline. "	2
Though the proposed methods might be fruitful only for object detection of wider range of object scales, the experimental evaluations showed that they successfully improved the detection accuracy for small faces.	3
The paper uses computation redistribution and sample redistribution for searching the best architecture based on TinaFace.  I'll say that there is enough innovation from the paper, and the result is very good. Although I have some questions posted in the main review, I think the paper is acceptable in the conference.	4
The idea of search an optimal structure considering all components to achieve both computational efficiency and performance is novel and the experimental results have supported the claim. However, details are missing in the main contributions -  searching network structure and searching scale. The search strategies implemented are rather straightforward.	2
"Overall, this is an interesting paper to explore audio-visual navigation in more complex environments and the proposed sound adversarial audio-visual navigation is well-motivated. The extensive experiments can demonstrate the superiority of the proposed method in handling sound attacks. But, I do have some concerns about the method and results. My current rating is borderline accept. But, I would like to upgrade my rating if the authors can address my concerns during rebuttal. 

***Post-Rebuttal***

The authors successfully addressed my concerns in the rebuttal. I am happy to upgrade my rating. 

There is a very recent work on audio-visual adversarial robustness. I'd like to suggest the authors to discuss this relevant work in the revised paper. 

[1] Tian, Yapeng, and Chenliang Xu. ""Can audio-visual integration strengthen robustness under multimodal attacks?."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021."	3
"The environment with the attacker is practical and interesting. The authors well-formulated the action of the agent and attacker.
Also, including the appendix, they tried to provide various experiments and analyses.
BTW, I have some questions to convince the efficacy of the proposed method."	3
The paper presents interesting solution for AVN in noisy conditions. The results are better than baselines, but the paper would benefit from more ablations. 	3
The paper does not give enough information to claim the importance of this paper, neither the method nor the results. 	1
Overall the paper is well written and I am not aware of this exact method being used in the literature, but the improvement strikes me as marginal in results with unclear motivation.	2
"The introduced method is somehow novel, but the experiments have very limited breadth and analysis don't go beyond discussion.
Gains are also limited, and it's not clear looking at the current results that the solutions proposed really answer the problems exposed in the introduction (exposure bias, task-relevant parts, overfitting in low-data regime)."	3
Because of my concerns in the rigor and clarify in both the technical approach and the experiments, the paper needs further development and is not yet ready for publication at ICLR. Therefore, I unfortunately cannot champion acceptance for this paper at this time.	2
In sum, the reviewer think the setting of this paper, the writing clarity, and the simulation study may need further revision and improvement before getting published. 	2
"This paper presents a very interesting idea with competitive performances to a very relevant problem in real world applications.
The exposition of the paper however needs to be greatly simplified by giving a lot more intuition and implementation details. Due to this, in this current state I am afraid that the paper would not have the impact it deserves in the ICLR community.



=====================

*Updated review after the author's rebuttal and revision of the paper*:
I have read the paper once more, the authors did a great job improving the exposition of the paper, which is now way easier to understand and accessible to a wider audience. I have therefore increased my score to an accept."	4
"The proposed formalization and method are interesting. However, the presentation is not focused on the core contribution, the method is lacking detail, and the experiments do not characterize the method well.
"	2
"Overall, the paper is very rigorous in proving the mathematical guarantees of the proposed method, but the method itself is not performing that well for harder real word problems. My evaluation is based on the simplicity of experiments for scenarios where NURD worked. For the harder case (even when classification bias is introduced to the data), NURD does not perform particularly well.

Post-revision Summary
The authors have nicely addressed most of my comments. The text is still heavy on math over intuition, but improved. I am thus increasing my score."	3
This paper proposes an interesting idea for an important problem. It provides a good theoretical and experimental analysis of the proposed method. I recommend accepting this paper. 	3
The paper proposes an incremental method that improves over past work for to ignore the infeasible state transitions. The idea is incremental, but has a good potential. The evaluation could be improved to show the impact of the given method. I think the paper can also use a higher level use-case of reward comparison where the proposed method is used to design a reward function to learn a problem that is otherwise unsuccessful.   	3
"Although the overall method is still quite similar to EPIC, this paper proposes a reasonable solution to overcome a clear limitation of EPIC. In addition, the experiments are quite well-designed, and the paper is very well-written. 


---
Update after the rebuttal: Thank you for adding experiments with learned transition models. The results look good. Since my major concern is addressed, I increased my score."	3
Overall, the paper is well written. My current score is 5 and I look forward to further discussion with the authors.	2
This paper makes progress on the important problem of comparing reward functions by introducing a new reward distance that addresses a significant shortcoming of the EPIC distance. The results are significant and novel, and I would recommend accepting the paper.	3
Overall, I think the paper provides a new perspective of backdoor defense, but it could be made stronger by addressing some critical aspects as listed above.	2
The paper proposes a good idea for using adversarial attack patterns to diagnose if backdoors exist in models. My only concerns are twofold. First, some adaptive studies could be conducted, analyzing the scenarios where the proposed detection method could be circumvented. This may not be very difficult since this paper assumes that the backdoor patterns are focused patches. Second, the proposed idea seems to be similar to using interpretation (e.g., heat maps) to detect backdoor, since adversarial attack = inversed interpretation. From this perspective, the proposed idea does not look that novel.	3
"While there are some gray areas around the proposed method (please see the strengths and weaknesses), I believe that this is a well-written, thought-provoking paper that can be interesting to the community and bring forward fruitful discussions.
As such, I vote for borderline acceptance of the paper.
If the authors can provide convincing answers to my two questions in the main review, I would be happy to increase my score."	4
A practical and novel backdoor detection with theoretical guarantees.	3
"The proposed benchmark would be worth in a different way: not for a universal OOD detection, but adversarially robust OOD detection.
"	2
"This paper provides an interesting angle for investigating/evaluating OOD detectors. The paper is well-justified, with a mostly reasonable setup, and would be of interest for broader OOD community as a new metric.
"	3
This paper presents a very interesting approach towards an objective and unbiased evaluation of OOD detection. While the proposed approach does not fully address the problems of existing evaluation protocols it identifies, I find the solution to be novel and intuitive, and the empirical results to be worthy for the community. Overall I am leaning towards accepting the paper, and would appreciate if the authors can address the questions enumerated in the Cons section above during rebuttal period.	3
I am worried about the handling of previous work in this paper, and it seems to me to be overselling its contributions. For these reasons, I would vote for rejection.	2
Although the paper studies an interesting setting and identifies an interesting failure mode of momentum methods, I believe the theoretical contributions as well as implications to practice are not strong enough to grant acceptance (comments 2 & 5). There is room for improvement in terms of presentation (comment 3), and also the analysis is carried out only using the expected gradients which I find somewhat doubtful (comment 4).	2
I recommend accepting this paper. The empirical results which I am able to evaluate are correct, useful and informative. I cannot attest to the mathematical validity of the paper, unfortunately.	4
The paper studies the influence of data covariance shift to the performance of SGDm. It is important to some machine learning applications. The authors uncover a resonance effect as a mechanism behind this influence. The insight is novel, the numerical experiments are extensive, while the theoretical study can be improved. 	2
Great and thorough paper, but empirical example that clearly demonstrates relevance would be appreciated.	4
"Interesting paper which suffers slightly from overpromise.
Landscape analysis is insightful but the gradient descent analysis is limited and does not explain the gap between architectural error bounds and actual error obtained."	2
"# Post-rebuttal update

I thank the authors for correctly pointing out that I misunderstood their work I my initial review. Given their correction, I see this as a stronger submission, and raise my score. However, I still find it lacking comprehensive experiments to claim that their bound/estimator reflect the implicit bias of the architecture, and the discussion about margin is either lacking rigor or is not completely clear to me at this time (namely, I see why for NNGP concentration on the mean is beneficial, but I struggle to understand the precise analogy with the GD, given that NTK posterior mean can often underperform compared to GD networks). 

Given that I post this update late in the discussion period, the paper contains otherwise decent ideas, and I may be misunderstanding some aspects of the margin discussion, I am willing to score this as a weak accept but with low confidence.

# Original review

While I think the paper investigates interesting and important ideas, and the writing appears of high quality, I find that experimental evidence for the claim that GD provides a substantial benefit beyond NNGP (section 5.2 and 5.3) is limited to a highly specific setting. I see this as the  the main theme of the paper, and believe it requires much better evidence in terms of both quality (""Nero""/SGD/GD/NTK/... ablations, NNGP regularization tuning, controlling for trivial settings like dataset rescaling) and preferably quantity (different datasets, sizes, architectures etc).

Otherwise, the paper also proposes a formula for computing the average classification error of a GP in section 5.1, but I am not sure how this is better than just Monte Carlo sampling the error (perhaps the authors could address this in the rebuttal, although my main concern remains the lacking experiments above).

Finally the authors also propose two bounds on the average GP classification error. However, these bounds aren’t compared to NNGP performance (instead, finite-width GD NNs are used). If NNGP performance is similar to GD NNs in those settings or worse, then IIUC these bounds are not tight and do not appear very practical. Since I’m not an expert in this area, I am also open to being convinced of their importance in the rebuttal, however, I again emphasize that my main concern is the limited experimental setting.

Together, I commend the three efforts above, but I do not find them significant enough for publication at this time.
"	3
"I acknowledge that this work introduces some relevant contributions, but, based on the above weak points,  I lean to reject this paper. I find that the main contribution (Theorem 2) to be incremental and that the rest of the analysis do not provide novel and relevant insights. 
"	2
The paper presents some insightful results on the implicit bias on architecture and gradient descent as an optimization method. The theoretical discussion is interesting but the experimental validation could be improved.	3
Based on the above concern described above, I currently tend to weakly reject the paper but would like to hear more feedback from the author and discussion from other reviewers towards the final decision.	2
In general I believe the idea is worthy of publication. However the concerns I presented, especially the fact the evaluation does not completely match DAGNN's and the relevant graph Transformer literature is not compared against, means the paper could benefit from another iteration of improvements. If the authors satisfactorily update the paper in response to my comments, I will back the paper for acceptance.	3
"The proposed method shows favorable benefits compared to previous work. Overall, the strengths outweigh the drawbacks. Thus, my current recommendation is ""Weak Accept""."	3
"The paper misses a significant body of related work on directed graphs and graph adaptations of Transformers, which would be necessary for a fair evaluation of their approach. The specific architecture choice seem fine but not especially novel or well motivated, and there are many missing details regarding the experimental results.

**Edit (Nov 23):** The authors have provided experimental results for other Transformer-based graph models, and PACE still shows strong performance on their tasks. Many of my questions about missing details have been answered in the discussion, although it is difficult to evaluate this fully without a revised version of the paper."	2
Interesting and novel perspective on various gradient estimators for the sign function used in binary activations. The main drawbacks are that there is no empirical data to support any of their claims, several aspects are unclear based on the manuscript and the paper could improve in its overall clarity. In the current version and my understanding of it the drawbacks outweighs the strong points.	3
I vote for rejection mainly due to the questionable analysis (lack of rigor) and the difficulty to interpret the relevance of the result. Also the work does not go beyond one-hidden layer network as it claimed in the abstract.	2
The paper provides a convincing in-depth analysis of STE properties for a particular simple combination of model and data distribution. However, the novelty is limited in view of the previous work Yin et al. (2019). Its significance is restricted due to the very specific assumptions on the model, data distribution and loss.	2
"While there is some novelty to the paper, I don't think the importance of the results and the novelty level
are high enough for the ICLR standards.
Moreover, there is a complete lack of experimental validation and some poor writing. 
I cannot recommend the manuscript for publication on ICLR. 
"	2
"The paper presents an approach for improving self-supervised learning approaches using a supervised teacher model. The problem is important, but the proposed method and the results aren't convincing.
"	2
Authors add more labeled training data, as an external scorer, and still results are significantly better. Authors need to compare against SoTA models where all models see exactly the same data. 	2
The paper uses ASR based scores to mask high-confident regions in masked speech modeling. Using this weighting idea is not very novel even though it may be the first implementation in the speech domain. The main strength of the paper is that it provides extensive comparison for different aspects of the approach. The writing may benefit from some notational fixes and addition of some implementation details as discussed in the detailed review. From the discussion above, it can be seen that there are more weaknesses as compared to the strengths.	3
"This paper explores the use of weighted masking for learning speech representation. The proposed scheme relies on confidence scores obtained from external ASR systems to decide which speech segments should and should not be used for masked modelling. An extensive empirical investigation shows that weighted masking is a promising direction for learning accurate speech representations. 

Although the technical novelty is limited, this paper introduces weighted masking by means of confidence scores (very popular subject) into speech representation learning area. Unfortunately, this paper currently contains a number of technical issues (see above). Furthermore it offers a limited insight about the quality of confidence scores used and sensitivity of the proposed scheme to this important in practical applications factor (e.g. limited resource languages where WERs of typical external ASR systems range wildly).

"	3
Overall, this is a well-written paper which uses interesting techniques and makes decent progress on an interesting problem, and hence I am in favor of acceptance.	4
"The paper makes progress towards answering, for randomized
algorithms with on-average uniform stability, such as stochastic gradient descent
(SGD) with time decaying learning rates, wheter
these deviation bounds still hold with high confidence over the internal randomness of algorithm.

However clarity can be improved in showing how far paper's results are from lower bounds."	2
"Overall, the authors proposed an interesting idea to improve the stability-based generalization bound for randomized algorithms. I thus tend to comment for acceptance.
"	3
 The paper proposed to use the confidence-boosting/bagging techniques to boost the dependence of learning algorithm from $1/\delta$ to $\log (1/\delta)$.  Specific applications to SGD are derived.   Overall, the paper is well written and the results are new and interesting. 	3
I would tend to accept this paper as it is novel enough and supported by empirical experiments.	3
The paper introduces novel frameworks to evaluate and perform slice discovery. Though technical novelty in the individual parts are limited, the overall framework seem to be interesting and perform well according to the experimental results. The understanding of the paper suffers from the way how it is organised and should be further improved. 	2
The paper addresses an interesting problem but is hampered by the ad-hoc nature of the approach and the lack of clarity in the problem formulation and writing.	2
The provides a variation over  confidence calibration that takes into account the identity of the top label. This variation makes sense under some scenarios, but is still a quite a weak notion on its own. While it's an interesting notion to discuss, I think the contributions of the paper are too thin to justify publications.	3
Excellent paper with good results and clear practical applications. The work is also well placed within existing literature.	3
There are several problems with the empirical evaluation and the analysis. The submission (including the many appendices) also seems heavily overloaded, to the point that the main message becomes quite unclear.	2
Interesting and well-motivated idea. Datasets in the experiments are quite simple. 	4
Overall, this paper seems to have limited contributions for adapting pre-trained vision-language models to downstream tasks. The depth of the analyses is limited and does not provide interesting and novel findings to the community. 	1
"The paper presents an empirical study of how to fine-tune CLIP for downstream tasks. 

My main concerns are: 1) from my subjective view, I am not sure the conclusions are significant enough; 2) full fine-tuning results are missing.

I recommend adding the full fine-tuning results for a full picture and being more specific about the reason to exclude full fine-tuning. It would be good to provide a compelling reason to favor these methods over full fine-tuning in practice."	2
This paper had been well written if entitled “How to Adapt CLIP to Downstream Image Classification Tasks”. However, considering the insufficient experimental evaluation and limited technical novelty, I could only give it a score of 5. 	2
The paper proposed a useful technique LN tunning for fine-tuning the pre-training CLIP model for downstream image classification tasks. However, the generalizability of this approach is not demonstrated, and the proposed recipe of combing LN tunning with other fine-tuning methods seems also not useful as it typically shows even lower performance than only using LN tuning. Overall, this paper does not provide enough support for it to be published at ICLR.	2
"In the sub-field of high-dimensional random feature regression, the paper present good contribution and results to consider the anisotropic data distribution, though the proof techniques mostly follow from previous works. Therefore, I think the paper has its pros and cons, depending on the ""weight"" of technical novelty. In all, I feel the paper is above the borderline of acceptance."	2
"The topic of this theoretical paper is of interest to the machine learning community, and the contributions given would extend understanding of the generalization behavior of high-dimensional, overparameterized models. My current recommendation to reject is because of the concerns mentioned above about the correctness, rigor, and clarity of the proof of the main theorem, Theorem 3.1. 

======================

Update on review (11/29)

Following the discussion and revision, my confidence has improved on the correctness of the results. Further the surrounding literature that was overlooked is now addressed, especially in Appendix A. 

This work does have some lack of novelty since versions of Theorem 3.1 were known and multiple descent phenomena and some effects of alignment had been recognized in anisotropic random feature regression. Still the main contribution of this paper is the new phenomenology that it identifies, which to my knowledge include the behavior of the bias and variance under overparameterization and (mis)alignment, steep cliffs in the learning curves and their relation to spectral gaps in the covariance matrix, and that parameter-wise descent is limited to double descent. This in itself is a noteworthy contribution. 

This paper would be further strengthened by establishing the equivalence of the formulas here with those of d'Ascoli et al '21 and Loureiro et al '21. This would help compensate for the loss in novelty. Also, while the relationship with d'Ascoli et al '21 seems well explained, not as many details are given about the connections with Loureiro et al '21. I also think that this paper needs to be careful regarding claims that it sets these prior works on rigorous footing because (i) the proof of the linearization invariance in Theorem 3.1 is not contained in this paper,  and (ii) to my understanding, the formulas of d'Ascoli et al '21 and Loureiro et al '21 are rigorous modulo linearization invariance. To summarize, the manuscript would benefit from further revision to clarify its contributions and their place in the literature.

All things considered, I am now slightly leaning toward acceptance."	3
"Some detailed typos and remarks:
* page 3 on the model: I do not understand why one needs to consider the setting of random coefficient vector $\beta \sim \mathcal{N}(0, \Sigma_\beta)$ instead of, e.g., considering deterministic $\beta$, as in previous efforts. I find it somewhat confusing since the authors say after Equation (6) that ""the outer expectation over $\beta$ has been suppressed since ..."" Could the authors comment on this?
* page 3 ""namely $\beta \sim \mathcal{N} (0, \Sigma_\beta)$.."" 
* page 5: after (11) there should be more discussions and intuitions to help the readers understand the definition of $d$-scale LJSDs introduced here, which turns out to be of crucial significance in this paper.
* page 5 Equation (14): perhaps add a few words to define the (partial) derivative.
* page 7 Figure 2 covariance model with $\alpha = 10^3$: a typo here? Also, we see the sample-wise multiple descents appear for relatively small values of $\omega$, which, I imagine, implies a close-to-linear regime, could the authors comment on this?
"	4
"This work provides some interesting insights on the effects of anisotropy in the RF ridge regression setting, but would strongly benefit from a major rewriting to account for a discussion of the relevant literature in this topic.

---------------------------------------------
**Update after the discussion period**

My main concern with this work was the disconnection with the existing literature dealing with anisotropic features on the asymptotic proportional regime. At first, I was surprised the authors had missed a long line of works based on a different approach than RMT to treat the same regime (Gordon inequalities), and the corresponding Gaussian equivalence discussion in a subset of these works. However, given their engament during the discussion period and their willingness to acknowledge this gap, I believe they have acted in good faith.

The revised manuscript from 23.11.2021 (including the revised appendix A) goes in the right direction. However, I join reviewer *ot31* in the opinion that it could be further improved by:

- Including the explicit connection with [5] in the case of the square loss with $\ell_2$ penalty (which the authors said they are already working on). This would be an interesting result in itself, bridging a gap between the two approaches in the existing literature and opening a possible direction of future research consisting of exploring whether the alignment discussion in this work hold for other losses. 
- Being more explicit about the linearisation assumption (c.f. *ot31* updated review). 

Assuming these suggestions will be incorporated in the final manuscript, I am changing my recommendation to an accept.


"	2
The proposed work is well motivated, well written, sound and contributes a model that solves a very interesting problem in timeseries prediction with strong results. Its capability of use along with any model adds to its value. I would be happy to see this work accepted.	4
The paper contributes both new data and a new problem setting, motivated by forecasting COVID-19 deaths over time. Due to the high novelty and very general proposed B2F pipeline for refining model predictions, this paper could have high impact, although it may not be the best fit for the ICLR audience.	4
This is a very well-written paper with good explanations, proper motivation, clear objectives, and results. 	3
The review score is all about the battle of lack of novelty versus the impact of implementations. I judge this work to be under the bar of acceptance for now but am willing to raise it depending on the author's feedback on the importance of computing finite width NTK.	1
Nice code which should be used by some researchers. Using methods/ideas that are established. Concerns that calculating NTK is still too expensive (which it is by nature, not fault of this paper) to prohibit some researchers and also that uptake in using the code will be only by a subset of the community.	2
I would not recommend the paper for publication at such a top-tier conference as ICLR. The simplicity could be justified, in principle, by the novelty. But this paper is too simple, and the novelty is moderate.	1
"Overall, this article is clearly written and well structured. If the author can solve the problem of unclear dotted line in Figure 1, it will be more perfect.

Besides, this work builds on JAX. However, the most popular deep learning framework used is PyTorch and Tensorflow. It would be better if the authors discuss why they choose JAX instead of other frameworks.

"	3
"The paper is well-written and well-structured. The proposed method is theoretically and empirically verified. Also, the paper is practically useful because it is easy to implement and plug into any tasks handling (long) sequences for better accuracy.
"	3
Both the technical part and empirical evaluations can be improved. 	2
The augmentation procedure is very similar to word dropout. It is not clear if similar benefits can be obtained by using word dropout at training time. The experimental results on real datasets are weak. 	2
I have two major concerns (1) limited novelty. (2) The lack of comparison of other baselines. In section5, the author discuss a bunch of related works on data augmentation, however, in natural language experiments these are not compared. I know that SCANDROP is easier to implement, but that does not mean you don't need to compare with other algorithms.	2
the paper is inspiring but major revision is needed	3
At the current stage the contribution of this paper is unclear, both theoretically and experimentally.	2
Overall, I think the problem of CSC is a neat problem. However, its connection to word embedding techniques has only been explored at a surface level. Hence I feel there is not enough in the paper to merit acceptance as a theory paper. However, the connection could bear fruit if the authors do provide deeper connections either in parameter space or provide results on the properties of the embedding learned. While I cannot vote for acceptance at this time, I do hope the authors revise with new results and resubmit. 	2
"In general, I feel that there is some value in this paper and that the contribution is interesting since,  to my knowledge, no paper so far has established any connection between the Weisfeiler-Lehman test and embedding approaches. However, I have several concerns about this work. The writing is not clear, while the significance of the theoretical results seems to be low. The authors do not perform any experiments, thus there are no empirical results and it is not clear how the proposed approach would work in real-world scenarios. Finally, I am not sure whether some of the claims made by the authors are actually true.

"	2
I think it is a very thorough work, with both lengthy discussions, experiments and theoretical analysis. The question of what accuracy can be achieved by kernel methods is important in its own right, and can potentially inform us on the performance of other methods (CNNs). Also we can hope that understanding theoretically the RKHS of good performing kernels can give us some clue on the properties of the image function classes. For these reasons, I recommend the paper to be accepted.	4
The paper makes a valuable contribution to our understanding of convolutional networks for image classification, via expanding the theoretical characterization and generalization guarantees of their associated GP kernels, and offering experiments with simple architectures that match the state of the art for a kernel method on CIFAR10. In the main review I offer some suggestions for improving the clarity of the presentation.	3
The results are interesting, but not clearly written. 	4
The paper addresses an important question, is well motivated, and carried out very thoroughly.	3
The main idea of the paper is interesting and as far as I can tell novel. The results indicate that it is working well. However, the most difficult part in an hierarchical RL setting is actually discovering the low-level skills and how to identify them automatically. The paper is not touching this at all and assumes access to pre-defined (well-selected) low-level skills.	2
"I find value function spaces to be a simple but potentially effective idea. The comparisons have largely centered around representation learning frameworks applied to deep-RL, but not around reinforcement learning techniques aimed at option reuse and option discovery. While the approach proposed is different, these works deserve to be discussed in context. As such, I urge the authors to address these concerns qualitatively or empirically.

Post Response:

I am quite happy with the updated version of the paper, and willing to upgrade my score."	3
This paper presents an interesting approach by which one can get more out of trained skills; their primary contribution (the novel VFS representation) is easy to construct for skill-driven agents and could therefore serve as a relatively general representation to allow for performance improvement across a range of applications in which skills are already being used to make progress. One of my primary concerns is the lack of inclusion of cost-based performance metrics for their experiments, which I feel are somewhat important for understanding the performance of the approach. Even without them, I think this work represents a (mostly) well-executed, interesting idea, and so I am generally in favor of publication.	2
The idea of using the value estimates of pre-trained policies is interesting and the empirical results look promising. However, as I pointed out in the weaknesses section, I do not think that the comparison with the baselines is entirely fair. Additionally, I would like to see the method evaluated in more environments. Because of that, I tend towards a weak reject, but I am willing to change my score if the authors can address these issues.	3
Please see the detailed comments above.	2
In summary, I think this is a decent submission. I have some questions (the central one is the strength of the adaptive attack) which I would like to discuss with the authors and I am open to changing my evaluation accordingly.	3
"For now, the authors need to add more theoretical analysis and verification experiments. The reason is that there are still unfair comparisons in the comparative experiments in this paper. If the author can address the reviewer’s comments, I will consider giving it a score of ""6""."	2
"The paper tackles a very interesting problem and the many security considerations as well as the experiments on various datasets and comparisons with existing defenses are commendable.

Some issues (unclear threat model, flexbile perturbations in the feature space) still prevent me from recommending complete acceptance. More clarifications are necessary, and by adding some more ""realistic"" experiments I believe that the paper could be turned into a significant submission of ICLR.

I am recommending a ""5"", but my score can be easily increased to 6 by addressing the many clarifications expressed in my review. Further experiments and the concrete use-case would further increase my score.

___

AFTER REBUTTAL: score increased to 6 which I will increase additionally to 8 if the authors are willing to support the claim that Tesseract is a ""secure by design"" defense.

FURTHER UPDATE: score increased to 8, and I stand by my decision unless other reviewers point out that the claim of Tesseract being ""secure-by-design"" is flawed.
"	3
The paper provides a generally reasonable framework to handle the nudge recommendation problem, but the proposed method is incremental upon existing techniques. It could be seen as a good exploration of applications of those techniques with proper integration. The paper also remains to be improved in its technical details, presentation and experiments.	2
"I would suggest a revision for the paper involving a set of results (recommendations) of deep neural network, with clear illustration of input data (ontology of diabetes care). I would also suggest hyperparameter tuning, and comparison of results based on this. Since the recommendations are proposed to manage diabetes care, I would suggest inclusion of a validation approach (e.g., correctness) to make sure that the inferred knowledge can be used as a recommendation and safe for users. 

Some minor comments. In general, the relations of knowledge graphs are represented in the form of is-a, part-of, or has-a. In the given example (Fig 3) on page 12 “theme” is stated as a relation, and the ‘nutrition’ (on page 12, Table 2) is its an exemplar value.  Because theme is defined as a relation, so nutrition should not be its instance.  Also, is “encouragement” an instance of a concept (node) or is it a concept?  The defined ontological concepts and their values seem confusing. I think these need to be clarified. These are used in mathematical model of the paper. That would be also nice if development/implementation of the framework platform/software (e.g., python) is specified for readers.
"	2
This paper suffers from the issues of insufficient contributions, flawed experiments and missing details. Thus, my recommendation is a rejection.	1
This paper proposes a graph-based recommender system for diabetes self-care management, which is based on the knowledge graph embeddings techniques. However, the technical novelty and significance are limited. The reviewer tends to reject the paper. 	2
Overall I think this work is well-motivated, technically sound, and showing promising results that support potential applications for interpreting and improving deep learning models for computer vision. Some minor changes could be made to improve the clarity. More details about how authors control / validate the quality of the MILANNOTATIONS dataset could be included.	3
"* A higher level of explainability of the regions responsible for network final result
* A thorough analysis of the actionable insights that can be used for the model
* Limits of the approach needs to be discussed further
"	3
Strong paper demonstrating how to generation descriptions of image features and how to use method to analyze model behaviour and filter out neurons by description.	3
Overall, the paper is well-written and the results and performance look solid. My major concern is the novelty, specifically compared with the GOAD method. In addition, the code is not uploaded, so I am not sure how it works for reproducibility and time cost.  	3
"Though I think the methodological novelty of the proposed approach is rather low, and that the experimental comparison should be somewhat extended (where I expect the improvements of SRR to hold up), I am overall positive towards accepting this work since robust anomaly detection is a relevant problem of high practical significance, for which the proposed SRR approach demonstrates significant improvements over current state-of-the-art methods.
"	2
"The paper proposes a framework to refine data and train contrastive models for unsupervised anomaly detection problems.
The extensive experiments show the effectiveness of the method.
However, the main experiments require prior knowledge of the true anomaly ratio which is unavailable in real-world problems.
Discussions about the important difference between the proposed method and the previous iterative methods would make the paper more convincing.

[Post rebuttal]
The authors addressed all my concerns.
I raise the score, assuming the rebuttal materials will be included in the revised version.
Rebuttal materials here means,
- OTsu's method experiment
- detailed discussion on the difference between SRR and the previous iterative works
- Convergence analysis (maybe this one in the appendix)"	3
The paper is ready for acceptance.	3
"  While I think that there is definitely a lot of potential
  in the paper, there are several areas that remain open to
  improvement. First, the comparisons to related work are a
  bit unclear to me, and explaining the trade-offs between
  different kinds of inference methods, especially through
  the lens of data-driven factor learning, would help
  position the paper better in this area of ML.

  Second, the empirical evaluation is not very thorough,
  and its results don't improve much over simple
  baselines like an LSTM. A broader evaluation, including
  on tasks where formulating ""hand-engineered"" potentials
  is more difficult would strengthen the paper in my
  opinion.

  Because of this I would recommend rejection for now
  unless the above issues are fully clarified in upcoming
  communications. That being said, I am likely to be the
  least experienced of this paper's reviewers when it comes
  to graphical models, so please take this into
  consideration when evaluating my reviews (this is
  addressed to both the authors, as well as to the ACs).
"	3
The proposed approach for learning MRF parameters by combining neural networks with an approximated belief propagation method is not concisely explained and involves (in my view) too many ad-hoc approximations. The experimental results are not convincing enough to compensate the conceptual weaknesses.	2
"While I'm not an expert on BP techniques and haven't verified the math (hence the low confidence), I've found the paper to be weak on experimental results. The LSTM baselines are very weak, and some important citations are missing. Uncertainty visualizations for hands do not seem sensible, and the uncertainty estimates are not analyzed in depth. There are no comparisons with other BP methods, as well as other uncertainty prediction techniques. Even if the approach seems sound, the results don't justify adopting this challenging-to-implement method over any other regular method.
"	3
Currently, the weaknesses outweigh the strengths. Especially the fact that the handcrafted graph structure is probably the reason why DNBP is not able to outperform the considered baselines.  	2
Overall, I think this work is marginally above the acceptance threshold. The improvements made to NS-RNN are well-motivated. While this may seem to be incremental, I think the contribution of the memory-limiting approach enhances the paper's relevance by allowing for language modeling over natural language.	4
"This paper makes straightforward (this is good) changes to an existing stack-augmented RNN model, the NS-RNN, evaluating the new model on some formal languages and even one natural language dataset. The improvements are modest (and on the natural language dataset, unclear), but sufficient to be of interest. The paper is well written, though it could do with some expansions and clarifications as elaborated in the main review. 
"	3
Overall the paper proposed useful improvements to the NS-RNN. However, the novelty and impact of this paper are not significant enough.	2
The proposed method is sound and the experiments are designed nicely to prove the improvements of this work. Although the gains are not significant, I'd rather like to see this paper accepted given that it is an interesting contribution to computational linguistics, and might have an impact in the future.	4
Theoretically, the paper seems nice but it could be made stronger by providing motivations for the specific problem being studied in the paper. For the latter reason the paper is not a clear accept for me.	3
In this paper, the author shows good results for an Lp regression on structured input, but the motivation towards this technique is not well presented. It also lacks a discussion on related work and a decent experimental comparison with existing techniques. 	2
"This paper takes an interesting, practically well motivated, study of sketching p-norm regression. The connections with polynomial regression is quite powerful, and I'm only slowly getting a sense of its value.

Both its theoretical and practical results are quite surprising and interesting to those working on optimization/randomized numerical algorithms."	4
I vote to accept the paper considering the strong empirical results and the interesting method.	3
Overall, I think the paper presents a novel and interesting approach that shows significant improvement, however there are still engineering challenges related to batching.	4
An interesting and well written paper that suggests a solution to a long standing problem in program synthesis from input/output examples. Some machine learning design decisions appear overly complex, although motivated. It would strengthen the paper if authors could rethink the ML design having general purpose programming languages in mind or different DSLs (would including an entire search history as context still scale?). An ablation study connecting this approach to the left-to-right models (e.g. use beam search with backtracking) could give ideas about feasibility of practical applications of such models.	3
The paper presents an interesting neural architecture that allows the search to account for the search state while deciding which program to evaluate next. The paper has two main weaknesses: the proposed method is much slower than other methods from the literature and the search algorithm can stall during search and not make progress, despite being allowed more computational time. The experiments possibly miss the fact that CrossBeam might suffer from getting stuck even when using the UniqueRandomizer. This is because the search budget allowed to the systems is suspiciously small. 	3
The paper presents a novel and well motivated approach to privacy preserving generative modeling. The approach has nice theoretical properties and compares well against other approaches on the few benchmark scenarios that the authors have looked at. 	3
Paper proposes a new method (PEARL) to generate synthetic data based on CFs. It is a nice approach that seems to work.	3
"The paper tackles a very interesting problem, and the many technical considerations and the experiments on various datasets and comparisons with existing data synthesis techniques are commendable. Some issues (unclear privacy-preserving optimization method, need to add baselines and benchmark datasets) still prevent me from recommending complete acceptance. More clarifications are necessary, and by adding some more ""realistic"" experiments, I believe the paper could be turned into a significant submission of ICLR. I recommend a ""5"", but my score can be easily increased to 6 by addressing the many clarifications expressed in my review. Further experiments and the concrete use-case would further improve my score."	3
The intuition of the proposed method is not really making sense and the empirical results are not quite convincing, therefore, I recommend reject for this paper	2
"Overall, I vote for weak reject. The paper proposes I-PGD-AT to imitate PGD virtually through single gradient calculation. My major concern is about the margin improvement and the limited compared methods in experiments (see cons below). Hopefully the author can address my concern in the rebuttal period. 

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

UPDATE

After reading all the response and comments from other reviewers, I will keep my scores. My main concern is the insufficient experiments."	2
"Reasons for score: 

 
Overall, I vote for weakly accepting. I like the idea of imitating the PGD perturbation to improve the performance of one-step adversarial training. Experiments show that the performance of the proposed method is better than fast adversarial training and has similar training
time. My major concern is that the proposed method is not evaluated on large image dataset, like ImageNet. Some methods perform well 
on small datasets like CIFAR10 but do not perform well on large datasets. Since Fast adversarial training can be deployed on ImageNet 
(experiments can be found in the paper), why not compare I-PGD-AT with FAT on ImageNet?

##########################################################################
Updates: Thanks for the authors' response. Part of my concerns are addressed by the author. The concern about lack experiments on ImageNet is hard to address during rebuttal period. Therefore, I'll keep my rating."	4
The paper proposes a method which could be very useful to the community as it provides a single-step adversarial training method with clearly improved performance compared to existing methods like Fast-AT with the same runtime. I am vouching for acceptance due to the great experimental results but currently I am not yet satisfied by the theoretical justification of the method as a few statements are not well-supported. I think that it can be clarified in the rebuttal as using the quantiles of the gradient magnitudes to produce multiple step sizes  within a single step adversarial method is a clever idea.	4
"I recommend reject for the reasons detailed in the full review.

If the authors could clarify the following things, I would consider raising up my score:
* How does their algorithm differs from Algorithm 2 of [Osband2018]?
* Why would the LCB of independent networks would have a better uncertainty estimate than an ensemble network learnt on common LCB targets?
* Why were the authors unable to train effective policies of well established algorithms?"	1
The paper addresses exploration with DRL algorithms, and explores estimating confidence interval of Q values using ensembles. Some discussion on the use of ensembles, the connection and difference from distributional RL, and comparisons to a good baseline (set) can make the paper much better and well supported. 	2
Although this paper proposed an algorithm that can be applied to real-life applications without imposing strong structural assumption to allow uncertainty quantification, the theory part of this paper should be improved to be more complete, rigorous and self-contained.	2
I found the paper quite hard to follow and overall there is a lack of rigor. However, looking at the experiments, there might be something to explore.	2
The proposed method to use independent initializations to capture the uncertainty is interesting, validated in simulations and performs well in experiments. However, the credibility of the work is compromised by the confusion in the type of uncertainty it targets at, which is the key theme. The writing could also be improved by another check to fix typos and missing references.	2
The paper does enough to justify to me that LARS and LAMB are not strictly better than simpler optimizers. But it doesn't do much more. There aren't many generalizable insights here. I highlight some areas where the paper can improve. I might be open to increasing my score if those concerns are addressed.  	2
I appreciate the authors effort in correcting the common misconception that large batch training optimizers works better than standard optimizers through experiments, and I give a weak accept for now, but honestly I am not entirely certain if the paper contains substantial improvements or novelty compared to previous relevant works.	1
"The novelty of this paper is very low. The key technical part is just tuning hyper-parameters. The authors also did not provide any deep analysis. If we don't need deep analysis, there are actually many blogposts on this topic (e.g. https://medium.com/fenwicks/tutorial-2-94-accuracy-on-cifar10-in-2-minutes-7b5aaecd9cdd). They can also significantly improve the performance of SGD with different optimization tricks. Similar tricks will likely improve the performance of LARS/LAMB.

The key advantage of this paper is that the authors can use a significant amount of computing resources while other researchers can not. I worry that this paper may mislead the ML community and have a negative impact on academia's budget/planning.

In my humble opinion, this paper is not a fit to top ML conferences like NeurIPS/ICML/ICLR."	2
"As stated above, the paper is well-written and clear. All claims are adequately
scoped and supported by the experiments provided in the paper.
This paper is not novel and it isn't trying to be. I expressly welcome papers
that double-check and critique prior work and try to disentangle effects with
carefully-designed experiments. However, I feel that the limited result of matching LARS/LAMB
instead of a full-fledged comparison of between LARS/LAMB and Nesterov/Adam greatly
diminishes the significance of the paper. With this shortcoming, in my opinion,
the paper narrowly misses the mark for publication at ICLR."	1
The authors propose an extension to the vanilla Coreset which is novel but relatively straight-forward. They provide some theoretical grounding for the designed algorithm, and show with some preliminary result. However due to the obvious lack of comparison to SOTA and insufficient discussion on related work, this reviewer believe that the work is still premature to be accepted.	2
"This paper presents an extension of the original greedy core-set algorithm.
The proposed scheme is relatively simple yet well-motivated, and the experimental results show that uncertainty-based distance scaling can improve the sampling efficiency of the resulting active learning scheme.
However, the authors compare the performance of the proposed algorithm only to the original core-set algorithm, although the original algorithm has spawned a large number of more recent algorithms that have been shown to improve performance.
Comparison with more recent coreset algorithms - especially, Bayesian coreset algorithms that naturally incorporate uncertainties into their predictions - would strengthen the paper by demonstrating its potential advantage against the current state-of-the-art.
Furthermore, it would be beneficial to include further discussions about alternative ways for assessing doubt/confidence/uncertainty and their pros/cons as well as any limitations of the current approach (of estimating doubt).

 
"	3
Well written paper. However, method is incremental, which is not necessarily bad, but paper does not assist with a weak experimental section and analysis of results.	2
This paper needs major revision for judgement.	3
"Strength:
+ Focusing on an interesting problem
+ The paper is of good structure

Weakness:
- Incompelete evaluation
- The novelty needs to be further justified"	3
This paper studies an interesting open problem in deep graph learning. However, it has limited novelty, and theoretical results do not support some of their claims.	2
"In summary, I appreciate the thorough experimental study in the paper and believe that – even though the paper does not propose a new method – the findings have relevant implications for GNN design choices. There are some additional experiments and discussions I would be interested to see (see main review). Overall, I'm not opposed acceptance currently.

After reading the authors' response to my review I increase my score."	2
This paper aims to investigate an interesting question about the relationship between robustness and heterophily of graphs, however, the novelty and significance of both theory and empirical results are not clear.	2
The paper proposes fairly straightforward, yet effective approach based on BERT. Intinsic evaluation setup and the structure of the paper is more or less identical to that of Bartunov et al. (2016). The section about the extrinsic evaluation of the proposed method should be extended (adding more details about the topic modelling), but preferably another use-case of the proposed approach. 	3
The authors build a new type of obtaining multi-sense word embedding is novel. Although the model is just comparable to existing methods in contextual word similarity tasks, the model and derivatives show potential for both the word sense induction task and the embed topic model. Thus, I found this paper is marginally significant.	4
The technique adopted in this paper is simple and not novel. The improvements are not significant. 	1
"Overall Recommendation: Lean Accept
The paper proposes a new noncontextual model for handling multiple word senses by constructing a sense embedding model with improved disambiguation learned via BERT knowledge distillation. As compared to existing work, this work is able to both perform word sense induction while also having strong performance on word similarity. This suggests that the model is able to both disambiguate and also learn strong underlying representations.

I would like to see my above concerns addressed by the authors. For example, it would be beneficial to report the latency of model inference and parameter counts (as relevant) for the proposed sense model, BERT, and other baselines averaged over the test/val set in the word sense induction and word similarity tasks. Likewise re:Concern #2, I would like to see a histogram over the top word sense probabilities. 
"	3
The effectiveness and generalization of the proposed method are somewhat limited due to the fixed sense number. The novelty seems not strong, and missing high-related works make this paper difficult to position in the research background. The clarity also has room for improvement.	3
"I find that this paper presents promising evidence suggesting that divisive normalization in combination with batch normalization can improve classification performance. The authors attempt to explain the performance gain by analyzing the representation of divisive normalization model in comparison to baseline models without normalization (or with other types of normalization). The findings are interesting but I am finding it difficult to grasp the main takeaway from the paper, especially since the gains produced by divisive normalization are quite small. I do not recommend acceptance of this paper at the current stage, but I am willing to change my decision if my comments above are addressed by the authors during the rebuttal phase.

Key factors I am concerned about that can influence my score:
1. Answers to the clarification requested in the main review above.
2. More clarity on connecting the representational analyses of divisive normalization models and other baselines with functional advantages (such as the performance gain on ImageNet and CIFAR-100).
3. I would be very impressed if the divisive (or divisive batch) models perform significantly better than other baselines on the OOD generalization benchmark referred in my main review (or any other comparable OOD generalization task that the authors find relevant).

======== UPDATE AFTER AUTHOR RESPONSE ==========
I am increasing my score to 6 and am recommending acceptance of this paper. I am updating my initial opinion of this paper, in my opinion the paper performs interesting experiments and analyses on OOD generalization and feature selectivity benefits produced by divisive normalized networks. I am not giving the paper an even higher score since I feel that these benefits / gains are quite small compared to just using batch normalization."	3
"The incorporation of biological nonlinearities in artificial networks is interesting both to enrich architectures in machine learning and to better study computational principles in neuroscience. Therefore, studies like this are (and will be) of interest for the ICLR audience.
I advocate for the publication of this work in the conference when the authors address/comment on the following concerns:
A. LIMITATIONS OF THE CONSIDERED NORMALIZATION AND TASK OBSCURE THE MESSAGE: clarify discussion!
B. IS THE FINAL DIVISIVE NORMALIZATION BIOLOGICALLY SENSIBLE?
C. OTHER ANALYSIS
D. LITERATURE (missing citations)"	4
These findings could help explain a largely unanswered question in neuroscience, which is to understand why divisible normalization, which is first and foremost a phenomenological heuristic, exists in biological networks.	3
The paper explores an interesting but not novel idea. The analysis is extensive, but one is left wondering what we have learned from the study. More generally, there is a mixture of ideas and analyses which are interesting but somehow not well connected in this work.	2
"The paper introduces a creative way to combine an implicit shape representation with parametric functions and a neural volumetric renderer to deal with document unwarping. The authors further employ deformation regularizers which reflect the real behaviour of a material on which the documents are usually printed. The paper is written in a clear way, I believe it should be easy to reproduce, and the qualitative results seem compelling. I see a couple minor problems, namely (i) exclusion of two seemingly relevant approaches for comparison (see above), (ii) a little confusion about the column ""best view"" in the table not being used for the proposed method too and (iii) unsupported claim about the sensitivity of LD and MSSIM and I would like the authors to comment on these. In general, I find this submission to be compelling overall and upon clarification of some of the issues mentioned above I lean towards acceptance."	3
"This paper does a good job demonstrating the applicability of the proposed neural rendering technique to the problem of document unwarping, both in the formulation of the problem (learning the UV grid *is* unwarping the document) as well as in the empirical evaluation (OCR). Design decisions are well-motivated and clearly articulated, and the merits of the proposed technique are backed up by relevant experiments. However, the improvements over prior work seem marginal at best in the key metrics of Multi-Scale Structural Similarity (MS-SSIM) and Local Distortion (LD). Furthermore, the qualitative results do not make the improvements especially obvious. Lastly, the 18 hour training time per scene (Sec 12, supplementary) is pretty prohibitive for the document unwarping application explored by this paper. Given these caveats, it is hard to see how substantially this work might impact the research community.

Additionally, although document unwarping is an interesting application of this neural rendering technique, it is also very niche. Given the potential this paper suggests for learning editable texture-mapped surfaces, it would have been nice to see a more general application (e.g. 3D shapes, objects, or structures) flirted with, even if only as a teaser experiment or discussion of future direction. Despite this wish, it would be inappropriate to knock the submission for not exploring every possible application. However, it does leave an open question as to whether the proposed approach is more generally applicable."	3
"I think this paper is interesting for an experimental study, where implicit differential rendering is applied for document unwrapping. Although I doubt the practical value of the method. 

As an experimental study, the supplemental material seems to include much information about the study that is missing in the paper itself. My concern is that the paper deeply depends on the supplemental document. 

Overall, I would like to positively evaluate the trial of implicit differential rendering for document unwrapping, and showing detained results in supplemental document.

"	3
I like the overall idea of this paper and the results look really convincing. However I would not recommend acceptance to ICLR given its significant overlap with an existing technique (IDR).	3
Overall, I think the technique presented in this paper is well-motivated and addresses an important problem in RL. Perhaps the improvement in performance is small but I think the technical development and the final technique are valuable contributions.	3
The paper is well written and introduces a very interesting perspective on equivariant representations in RL. The method proposed makes sense to me, and the results seem promising. 	3
While I do find the idea of this paper novel and interesting, the empirical evidence does not convince me of its superiority over prior methods, in particular SPR. I will not recommend acceptance for its current form.	3
I recommend rejecting this paper due to a) the limited empirical benefit of incorporating equivariance into the proposed representation learning method and b) the lack of clarity in both the paper’s exposition and its claims about the proposed method’s properties. With additional work and a more informative evaluation setting, I think the paper will be in a much better position for consideration in future conferences.	1
The method is elegant and well motivated, hence a desire to see it accepted despite question marks about the experimental results. In particular, the broader applicability may be limited given the gains are clearest in the setting specifically designed to satisfy the properties which motivated the method. To increase my score I would need clarification around the evaluation protocol, which is already known to be challenging in RL without selecting the top 3 seeds (while saying 5 seeds elsewhere). I would also be happy to increase my score if there are additional baselines (e.g. AGAC for Doom) or benchmark environments (e.g. additional Atari games/MiniGrid environments), clarification around hyperparameter choices or an effective demonstration of combining AS with an extrinsic reward. Overall I could increase multiple times if this is improved. 	3
The paper presents an interesting method, but the evaluation protocol needs to be improved in order to properly assess the advantages of the proposed approach. 	3
"This paper introduces some interesting ideas. Especially, it provides a practical and smoother reinterpretation of asynchronous self-play (Sukhbaatar et al., 2017), a method holding compelling premises but with some clear limitations. Moreover, it tries to unify under a unique approach the pros of novelty-seeking exploration and surprise minimization, while mitigating some of their cons (such as noisy-tv and dark-room problems respectively). Overall, the proposed methodology seems valuable, and I believe it can help improving the state of the art in some specific settings (e.g., partially observable visual domains). Those might be sufficient reason for accepting the paper.

However, I think that the paper has some important limitations as it is, which might significantly reduce its potential impact. Thus, I am currently providing a slightly negative evaluation. I detailed in the main review above some of the concerns that the authors might address in their response."	3
This investigatory paper defines the problem that they are studying, develops a methodology for studying it, and clearly analyzes the results. The investigation yields interesting findings related to language models. This paper would make a great addition to ICLR - accept.	4
"Before author response:

My comparatively low score is a consequence of my concerns and very much given as a temporary score until we can discuss the three major concerns I have---if they are addressed satisfactorily, I anticipate raising my score significantly to champion the paper if needed as I found this paper a joy to read and thought-provoking in a good way (even if that lead to criticism).


After author response:

Thank you for all the clarifications and edits, they are greatly appreciated. I would strongly like to see this paper accepted."	3
"A nice experimental paper addressing the difficulty of neural LMs to approximate the probability of rare events from an underlying ""teacher"" LM. The paper could be improved by more analysis and discussion of some of the results.

*After authors' response*: thank you for your detailed answers to my questions and for your additions to the paper and to experiments. I aim raising my score and hope the paper will get accepted."	2
"While the methods presented are simple, the paper needs more work on motivating the methods presented and connecting it to the empirical evaluations.
"	2
New activation functions are introduced and tested in a very wide range of experimental settings, but I find the motivations not sufficiently developed and the experimental results under-reported and motivated; more theoretical development would help assess this contribution.	2
"I like the activation functions introduced by the authors in this paper. They seem like a nice extension to MaxOut with semantical meaning on the operations they carry.

The weakness I see here are that the empirical section should compare against other activation functions and the missing related work."	3
"In general, the paper provides great idea, insights, and analysis. I added my major concerns in main questions and suggestions above. I am willing to increase my score once my concerns are addressed. 

---------------------------------
### After rebuttal

I read the responses and other reviews. I generally like the idea of this paper and how it's written. However, my concerns are not fully addressed in the response, and I also agree with some of other reviewers' concerns. I keep my score. "	4
Nice idea but weak experimental baselines and results.	2
The basic ideas are well motivated, reasonable, and interesting. However, the motivation about how to approach the objective is improper, and the method is not carefully designed (or not completed). Experiments and analysis can be further improved. 	2
"Overall, I think the novelty of the method is limited, and critical insights and analysis are not enough, both theoretically and experimentally.
"	2
Applying the MOE to continual learning is a new idea, but it should prove that inferring the task-specific identity has noticeable advantages over other memory buffer-based methods that are listed in the previous section, at least from the numerical perspective.	2
The paper provides a novel provable method to produce samples that are uniformly distributed on the learned manifold, regardless of the training set distribution. The method is also well proven empirically through numerous experiments. The proposed method help address the problem fairness in samples produced from DGNs trained on not-so-well-representative training datasets.	4
"This method itself is novel and interesting, and warrants acceptance into the conference. However, the current wording of the title and abstract can be misleading, and should be edited to remove confusion.

Pros:
+ Theoretically motivated
+ Algorithmically simple

Cons:
- Seems computationally expensive
- Oversells the capabilities of the technique in the abstract, namely: mathematically uniform is implied to mean semantically uniform
- Needs proofreading"	4
This paper does not provide enough experimental results (both qualitatively and quantitively) to support their claims.	2
I think the result is intuitive and requires advanced techniques to construct the infinitesimal generator used for constrained sampling.	3
The theoretical contribution of this work is solid. Still, quality and clarity need to be improved in several points as mentioned in main review.	4
"This paper addresses an interesting problem and its development looks very exciting.
The work is very complete and addresses both theoretical and experimental aspects of the proposed methods.
The theory proposed looks sound with a strong background and the experiments look very convincing."	3
The contribution is novel and solid with a wide range of potential applications. The paper is slightly too dense and somewhat unfocused. However, the main contributions are presented in a clear and understandable way. 	3
A very good paper with a couple of areas that need some work in rewriting to make the method clearer. 	3
"Overall, this is an interesting idea, and I encourage the authors to continue working on this paper to provide sound empirical support.  If the claims were sufficiently supported, the results of this paper would be a significant contribution to MBRL. For now, I am leaning towards a rejection as the current empirical methodology is unsound.

### Final 
Thank you to the authors, for their response. I have read their replies and the other reviews. I will maintain my original decision to reject, because the biggest issue I have with the paper have not been addressed. This related to the empirical methodology.
"	3
I think that, as a matter of correctness, my 1st point needs to be addressed for the submission to merit acceptance. Simply qualifying the claim would suffice but it would obviously be better if the submission were actually able to substantiate the claim that Stochastic MuZero can match MuZero's performance with an equal computation budget. I also think that the 3rd point should be addressed, toward making the submission's experiments more reproducible. While I think the paper would merit acceptance even without addressing my 2nd and 4th concerns, I will be disappointed if the authors choose not to perform these experiments, as I think they would provide significant value to the community.	3
The paper proposes an extension of MuZero to stochastic environments, and achieves ~SOTA results on 2 stochastic environments, 2048 and backgammon, while retaining SOTA performance on a hard non-stochastic environment, Go. The proposed method is a significant step towards a strong, universally applicable, reinforcement learning algorithm.	3
"With the points I made above, I am leaning slightly on acceptance. I hope the authors address the concerns mentioned above.

========== Post-reponses ================= 

Thanks to the authors for the responses. I have read the comments of the other reviewers. I am staying with the current score.

"	3
"I am recommending accept because the theoretical results are interesting, and combined with the given very clear discussion about the assumptions in context, I think provide a decent baseline for future such analyses, even if assumptions such as the large network width are restrictive.
"	3
Overall, I think that the studied neural contextual bandit problem in this paper is very important. The idea behind the proposed algorithm is simple and effective. Theoretical and experimental results are provided to demonstrate the effectiveness of the proposed algorithm. However, this paper borrows the idea and analytical techniques in prior works and lacks theoretical novelty.	2
The paper is well written. The studied problem is significant. The theoretical results are interesting. The reviewer accepts this paper.	3
The paper is well motivated, but the theoretical analysis does not appear to be solid enough for the proposed method and the experiments are limited. 	3
"**Recommendation**

If my questions and concerns below are addressed, I think the paper is interesting enough for acceptance."	3
"Although the targeted problem is interesting and the proposed method is mathematically correct with reasonable experimental results, I think more analysis is needed to make their method more interesting. It's a reasonable draft of a promising line of work.

*Response

Although this paper has a good motivation and the authors improve the quality of experiments, it's still a bit hard to say the proposed method is novel enough as it is a combination of pre-existing methods.


"	2
As claimed by the author(s), this paper addressed some of the limitations of the existing approaches. Empirical experiments showed the superiority of PI3NN, and theoretical proven properties of PI3NN are provided. So I think this is an interesting paper.	3
In general, considering the significance of the researched problem, this paper can be accepted by the ICLR2022. However, some points should be clarified and strengthened in the revision. I would like to strongly support this paper if my concerns can be fully addressed.	4
Non-transferable learning is an interesting idea to explore, and this is the first step in that direction. I can imagine that there will be a lot of follow-up ideas both for attacking this, as well as improving upon it. I would definitely recommend accepting this for ICLR.	3
This paper proposes an interesting question and gives the corresponding solution. I recommend the acceptance of this paper. 	4
The details of the paper needs further refinement.	2
This paper presents a novel and interesting clustering method with theoretical explanations, and the detailed discussion with the similarity-based clustering and discriminative clustering literature is appreciated. I encourage the authors to add more examples showing the comparison between the derived discriminative similarity and conventional similarity used in data clustering.	4
The paper contributes some new ideas and the motivation of this paper is clear. This paper proposes a new clustering framework termed Clustering by Discriminative Similarity (CDS). Based on this model, the authors develop a CDSK clustering algorithm. The paper is well organized. The authors conduct a complete and detailed theoretical analysis. Experiments on real-world datasets validated the effectiveness of the proposed methods.	3
Although there are some concerns, the paper proposes an interesting and effective method and also provides some theoretical analysis.  Therefore I recommend for weak accept.	3
In all, this paper is well written and provided a novel formulation for goal-conditional automatic curriculum learning. The paper could be strengthened if its use cases can be extended to more general curriculum learning settings (beyond goal-generation). It would also be better if more theoretical analysis, such as sample complexity/convergence and be provided. 	3
I like this direction but I am worried that the main objective has problems that can prevent it from being an effective curriculum, even if they don't show up in these particular results. 	2
Overall I feel the paper is slightly above the acceptance threshold. It does a good job of explaining it's algorithmic choices with toy experiments and shows a reasonable bump over other curriculum methods on a series of goal conditioned tasks. However, I'm left with the question of why the method works at all and if there's a better formulation that achieves the same thing (see my first question in the main review). If this were clarified and some analysis included in the paper I think it would be a much stronger submission and would be a clear accept; in this case I would be more than happy to increase my score.	3
"The work introduces nice concepts and ideas that are not completely well supported nor fully explored. Thus although I like the  idea I am reluctant to give an accept at its current state

### Post-discussion and updated version
I think that the updated version supports much better author's statements and reflects better their contributions"	3
"This paper extends the recent online boosting technique to the RL setting. The current presentation has some problems and the technical novelty looks a little bit incremental.

I recommend the authors to (1) discuss the difference between designing boosted contextual bandits and boosted RL; 
(2) design some initial experiments to justify the effectiveness of the proposed algorithms. 
(3) consider to present the logic of this paper in another way (not motivated by the large state problem in RL). "	3
Weighing the strength and weakness of the paper, I am recommending weak accept.	4
I suggest the paper be added more clarification of notations and interpretation of main results. The paper is certainly not ready to be published due to its lack of clarity and unrefined narrative.	2
An interesting result, however I cannot recommend publication due to significant clarity and readability issues.	3
"I believe the results are interesting, decently significant, and relevant to the community. That being said, there are issues with clarity (noted above). I believe fixing these issues is an easy way to significantly strengthen the paper.
"	3
I set the Correctness score to 2 as I have several questions about the correctness of the theoretical part of this paper (see【Correctness】Section). If these questions are solved, I will increase my Correctness score. Regarding technical novelty, I think this paper gave novel tools for analyzing the expressive power of flow models. Introducing these tools is significant as they encompass many existing models.	3
"Well written. But the contribution could be clearer.
"	3
"While I believe the core idea and motivation are strong, I found the execution to be rather lackluster. In particular, I have questions about the core result (universality); in particular the function class they approximate seems very weak. I also found the presentation very confusing and hard to follow. Finally, the other experiments do not fit the overall narrative and are often underdeveloped.

UPDATE
----------
Lemma 2 alleviates my main concerns. I am greatly pleased by the author response."	4
"(1) Good work. Mathematically profound and results may have potential applications;
(2) Presentation too mathematical and might reach a larger audience if it can be revised to be more understandable."	4
"Overall an enjoyable read with interesting ideas. There might be some context or clarification missing, I am not convinced that the objective propose would explore available affordances/actions, and would not get stuck looking at a suboptimal subset.
If that is the case, it would be useful to know what are the requirements to prevent this from happening."	3
While I enjoyed the addition of affordance module in value equivalent network and found it interesting contribution to integrate learning and planning, the experiments do not analyze all the merits of the proposed approach as compared to prior work. It is unclear how design choices like the discrete set of actions/options is constructed from the continuous space.	3
This paper proposes a gradient-based approach for planning in continuous control problems. However, the experiments currently lack relevant comparisons to understand the significance of the proposed approach. I’m also curious how performance varies on tasks with different option spaces and different horizons.	2
I think this a good paper. It builds on existing ideas on incorporating affordances but does it in a way that is novel.	3
I think the paper has good experimental results and that the incorporation of the meta-learning framework is useful. However, I am concerned that the authors have left out important references, and would welcome a thorough evaluation of these references (and ideally other important references contained therein) and how they compare to the current work. Currently I will lean toward reject, but welcome a discussion with the authors on the novelty of their work. 	2
"The current paper addresses an important problem using a smart approach but requires significant rewriting as well as better empirical evaluation. Results on social network data are only marginally better than a much simpler approach (GL2Vec+ProtoNet) while the biological time-varying graph dataset is not actually different tasks since the underlying dynamics for all the 12 networks are learnt from a single gene-expression dataset which means train and test settings share the same underlying biological process and time-scale. I do not believe the paper is ready for publication in the present form. 
"	2
In general, it is an interesting paper. The experimental results look promising. However, the overall quality is not strong enough for acceptance. There is space for improvement in experiments. For example, the comparison between the proposed method and baseline methods. More discussion on the experimental results is also needed.	2
The problem is not clearly defined and hard to read. The method is general and claimed to be better than others due to its generality. I do not see too much novelty in the method. The results look good.  	2
The paper's presentation---specifically, the lack of a derivation that connects Eqs 10 and 11---makes it very hard for me to assess the paper's contributions.  Thus, I think the paper needs a major revision to improve clarity.	2
The paper includes a novel formulation of  multi-modal VAE learning, but the experimental section does not support the main claim of scalability of training and the results of the strongest reported baseline have large differences from the ones reported in the original publication. If these problems are addressed, I can see raising my score.	4
Overall, the paper does a reasonable job of explaining the background for multimodal retrieval as well as conducting experiments to demonstrate their superior performance. However, the motivation for their work could be improved. The motivation for choosing the specific objective should also have been discussed in detail.	3
This is an interesting approach to scalable, multimodal learning with promising results regarding the learned latent representations and coherence of generated samples. Nevertheless, quantitative results on the quality of generated samples (including random samples) are absolutely necessary for multimodal VAEs in my opinion. Therefore, I can only give a weak reject at this point.	3
The paper has major shortcomings with the experimental setup considered to evaluate the efficacy of pre-trained decision transformers, and falls short of outperforming simple baselines. For these reasons, I recommend rejection at this point.	2
I think this is a great start, but insufficient for ICLR, so I'm recommending reject here. This is motivated by two key problems: (1) The results don't convince me there is real improvement here, in particular vs. the CQL+BC baseline, and I do think the degradation in performance at the full setting is a major issue, and (2) I'm not convinced there is sufficient technical novelty / sophistication here. This submission in its current form (modulo some edits to improve the presentaition of the results and more accurately reflect the performance relationship between the various methods) might be a great fit for a workshop, and improvements to the PT/FT method to increase significance of results and/or expand the technical sophistication of the method would make it a better fit for ICLR.	2
The paper introduces a self-supervised approach to improve the performance of decision transformers in deep RL when data is limited. But the contribution and novelty is not enough. So I give the score of 5.	2
Overall, I vote for weak rejection. Despite its simplicity, the novelty and significance of the proposed method are limited. My major concern is the importance of fine-tuning is not well supported in the experiments. It can be enhanced by showing the significance of fine-tuning on more diverse downstream datasets that collected from related but more challenging tasks. Hopefully the authors can address my concern in the rebuttal period. 	2
In conclusion, this paper proposed a novel way to handle the issue of scalability of training a graph neural network model. However, the disadvantages of the paper mentioned above, especially the lack of a well-elaborated motivation and the weakness of experimental settings, hurt the value of this work. Thus, a reject is recommended here.	3
In general, I think the problem investigated in the paper is important, and learning from smaller GNN to approximate its limit object is of interest. Also, it is great to see the theorems and proofs are well stated in the paper to help understand the algorithm. However, I still hope that the authors could answer the question or make clarifications to some issues mentioned above.	3
"In its current state, the paper is not ready for publication: 

- The relation with Theorem 1 of Ruiz et al. 2020 is unclear.
- The theoretical analysis is not sufficient to justify Algorithm 1, since it is based on an assumption that might not hold in practice. 
- The paper does not show evidence of having solved the main problem that it sets out to solve (ie, reducing computational costs).
- The experimental evaluation is insufficient (not enough experiments, no baselines, and too small graphs) and the results raise some questions."	3
By construction, robust policies (i.e., those that solve max-min objective in robust MDPs) provide a robustness certificate that is more general than the two criteria provided in Def. 1 and 3. As such, I see this study as a specific case of prior work.  	1
A conceptually interesting approach to the verification of reinforcement learning in the per-state setting and beyond. However, the current manuscript leaves open questions about conceptual decisions and formal correctness.	3
"This paper focuses on an important problem of certifying RL policies against state perturbations. The proposed method makes intuitive sense, but as a direct application of the randomized smoothing technique, the novelty is a little limited. Although theoretical analysis of the lower bound of cumulative rewards is provided under 2 smoothing settings, the estimation of the lower bound could be either too loose or too expensive. The empirical evaluation is interesting and helpful for understanding recent robust RL works, but there is no enough evidence for the effectiveness of the proposed method (the tested environments are relatively simple). Therefore, I think this paper is around the borderline, and has the potential to be accepted. But at the current stage, I tend to reject it, unless more improvements are made (e.g. a more practical smoothing method, theoretical complexity analysis, or experiments in more complicated environments).


---
After rebuttal: the authors have provided more implementation details and experimental results, which I find interesting. So I increased my score.
"	3
overall I am convinced this paper is solid based on its problem statement and empirical evidence, but I will defer to someone who can formally verify the math better.	3
The motivations and methodology of this paper is not adequately explained in this paper.	2
"I think we should not suppose the trigger keys are already there. The mechanism to generate trigger keys is very important to make MTK framework happen. At least, authors can provide a potential approach to design such mechanism. I believe the appropriate mechanism would be a great contribution along with MTK framework to protect privacy of MTC in inference phase. Currently, I think this paper is below the acceptance threshold. I will read other reviewers’ comments and authors’ feedback to make my final decision.
"	3
"
In summary, the paper offers a novel approach for information privacy, relying on the lack of secret data augmentation to poison the model. However, the threat model is unclear and thus the utility of this method remains unclear. I currently recommend rejecting the paper, unless the authors can further clarify and justify their threat model. 
"	3
Please find our comments above. 	2
The proposed framework of DATASIFTER is interesting and novel with promising results. However, the method primarily depends on a set selection phase for which no empirical/theoretical evidence is provided.	2
"Promising paper with clear ideas.  I have concerns in particular around the evaluation as detailed above.  Author response is important for my setting of the final score.

Edit: The authors provided thoughtful detailed reviews and addressed many of my concerns.  Their responsiveness and turnaround time on feedback was quite remarkable and duly noticed."	2
The paper provides worst-case analysis of data selection using linear heuristics, revealing potentially hazards using valuation-based approaches, which is interesting and novel. However, it doesn't exactly corroborate the main argument that we should prefer DataSifter over coventional heuristics.	2
My decision is borderline accept for this paper only because of empirical success in some specific examples and theoretical results on the domination ratio of Shapley value based methods. Although, I should mention this paper has considerable weakness. Depending on discussion with other reviewers I may increase/decrease my score. 	3
"MGVAE is the first hierarchical generative model to learn and generate graphs both in a multiresolution and in an equivariant manner.  The experiments are extensive.  The paper needs a stronger motivation for why the authors' specific approach is important considering  that (i) the idea of studying the properties of graphs via the eigenvalues and eigenvectors of their associated graph matrices at different resolutions is not novel, and neither is the (ii) permutation equivariance of graph neural networks 


"	3
The paper could be more persuasive if it provides further justification regarding the necessity (and initativity) of designing an equivariance graph generation. The concern on model complexity should be addressed. Also, the paper should be carefully proofread before it is ready for acceptance.	3
"The authors present a novel approach for generating complex graphs, e.g., molecules by introducing a model that allows for sampling from a latent space at various levels of resolution.
Further, the paper is well written and generally presented in a clear manner. However, the evaluation of the method and the comparison to other work should be expanded upon. In particular, relevant work seems to be missing both for providing context of the work in the current state of the field, as well as in the evaluation and comparison of the model to other work.
Thus, while I find the method worthy of acceptance at  ICLR, I deem the paper marginally below the acceptance threshold in its current state.

Edit:
The authors clarified my question regarding the parameterized Gaussian and I appreciate their time for doing so. I would encourage the authors to include this motivation for their approach also in the manuscript (while the method is formally described in the main text and the appendix, I cannot find a discussion for why this approach is taken). This will help readers understand the importance of this aspect better. 

Unfortunately, my main concerns regarding comparison with and discussion of relevant related work remain unanswered and I thus maintain the score of 5."	3
I'm definately willing to raise my score if my concerns have been well addressed.	3
Although I think this paper has potential, in its current form I vote to reject it. The weaknesses raised in the previous part of the review are the most important reason for this vote. Although each one of them consists of a significant weakness from my point of view, they are listed in order of descending importance.	2
My bit concern is the way of how to handle the KL terms in the objective. 	2
Overall, this paper proposed an interesting idea and successfully demonstrated its effectiveness. Only a few visualizations should be done to better justify the proposed ideas.	4
The motivation looks OK but there still remain many questions. A bunch of experiments have been conducted and some of them look very promising, but more in-depth analysis are required to address my concerns.	2
The idea itself is interesting, and but I think these are several limitations of existing versions (cf. the weaknesses part), and the submission can be further improved by solving my concerns.	2
Overall, the proposed method is novel and reasonable. The experiments are quite solid by adopting the method to multiple structures and comparing it with SOTA pruning methods.	3
Overall, it is an interesting paper. An easy method to reduce the model size and increase the performance in the downstream tasks. Not sure whether it can be generalized to general transformer-based models. It looks more like some technology that is not so novel. There is also not enough detailed research analysis.	2
The results look very promising, but the presentation is bad. The method lacks crucial details for implementation. Their methods seem too naive to be effective, but the authors do not fully explain why they result in a good performance.	2
This paper proposes a new pruning framework for pretrained Transformers with good evaluation and analysis. Although the idea is not completely new, I would like to recommend weak acceptance for this paper.	3
A well-written and decent paper that struggles to meet the bar for novelty and significance.	2
The paper is well-written and easy to follow. However, the idea is incremental, building largely on previous work. Moreover, the gain in expressiveness is not quantified, e.g., by showing that the infinite-length RW models can expressive more permutation-invariant functions compared to the fixed-length RW models. Morever, the emperical results are not convincing, i.e., they do not clearly win over  the fixed-length RW models.	3
Although the authors make great contribution to the technical part, the experimental results have big flaw, which needs to be addressed.	2
The idea proposed in this paper is interesting indeed, which provides a new way to learn graph representations. However, the superiority of the proposed method to existing work is verified neither on classification accuracy nor runtime. Additionally, some related graph representation strategies are not considered or discussed.	3
I like the idea of encompassing different curriculum approaches inside a parameterized function. The authors show improvements over other approaches on three datasets with curriculum discovered by this approach. My reservations are mostly around how well this approach will scale to larger datasets and on unbalanced datasets. Some results show that other methods are better when it comes to full datasets. Due to these reasons, I will am recommending the current score. I encourage authors to address these concerns and I will be happy to bump up my score.	2
In my opinion, the proposed approach is too limited to be of practical usefulness, and its weaknesses outweigh the strengths so that the current draft is below the ICLR bar.	2
In my opinion, the weaknesses outweigh the strengths of this paper.	2
I think that this paper presents an interesting benchmark for developing RL agents which can utilize language. However, the proposed model, Text-DT, is considered to have a small contribution because the Transformer architecture is simply applied to the conditional sequence generation problem with language inputs. In addition, the authors explain the proposed model based on the decision transformer, but it is not considered to have much related to the decision transformer. I would like to recommend a more detailed explanation of the proposed benchmark and possible future studies.	2
The paper lacks novel technical contributions. The experiments are weak and do not provide new, meaningful and practically useful insights. The work further needs to be contextualized better in existing work. 	2
No comparison to prior tasks, unclear what new proposed environment tests and how it compares to prior environments.	1
This paper introduces a quite large corpus of text instructions for the language guided Atari Frostbite environment, which is good resources for the community, and it also presents baselines for pretrained text decision transformers (TDT), and shows some interesting empirical results. 	3
Interesting problem but insufficient theoretical contribution. The provided insights are made confused by the fact that both positive and negative $\lambda$ are considered, with completely opposite effects. The technical derivations are basic computations.	2
In my current understanding, the paper seems to study a simplistic setting (and hence the theoretical results of the paper are immediate) and it is not clear to me how the results/framework extends to the general setting. Hence, I do not think the paper is fit for ICLR in its current form. I will be happy to increase my score if my understanding is incorrect.	1
In my opinion, the contribution of the paper is quite marginal and the paper is quite hard to understand, which causes by the poor writing and lacks of proper explanation. 	2
"I think this is an interesting paper and vote to accept it. I think the observation carries value and might have several applications down the line. 

The only detracting aspect is that the theorems are relatively straightforward, and so I would have liked to see more consequences of this observation in the context of other algorithms that exist in the literature. 
"	3
"The paper is well-written and backs up the experimental results with mathematical intuition and analysis. I, however, have the following questions and concerns.

- Adversarial examples in RL: Studying adversarial robustness in RL scenarios is interesting and important. However, most of the current research on adversarial examples is focused on classifier models and image domain. It also seems that the arguments and analysis presented in this paper could be directly applied to the classification problems as well. In fact the baseline of Roth et al. and all the attack methods used have been designed for image classifiers. Is there a particular reason that the paper focuses instead on RL? How will the proposed methods work e.g., on CIFAR10 and CNNs? The paper notes the real-time constraint in RL setting as an added constraint for detectors. But please note that, even without any constraints, designing a reliable detector is still an open problem for image classifiers.

- The paper posits that, as shown in figure 1, the cost has larger negative curvature at clean examples compared to adversarial examples. The detection method is, however, specified as $|\mathcal{L}-\mathcal{\bar{L}}|>\tau$. Shouldn’t it be $(\mathcal{L}-\mathcal{\bar{L}})>\tau$ (without the absolute value)? Also, the objective function of the adaptive attack is defined as: $\min (J(s_{adv}) + ||s-s_{adv}||^2 + \mathcal{L}(s_{adv}))$. Shouldn’t it be $\min (J(s_{adv}) + ||s-s_{adv}||^2 + |\mathcal{L}(s_{adv})-\bar{\mathcal{L}}|)$ (or maybe $\min (J(s_{adv}) + ||s-s_{adv}||^2 + |\mathcal{L}(s_{adv})-\bar{\mathcal{L}}|)$) to match it to the detection rule? With the current attack formulation, the curvature of the adversarial example could end up being lower than that of clean examples, which would be flagged as adversarial. Plotting L(s) (as in figure 1) for adaptive adversarial examples with different objectives could help visualize this.

- The paper does not report the attack success rates on the undefended models. This is an important sanity check to make sure that the attacks are implemented correctly and also helps with better evaluating the effect of the detector.

- The adaptive attack results are reported for the case that the decrease in the attack success is at most 10%. This doesn’t sound like a good metric. The paper should instead define the attack success rate (ASR) as the rate of examples fooling both the model and the detector, and then report the results for the hyperparameters for which the ASR is highest. The same thing holds for the results of non-adaptive attacks. For example, Table 1 reports the detection rate, but it is not clear what is the ASR and whether the attack hyperparameters are optimized to maximize it. For instance, decreasing the attack budget reduces both the detection rate and the misclassification rate. Thus, there could be a setting that even a lower attack budget results in a higher overall ASR (fooling both model and detector). Overall, it is hard to evaluate the proposed detector without having the best ASR results.

- Table 1 shows that the second-order detector works better than the first-order one. But why is the detection rate of Roth et al., so low? E.g., <5% TPR for CW attack. It seems that their detection method didn’t really need an adaptive attack to break it. Their paper [ICML ’19] and a few follow up works, however, have shown it achieves a pretty high detection rate for non-adaptive attacks on image classifiers.

- Section 5 mentions that the “feature matching” has been used as the first attempt of an adaptive attack. Is this the same attack used for breaking the Roth et al method? If yes, did the authors investigate using the feature matching for designing an adaptive attack specifically for the proposed SO-DATA detector? Please note that the feature matching attack should be designed with the exact same features that the detector uses."	2
"The paper provided and adversarial detection method and a good attempt to break the method.
While strongly connected with methods in non-reinforcement settings, I'm unaware of a similar approach
for reinforcement learning. I enjoyed reading this paper.
"	3
Although they claim that they are the first to apply such an idea (detection of the adversarial example in RL domain), the contribution is marginal as it borrows a lot from the literature. Moreover, the performance evaluation is not comprehensive. 	1
"Pros:
- simple, theoretically well-motivated, and effective approach
- writing is easy to follow
- strong empirical performance improvement

Cons:
- Evaluations are only on Atari Games

"	3
The walk based approach is interesting but the paper lacks novelty, clear articulation about high or low level difference of WalkPool with GNN. Also, MSE  loss is chosen for a clear classification task. The results are weak too. 	1
"
Overall the paper is nicely written and organized, and provides useful thought in the problem of link prediction with or without node features. Experimental results are also promising. However I have a few technical details I would like to discuss.

(1) The authors used feature-based similarty (like in GAT type of algorithms) to define the transition probbaility matrix, which takes the place of the topology-based transitiona matrix by and large. Is there a way to combine the information from both?

(2) The pooling part is a bit confusing to me, in that it seems that the WALKPOOL indeed does not perform pooling but instead use pre-defined features, while pooling is usually for a set (of objects such as nodes) with varying numbe of elements. Can authors add more details of the pooling used in the SEAL and why WALKPOOL is considered a pooling scheme?
  "	3
"The paper proposes a pooling method (i.e., a latent structure feature extractor) for learning features that describe links (i.e., the surrounding subgraphs of the links). Building the latent subgraph from embeddings turns continuous information back to discrete graph space, which facilitates feature extraction. The framework remains end-to-end learnable. The extracted features summarizes multi-hop path information, which somehow reflect higher order structural information. Experiments on multiple real datasets are conducted.

In general, the paper proposes a neat idea with good experiment design. The current draft could be further improved by revising its Introduction section, as many claims are not clear and do not seem to motivate the method design. For example, Figure 1 does not provide much information about the idea. Also, I think the word ""topology"" does not well describe the proposed idea."	3
Overall, I believe it’s an interesting approach that offers a new perspective on the link prediction task. Despite building upon ideas presented in previous work (SEAL), the paper makes progress in presenting an elegant methodology. Some aspects related to the experimental evaluation should be addressed.	3
The details of the proposed model were not clearly introduced. The evaluation should be strengthened. 	2
The idea of this work is reasonable, but some technical details have not been clearly presented.	3
The idea of the proposed method is technically interesting, while its empirical evaluation is not sufficient.	3
"The justification for this work is that autoregressive models tend to ignore the existence of common substructures and hence a specific mechanism that is aware of larger fragments is needed. This fact is however never demonstrated and possibly incorrect (that is autoregressive models do not encode explicitly substructures but do this in the latent space given the compression constraints they are subject to). It would be of interest if the paper would analyse this statement empirically or theoretically, but the only results available are the usual weakly informative experiments on validity, uniqueness and novelty, and property optimization experiments (without any notion of statistical significance for the comparative results). 
The presentation is overall unclear and the literature review incomplete. "	2
"This paper proposes a new method for MARL exploration and demonstrates extensive empirical performance on a large set of tasks. However, the reviewer has major concerns on the core component of this paper, i.e., the motivation of influence functions. In addition, the reviewer thinks that the proposed method has some implicit assumptions and the discussion and comparison of related work need to be improved. 
"	2
I vote for rejecting this paper as many technical details of the proposed method are not very clear to me and many experimental results in the paper are not analysed. 	2
In sum, I think the method has the potential for high impact, but it would be considerably strengthened with more thorough head-to-head comparison with popular methods -- that different baselines are limited to different benchmark suites considerably limits the results. I also have clarity concerns, but these are more minor and can easily be rectified with an edit.	3
This paper needs significant improvements in terms of better motivation and conceptual justification of the proposed methods, and with better choice of baselines in experiments. Technical writing also needs improvements to clarity and precision.	1
Clearly, the paper adds to the existing literature and is interesting. It is well written and mathematically sound.  It is however, unclear how general the approach is and that brings into question the significance of the contribution.	2
The paper does not explain clearly enough the benefits of the proposed approach over regular Neural ODE models, nor thoroughly validate their claims empirically.	2
Since the paper shows interesting links between seemingly unrelated topics, I find the contribution significant and suggest an accept although the writing can be improved and the model can be investigated in more detail.	4
"The ""linear base"" part and the ""non-linear neural network base"" part are disjoint. Unlike the current wordings imply, I think the former is rather restrictive. On the other hand, the latter is interesting but needs more work. Although I cannot recommend the acceptance of this paper in its current form, I encourage the authors to strengthen the latter part and consult more with the literature."	2
Overall, this is a decent document and manual for MOO software package/platform introduction but not a scientifically methodological paper for ICLR.	3
"this paper is more engineering-oriented, lacking convincing theoretical works.
"	2
Overall, AutoOED is a well engineered BO package and it has some strengths and rare features such as multi-objective optimization. However, the novelty is rather minor, there are only little new ideas in the package and the claims in the experiments are clearly overstated. 	2
The problem studied in this paper is well-motivated. Multi-objective (MO) problems in the context of Optimal Experimental Design (OED) are pervasive. I like it very much that the authors would open source such a powerful and convenient framework to complement the current state-of-the-art packages. 	3
The conclusion that adversarial examples are dataset dependent is too general: (1) this is already observed in the literature so that it does not say anything new; and (2) the validation of such conclusion is not sufficient. That being said, the experiments and the corresponding observations are interesting to the community. A major revision with sufficient validations and refined and more specific statements/arguments is recommended.	3
The paper negates the widely held belief about high-frequency nature of adversarial examples through interestingly designed experiments with practical consequences of more effective adversarial training, and, therefore, I recommend an acceptance. However, the presentation of figures and minor grammatical errors should be addressed. 	3
I decide to reject this paper for now. However, if the author can properly highlight their contributions during rebuttal, I may well reconsider my recommendation.	2
Accept. This work provides beneficial insights to the community and the strong points outweigh the weaknesses of this work.	3
The major concern of this paper is that the baseline results are too bad, making the experimental result not convincing. I suggest the authors choose a SOTA method as baselines.	2
In summary, this paper is well-written and its key contribution technically sounds. Extensive results on the scene text recognition task show the effectiveness of the proposed method as compared with the traditional attention mechanism. However, the experiments are not thorough and enough to support the authors’ claim well. Additionally, some details of the experiments are not very clear to me. 	3
In general, this problem addressed in this paper is not a new one. There are many existing methods have been proposed to address the attention mis-alignment (or attention drift) issue. Although the motivation of this paper and the mathematical design of the sharp attention mechanism seem ok and reasonable, the proposed new Sharpener module does not bring me too much new insight and inspiration.  I do not see clear advantage of the proposed method against existing ones. Furthermore, I am not satisfied with the experimental part of this paper. 	2
This paper is well motivated to seek clear alignment in the attention mechanism. This idea can be understood overall, although some details can be added. For the experiments, it seems not sufficient, maybe more comparison experiments can be added to verify the effectiveness of the proposed method.  	2
"The paper studies a very relevant problem. There are no mistakes in the proofs as far as I can see. However, the main concerns are (i) the paper is hard to read and understand, (ii) the convergence result relies on many assumptions that cannot easily be extended to more general settings, (iii) the convergence is probabilistic and if the desired probability gets ""finer"" then the required radius of the infinity-ball will get smaller hence one needs to also increase the number of parameters of the network so the results do not work for a fixed architecture or a fixed error radius."	2
The authors non-trivially extend theoretical analyses previously presented in the context of natural training of overparametrized networks to the context of IBP training. While the assumptions that lead to the results are fairly restrictive, I believe the results are of great interest to the community and could lay the ground for further work in the area.	4
"This paper explores the certified robust training dynamics and proves convergence with high probability on a training set under certain assumptions. While the direction is novel, the writing and presentation should be improved. 
"	4
Novel idea of stabilizing ELECTRA-style language learning using a mixture of signals at different depth of generator hidden representations. Significant contribution to using adversarial training as part of the signal for self-supervised language learning. Good experimental results of the proposed model. Good set of ablation studies for each part of the model. Some unclear reasoning in the choice of adversarial training techniques compared to previous papers.	4
The proposed adversarial approach combined with the multi-generator signal which is composed from different layers of a generator provides a curriculum learning approach to train a better discriminator for downstream tasks. Despite better results on GLUE and Squad tasks compared to previous arts, the extent to which the different component of AMOS contribute to the results are less studied in the ablation. Only two tasks (MNLI and Squad) are selected for ablation of adversarial learning, mixture weights, gradient stopping, etc. The evaluation results can be extended to SuperGLUE as well. 	1
This is a meaningful contribution to Electra-like pretraining approaches and provides fairly significant downstream improvements.	3
Overall, I think this is a reasonable work by directly improving of the ELECTRA framework. The proposed ideas are clear, straightforward, and sound. The results can be further improvement. 	2
I recommend this paper for publication given a satisfactory explanation of the raised weakness. Overall, I think the concepts explored in this paper are novel and will be of interest to the NLP community, and might spur new research in this direction.	3
"The paper makes a novel addition to the ELECTRA-style LM pretraining direction by using multiple generators to guide the learning of the discriminator, and in turn use the loss from the discriminator to enhance the generation capability of the generators. The idea is simple, easy to comprehend and evaluated on the standard benchmarks used for LM pretraining research. The results are strong and show a 1 point absolute improvement in average GLUE score over previous baselines.

The results of the ablation study are not very conclusive, and raise some doubts about whether certain modeling decisions are essential (and if they are making the AMOS modeling too complex). There are some weaknesses in the experiments to justify the need for different number of layers of generators, and concerns over generalization to different values of k and the size of the transformer architecture."	3
Overall, I find the paper in its current state not ready for publication. I think that the methodologies for measuring smoothness can be improved, and the experiments should include experimental details as well as an ablation study of the role of different hyperparameters on the smoothness of the function obtained after training.	2
"Although I believe the proposed research question is of great interest to the community, and I admire the effort made by the authors in studying the effect of many training factors in the variability of the spectral bias in CNNs; I think that the overreliance on qualitative metrics and lack of quantitative results makes most of the observations in this work inconclusive. I am open to increase my score if the authors address these issues during the rebuttal, but at this stage a lean towards rejection.
"	3
Overall, I think the paper has merit and the proposed tools have potential to reveal novel insights on generalization properties of deep neural networks. On the other hand, the core novelty and significance of the findings are somewhat limited in my opinion, therefore I recommend marginal acceptance. I am however open to change my score either way after hearing back from the authors and after further discussion with other reviewers.	3
"In my opinion, the spectral bias can be a useful tool in analyzing the inductive bias of networks, but the current format of the manuscript is not ready for acceptance. There are a number of questions on the methodology and how this relates to the spectral bias, that is the main contribution of this work. In addition, the experiments are not considering several cases (e.g. other datasets, or alternative learning algorithms) that might have correlations with their results. Furthermore, previous results about the manifold of the data (ie. that it impacts the frequencies learnt) are not discussed in the paper. If the revised version addresses the improvement points, I can reconsider my rating. 
"	1
This work is well written and presents a very interesting conjecture/conclusion that the patch representation may be the key to the success of visual transformers and visual Milp. However, its research is not strong enough to support its claims.	3
I recommend reject due to the weaknesses above, and I will consider updating my score if seeing a strong rebuttal.	3
I believe the paper is good and it should be accepted. It will be a relevant contribution to the community as a baseline as well as a good datapoint to improve our understanding of SOTA models. However, I believe the paper would be more complete if authors evaluated in additional task. Furthermore, I think authors should address some relevant points when comparing to other models such as data scale and throughput. 	3
"I find the claims in the paper largely unsubstantiated due to the lack of comparison to baselines as mentioned in the Main Review above. 
"	2
"There is a mismatch between the title and the content, given that the proposed methods are only verified in one application scenario, i.e., emotion recognition. However, the multimodal learning is such a big topic, including but not limited to emotion recognition, action recognition, etc. The authors may consider to extend the methods in order to match the title or change the title to a specific area.

Besides, the experiments the authors have conducted is far from extensive, and the comparative methods are not sufficent to support the conclusion."	2
The experimental analysis is limited in terms of several aspects: a) applications: tested only for emotion recognition, b) fixed type of modalities: only audio and video, c) no comparisons with the SOTA 	3
The algorithm loses most of the features that make target propagation attractive as an alternative to backpropagation. As an optimization algorithm, its guarantees are not sufficiently strong.	2
The target propagation scheme proposed in this paper look novel to me. Even though the numerical result is a bit confusing, the algorithm has natural interpretation and more computationally tractable.	2
The paper explore an interesting alternative to BP. However, the current form didn't fully convince me its superiority over bp. I lean towards the rejection, but willing to adjust my rating based on the authors' feedbacks. 	2
This manuscript proposes an interesting and potentially useful algorithm, but needs to more fully explain its theoretical properties and provide more detailed experimentation and comparisons.	3
In my opinion, the paper contains good contributions that are worth publishing at ICLR. While I think the experiments section could benefit from additional comparisons, the current state of the empirical evaluation is reasonable. A score of 7 would reflect better my evaluation of this work as I find the technical contributions to be okay but somewhat limited.	2
While the paper is well-written and the results show consistent improvements, the paper is lacking in terms of any connection made to piecewise training (which the best version of the SMN essentially boils down to, as far as I can tell) and an analysis of why optimizing the proxy problem is superior to the original learning problem. These are potentially fixable issues. 	3
"If the scoring existed, I would give this paper a 9/10. Having to choose between 8 and 10, I have decided to go for 10, as I consider the paper to be much better than just ""good"".
The paper is very good under all aspects. Its contribution is clear, and the problem under consideration is real. One might argue that the innovation is slim because it consists of a single technical contribution; for this reason this might not be a game-changing paper, but it certainly improves on the state of the art by solving a tricky problem. The paper also has the merit of establishing a bridge between neural and graphical model techniques, thus encouraging further work which I expect to be fruitful."	4
I think the model is reasonable both theoretically and empirically (though some recent baselines are not included for comparison). Overall, the merits outweigh the flaws, and I believe the paper is a good addition to the existing literature.	3
Overall, I vote for acceptance. The paper proposes a method to train large scale graph neural networks and shows that very large GNN models can have quite large improvements in the Open Catalyst 2020 (OC20) benchmark	3
I did not identify technical concerns with respect to the proposed method. I will defer to other reviewers regarding the technical novelty of the approach. The paper could be made stronger by explicitly discussing how this approach is applicable to other GNN architectures that have been designed for 3D molecular structure.	3
Despite the good empirical results, it is not clear what is the benefit of the proposed approach over standard distributed training techniques (data/model parallelism). Since this paper proposes a new distributed training algorithm for graphs, it is necessary to compare the speedup of the proposed method against standard distributed training techniques.	2
"The paper is based on the idea that most of the computation costs to extended graphs come from triplets. The authors proposed a way to parallelize the computation of triplets in a distributed way. The author also discussed two models that fit into this framework and showed their increase in performance due to the larger parameter count enabled by their parallel framework.

The paper is well written and easy to follow. A minor part I am a bit confused about is that in the sentence “In many applications, the number of edges is one or two orders of magnitude larger than the number of nodes, while the number of triplets is one or two orders of magnitude larger still”, do you mean the triplets are one or two orders larger than the node or the edges?

In the experiment section, network structures and hyperparameters are given and the experiment should be reproducible. The results support the authors' claim that larger graph networks have better performance.

I do not know much about this field."	3
I am not completely convinced the proposed model of computing a recourse set to minimize the expected cost for the user is always effective in the absence of knowing the cost function even approximately.   The experiments to show the effectiveness with respect to the proposed baseline are inconclusive with respect to natural measures like diversity.	2
In this paper the author proposed a new way for evaluating and optimizing user satisfaction. The technical contributions are solid and the results are rather promising despite the potential bias toward EMC. The paper contains fruitful discussion and ablation studies, although it can be further improved in terms of clarity. Therefore, I would like to give a weak accept.	4
This paper studied an interesting problem. To improve the paper, the author may want to illustrate the advantage of using FS@k and how it is very different from state-of-art measure functions, both conceptually and numerically.	3
I believe the claim that the paper relaxes the assumption of knowing a global cost function is not true. However, it still introduces an interesting new objective to optimize for when finding algorithmic recourse.	3
I believe this paper still needs a lot of work to be ready for publication.	2
The ingredients of the algorithm are known for smooth, non-noisy functions, though the specific combination seems new.  However, the details of the algorithm are not fully justified even in this setting; and the actual setting of interest seems to involve stochastic function and gradient estimates and possibly non-smooth objectives (depending on the activation).  The experiments that are in the current version are intriguing because they suggest that the algorithm works well (significantly better than L-BFGS) in this setting.  But to accept, I would have liked to see more exploration of these aspects of the problem, either theoretically or empirically.	3
See above	2
Even though the paper has a neat idea to transform the ARC subproblem into an easily solvable form, the paper has many shortcomings when it comes to novelty and significance to the ML community detailed above. At this point the paper reads more as a draft, and would require very significant modifications in my opinion to improve its quality. Therefore I recommend rejection. I am however open to update my score if the authors or other reviewers address my concerns.	2
This paper proposes a simple but efficient approach to the resource constrained deployment of offline RL policies. I would like to hear more about the contributions of this work compared to prior works in similar settings in online RL and sim2real. I appreciate the experiments and new dataset construction, but I would like to see a broader set of experiments on a larger variety of tasks to be able to make conclusions about the efficiency of the method.	2
Overall, the paper makes a first step in a novel and relevant setting. The actual approach appears to be general, as the transfer learning objective can be added to any existing offline RL algorithm. And though the results in the paper are perhaps unsurprising due to how much weaker the baseline is, it is understandable because, to my knowledge, no other algorithms that could leverage the offline dataset with rich features. Hence, I recommend that it be accepted. 	3
The framework introduced by the authors represent an important part of offline RL that remains under explored. The proposed algorithm is a minimal extension to a popular offline RL and the experiments/results are well motivated. However, at the current state I would lean towards rejection as the paper still lacks detailed investigation of possible approaches to solve this problem as well as results on a more compelling set of benchmarks(datasets).  	4
The new problem setting could be better motivated and supported by experiments. The proposed algorithm is a simple modification to the existing method.	1
Overall, the experiments are too narrow to support the main claims, so I recommend against accepting the paper.	2
I think this is a good paper and is heading in the right direction, but needs a bit more work to realize its potential impact. In particular, I believe it needs a bit more discussion and evaluation in order to fully understand where this method falls in relation to previously proposed methods. 	3
I find this paper marginally below the acceptance bar because the problem setting is not that well motivated, and the approach itself is incremental over the prior work PROVEN.	2
see above.	1
This paper presents a parameterization scheme and optimization procedure for improved training of shared multi-task networks.  The technical approach is concise, well-motivated, and novel with respect to existing work.  Experiments across synthetic and real datasets show convincing results.	4
Overall it is a good paper with clear motivation and an intuitive approach. It tackles a major challenge in MTL optimization. Strong empirical results highlight the benefit of the approach compared to previous related methods.	4
The paper is solid and it looks has received several iterations. It is a nice contribution to the multi-task learning field, although I think there are still some questions to be addressed first. I'm leaning towards acceptance and I'll be more than happy to increase my rate after the rebuttal and discussion with other reviewers.	3
"I think this is a good paper. It is clearly written, the empirical evaluation is fair, and presents a novel perspective (rotation) to reduce inter-task conflict in multi-task learning paradigms. 

I recommend it for acceptance and have no additional or follow-up questions. "	3
The presented approach introduces interesting and novel ideas and shows that the model is able to generalize to tasks that it was not trained on. Only using one exterior baseline method that does not perform well seems not sufficient given that three related publications are evaluated using a similar setup. While the experimental section needs to be improved, I would rate the positive aspects the approach, generalization abilities and the novelty higher and would, therefore, argue for the submission to be accepted.	3
"This work proposes a pipeline to learn factored world models to predict the robot actions' effects leveraging the graph neural network to encode the latent space and the contrastive loss to update the neural network's weight. However, the technical novelty is limited, and the current setting is too simple. It's unclear whether the proposed pipeline could work with complicated manipulation tasks.

"	2
"This work presents an adaptation of an existing factored world model specifically tailored for robotic manipulation, using motion primitives (picking/placing objects) as the action space and pre-segmenting objects in order to factorize them into the state representation. The paper's novelty comprises the addition of an attention operation to contextualize future actions with latent object states. Even though experimental results suggest positively towards the potential of this method in task generalization, the lack of extensive comparison with previous factored
(structured) world models do not convince me 100% of the merits of the proposed methodology, especially when considering the capability of the standard C-SWM model to explore new object categories which are lost in the proposed model in favour of a ""black-box"" object detection system that factorizes the scene as input to the model."	2
Overall I feel that the paper lacks a concrete quantitative evaluation of the action attention module (which is the core contribution of the paper). The experiments don't prove that this module is helpful for robotic manipulation tasks (though it can be useful for some other tasks which the author(s) have to investigate further). Hence, I would like to vote for a weak rejection of the paper at this stage. I will be making a final decision after engaging with the author(s) post rebuttal.	3
The paper is well motivated and the idea of OAT technique generally sounds. However, the paper reads like a manuscript in a rush: we see many typos, unclear subscripts. Moreover, we see unclear equations, such as Eq. 6, which is hard to follow, but the most important equation for the paper. Weird Table 1 columns and not strong experimental metrics. Overall, the paper needs a polish to improve its clarity and readiness to publish.  	3
Learning a disentangled and interpretable representations is an important topic in the machine learning. The proposed method tried to address this challenge. However, I think the contribution is not notable and the paper lacks theoretical and empirical justifications to support its claim. 	2
Although this work presents an interesting approach to learning disentangled representations, in my opinion it is not ready to be published at ICLR at this stage. It lacks convincing arguments why a researcher in this area should choose the proposed method. In my opinion what is needed to improve the paper is more clarity and details regarding the training procedure, e.g. model selection criteria for the different training stages, ablation studies on the learned number of disentangled factors $k$ and a better description of their central and only results table.	2
In summary, (1) the main selling idea of 'disentangling one factor at a time' is not new, (2) the results are worse than the state-of-the-arts, and are not sufficient to show the benefit of 'disentangling one factor at a time'. Therefore, I have to give a negative score.	2
Based on the limitations above, I think that this paper, although having technical contributions to the community, is still not ready for publication due to the lack of fair evaluation. Therefore, I recommend rejection. I would suggest the authors address my concerns (as well as those from other reviewers) with additional experiments and submit it to a future venue.	2
Overall, considering the weaknesses, not good enough experimental results and the limited novelty, I am inclined to reject the paper unless the authors can provide more interesting discovery and explain a compelling reason why it cannot perform well itself.	2
Overall, the paper is well written. I especially like the way the authors have motivated BiLAW, and appreciate the detailed comparisons with related work. The improvements while not significant, do merit publication and further discussion.The experiments are also comprehensive and support all claims. I am therefore inclined to recommending an accept. 	3
Despite the weaknesses and the limited novelty from previous reweighting algorithms, I am more inclined to accept given the improved empirical results.	2
Overall, I think this is a nice contribution to the literature. The only drawback I can see is the lack of justification for their input encoding to the meta net, leaving the readers pondering where the improvements come from. I'd raise my score if the author can provide more justification on their input encoding (Def 3), or an ablation study.	3
The paper is well-motivated and has a solid evaluation showing consistent improvements on a well-studied task. Even though the authors could try to push their method's performance by additionally combining it with other baselines, in my opinion the results are significant enough to warrant a publication.	2
Thia paper proposes two strategies specific to the architecture of ViT models, i.e. Self-Ensemble and Token refinement to enhance the transferability of the ViT models.  The experiments conducted on ImageNet/COCO dataset, and PASCAL demonstrate that the proposed method can outperform the baselines on some of the experimental settings. 	3
I find that the paper presents an interesting study on the transferability of adversarial samples of ViTs. In general, I think it is a good paper, but also have some concerns about the generality of the work.	3
"Overall, I vote for marginal accept. I like the idea of mining the relation between blocks and handle it by the proposed self-ensemble and token refinement method. My major concern is about the clarity of the paper and discussion as when and why the method works. Hopefully the authors can address my concern in the rebuttal period. 

"	3
"In summary, this paper proposes novel methods and shows good reproducibility. Proposed self-ensemble and token reinforcement gets good results in many situations. But I still have doubts about whether this method is for Transformer’s inductive bias.
I am inclined to accept this paper, but I would like to see a more detailed discussion."	3
This paper seems like a clear accept to me, unless there is some serious issue with the experiments that I missed. (I am not an expert in this field but they seemed quite convincing to me and basically speak for themselves.)	4
Interesting and relevant empirical results on predicting generalization with a simple technique based on synthetic data generated by GANs.	3
Overall, I think the proposed method is quite interesting but lacks well-conducted experiments and evidence to support it. Based on the current state of the paper, I give a weak reject; however, I am willing to increase the score if the authors can address the weaknesses and update their draft during the rebuttal period.	3
The paper offers a simple method for estimating the performance of deep models without test data using GANs. The empirical results are strong and the analysis is interesting. On the other hand, the paper is unpolished and the presentation could be improved.	3
This paper proves some theoretical optimization results about FA under some special settings, but I think the one-dimensional setting may oversimplify the original problem, thus the novelty may not be significant enough.	2
The paper is nicely written, and the phenomenon of anti-regularization is interesting. However, the convergence results seem to assume strong conditions that only works for specific cases.	2
"Feedback alignment has attracted some interest in recent years after it was
shown that it can train a range of neural networks, and some groups have started
investigating the mechanism that allow the training to proceed with random
matrices. This work is thus timely and should be of interest to the ICLR
community. It provides several interesting contributions regarding (1) the
mechanism behind feedback alignment algorithms and (2) the implict bias of
different learning algorithms, through an analysis of linear neural networks, a
workhorse in neural network theory. I therefore recommend acceptance at the
conference.

PS: I am not sure I fully understood the questionnaire below regarding the
difference between ""technical novelty..."" and ""empirical novelty"". Since this a
theory paper, I only voted on the technical novelty.
"	4
"My initial rating is ""weak reject"" because:
Strength: 
1. clear writing
2. detailed analysis
3. interesting observations

Weakness:
1. Significance, need connections to initialization schemes used in practice
2. The results are hard to interpret in general non-spectral settings
3. little numerical example
4. presentation can be improved by visual tools"	3
The paper presents an interesting way of enforcing consistency constraints in object detection. However, I feel that the experimental evaluation and overall novelty when compared to existing approaches in video object detection/tracking is lacking. Therefore, I feel that this paper, in its current state, is not ready for publication.	2
I think this is an interesting paper that could merit publication but I am concerned that the experimental side is lacking for the reasons mentioned above. I would say it is below the threshold for publication as I read it right now, but I look forward to hear clarifications from the authors and will take into consideration the Q&A with all the reviewers.	2
"
In summary, this is an interesting research topic, but the significance of the article is limited by the rather simple model choices and experimental setup. Because of those choices, it is unclear that the proposed method would generalise to other setups. "	2
"Overall, my point is on the fence. As the limitation I mention above, my pre-rebuttal score is slightly below the acceptance bar. If the author can validate the effectiveness of the proposed method on the challenging public benchmarks, I'd like to change the score.
"	3
A potentially novel perspective for understanding the bias of SGD, but more discussion on the preliminary results are needed for non-expert readers to understand the derivation.	4
Due to the lack of novelty and the use of empirical justifications/lack of rigorous definitions, I would recommend a significant rewrite.	2
This paper gives more realistic mathematical characterization of SGD's behavior around minima, by studying the location-dependence of SGD noise. The noise covariance is simplified in a smart way (with numerical justification) so that SGD dynamics can be formulated as a simple diffusion process on a log-loss landscape. Then, escape rate and stationary distribution are derived depending on a power law, instead of an exponential law. 	3
"# Post-rebuttal update

The authors have answered my questions and clarified that we may have some differences in interpreting their results, but I don't think it's significant enough for me to change the original score.

# Original Review

The paper presents an interesting setting in which the approximation to the Bayesian prior $P(f)$ correlates remarkably well with generalization (while flatness does not). 

As I detail in my review, I find the setting to be somewhat constrained and unrealistic, and therefore the claim that the prior ""is a significantly more robust predictor of generalization than flatness"" is too strong, especially since there is no solid theoretical or qualitative explanation for why exactly $P(f)$ works.

Nonetheless, I still find this to be a novel and meaningful result, and look forward to future research investigating / explaining this effect further, so am inclined to accept (assuming the authors soften their claims or convince me that my concerns are overblown).
"	2
This paper propose a new criterion to measure generalization and empirically show that it correlate with generalization, even in cases where flatness fails to maintain the correlation. It would be more solid if direct theoretical link could be shown. It is OK for the paper to be primarily empirical studies, but in this case, a more systematic set of experiments could help strengthen the paper. While it is impossible to enumerate all possible comparison cases, a number of canonical settings like different parameterizations (see above for details) are very important.	3
The paper is well written. However, the main contribution in the paper seems to be identical to previous works. I would appreciate it if the authors elaborate on what their contribution is compared to the works of Valle-Pérez & Louis (2020) and Mingard 2021. 	2
"Overall, although the two observations that the Bayesian prior is empirically linked with generalization and that when using Adam or Entropy-SGD flatness is no longer linked to generalization are interesting, the contribution of the paper is not enough. The novelty of the work is very limited and the empirical results are insufficient.
"	1
Overall, its very long, there are a few critical concerns about the soundness of the claims, there are some typesetting issues. The figures could use work, a wall of dozens of hard-to-read graphs at the end aren't helping the focus. The conclusion for me is quite ambiguous, I'm not sure what to take away from this paper in terms of if flatness makes sense to use as a predictor of generalization. The title doesn't entirely reflect what's being claimed in the paper, since much of the paper focuses on the Bayesian prior.	2
Good empirical findings from extensive experiments, but limited to the individual setup in specific conditions without an overall picture of dataset dynamics.	3
"For an experimental paper, it is interesting, and might point to some new directions for AL.
However, more experiments to clarify some points (see above) should be added.
"	2
In summary, we would encourage the authors to more rigorously examine the heart of their work -- the source of the instability they uncover: is pruning really ineffective, or are we removing too many challenging examples along with the outliers? Can we qualitatively diagnose the problem and explain what makes an example “informative” but unstable? And as accuracy margins are so small, it may be unfortunately necessary to justify or relax some hyperparameter choices, and run additional experiments to confirm these findings generalize.	1
This paper conducted a large amount of empirical experiments on AL with pre-trained LMs. However, the research problems they explored are somewhat covered by existing research outputs and their conclusions are less attractive compared with [1]. Additionally, the writing of this article should also be improved.	3
"Generally I find that this work provides a reasonable study on the usefulness of AL with pre-trained models, there are lots of experiments and there are certain interesting findings. Nevertheless, I still have concerns with the training setup and there are still some analyses that I would like to see.
"	2
This paper makes interesting and novel observations about generalization gap between adam and GD. Their observations are somewhat limited in scope. 	3
Although I find this paper to be novel and interesting, I have to admit that some of the settings in the paper look strange to me. However, I would like to encourage such explorations and vote for acceptance because the authors are not using NTK, and I could imagine how difficult it is when trying to analyze Adam on neural networks. However, I would also like to encourage the authors to further generalize and improve their analysis.	3
This paper presents a promising approach to explaining the generalization gap between Adam and gradient descent, but many of the assumptions on its data, model, and optimizers are far from realistic.	3
"Interesting example where Adam generalizes worse than GD. The assumptions are very far from a real network, and removing any of them make this example incorrect: second layer is not learnable, very high level of sparsity, and activation function has an amplification behavior.

As far as I can say, claims are correct but the proof is quite hard to verify.

To summarize, I'm recommending weak acceptance, because the example is interesting but the assumptions are unrealistic."	4
"As mentioned above, I am excited about this paper in light of the importance of studying TD and related algorithms in settings other than standard under-parameterized settings. The demonstrated scholarship, or lack thereof, gives me doubts, but overall I like this paper and lean towards the acceptance side.
--
post rebuttal: I think this paper makes a respectable contribution to the theory of temporal difference learning. I believe authors will address my concerns, so I like to see this paper accepted."	3
This paper proposes a novel convergence analysis of three classical value estimation algorithms and reveals their implicit bias. However, the validity of its setting is questionable and the experiment results is not very persuasive.	3
See main review.	2
This is a theoretical paper about the overparameterized linear representations of TD, FVI, and RM. This is an interesting question. On the downside, the linear representation might be too restricted and the technical contribution is not significant.	2
Even though this paper approaches an interesting problem, it needs improvements and clarifications.	2
"Assessing fairness is important butt non-trivial. It is interesting to exploit SHAP and BShap, two approaches based on the theory of Shapley Values, as the reference of ""fair"" methods. Many concepts are not clear, and details are missing. Experiments are not comprehensive; please see the detailed comments.


"	3
The theoretical analysis part (section 3) is somehow hard to understand for me. This lowers my score.	3
The paper provides novel results analyzing optimization and generalization of three-layer neural networks. However, my main concern is that the paper does not provide theoretical evidence that the bounds are indeed strictly and significantly stronger than what can be obtained by learning with the NTK, or more generally using kernels. I am willing to raise my score if my concerns are answered.	4
"Summary of Review:
1. The paper makes a fairly interesting technical contribution: the proposed generalization bound poses adaptive nature, it improves the current kernel based bound (NTK) by selecting the best kernel that tradeoff data fitting and complexity.
2. The presentation is mostly clear, and the author clearly make efforts to compliment technical results with rich discussions. However, some technical conditions imposed in the loss and algorithm are not sufficiently motivated or explained. 

"	3
Theoretical paper defining a a novel bound for a 3-layer network. Since I'm not an expert in this field, I can provide only superficial comments to the paper.	4
The paper studies the optimization and generalization of a three-layer neural network trained by a projected SGD algorithm. In this setting, the network runs beyond the NTK regime. Hence, it can achieve feature learning, and better generalization performance than NTK. Specifically, the generalization error is bounded by a complexity depending on the minimum of a family of RKHS norms. This paper explores the behavior of neural networks out of the NTK regime, which is important due to 1) the existence of feature learning, and 2) closer settings to practice.	4
The paper improves upon NeurADP by proposing a way to incorporate agent interactions without resorting to joint action values. While there's some merit in technical contribution in this regard, there are a number of major issues in algorithmic justification and empirical validation. 	2
In general, the paper makes very solid work and is suited to be published in ICRL.	3
"To the best of my knowledge and experiences, the achieved performance of CEVD seems to show important progress on the ToD services. The computational times and settings are reasonable.

However, the paper contains some unclarified notations. It is hard to follow the details of the proposed method. I want the authors to give more explanations and clarify some parts to help readers, which is also important to increase my review score."	2
I think this work is a valuable contribution which captures an interesting feature of the training dynamics of generalization error in deep neural networks in a simple, analytically tractable model. The close qualitative match between the behavior of the simple model and a ResNet on CIFAR10 suggests that the mechanisms identified here may be general, and the theory derived by the authors lays the groundwork for deeper investigations into the training dynamics of generalization error in neural networks.	3
It is a solid work with theoretical analysis and empirical evaluation. Though it may connect to many pioneering works in the field, authors should discuss these connections.	3
"My slightly lower score is based on the following factors:
--Since the focus of this paper is not empirical (e.g. the observation in realistic networks has appeared before), the contributions come primarily from the theoretical side. In this respect, I am not sure if similar insights in linear models & control of multiple scales for epoch-wise double descent have appeared in earlier works and would appreciate if the authors could comment on this in detail. 
--The typos / errors made the calculations more ambiguous and harder to assess the correctness of the final result.

I would be happy to consider adjusting my score based on the author's response.
"	2
This work has merits, but its weaknesses are also salient as mentioned above. I am looking forward to author responses for addressing my concerns, or I vote for rejecting this work.	3
My major concern of this work is the technical novelty. In addition, the writings of this paper also needs to be improved. 	2
"1. Overall, the paper is easy to read and the ideas to improve content preservation between the source and the target sentence is simple and effective. The ablation studies and parameter sensitivity studies are well performed.  
2. The reviewer would require more details on the entities that are preserved between the sentences, examples on the inherent parallel sentence between the two corpora in the introduction.
3.  Problematic evaluation measures used in the literature need to be revised 
4. The evaluation section needs to focus on the main contribution of the paper. Splitting Section 3.2 into more paragraphs might be a starter. 
5. The benchmark table needs to have multiple runs with mean and standard deviation reports.
"	3
"I like the proposed approach and the clarity of the paper and code.
I'm slightly concerned with the paper's novelty since it presents a combination of previously known methods, but still, I think it can be useful for the community.
The main concern is somehow ambiguous motivation and definition of the scope of applicability of the proposed approach (check the questions above). 
I believe this should be clarified.

My score for the paper is ""marginally above the acceptance threshold""."	2
"Overall, I'm in favour of acceptance since the paper has several interesting new ideas, a large new dataset for political stance transfer, and automatic + human evaluation comparing the benefits of their approach against baselines. The weakness #1 prevents me from giving the next higher score (of 8).

-----------

**After author response**: I'm pleasantly surprised by the few-shot style transfer results, and commend the authors for their rigorous rebuttal to all reviewers. I think the paper will be stronger with results on a dataset with a different content distribution between the two styles (irrespective of size, such as Shakespeare <---> Tweets). My overall assessment is more like a 7 or 7.5, which means I'll increase my score to 8."	3
This paper provides a novel idea for video action recognition. Their claims are well supported by solid experiments and ablation studies. I believe it would inspire others in this research field.	3
The paper presents a simple yet effective idea to transform simpler image classification models into video classification models. Even being simple, the approach manages to achieve surprisingly good results when compared to SotA video-classification models which explicitly handle the temporal dimension. The paper may thus contain findings that should be of interest for the ICLR community.	3
"Based on the comments in the Main Review part, I tend to reject this paper. The main reasons are:
1) Limittd novelty;
2) Lack of considering the efficiency of the proposed method, i.e. the input image size it too large;
3) Unreasonable model design;"	2
Overall, I think the proposed method, though simple, leads to surprisingly good results. It not only provides a new way of thinking about modeling temporal relationships, but also better connects action recognition and image classification. Therefore, even though there may not be as much technical novelty in the paper, I still vote for acceptance of the paper.	2
The paper proposes a modular and efficient framework along with its JAX implementation for the implicit differentiation of optimization problems. This framework allows to abstract away low-level details and significantly lowers the barrier to use implicit differentiation. Bi-level optimization problems are ubiquitous in ML (hyperparameter optimization, meta-learning, NAS) and a procedure to automatically and efficiently tackle them is much needed by the community. I believe that this paper should be accepted and highlighted at the conference.	3
I enjoyed the paper and think that the package can contribute much to popularize implicit differentiation tools. However, I feel that the paper is not sufficiently novel on any front to warrant publication. Furthermore, I would encourage the authors to present the main contribution, the package, in a more digestible manner.	2
Overall, I feel this work provides a useful tool for implementing the implicit differentiation in various scenarios. Although I feel the novelty of this work is not that high, I am still slightly positive about it. I am open to increase my score if the authors can address my concerns and add missing related works. 	3
I found that the goal of the paper is not well motivated and the presentation should be improved before it can be accepted. 	1
In summary a few things are a bit unclear, e.g. to me why the method would be restricted to fine-tuning. Can certainly be improved by re-structuring, some suggestions given above. In the current state for me a bit hard to grasp the contribution. 	3
From my perspective, the proposed approach is straightforward and not fully explained. Thus I tend to reject this work. But I am open with my score, according to the authors' responses and other reviewers' comments.	2
This work raises and addresses an interesting artefact of robust training, which was disregarded in previous work. The proposed approach seems technically sound, with a few novel elements. Although the analysis could be stonger, it nevertheless provides sufficient evidence for improved accuracy-robustness trade-off over state-of-the-art methods.	3
See Main Review	4
"Overall I think this paper provides nice theoretical contribution with an improved rate for no-regret learning dynamics that converge to extensive-form correlated equilibrium. The paper is well-written. However currently the related work is missing some recent relevant results, and on the technical contribution side there should be some more details differentiating the new results and existing frameworks.

Post rebuttal update: I have read the authors' response and will keep the original score."	3
Overall, I am slightly in favor of accepting this work. The problem is worth studying, the main result the authors present is exciting, and the empirical results support their claims. My main concerns relate to the presentation of this work. The text is extremely technical and notation-heavy, and I believe many concepts should be more adequately explained for a reader less familiar with no-regret learning to understand this work. The manuscript also completely lacks any examples. I remain unsure if a better exposition is possible given the ICLR's strict page limit and if this work would not be better suited for journals or conferences that allow longer narratives.	3
While I appreciate the headline theoretical contribution, given my concerns I am not convinced that this version of the paper makes a sufficient novel technical contribution.  One possibility is that it does but the technical exposition needs to be improved to more clearly highlight the contribution.  Another path forward, since this paper is heavily based on the approach of [F21a], improves its results, and that paper appears to be still unpublished, would be to consider whether the authors of {F21a] would be amenable to merging the papers.  A third possibility to extend the results and demonstrate the power and flexibility of the approach would be to show that it can also be adapted to work with the line of work which uses dynamics based on carefully-designed distance-generating functions to achieve an O(T^{-1}) bound.	2
"The main difference between local attention and the inhomogeneous dynamic depth-wise convolution (sec 2.4) is in terms of how the dynamic aggregation weights are computed. The on par or superior performance of I-D-DW-Conv when compared to local attention suggests that dot product-based weight computation (which is core to attention)  is not crucial. Dynamic weights that are predicted using only the query/center pixel features also work well. I think this is an interesting for the community to know.

"	3
The paper qualitatively connects local attention and dynamic depth-wise convolution and they validate this connection empirically.	3
I think this paper provides valuable insights on the connection between local attention and dynamic depth-wise convolution and I suggest accepting it without a doubt. 	3
In summary, I think this is a good paper and leaning towards accept.  I think if the authors address some of the weakness discussed above then I will raise my score. 	3
"
This paper investigates the role minibatch SGD noise from a different perspective and provide novel insight about the strength and the shape of the noise. Paper finding could be of interest to the community, I therefore recommend acceptance. 

"	3
This work makes a clear contribution and is very well written.	4
This paper finds the novelty of discrete sgd noise. Compared with previous with Hessian noise approximation, this paper found the shape of sgd noise on a generic setting. Experiments shows much better consistency with this work. Throughout the paper, the theoretical derivations are solid and numerical experiments are well-design. Suggest acception.	3
The paper is well motivated, provides a simple and theoretically sound novel reward bonus for overcoming the stated “pessimistic exploration” problem and demonstrates positive results across the board for the 4-room and the 57 Atari game environments. No experiments are presented on continuous control tasks such as MuJoCo locomotion environments, which would have strengthened the paper further.	3
The motivation behind the method is well-justified and the approach is interesting and practical. The discussion on pessimism in skill discovery is insightful. The paper can be improved by further comparison with methods that are not focused on skill discovery.	3
"The paper introduces and solves an important problem, proposes a technically sound method, has a very clear presentation of related works, ideas, and results, and offers good experimental evidence for the claims.

Nevertheless, I believe certain clarifications/additions could improve the paper.  "	4
Overall, I think the paper is well written and has extensive experiments. My main concern is about the novelty of the method. These concerns are pretty important and thus I encourage the author to engage in the discussion period and clarify these if there is any misunderstanding. I am happy to re-evaluate if the author convinces me. 	2
Overall, the paper explores an interesting and timely problem. The paper presents a simple and effective scheme that has provable performance guarantees. However, there is significant room for improvement to make the contributions of the paper clear. Especially, the discussion around privacy guarantees needs to be improved. The authors also need to discuss missing prior work on communication efficiency + privacy for federated learning.	3
The paper lacks empirical results and the theoretical analysis is somewhat weak. Thus I recommend to reject the paper.	2
"---------------------------------
Post rebuttal:

I have read author's response as well as other reviewer's suggestion. I still think the paper is of value at certain aspects. But as other reviewers pointed out, I suggest the authors include a more detailed discussion on related work."	2
GIven the lack of novelty in the paper's main algorithm (A.2), the lack of comparison with other published works using similar approaches (A.1, B.4), the fact that authors themselves show a lack of understanding of the motivation behind using sketches (A.3), and multiple presentation issues (B.1 - B.6), I  recommend the paper be rejected.	1
The authors present a new method named ZeroSARAH where the main advantage is that they do not need to compute any full gradients. However this algorithms may requires more storage/computational cost than the previous methods that use minibatch to update. Therefore I do not see any advantage and do not support publication at the moment when these issues are unresolved.	3
Overall, I agree that an algorithm with zero full gradient computation is definitely interesting, it is not clear if this brings any advantage in theory (due to different parameters in the bounds) and also in practice (experiments are only compared to SARAH and not with the SOTA solvers in distributed settings.)	3
"Overall, the paper proposes ZeroSARAH and its distributed version D-ZeroSARAH, as variants of SARAH that avoid full gradient computation. Under the distributed setting (where such feature can be beneficial), requiring less number of gradient computations per iteration means that D-ZeroSARAH requires more iterations, i.e. communication rounds, than distributed SARAH to converge, since the  gradient complexity of these 2 algorithms are asymptotically the same. This provides some interesting tradeoffs, but in general, communication complexity is usually more prioritized in practice. D-ZeroSARAH can address certain situations where synchronizing all clients is impractical. This is a new perspective for this class of problems and can facilitate future work in this direction. The proposed methods are empirically validated. 
"	3
See above	3
The proposed model is interesting and in-line with recent work like BEIT etc, and obtains some decent results. However I find it hard to draw meaningful conclusions from the paper due to lack of proper apples-to-apples comparisons. Moreover, given the architecture is likely to have limited impact in the field (as I discuss in my weaknesses), I am borderline on this paper.	3
With strengths and weaknesses I listed in previous section, I think this paper makes some reasonable contributions but is marginally below the iclr acceptance threshold.	2
Given the aforementioned weakness of this paper, I think the novelty of this paper is currently limited, and its performance is not good.	2
"This is a well-written paper with good reconstruction results. In its current form, the paper lacks important comparisons/contextualization with a very closely related work (Rippel et al.), which previously introduced many of the main technical ideas in this in this work. Also, for the lensless imaging task in this paper, more details are needed to understand whether the resources required for this method and the state-of-the-art are comparable. 

Overall, I would be inclined to improve my rating if the paper is extended with (1) a more careful comparison with Rippel et al., demonstrating which specific aspects of this work provide innovation and improvement over previous approaches and (2) more details about the resources required for the lensless imaging experiments, to better understand whether the comparison provided is fair."	2
"This paper proposes an interesting architecture for the end-to-end optimization of the optical encoder and deep learning decoder for 3D snapshot microscopy. In particular, the authors introduce the Fourier network to handle the global PSF in the encoder with a lower computational cost compared to conventional UNet architecture. However, the method is evaluated only on simulated data, with a single type of object, and the explanation of conventional UNet result is insufficient to demonstrate the effectiveness of the proposed architecture. So the decision is weak reject.
"	3
In summary, there are some major issues better be addressed in the paper. However, I don't see them as a critical barrier for blocking this paper from being accepted, given its high-quality execution and potential impact on computational imaging research. 	2
"In my opinion, the foremost contributions of this paper are in the application and the pipeline, and not quite so much in machine learning methodology as the main idea (Fourier convolution) has been around for quite some time. The proposed pipeline is thoroughly motivated, described and evaluated. To me, this looks like an acceptable paper.

That said, I'm not an expert on the application (though it does feel like a bit of a niche). My relation to machine learning is that of a casual user rather than a contributor of hardcore methodology. So I'm looking forward to reading the other opinions on the work."	3
The authors provide a simple and interesting extension to VAEs for multivariate timeseries. Although I like the idea of the paper, the limited empirical evaluation can be improved in my opinion. Although the authors perform experiments on 4 datasets to compare to previous works, the hyper-parameters of the core building blocks are not evaluated properly. Ablation studies and the selection of values are missing. Therefore, this paper is not ready for publication at ICLR in my opinion.	3
The proposed methods simply do not seem to be presenting any sort of substantial originality that would make for me to recommend acceptance.	1
The paper has some interesting ideas regarding incorporating domain knowledge and interpretability into a deep generative model for time-series, but falls short in a large number of aspects. The literature review is insufficient, the background and methods should be described in more precise terms, and the experiments are not applied to well-known datasets, or extensive enough such that we can robustly see the benefits of the proposed model compared to the state-of-the-art. The experiments also lack any type of evaluation of the interpretability aspect of the proposed model, thus I do not see it as an important contribution. Overall, I suggest a major revision to the paper before recommending its acceptance.	2
"The authors proposed a synthetic data generator for the time-series domain. Based on the results, I didn’t see significant performance achieved compared to the other state-of-the-art methods. Meanwhile, it didn’t discuss the interpretability of this method which is claimed as one key contribution of this method. 

Strengths

Well written paper

Weaknesses

No interpretation was discussed since it was claimed as the main contribution 
The overall contribution to this field is limited

"	3
The paper is written very poorly, and is hard to follow. Even the problem statement is not clear. The authors study the controlability problem for SBMs. It is not clear how well this is motivated from the applications they mention. The technical contribution is not very clear, and the experimental section is very weak. The fit for ICLR is not very clear	3
This paper established a framework for inferring the global controllability measurement based on the community information. The scope might be narrow for the computer science group. But the topic itself is very interesting and lays on the frontier of control theory and network neuroscience fields. 	3
"The paper makes progress in an interesting direction: learning of network properties from aggregate observations.

What should be improved in a revised version  (see above)
a) the relations to previous work in the literature should be explained more clearly (ideas from concentration of measure, blind community detection and MOR)
b) a more careful discussion about the SBM and the weaknesses of this modelling assumption should be given.
c) a number of technical mistakes should be fixed"	3
I think the current version of the paper needs to be strengthened from novelty, evaluation, and presentation perspectives.  	2
"
The paper shows an interesting way to apply ""high capacity"" transformers to generate the weights of a low-capacity CNN model for few-shot image classification. While results are comparable to SOTA high-capacity classifier networks, the meta-network generation is interesting and able to produce very low-capacity classifiers."	3
This paper shows that the recent transformer architecture works for meta-learning setup as a hypernetwork. Although I believe it is a good information, but I am not fully convinced that this has enough contribution to be a full ICLR paper. Experimental results shows some improvements, but it is not convincing enough so that researchers will follow up on this work later. Therefore, I tend to reject this submission.	2
"While I generally appreciate the problem tackled in this paper (provable optimization of two-layer ReLU residual networks) and the idea of re-writing the learning problem as a QP (or even LP), unfortunately the paper in its current status appears to be (slightly) below the acceptance bar, because of the weaknesses (1)-(2) mentioned above.

On the one hand, weakness (1) seems to be inherent to the approach pursued in the manuscript (but I would still appreciate if the authors could comment on it and suggest possible ways to overcome it). On the other hand, weakness (2) does not seem fundamental, but it would require additional work. I encourage the authors to pursue the direction of giving concrete sample complexity bounds since, in my opinion, it would add value to their paper. This would mean not only to show that the sample complexity is polynomial, but also to understand the degree of such a polynomial. It would also add value to provide simulation results that indicate a certain polynomial dependence of the algorithm on the number of samples (e.g., linear, quadratic, etc) -- even better if the simulation results confirmed the theoretical bounds.

A couple of other comments are below:

(a) The discussion on the computational complexity is deferred to Appendix B. I think that this part is actually important and I would suggest to discuss it directly in the main paper. This is especially important if the authors are able to obtain sample complexity bounds, since the computational complexity bounds would be complementary to them.

(b) The input distribution chosen for the numerical experiments (mixture of N(-0.1, 1) and U(-0.9, 1.1)) seems a bit ad-hoc. Do the results change for different input distributions? (say only Gaussian, only uniform, and as a function of the mean of the Gaussian and of the uniform distributions)

(c) Page 7. nonparamatric --> nonparametric "	2
Understanding theoretical properties of neural networks is a major unsolved problem in learning theory. Considering that, this paper makes an important progress using simple algorithms and techniques. Although setting is somewhat simple, I felt the results should be encouraged. It would be interesting to extend this result to more complicated classes of neural networks.	3
The major novelty and contribution of this work lies in the study of the residual unit model, without relying on the assumption on the (specific) data distribution. The paper contains some interesting ideas that would be of interest in the study of more involved neural network models.	3
I think that the main contribution of this paper is incremental due to the following reasons. First of all, the model seems to be so simple that it even doesn't allow permutation and/or scaling invariance. Moreover, the paper lacks satisfactory comparative experimental analysis. 	2
Nice experimental results. Theoretical claim misleading/inaccurate as stated. If theoretical claim is fixed, then this could be a good paper.  	2
The biggest contribution of this paper is that it derives a more general GCN from the perspective of PCA.  The paper is well written and easy to follow. However, due to computation bottleneck and insignificant empirical results, I would rate this paper with a weak accept. 	3
Basically, the whole paper is based on posing an orthogonal constraint on GCN. However, they do not provide any convincing theoretical justification for that. Although it is a good paper to read, I tend to reject this paper because of a lack of novelty.	2
In general, there is no theoretical flaw in the paper, and the paper is well presented. However the novelty is limited as the proposed model is a parameter constrained NNPN or GCN.	2
The authors propose an approach of data augmentation for code translation for document similarities. However, the results show the approach suffered for independently developed code (CodeNet), which is the most realistic case. The approach is also susceptible to noise. 	2
This paper is interesting, but the research is somewhat superficial. 	2
"- The alignment algorithm need to be improved. This is not critical: this is still acceptable for the first observation.
- Conclusion sounds sometimes not reasonable according to experiments. Some results are hard to discuss due to unnormalized data."	2
The method and data set in this paper in its current form are rather trivial and more sophistication is required. The techniques seem highly tuned for mining well-organized data sets from coding-problem websites. A better and more improved model that can scale in the wild is recommended and would be a better fit for this venue.	2
"This submission is a follow-up to [Deng, 2020] that extends the previous work to the case of multiple parallel chains. Although the extension seems quite straightforward, the authors provide a theoretical analysis of the proposed scheme and perform its empirical study. The main issues are the clarity of the exposition and one inconsistency in the experimental section.

[Deng, 2020]: Deng, Wei, Guang Lin, and Faming Liang. ""A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions."" arXiv preprint arXiv:2010.09800 (2020)."	3
I believe that the paper is worth of publication since it presents an efficient importance sampling method that scales to big data problems and possibly leads to additional future investigations. 	4
This paper proposes an interesting extension to CSGLD whose advantages are shown both theoretically and empirically. However, I believe many questions deserve further investigation before applying ICSGLD into practical machine learning. I would like to hear authors' response on how theoretical implications are connected to practical concerns.	4
"(1) Page 2. For ICSGLD and CSGLD, I have one naive question. You are sampling (2) with a flattened density compared to the original Gibbs distribution $\pi$. If you are sampling a modified distribution instead, do you need to show that the new target distribution is close to the original distribution?

(2) Page 2. I am a bit confused with the notation here. In equation (2), you have $\Psi^{\zeta}_{\theta}(U(x))$ and then in the bottom of page 2, you have $\tilde{U}_{\Psi_{\theta_{k}}}$. What's the relation between these two different notations?

(3) Page 3. In your Algorithm 1, there is a stochastic approximation step for $\theta_{k}$. In your Assumption A2 in the appendix, you assume that the space $\Theta$ is compact. How do you guarantee your $\theta_{k}$ lives in a compact space since you are doing $k\rightarrow\infty$ analysis.

(4) Your Theorem 1 and Corollary 1 for the asymptotic normality is nice. But do you have a result saying that $x_{k}$ will converge to the Gibbs distribution corresponding to $\theta_{\star}$ or something like that, e.g. ergodic mean etc.?

(5) Lemma 2 (Lemma 7) is very similar to the result for CSGLD (Theorem 1, Deng et al. (2020).) What's the technical novelty here?"	3
"A new framework of population training is proposed, the key idea is optimizing a shared conditional network to learn and represent diverse policies and embedding self-play. 

The paper is well presented with just a few confusing points. 

Experimental results are strong and have many interesting empirical findings. "	3
I think NeuPL is new and interesting. Also, NeuPL is also rigorously studied empirically. Although I have the question above mentioned, currently I feel the strengths of the paper outweigh the weaknesses. 	3
The authors provide a framework called NeuPL to improve the performance and convergence speed of population-based training algorithms. The authors also conduct extensive experiments to validate the effectiveness and generalization ability of NeuPL. Although some important related work is missing and the writing can be further improved, this paper does solve some key problems in population-based training algorithms. Therefore, I recommend its acceptance.	3
"Overall I think the paper is sensible to appear at ICLR, but would not fight for it or be upset if it was not accepted.

The ideas presented within the work are interesting and provide a way to frame PBT methods and could facilitate the design of many future algorithms. These ideas are supported with some empirical demonstrations; however, my concerns regarding: insufficient discussion of place in literature, absent discussion of how hyperparameters are arrived at, and lack of quantitative analysis on the later experiments; leave me with reservations."	3
"I think the introduction about the PKCAM module is very unclear. The experiments are not sufficient and convincing.
"	2
In my opinion, the technical contribution of this work seems limited, while experiments and writing could be further polished. 	2
"-	What is the main contribution or motivation of this paper. If it is only about using the previous knowledge, it is not creative enough.
-	Try to explain or proof the working mechanism of the proposed PKCAM, which should be made more solid.
-	Try to polish the paper.
"	2
The paper is not ready for review, also the contribution and analysis are limited. Thus, I vote for rejecting.	2
This paper presents practical algorithms for MARL by utilizing the data-driven dynamic modeling by PSR.	3
The paper presents a clearly motivated and well founded approach for making multi-agent RL more stable and robust (in particular to non-stationarity) by use of predictive state representations that incorporate inter-agent interactions. In my opinion this paper is of very high quality, relevance and novelty.	4
The paper gives solid technical contributions and substantial experiments. I recommend acceptance of the paper.	4
"I think the idea of compositional learning is interesting. However, I didn't fully understand why the proposed representation is helpful for compositional learning? I hope to see more explanations from the authors in their feedback. 

The experiments are not sufficient as well. The authors simply compared several baselines without extra ablation study of their proposed method. It is hard to know why LSTM+Rec is better than other baselines. The authors just use 2 seeds for runs with DNC+Rec and LSTM+Rec, they should report the results of using more seeds in Figure 4. 

I think this paper needs to be improved and it has the potential to become a promising paper."	2
I think the overall problem of compositional learning through meta-referential games is interesting but the paper is hard to follow and its main contributions should be emphasized more; especially the benchmark should be detailed with more results, ablations and the new representation, SCS, should be clarified. 	2
I wasn't convinced of the utility of the three main contributions of the paper outside the limited setting described in the paper: SCS, the meta-referential game and the failure of models on the same. Further, the paper is unclear in several places and does not provide enough details. Thus, in its current form, I recommend that the paper be rejected.	2
"This is an ambitious paper that could offer some promise. Some of the ideas are intriguing. However, to achieve its potential, this paper needs substantially more development conceptually, in terms of evaluation, in clarity of communication, and in engagement with the existing literature. 
"	2
This is a good paper, with some aspects of the presentation that should be improved. 	3
Well motivated method that is provably more powerful than 1-WL, which incorporates structural information in a principled manner using the notion of overlapping-subgraph ismorphism. Extensive experimentation shows strong performance compared to the competition (including the higher-order WL procedures). 	3
I would like to recommend to accept this paper, for its expressive, efficient and easy-to-use GNN component with intriguing performance.	3
"A well written paper with strong theoretical exposition and results. 


---
**Post Rebuttal**: I am satisfied with the authors response and keep my original score. "	3
"The authors proposed a novel method that allows Deep Koopman Operators to handle noisy systems, and demonstrate that their approach beats various baselines. However, there are not enough experiment varieties, and the results are close to the SAC baseline (not by large margin).
"	2
Although the building blocks of this paper are not new, the paper leverage these ideas and empirically shows the advantage of the proposed framework. I recommend acceptance of this paper. 	2
If the issues with the theoretical statements are adequately addressed, I find this to be a nice paper with convincing empirical results.	3
"This paper presents an elegant solution to the modeling of uncertainty in dynamical systems for the puprpose of designing closed-loop controllers. The model is backed by control theory and has a guaranteed stability. The method is evaluated on several tasks showing its properties.
"	3
Although the question raised by this paper is interesting, the proposed answer is not clearly exposed, not rigorous enough on the math side, and not enough significant to warrant acceptance.	1
"Unfortunately, I have to recommend that the paper would be rejected for the following reasons:

1.  the analysis concerns binary classification, which is one among many learning tasks to study. It is not clear how to deal with other learning tasks. 
2. The presentation of the theoretical results (Theorem 3.1 and Theorem 3.2) is poor and informal, and the proofs lack rigor. For example, the threshold 0.6 in Theorem 3.2 Is mysterious and does not seem to stem from any grounded argument. "	2
"Correctness: 
This paper only considered limited scenarios in classification problems. Therefore, the title and the statements in the paper may be exaggerated by saying quantum kernel methods instead of quantum kernel based classification methods. The proofs of the major theorems are not convincing to me. The experiments are not strong enough to support the claims made in the paper.

Technical Novelty And Significance:
The theoretical contributions of the paper are limited if there are some at all. 

Empirical Novelty And Significance:
I am not sure if the authors have proposed any new quantum methods for the experiments. The results are hard to understand."	1
"- Overall a well-written paper.
- I have some problems with the proof of theorem 3.1.
- I am also curious how the threshold value 0.6 is arrived at, and whether this is purely experimental or if there is some alternative intuition.
- The experimental section is very thorough and quite convincing.


-- EDIT AFTER FURTHER CONSIDERATION --

After further consideration and reading other comments I am persuaded that the informality of proofs is a more serious problem than I initially thought and adjusted my recommendation accordingly."	3
Overall, I found the approach to efficient DDPM sampling employed by the authors to be sensible and reasonably novel. While their method can indeed effectively increase log likelihood for DDPM with a greatly reduced grid of time steps, this did not appear to translate to improved model quality in terms of actual generated samples. The final conclusion is therefore somewhat unsatisfying because an ideal DDPM schedule would be short and efficient, able to produce high log likelihoods, and able to produce low FID scores compared to other methods. Since this goal is not achieved, I recommend that the authors revisit their approach to identify if there is a way to more effectively incorporate sample quality (rather than log likelihood) in their DP algorithm.	3
Overall, the paper gives a nice overview of the literature and present a new inference scheme in post-training scenarios. I have some remarks above that can make me reconsider my evaluation, and I hope for a nice discussion with the authors.	2
Possibly with some refinement, the paper has the potential to be very good. As it stands it presents a dramatic speed improvement that may-or-may-not produce compromise the final model. Overall I recommend acceptance.	2
"I evaluate the paper as a reject for the following reasons:
-	The paper is not well-motivated and does not seem to fit within the general scope of ICLR (see more below).
-	The analyses are univariate and possibly misleading (see more below).
-	The study is non-conclusive and lacks a coherent structure.
-	No in-depth study of the observations is provided and the results raise more questions than answers.
"	1
This paper touches on the very important topic of investigating explanatory effects of demographic and meta-factors in conference attendance, submission, and acceptance. This is an extraordinarily wide mandate, and the paper attempts to cover a fair amount of ground - this also results in several intuitive, unanswered questions when reading. Despite the opportunity for deeper analysis, I believe this paper is valuable in understanding representation at conferences (specifically ICLR) and provides some starting points for improving the same.	3
This paper does not fit the typical mold of an ICLR paper, in that it is mainly empirical work rather than presenting novel methods.  As always, more analysis could have been done.  However, its findings regarding participation and review in ICLR are valuable to the ICLR community, and it would likely obtain substantial attention in the literature and have an impact on the machine learning community at large.  	2
"The analysis presented in the paper contains some interesting results. However, many of the findings overlap with those of (Tran et al., 2020), which conducted a similar analysis. In addition, I raised several concerns and questions on the methodology used in the paper. 
"	1
As said in the main review, this is an interesting and insightful work. Its main focus is theoretical, and clearly expands on previous work in a non-trivial way. Empirical results are somewhat simplistic, but as their main purpose is to demonstrate theoretical results this is fine in the context of this submission. Therefore, I recommend accepting it and look forward to seeing it presented in the conference.	4
"The work represents a theoretical generalization of the results of Hanin and
Rolnick on linear regions in random ReLU networks, but it is an incremental
one, with proofs whose structure nearly agrees with those of the prior work.
The empirical results could be more systematic in helping to illustrate the
key novel differences between this setting and the previous setting, and why
they are important.

"	2
Overall I think the paper can benefit from more empirical results and discussion. While the extension to the manifold setting is a valuable contribution by itself. The significance lacks empirical support. 	3
The paper displays a systematic treatment of studying the number of linear regions in ReLU neural networks. However, the theories and experiments have some issues, which undermine the overall quality of the paper.	3
"Author's goal: Create a ML model that generates de novo sequences that fold.
- That fold could be anything at all. Simply ""a folding predictor is confident that it folded my sequence accurately"".

I think this is a fun idea, but far too flawed. I am curious what the generated sequences look like... Not because the are likely to fold, rather because they are ones that ""fool"" AlphaFold.

Critique in summary:
The work boils down to an adversarial attack on DMSfold2, which proves to enrich for high scores in AlphaFold. But, adversarial attacks commonly create nonsense examples that exploit biases in the target model.
AlphaFold is not convincing validation, because it is too similar to the oracle used for optimization.
Both DMSfold2 and AlphaFold are misused. They've been trained & tested for multiple sequence alignments of natural sequences. There's very very little evidence that they accurately predict the fold of individual non-natural sequences.

The method and my problems with it:
1. Sequence generation = autoregressive sequence model. First, the AR model is trained on seed sequences. It then generates new sequences.
As a seed, the authors use sequences generated from trDesign (Baker lab 'hallucinations' model). Effectively, these are sequences predicted to fold according to Rosetta.

Problem = They've started with the answer, Rosetta designs.
- In the output, 922 / 950 seqs were ~33% identity to seed sequences.
- Need authors to include more detailed sequence comparison.
- That 33% might be enough 'cheating' to get high 'validation' scores, in which case they've just used Rosetta.
- They need to report the validation scores for the trDesign seed sequences too.
- Are the predicted folds for final output the same as for seed sequences?

2. Generated sequences are then mutated to optimize ""folding score"" = KL divergence of distograms (binned pair distances) according to DeepMetaPsiCov fold 2.
This should mean ""optimize sequences so that predicted contact maps are highly non-random"". With each iteration, the AR model is then trained on the optimized sequences plus the previous iterations' sequences.

Problem = They are optimizing a sequence generator to exploit a model OUTSIDE of its use-case. This is like an adversarial attack. I would expect to get sequences that exploit false biases. 

3. For validation the final output sequences are run through AlphaFold prediction & scoring = pLDDT (predicted Local Distance Test score). Authors claim AlphaFold is independent validation from the loss function they optimized against.

Problem =
AlphaFold is not an independent validation, because it is virtually the same model, trained on the same data as DMSfold2, and outputs the same sort of score.
- Trains on MSAs
- Trains on natural sequences
- Very similar architectures
- Scores, AlphaFold pLDDT vs DMSfold2 confidence. Both are predictions of distogram measurements, LDDT and TM, respectively. And trained on PDB structures of natural sequences.
Also a misuse of AlphaFold the same way as in #1. AlphaFold uses MSAs, and is trained/validated 99% on natural sequences.


Results, boil down to 2 weak observations:
1. The basic training task is possible: training the AR model indeed creates sequences that improve KL Divergence of DMPfold2 distograms.
- Non-result.. should be guaranteed that the ML training will improve objective score.

2. The output sequences have higher AlphaFold predicted LDDT than random sequences, sequences output by an AR trained on UniRef50, or two other models.
- Comparative advantage is versus other unproven models, but not e.g. natural proteins, or Rosetta designs.
- Advantage is all but guaranteed by similarity to DMPfold2, which was optimized for.
- Unclear that the AlphaFold pLDDT score is meaningful for non-natural sequences.
- Unclear if higher scores are byproduct of using trDesign seed sequences.

"	2
The core methodological idea is worth exploring, and ultimately publishing, in the context of sampling protein sequences. But, the novel part of the methodology is conceptually similar to other population-based sampling strategies like EDA (estimation of distribution) and to self-training, and those connections are not mentioned or contrasted. Importantly, the baselines are different in ways that aren't relevant to the core novelty, making it hard to draw strong conclusions about the core idea itself (versus the influence of all the other maybe-helpful-but-not-methodologically-interesting changes in the system proposed). I'm open to changing my rating, but this is my take.	3
Overall, the authors present an interesting method that clearly achieves their goal of generating sequences predicted by AlphaFold to fold. I am left feeling that this problem is not clearly motivated and that the solution is not significantly novel from an ML perspective. 	2
The paper is clearly of interest. The approach is, up to my knowledge, a novel and wise way of assembling building blocks from various recent articles. However, in the current state of the article, there are still a few points that need to be clarified in order to be fully equiped to assess the potential of the approach.	2
"The problem of generating novel protein sequences is a popular one. Although this submission develops a somewhat novel approach to solving this problem, the method's success is dubious, if not absent. The overarching claim that the method is unconditional is not quite true, and the sequences generated are almost certainly highly biased and limited. There is also no real validation of the generated sequences, and minimal comparisons to existing work in the same space. For these major methodological and technical issues, I unfortunately cannot support its acceptance.
"	2
"Details of the proposed method and experiments are important for readers to follow the main concepts. It would have been better to provide more details in the paper. 

"	3
"Based on my main review comments, I suggest ""reject""."	1
"In my opinion, this submission is not yet ready for publication. The novelty of the proposed method is limited, and more thorough experiments are needed (Please refer to main review).

------------------------------------------------------------------------------------

Update:

I will keep my score unchanged because the authors have not provided a rebuttal so far."	1
Please see the detailed comments in the main review section.	3
The motivation for the proposed algorithm is interesting and promising. However, the empirical evaluation raises important concerns about the applicability and usefulness of the method to defend against backdoor poisoning attacks. On one side, the drop in performance on regular inputs when using the proposed technique is non-negligible and, on the other hand, the selection of the parameters of this algorithm can also have a significant impact both on the performance on the clean and the poisoned dataset (and it is not clear how to set this parameters properly). The experimental evaluation should be more comprehensive. 	3
The authors presenting an interesting approach towards defending against backdoor attacks involving noisy label algorithm. However, a more thorough comparison with previous defenses and other forms of attacks is required to understand the significance of the method. An optimization based approach towards defending against both clean and noisy label attacks would be interesting.  	3
The authors theoretically proved that noisy-label defense algorithms can be leveraged to against backdoor attacks, but the method seems not very innovative, and the computational cost would be high compared with other methods.	3
"Since the novelty of the paper currently needs further clarification, I am providing a borderline rating, subject to the author's rebuttal. 

AFTER REBUTTAL: The author's rebuttal did not sufficiently convince me of the novelty of the contribution, and thus the paper remains borderline in my estimation. However, I will be raising my score. "	3
"The idea of degradation attacks on certifiably robust AE detection is novel. However, the experiment setting is a bit too simplified. I recommend the authors to strengthen the experiment part so that the severity of the threat is better motivated. I'm giving a marginally below acceptance for now, but I'm willing to increase the score once my concerns are addressed.

================================================================

After viewing the author response, I'm raising my score to 6. The technical aspect is sound, and the author explained a relatively intuitive concept in a reasonable manner. Thanks for the clarification."	4
"I recommend rejection due to lack of novelty and comprehensive evaluations over SOTA verification methods. Also, I am not very convinced by the motivation of the proposed methods.

=== Post Rebuttal ===
Thank the authors for the extensive discussions. Some of my concerns have been clarified in the discussion and the revised submission. Some insights of the paper are quite interesting. However, I am still concerned about the novelty and SOTA evaluation of the proposed methods. Overall, I decided to increase my score to 5."	2
"Though the idea of the paper is interesting, and the basic attack techniques proposed seem effective, I had, besides some more semantic issues, some concerns with the correctness of the argument of the paper with respect to the upper bound determination. This means that I cannot accept the paper in its current form (unless of course the authors correct any misunderstanding on my behalf – which is very possible). 

Edit: The author revisions have addressed some of the main issues with the upper bound determination (including a discussion of monotonicity). Interestingly, the inclusion of the PGD attacks in the supplementary materials, as per my suggestion, seems to perform about as effectively as the proposed algorithms in the main paper. Given the improvements, I have increased my rating to reflect that and the fact that the underlying idea is interesting."	2
The paper seems to be mathematically solid, but I see room for improvement in the experimental section. Since the experiments focus on small architectures, it is unclear to me whether the proposed algorithm can be used to compress larger networks. Since theoretical work has an important value on its own, the compression algorithm is not required to be of immediate practical interest. However, it should be made clear what the limitations of the presented method are. Judging from the presented experiments, the last sentence of the abstract may be overclaiming the experimental results. Nonetheless, the tropical geometry section seems intriguing and may be of interest to theorists.	4
"As this work is outside my domain of expertise, this is only an educated guess type of review, and I am willing to hear how other reviewers' opinions. 

===Update after the authors' response===
The summary of the paper in the authors' response is very helpful for the reader to grasp the main message of this work. Still, I have decided to keep my score as a) the empirical contribution is a bit limited, and as mentioned by the authors, the experiments are mainly proof-of-concept; and b) though the theoretical contribution is novel, it still needs a bit more work to justify its significance."	3
This paper has some interesting results (such as Theorem 2) and the techniques are interesting and seem well-motivated (although this paper is not the first to study tropical geometric neural compression). However, the bounds from Propositions 4 and 5 seem to be not practically useful, and the experimental results have limitations. Thus, I hesitate to recommend acceptance at this time. It would be great if the authors can provide clear responses to my questions above.	3
I am leaning towards acceptance of the paper as it provides a very interesting and new idea. As I am not a domain expert in tropical geometry, I would wait for other expert reviewers to judge the theoretical part. If authors can provide pseudo-code or code segment that makes this paper more accessible for me leading to better understanding, then I am willing to change my score.	3
The paper is novel enough, methodologically sound, and empirically well-validated. 	3
The major concern goes to the experiment setting. The benchmarks are too simple to show the potential of the model and the baselines compared can be more up-to-date. I lean towards reject and may reconsider my rating based on authors' feedbacks. 	2
I find the idea of including consensus clustering in a VAE an interesting and potentially useful idea to explore, but the theory explicitly suggests that this model only helps in the supervised case, not an unsupervised case.  Additionally, I would argue that supervised information leaks into cpl-mixVAE through the data augmentation, as is the explicit goal, and it is unfair to compare to purely unsupervised methods on only supervised metrics.	3
My main issue is that as the paper is it is difficult to assess whether the strength of the method comes from the concensus mechanism or whether it comes from the factorisation. In addition in the discussion about the number of arms being a strength when the distribution of the categorical factor is not uniform I miss the justification. 	3
Hard to say how novel the contribue is. I want to see the author respond to my question about table 1.	2
"The paper provides a unique way to test generalization abilities of models and offers some novel recommendations and insights.

My score reflects my view that the data generation process and test environment that the authors propose are important and novel contributions."	3
While the paper proposes some interesting ideas (iteration, log-polar convolutions) and attempts to understand generalization across 2D transformations in a rigorous manner, it is currently not mature enough for publication. I would advise (1) extending the experiments on some more realistic datasets listed below, (2) paying greater attention to the training/testing distributions used, and (3) placing the work in the context of existing works above all to make this work more solid.	1
A technically sound paper that has limited novelty. More baselines should be compared against on more datasets.	2
Though it is useful to investigate the OOD transformation on toy tasks first, it is important to scale it up to more realistic settings later. The observations in this paper are not likely to generalize to realistic settings that we really care about. 	2
The paper as it's well written, and shows novel analytical and empirical results. I think the losses are simple and easy to implement in practice.	3
"This paper presents several theoretical and practical materials regarding large-margin based losses.
They, however, are of limited novelty and are less contributive to theoretical understanding of margin-based losses.
Thus, my rating score of the paper is leaning toward weak rejection.


-- After rebuttal --

I appreciate the authors' effort to provide detailed response.
It nicely addresses my concerns about Theorem 3.2 and the practical methods.
Based on the rebuttal, I upgrade my score to 'weak accept' and recommend the authors to put those materials into a revised paper for clarifying the theoretical and practical contributions."	3
The theoretical explanation in this paper is nice and the experiments on person Re-Id and face verification confirm the effectiveness of the proposed method. In the NeurIPS 2021 review, this paper accepted two marginally above and one borderline result. In this ICLR submission, the main concern from the former reviewers has been fixed. In ICCV 2021, face-related papers have the highest rejection rate (>80%). There are two main groups of researchers in the face recognition community. One group of researchers focus on high performance under large-scale settings. Their papers look like experiment reports, but the performance is impressive. Another group of researchers focus on theoretical analysis. Their papers look more rigorous, but the performance is not satisfying and the proposed methods are not orthogonal to current state-of-the-art solutions. Even though I am from the first group, and I am not persuaded that the proposed method can obviously improve verification accuracy (in Table 4), I believe the theoretical explanation in this paper can enrich the research community and enhance the deep understanding of margin-based softmax loss designs. Both of the researchers from the practical and theoretical sides need to appreciate the contributions and novelties from the other side.	3
The paper is well-written and the novelty/contribution is significant and somewhat new. However, the weaknesses of the paper should be addressed to improve the paper.	3
The paper is very clear, well motivated, the approach is simple, and problem is important.	3
"Overall, the paper presents novel ideas and tasks and places the work well within existing literature.  This could have significant impact on unsupervised domain alignment and more generally on thinking about alignment problems. The main weakness is the potential for the proposed algorithm to collapse to standard distribution alignment and the lack of explicit evaluation of support alignment.
"	4
It seems that the proposed method (SSD) is closely related to Chamfer divergence if not the same (however, there are no discussions about their relationship yet). For experiments, in my opinion, the proposed approach is similar to optimal transport. Therefore, the authors should compare the proposed approach with other optimal transport-based approaches for domain adaption.	2
"The theory established in the paper is interesting for the community, especially with the general result of reducing the comparison between distributions to a uni-dimensional one. However, there are some gaps between the theory and the proposed algorithm, especially concerning the use of the unsupervised conditional entropy loss on the target domain.

# Post-rebuttal 
As a result of the authors's detailed response, I update my review score to 8."	3
I believe that this is a great work and useful tool for scientific research, but I doubt that it is suitable for ICLR main conference: it's rather a software engineering paper than a novel research. 	2
The paper makes processing geospatial datasets easier with deep learning algorithms, and is worthy of publication at another venue after responding to some concerns. The paper does not contribute to any aspect of representation/deep learning, and hence ICLR would not be a right fit. The contribution, the TorchGeo package, is more relevant for the remote sensing field.	3
"I work with geospatial data, and this paper does a good job at describing the challenges, and I think it's important that packages like this are developed and publicized. The paper doesn't introduce anything particularly novel, but I think the goal of providing reproducible methods for experimentation is technically significant enough to warrant publication. 

There are a few issues in the weaknesses that I would like to see resolved, but if these are addressed I think this is a above the acceptance threshold. Particularly what I'd like to see is more discussion of how torchgeo handles MSI/SAR data and how (or if) these disparate modalities can be rectified and combined when merging different datasets.

Lastly, I'd like to mention that I've done a lot of work in this domain, and after the review period is over I'd be interested in connecting with the authors to share ideas and software in order to help make this package as generally useful as possible."	2
The paper establishes a new domain specific library that is ML-ready	3
Even though the library is interesting, the overall contribution of the paper is low and does not advance the field	2
I believe this work is solid and provides insightful finding, and thus I vote for acceptance of this work.	3
This paper makes an important contribution on identifying a particular vulnerability of ViT against patch attacks compared to CNNs and has comprehensive empirical results, though this paper does not consider robust training for the models.	3
Ultimately, I think there is a good idea here but the lack of qualitative examples makes a fair evaluation of this technique not possible. I would not recommend publication of this manuscript in its current state. 	3
This paper proposes an effective Patch-Fool Attack for evaluating the robustness of ViTs, which is novel and the key finding of this paper is significant. I suggest acceptance.	4
"The idea of a survey on evidential deep learning is excellent and valuable to researchers. However, in its current state, I do not believe the paper is ready for publication at ICLR. With improvements to the broader insights and takeaways from the survey as well as paper presentation, this work will become a good contribution to the academic community.
"	2
Overall, the paper is informative and did a relatively good job in introducing the basic concepts as well as the motivation of evidential deep learning. However, it seems there are still some important related works that are missing from the references. Besides, the paper in its current form looks more like a review rather than a comprehensive thorough survey, partly because of the space constraints in ICLR. Therefore I am not sure that ICLR is the right venue for it. Perhaps it is more suitable as a long journal where more details and taxonomy could be included. 	1
"Contributions of this paper are not novel, which is normal for a survey paper. 
I think survey papers are not suitable for conferences as they need to be evaluated using a different set of criteria. 
Therefore, my recommendation would be to reject this paper purely based on its type. "	1
Having said that the paper touches the important topic of making a new model family more accessible to the audience, I am afraid I do not think it does it in a way that would add sufficient value on top of one reading the material from the original papers. I think this way because the paper adopts most of the content verbatim from the original sources and does not add a level of abstraction that helps the reader draw new insights that are not straightforward from the original sources. Nor does it highlight overlooked positive or negative properties of the evidential model and nor does it present any empirical outcomes that one would find surprising. Under these conditions, I am not able to recommend an accept for this paper in its current shape.	1
This is a good paper overall. It proposes a simple yet highly effective method to address various DA problems. There are some issues in the experiments, which I hope the authors could address in the response or in the future version. But overall I like it and recommend for acceptance. 	3
"Although the technical novelty is somewhat marginal, this study has a significant impact in the literature of SSL and UDA by empirically showing a unified scheme capable to solve both SSL and UDA. I vote for ""weak accept."""	2
The contribution is significant and somewhat new, and the experimental evaluation is extensive. However, three concerns in the weaknesses of the paper should be clarified in the paper.	3
"The proposal extends FixMatch to work on three related tasks, and outperforms existing methods on two datasets.  The proposal mixes existing approaches for normalizing label distributions to train the model with a cross-entropy loss on different versions of the data to provide regularization. Moreover, the paper also proposes a random logit interpolation as means of regularization.  However, the intent of how these pieces were selected and how they work together is not clear to me.  The paper demonstrates empirically that the parts are necessary through an ablation study, yet I don't know why they are needed.

Overall, the paper presents an engineer method that works, although why escapes me."	2
Promising verification procedure, but a more thorough evaluation is needed.	3
The paper presents its new idea cleanly, and the improvements appear substantial enough for publication.	3
"Whilst the paper introduces some novel branching heuristics for the complete
verification of ReLU neural networks via branch-and-bound, the overall
contribution lacks in my opinion the sufficient novelty to merit an ICLR
publication. I think that  the authors should include additional empirical
comparisons with related tools on bigger networks and consider submitting a
tool paper in a top venue."	2
In conclusion, I feel the paper presents an interesting idea but needs a more thorough evaluation and comparisons to related works to justify the claims made. Hence,  the current submission is marginally below the acceptance threshold for ICLR.	3
"This paper combines existing techniques to improve the performance of complete verifiers. The contributions to the bounding process (incorporating the additional constraints with Lagrange multipliers) are relatively minor. The improvements to the branching procedure are interesting but should be clarified further.
It is however disappointing that the empirical evaluation has several weaknesses, mainly due to the lack of comparing to strong baselines."	2
The key idea of the paper is very interesting. Access to a data-driven methodology for selecting near-optimal action set in a generic and principled way would be very helpful in developing an RL based solution for a real-world challenging problem. However, I am afraid that the presentation of the paper is not up to mark at this moment; I failed to understand the key technical contributions and empirical evaluation methodology. Several important technical details and experimental settings are also missing.	3
"The paper is confusing at some points and the actual contribution is only small.
There is no theoretical justification of the method.
The experimental evaluation has some shortcomings.
The paper is not ready to be published and needs a thorough revision.
"	1
This is an incomplete work. The method is not explained in detail and is not evaluated in a sufficient manner.	2
This paper has major weakness in several aspects, including both the algorithm design and experimental analysis. The paper is not ready for publication with its current shape.	1
I lean towards rejecting this paper. The authors didn’t make a compelling case for why they need a new LaTeX-based language to enable auto-batching. The evaluation also lacks several details. 	2
"Pros:
. The proposed idea is interesting.
. The proposed tool is implemented and integrated with Jupyter Notebook. 

Cons:
. The benefits of the proposed tool are unclear. 
. The evaluation is limited.
"	3
The paper is good. However, it presents a system that cannot be tested.	4
"The whole paper (Introduction, Fig 1, etc.) implies that this study has a smart device that can be attached to the left ear that will inject well-designed perturbation to collected EEG data. I was excited when reading the paper and I know how hard it is. It requires a multidisciplinary team to design the wireless smart device, manufacture the device and make sure it really works, break the EEG collection equipment (like Emotive headset) which is difficult as the signal transmission is generally encrypted, and so on. I was intended to give an accept (or even a strong accept) as long as they can implement the whole system in the real world. 

But it's not the case! The dataset is not locally acquired but is the most well-known public EEGMMIDB dataset, which takes little effort to get. The attack is not really deployed with a smart device at the left ear which can emit perturbations to the source signal, but is simulated. There is no device, no signal transmission, no poisoning attack on source signals. 

Without the effort on physical systems and real-world attacks, this paper is still good but not good enough for ICLR. It's a good story, but still a story. So I decided to give a rejection."	2
"I consider this paper as an interesting work with a proper motivation and novelty component. The topic addressed (adversarial perturbations on wearable BCI ) can be definitively of interest for the conference audience. The main improvement point in my opinion is the description of method and experimental setup. Further details on the adversarial approach I think would improve the paper. I consider the overall contribution of the work as adequate, and hence my recommendation
"	3
 This is an interesting study, but in my opinion requires some additional work to make the contribution more solid, specifically comparison with other smooth adversarial attacks on electrical signals should be provided, and selection of CNNs vs transformers should be explained. It would be also beneficial to verify the proposed methods with other datasets/other EEG-based tasks if possible. 	2
The paper is well written with a clear narrative, and used methods are well described. Main limitations of this work is on the presentation of the general motivating perspective, as well as a general lack of methodological comparisons/evaluations and discussions on possible countermeasures that one can consider for DoS attacks on BCI systems. Some experiments/comparisons are highlighted and suggested in my main review for the authors.	2
Though some points of the presentation could be improved, I think this is an interesting idea illustrated by good experience.  I would like to see it appear in the conference.	4
The proposed algorithm is certainly an option that has not been explored before but the practical value is at least questionnable due to the missing baseline experiments.	3
"In general, the paper is well-written. However, the proposed model has not been explained appropriately in Section 3. 

Section 3.2 is the main contribution of the paper which proposes the computational graph that estimates inducing points and variational parameters. Here it is necessary that the authors address the major issues of their method: why maximizing the lower bond (Equation (4)) in the proposed DNN leads to better results than the available baselines (e.g. SWSGP and SVGP which maximize the same objective function)?  Why does a small number of inducing points provide a better posterior than other baselines which find more inducing points (for instance SWSGP provides more inducing points but uses only H nearest inducing points for each input point)? Indeed, it is not clear how the batch permutation-invariance is achieved.  Something like a sum over the batch dimension is clearly not a good idea because the encoder is sensitive to the ordering of the batch. However, the paper does not mention how this issue is considered.

Besides, here are some omissions of related work that are not much discussed but could be mentioned, e.g. 
https://arxiv.org/pdf/1303.0383.pdf, 
https://mlg.eng.cam.ac.uk/zoubin/papers/aistats07localGP.pdf, 
https://www.dbs.ifi.lmu.de/~tresp/papers/bcm6.pdf,
http://proceedings.mlr.press/v84/salimbeni18a.html."	2
"The paper proposes to improve the training time and flexibility of sparse GP approximation using deep neural network. While it makes sense to optimize the ELBO using deep neural network, the proposed framework may have certain limitation, such as learning the mapping from one (high-dim) input to multiple (high-dim) output can be inefficient, especially when we have limited data. Another suggestion is to demonstrate the uncertainty estimation of the proposed approach.

The paper is well written and presented. The related work section is well covered.

The paper is currently in the border-line.
"	2
"## Summary of Review
The method is interesting and the empirical results seem promising. I do have some concerns that the posterior distribution defined over $f$ is not consistent. Perhaps this is not essential if only marginal predictions are needed but it does seem to be a significant drawback in cases when covariances or samples from the approximate posterior are needed.  This also makes me slightly concerned that the objective function is not quite as well founded as that of Titsias, 2009 (see the discussion in Matthews et al 2015). I think some of these critiques apply equally to Tran et al, and I am curious to hear the authors thoughts on these limitations. 

The writing is generally reasonably clear, though aspects could be improved. While I have rated the paper as marginally below the acceptance threshold, I think it is quite close to the threshold.
"	2
"The paper presents a novel way of combining information from text and knowledge base modalities. The results on 3 datasets show the models improved performance, and ablations show quantitatively that the model performs better on complex questions. While I believe the paper can benefit from some qualitative analysis, and there are some open questions that need clarification, I believe the contributions presented are enough to merit an acceptance.

Edit: I have updated my score after reading the author response and edits made to the paper."	3
A good paper with a small and easily fixable issue regarding LM initialization.	3
In summary, the paper has some nice contributions, such as a novel architecture to enable deeper interaction between LM and GCN, clear writing, and extensive analysis. Although there remain some issues and questions, the paper should be able to bring to the community some new aspects. 	3
In summary, this paper proposes an interesting idea to combine language model and graph neural networks for question answering and shows good results. But more experiments and analysis should be conducted to justify their design on modality interaction modeling. Adding them will make it a strong paper. 	2
"In general, the paper's contributions are unclear, while the paper tackles a very interesting problem in the context of diffusion-based generative models and provides empirical improvements. Unfortunately, the proposed method is not well-motivated, and the discussion about it is insufficient. Thus, I opt to reject the paper. However, I'm also inclined to improve my evaluation if the aforementioned weak points are well addressed.

===== POST-REBUTTAL COMMENTS ========  
The rebuttal had addressed some of my concerns. Consequently, I raised my score from 3 to 6, and I also raised the ""Correctness"" & ""Technical Novelty And Significance"" scores from 2 to 3.

"	3
This work proposed a relatively simple, if computationally difficult optimization procedure for finding the parameters of a sampler able to generate compelling images in fewer iterations. I found the approach reasonable given that a more flexible family of samplers is proposed to be amenable to the optimization process. The results are compelling, achieving better sample quality in fewer timesteps. In the regime of fewer than 10 steps, this work achieves better sample quality in half the steps. 	3
"1. Although the proposed approach can generate high-quality samples, the objective used is hand-crafted and might lack theoretical guarantees. As mentioned in the paper ""GGDP family does not guarantee that the marginals of the new forward process match that of the original DDPM."" Although the sample quality can be reasonable, it is unclear what the model is doing theoretically (e.g., is it optimizing likelihood, or doing score matching).  
2. The samples, after optimizing perceptual loss, might not correspond to the samples of the underlying DDPM model. Optimizing via perceptual distance is expected to have better performance under FID/IS scores, but it would also change the statistical property of DDPM. In some sense, the samples are not from DDPM, and it might be useful to discuss in more detail the statistical properties of GGDP.
3. As mentioned in the paper, GGDP can use pre-trained DDPM models. It would be more convincing to also show results on more challenging high-resolution datasets (e.g. LSUN, ImageNet, CelebA) using pre-trained models, since the inference time of high-resolution models are ofter longer, and improving the inference time of larger models is more crucial.
4. The samples on ImageNet 64 in the appendix are not very good compared to the samples on CIFAR-10 in figure 2, which also raises questions on how general and robust the proposed approach is.
5. Given the existing work (e.g., DDIM), conditioned on all the previous noisier images in the sampling chain might not be very novel.
"	2
"I think the problem of accelerating diffusion models is important and believe that the paper makes a novel contribution towards it. That being said, I believe that the paper has serious issues regarding clarity and minor issues on experiments over larger datasets. 

I am willing to increase the score if the authors address clarity issues well enough in the rebuttal and especially in the revisions that are allowed by the ICLR format. If the revision does not change by much, then my score will be around 5 and 6 (good idea, relatively poorly polished paper)."	3
While simple in hindsight, I think the new idea of this paper deserves being published in this venue.	2
As mentioned earlier, the paper makes a good contribution and thus I recommend acceptance.	3
"Even though the framework and analysis is simple, I believe community can benefit from these faster thinning meta algorithms. I am not sure how well ""distribution compression"" fits ICLR. "	2
The novelty seems to be fairly limited as the paper mainly combines two known ideas: bucketing and secure median. The threat model is somewhat non-standard, and experiments are conducted on only 3 clients.	2
"The paper proposes an interesting approach approach for tolerating malicious users in FL, but it does not consider some prior works. Hence, I recommend revising the paper to include all relevant works and adding the comparisons. 

-- Post Rebuttal  --

I still do not see how the proposed approach in this paper compares to the related works I mentioned. While [1] does not provide server side running times, you should still evaluate their protocol and compare with your work. The responses to the non-IID issue and the scalability issue that I raised are not satisfying neither. Hence, I am keeping my score. 
"	3
The paper studies privacy-preserving and robust federated learning. I think the paper would be stronger if my comments are addressed. 	3
"
While the topic is interesting and the paper may have a valuable contribution, I don't succeed to verify the proofs of its theorems, and the way in which the theorems are stated lets me believe it is possible they are not correct.  "	3
The paper makes an interesting hypothesis and goes about validating the hypothesis. However, the improvements due to the proposed method are marginal.	2
Overall, the paper indeed has some merits. The paper can be made stronger by considering some comments mentioned above.	2
Same as above	3
In my opinion, the paper makes a minor contribution by proposing a straightforward way to add structural locality to kNN-LM models. The contribution is possibly useful, but not major. For tested tasks, structural locality information improves results but not significantly. I believe the paper is marginally below acceptance threshold.	2
The paper is well-written with a clear and simple idea inherited from prior works. LTH does perform well on ASR and this is a very nice finding. Though the paper claims pruning RNN-based models is different from pruning CNN in CV, in the end the method is still quite similar to the original LTH, which makes the paper's novelty a bit thin. But the work does have a pretty thorough experimental section including all kinds of verification tasks, and proves the effectiveness of LTH on ASR models. The paper would be better if you can give more details on how the smallest weights (or block sparsity) are pruned for different models.	2
Although the paper shows significant performance achieving high model compression rates, the algorithmic novelty of this paper is not strong. The application is only ASR and it may not attract so many AI/ML researchers. 	2
This is a paper with limited novelty and some weaknesses as explained above, but it is still very interesting.	2
Given the above comments, I think this paper do have some strengths such as the paper is well-written and self-contained, and is the first to study on ASR. However, I have concerns on the soundness of the results since the WER is not close to SOTA and the novelty of the method is limited. Overall, I recommend weak reject of this paper.	2
Overall I think the paper is nice, the explanations are not hard to follow and it is overall well written. While the novelty of this model is not outstanding, the results section covers an interesting range of experiments with the right baselines. I would personally recommend adding more intuition as to why this extension improves the performance the way it does, as it is the main selling point. 	2
My opinion is that the paper is on borderline. The work seems neat and probable while the novelty is limited. Considering the quality of the venue, I'm bit negative on the direct utilization of existing work. Also, there is a disalignment (probably my misunderstanding) of the motivation and the experiment result, which I discussed in the weakness, want to be addressed. 	2
This paper proposes an efficient way of modeling functional uncertainty building on recent work of NeuBoots and BANP. On the benchmark of sequential decision making, authors demonstrated state-of-the art performance which shows great promise. Method is presented in a clear fashion and evaluation is done on standard set up. There are some desired analyzes pointed out in the main review that could improve scientific understanding of the proposed method. Overall, I believe the proposed idea is sound and interesting to be shared among the ICLR audience. 	3
The authors propose a new bootstrapping method in NP family called NeuBANP. The authors acknowledge the limitations of BANP and used the neural bootstrapping method to make development from BANP. The experimental results show great improvement over other NP methods. While the paper is written well and the figures are clear, there is some room for improvement. In particular, the combination of existing ideas may not hold up to the high standard of ICLR.	2
"This paper proposes a method to extract meta-features based on optimal transport to improve the performance of AutoML, which is a hot topic in the field of machine learning. The usefulness of the method is shown through benchmark data, but the theoretical argumentation is not sufficient.
"	2
"The topic and claims in this paper may be interesting to AutoML community. It contains novelty and was well written. 
Overall, this is a good paper, which is marginally above the acceptance threshold."	3
"Overall, I would like to recommend this paper for an accept for the following reasons:

- I think this is a well-written paper that positions itself well relative to existing work, and presents the novel scheme is a clear and easy-to-follow manner.
- The paper presents a significantly new meta-learning scheme building upon existing meta-learning techniques. Hence the novelty is significant in my opinion.
- The empirical evaluation is done against valid baselines, and the proposed scheme shows significant margins of improvement.
"	4
"While the results seem to be very promising, due to the issues raised above, I feel that this paper is not ready for publication yet. Therefore, I cautiosly recommend reject.

# Update after Rebuttal
Due to the strong responses by the authors as well as the updated version of the paper I recommend accepting the paper."	3
"Overall I like the presented idea very much and I deem the paper to be overall well written, except for some debatable statements (see Correctness). As such, both the methodology and the experiments are well described. Unfortunately, the part of the evaluation assessing the quality of the learned meta-features is insufficient (see Correctness) and needs to be revised in my opinion. Accordingly, I cannot recommend acceptance of this work in its current form.

-----
# Update after Rebuttal
All my concerns have been fixed or ruled out by the authors in the revised version. I recommend to accept the paper in its revised form."	3
This paper provides a great theoretical framework for analyzing domain transferability and explains why improving adversarial robustness can improve domain generalization. The empirical experiments are very interesting. However, the method of evaluating the adversarial robustness is not very appropriate. This is very important, as it may alter the conclusion of this paper, especially for the norm controlling experiments. If the author can obtain the same conclusion with a more advanced attack method (e.g., AutoAttack), I will raise my rating. 	3
This paper is overclaiming their contribution. Details are in the main review.	2
"The proposed analysis, in its current shape and form, might not be ready for publication at ICLR. While the proposed framework in this paper is interesting and definitely worthy of more analysis, the current results including all high-probability concentration bounds and etc. are still incremental and need more work.

The other problem is writing. There are grammatical errors throughout the paper and the quality of writings can be greatly improved.

Overall, I believe the paper is not ready for publication yet. My vote is weak reject."	3
Overall this paper aims to address an important issue: does robustness help generalization? However, in practice it mostly does experiments on regularization and data augmentation, which may not be directly related to robustness. Comparison with existing work is needed, and the presentation (especially some introduction of Theorems and Definitions) needs to be improved. Therefore I would not recommend acceptance.	3
The ideas and analysis of the paper to find the duality gap in training deep neural networks and when it achieves zero (at least for over-parametrized deep NNs) can be useful in understanding the optimization and training deep NNs and their properties. The paper adds non-trivial and solid results to the previous works in this domain, esp. works by Ergen and Pilanci. However, the writing and presentation of the paper falls short from the expectations.	3
The statements of main theorems are interesting and important, but the quality of the paper should be much improved. See the main review for detail.	3
I found the manuscript to be clearly written and technically sound. Although it has some weakness points, I think it still worth a publication. 	3
"The above concerns on mathematical clarity, technical novelty, technical correctness, and practical vadility lead me to form the opnion of reject. However, I am not an expert in this field of ""theory of training neural networks"", so I am looking forward to rebuttals from the authors, as well as comments from other reviewers and the area chair."	2
This paper provides some promising theoretical results for the variants of stochastic gradient algorithms without replacement.	4
I think this is a good paper with solid theoretical analysis and contribution, the analyzed problem is of great importance and needs more thorough understanding like this work.	3
"This paper introduces a deep and wide theoretical study of permutation-based variants of local SGD and minibatch SGD. Authors provide upper bounds, as well as lower bounds, which makes the contribution solid. The novelty and tightness of results are significant, so the paper should be accepted. My grade is ""accept, good paper"". "	3
This paper presents a strong contribution. My main concern is lack of test set results, however I consider this to be a significant issue as the test set is the standard for reporting on this task/dataset. 	4
As mentioned above, I feel that the paper has sufficient novel contributions to warrant an acceptance. I encourage the others to include a discussion on comparisons with SoTA from the leaderboard. Also, there are some minor issues that I hope get resolved as part of the discussions. But overall, keeping in mind the insights, novelty and usefulness of the ideas presented, my vote is to accept.	4
"The high-level ideas the paper is based seem natural and interesting. However, fairly similar ideas have been proposed in prior work which is cited by the paper. It seems to me that the distinctions are mostly in the details a lot of which emerge from the slight differences in the problem setup. 

Post-discussion: The authors have addressed the main concerns about distinctions from prior work. Therefore, I am improving my rating. "	3
"Overall, Crafter – while a straightforward simplification of another existing environment (Mine RL) – could become a useful benchmark task in the field. While it is not fully novel, it positions itself uniquely relative to other benchmarks across various axes relevant for research (pixel vs state based, memory vs not-memory intensive, 2D vs 3D complexity, small vs large required compute). The low amount of compute required for training (while preserving environment and potential behavior complexity) is worth emphasizing, as it could speed up the rate of progress of further research in this space.

It would however be nice to see addressed the limitations mentioned above.



"	3
"The paper is well written and presents a new Minecraft inspired environment for RL research which has some novel features. However, the paper needs to do a better job explaining the research contribution.
Similarly any discussion of limitations or future work is lacking."	2
It's hard to objectively referee this type of a paper because as with many new environments, beauty is in the eye of the beholder. Overall, I'm not particularly excited by another 2D gridworld-style environment - but I will admit that Crafter has several interesting aspects that aren't featured in other environments. Similarly, I'm moderately interested by the emergent behaviors demonstrated by the Dreamer-V2 agent, but not blown away (unlike emergent behaviors in OpenAI Hide and Seek for example). So overall I'm lukewarm on Crafter - I could see it possibly appealing to a subset of the RL community and perhaps helping to drive research in particular subfields of RL, but this is just my guess. Should it be accepted as a conference paper? Perhaps, but it may be better promoted through other means like NeurIPS competitions, blog posts, and similar.	3
"The benchmark developed will be valuable to those seeking to test agents in Minecraft-like environments but without sufficient computational resources to test at scale in MineRL. However, this use case is somewhat narrow, and additionally the choice of evaluation metric (aggregate score) is not well justified (especially in situations where it may diverge from the reward). I would assess the current submission as being borderline, but I lean slightly towards reject. I would consider increasing my score if the evaluation criteria were more clearly justified.

Update: I have increased my score to marginal accept after the author's explanation of the choice of GM, and their inclusion of returns in the main paper."	2
"My current recommendation is borderline leaning to accept. I may change my score in the rebuttal if my concerns are not addressed well.

--------
The authors addressed most of my concerns and hence I raise my score to 8. 
"	3
Overall I think the paper is interesting, as it proposes a novel way for one-shot GAN adaptation. However I do have some concerns about insufficient experimental validation and clarity in details.	3
The results in the paper are a little amazing and can be used in style transfer on human faces.	2
See above.	3
This paper provides novel tools and insights in revealing the convexity structure of non-convex loss landscape of two-layer neural networks. The ideas developed here can be used to explain the global convergence of gradient descent algorithm in training neural networks. I recommend accepting the paper. 	3
"The paper contains some typos and grammar errors. 

in (1), the second summand should be m terms instead of d terms?

I do not understand why the positive homogeneity of the ReLU activation function directly leads to σ(Xui)αi = σ(Xwi), as \alpha_i's is not necessarily positive in general?

The paragraphs regarding the partions of D_i's are not easy to understand, which shall be improved. 


----------------------------------------------------
After rebuttal: I have read the other reviews and authors' replies. My minor issues were addressed. I will keep my score."	3
"Clear accept.  It significantly clarifies the relationship of the solutions of the nonconvex ReLU neural network training problem with a corresponding convex program.  That relationship allows them characterize all global minima and to test whether particular networks are indeed global minima.  While the work only applies in the 2-layer case, it is nonetheless a significant theoretical extension that can inspire extensions to multi-layer cases.
"	4
This paper utilizes “supermask” technique in FL to improve communication efficiency and robustness simultaneously and provides thorough empirical studies. However, it seems simply combines “supermask” and FL without solving any challenges nor providing theoretical analysis about why this can work. In addition, the experimental settings are not hard enough to judge the usability of the algorithm proposed by this paper. Based on the experimental settings and the lack of scientific contributions, I would give a negative score. If the authors can solve these two major concerns, I am delighted to increase my score.	2
Overall, the well designed structure makes the workﬂow clear and easy to follow, but the further analysis and discussion are expected to clarify the contributions in the techniques as well as in the evaluation section.	2
In general this paper is interesting; but before the acceptance of the paper, the authors need to provide additional numerical comparison over other strong baselines in the rebuttal (as pointed out in the main review section).	3
In Summary, the authors apply an existing algorithm to the FL setup, including a novel voting-algorithm that applies to this algorithm. The execution of the paper has some flaws that I would like to see corrected before the paper is ready for publication.	2
Overall, the authors study an practical problem at scale. They establish a large scale benchmark and propose a simple yet efficient approach for OOD detection. But they need to study more SOTA baselines. I would like to give an initial positive score and keep tuning during discussion period.	3
"Creating datasets requires a lot of hard work. Yet, the weaknesses outweigh advantages. Hence, I must conclude that this manuscript is below ICLR acceptance threshold.
"	2
Overall, the paper presents some good empirical results of large-scale OOD detection in three settings, but the work ignores a number of closely related studies and it is weak in terms of both technical novelty and empirical evaluation, thus not good enough for publication at ICLR.	1
"Overall the paper proposes a metric and a large scale setup of out-of-distribution detection. Both contributions are valuable and can be considered novel too. There are few a points to be addressed in the paper structure, as discussed above. The major issue is to add more related approaches in the comparisons. Moreover, the small-scale evaluation would be useful to show the generalization of the proposed metric to different types of setups. Even if MaxLogit does not deliver state-of-the-art results on a small scale, it would still be important to show the results and interpret the outcome.


Post-rebuttal: 

The rebuttal showed that there are still changes to be performed in the paper. It would need further work before acceptance."	3
This paper is well motivated and solid, but the writing needs to be improved.	2
"The paper is well written and contributes some new thoughts and approaches for
the evaluation of GGMs in an unbiased manner.  I solely have minor
considerations with regard to the types of graph perturbations selected and the
selection of the MMD hyperparameters. Further, I question whether the
comparison classical evaluation approaches to the GNN based approaches is
completely fair."	3
While the presentation of a new method/framework is important for the GGM, the paper lacks theory, and the new metric is impossible to interpret. Moreover, the three critical points mentioned in the paper have already been solved by the KS-multidimensional distance. 	2
"I recommend rejection for now, for the motivations above. I will however await for the authors response and will update my claim accordingly.

EDIT 1
Raised my score to 5 after the first round of review and a first discussion with the authors.

EDIT 2
Raised my score to 6 considering the improved quality of the paper and the effort shown by the authors to fix what was wrong in the first version of the paper."	2
I think the paper has evaluated different metrics extensively and they have provided abundant information. However, I feel a bit lost when I read this paper due to distractions from the details. I think authors should make their conclusions more clear before providing details. Besides, a few more concerns about this paper have been mentioned above.	1
Due to the reasons I stated in the main review, I am currently in the borderline accept; however effectively rebutting my comments will make me raise my voting towards higher score.	3
The contributions of the paper are not clear. The paper is also missing important related works and comparisons with recent approaches in this domain. The paper has only marginal novelty both in terms of the data representation and the proposed approach.  	2
In general, the paper suffers from clarity as well as comparison with existing baseline other than transformer-based approaches and feed-forward networks. Assessing the performance of the model on different subpopulations based on demographics would be helpful to identify how the method performs subgroups rather than on average since it is known that there are differences in lab-orderings based on demographic attributes such as gender and self-reported race. This could strengthen the contributions.	2
This paper addresses an important problem in machine learning that is of great interest to many researchers and practitioners. The authors propose a method called selective ensembles and evaluate its effectiveness from both theoretical and empirical perspectives to solve this problem. Experiments on benchmark data show that the proposed method seems to work well for the problem. In conclusion, we believe that this paper is worthy of acceptance.	3
It's unclear how the proposed method is beneficial in classification problems. The authors should provide the practical meaning of their method and compare their method with some proper baselines if necessary.	3
While an interesting method the lack of clarity in the empirical experiments reduces the impact. Furthermore, the method reduces disagreement by essentially avoiding the problem through abstention; it's not really demonstrated that this has a practical advantage other than through highlighting specific cases for manual intervention or inspection.	3
The paper presents a highly completed study on the improvements of model stability and feature attribution thanks to the contribution from selective ensembles with abstention option. The paper could use a bit more theoretical discussions regarding the inner workings of the proposed method as well as the relationship between the work and existing ensemble studies to highlight its novelty. 	3
"- Pro: The method is sound.
- Con: The method is similar to the existing gradient matching method, but the graph setting is new.
- Con: Experiments can be improved:
-- Ablation study is needed to verify the effectiveness of the proposed parameterization.
-- Experiments on deeper GNNs can better demonstrate the method.

My score is actually between weak rejection and weak acceptance. I will consider increasing my score to weak acceptance if experiments can be improved.

---post rebuttal

The new experimental results in the revised paper have addressed my concerns so I choose to raise the score to 6.
"	2
Overall, I think graph condensation is a new and interesting direction, and the empirical results presented in the submission are generally encouraging. On the other hand, given previous work on data condensation, the novelty of the current method is not significant. 	3
The paper's idea is interesting, as evidenced by the experimental results. However, the theoretical depth and novelty may not be enough to meet ICLR's standards. 	2
The paper proposes and addresses the problem of graph condensation. While the formulation of the problem is interesting, I didn’t find the approach proposed by the authors to be mature enough for ICLR due to the limitations and issues highlighted in the main review. I thus consider the paper a weak reject for the conference and I kindly ask the authors to reply to the points highlighted in my review in their rebuttal.	2
This is a promising method to the well studied problem of identifying the weighting of the datapoints in a learning task that aid generalization on a given task. The current method while promising is missing some vital comparisons which limit the ability of a reader to assess the true impact of the work, along with experiments which make a lot of the methods sections and details in it seem like overly complicated detail. Given this I do not think the current paper is ready for publication. Important points for the rebuttal are marked with a (*).	2
I think the paper is in good enough shape to warrant publication with only minor changes.  I believe the focus of this paper would be of sheer interest to a broad audience.	4
This paper is a valuable and interesting contribution. It raises awareness for the importance of going beyond a single validation split for evaluation of neural networks and presents an interesting closed form solution for the leave-one-out (LOO) error that is, to the best of my knowledge, novel. The similarities of the proposed approach with efficient LOO cross-validation approaches for kernel machines suggests that there could be synergies that would allow to improve the optimization procedure of DIVA. 	3
"Although the motivation of this work is interesting and the paper has some technical novelty, experiments can be improved.
Thus, I'm slightly leaning towards the reject side at this moment.

---After rebuttal---
Thanks for the response.
I think there is room for improvement in the experiment (comparison with existing methods for each task), 
but considering the promise of the proposed method in a number of tasks and other reviews, I decide to increase my score to 6."	3
Overall, the paper is an empirical work with little justification for the proposed method. The empirical studies have limited settings. It is not clear when it works and when it fails.	1
The concept of creating alternate triggers that can still fool the model is an interesting idea and can enable researchers to defend against such attacks in a generic sense rather than focusing on a particular trigger. However, it would be more beneficial if the human-in-the-loop system can be relaxed or shown to be absolutely necessary to create such triggers. Authors are also encouraged to discuss possible defenses to reduce the effect of such poisons. An experiment that shows that such an attack can be performed without complete access to the classifier would also be interesting. 	3
The paper proposed a more efficient backdoor attack method without access to the training dataset and original trigger. However, this method highly depends on manual interaction. By answering questions in the above section, the authors can justify the necessity of human interaction. This justification is the important factor for changing the overall score. Also, please consider propositions and miscellaneous to clearly deliver the main idea.	2
"The technical contribution of the method is interesting and the experiments are thorough and support the majority of the claims made in this paper. The threat model is curious at best and should be addressed -- stronger motivation that this attack would concern anyone would be good.

I am initially giving this paper a borderline score. I think there are real strengths and some weaknesses that need to be addressed. I look forward to the author response."	4
As far as I can tell, this work makes an interesting observation, but it does not provide a very compelling explanation, and it is not clear to me how this observation or the related method provide practical utility.	3
"The authors present a wide range of results on efficient algorithms for zero-sum markov games.
These results leverage several previous works on value-function elimination using the Bellman factorization and Bellman Eluder Dimension.
The contributions seem to be significant. The correctness seems good but it is possible i did not understand some parts of the submission and unfamiliar with some piece of relate work. The technical novelty seem high, providing new theoretical insights into minimax markov games."	3
"While I listed several critical comments to this work, on the whole I liked the work because it still makes a non-trivial contribution to the theory of Markov games.
"	2
I think this paper is blow the bar of acceptance.	2
"The paper comes with a very detailed appendix containing all the necessary proofs. This is a big plus of the paper. I am not quite sure, though, how novel the presented results are. At the same time I am admitting that I am not an expert on MDPs or RL. From an algorithmic perspective I have some doubts about the computational complexity of the algorithms presented here, but it seems this is a general problem in this literature, so again I would not put much weight on my critique here. 

What I find quite confusing is the use of classical concepts, like the Shapley operator (the analogue of the Bellman operator in stochastic games), is defined in a way that I have not seen before. In the operator approach to stochastic games, it is used to define an extension of dynamic programming to competitive settings. A key property is that the Shapley operator is a contraction, and this drives a value iteration kind of algorithm. I am not sure how these classical results translate in the present formulation, as the operator is different and it is not clear whether contraction properties can be used. The question is then to me how one can relate regret and equilibrium. Note that we know that regret minimization and Nash equilibrium are not the same, and even in zero sum games usually regret minimization implies something about the ergodic average, but not the last iterate. This confuses me quite a bit in this paper (and the cited references, to be fair).

"	2
This paper is technically very strong and well-supported by experiments. I believe it is among top 20% of accepted ICLR papers. However, the motivation of the problem is less clear which requires non-trivial improvement.	3
After my first read of the paper, I am recommending a weak accept. My concerns are more about the assumptions and the lack of discussions about them. I would appreciate it if the authors can comment on this matter.	3
The problem of multiple descent is an important scientific question which is relevant to the ICLR community. The contributions of the current paper are novel. The techniques used appear novel as well. However the degree of significance of the results is not fully clear. The paper would benefit from a deeper discussion around this.	2
Due to the significant overlap with prior results, I cannot recommend acceptance. I'm happy to adjust my score if the authors can clarify how the current submission differs from (and improves upon) all the aforementioned papers. 	1
"Overall the paper is theoretically sound experiments are sufficient in my opinion and shows that the proposed method works.
I would love the authors to enhance the accessibility/readability of the manuscript by unifying mathematical notation, defining the rationale behind using $E_{local}$ and $E_{norm}$ rather than long paragraphs referring to previous work instead of motivation. It would be great to simplify the connections between them in layman terms (sum of scores and sum of softmax outputs).  "	4
"Overall, I think it is a good paper. I believe my concerns are more about presentation instead of the actual methods. 
"	3
This paper interprets the masked language model with a novel perspective, and introduces  energy networks and Metropolis-Hasting sampling to text generation based on MLM models. It proves the effectiveness of MLM models on text generation, a good paper to accept.	3
see above.	2
This paper explored a variety of approaches for iterated batch reinforcement learning. There is no obvious contribution other than the demonstration that experimentally both planning and exploration are important to improving the performance of Acrobot policies. 	1
"I am on the fence regarding the significance of the paper results. On one hand, it adds new ideas and establishes an interesting line of research. Eventually has a significant potential of being useful in the research on MBRL. 

However, the presentation of the paper is not mature enough. I think the paper has a few significant drawbacks including: lacks a proper mathematical formalization of the studied problem, and validation within a varied set of benchmarks and different scenarios. I explained my concerns in three paragraphs above. Therefore, I am leaning towards its rejection. An improved version of the paper may have a major impact.

More adequate score is 4.5"	3
The new algorithm is interesting and seems to make some improvements over some of the existing approaches, but the gains seem marginal, some variants re not shown in the main results, and the reported statistics could use improvement.  there is also no theoretical justification or proofs provided for these changes,	2
This paper is different  and topical, with still has a contribution to the research community and hopefully will provoke discussions	3
The paper can be regarded as a particular application for synthesizing flooded images with street-level cityscape images, however, since this work is not well motivated, yet the contribution and comparison are not strong enough, overall, I vote for rejecting.	2
I believe that the purpose of the proposed model is topical and of interest. However, from a technical viewpoint, I believe that the contribution is not novel enough for acceptance at ICLR as ClimateGAN is given by combining two already existing models (DADA and GauGAN) with minor modifications.	2
This paper is a solid extension of the Split Federated Learning work, which improves transmission efficiency with an acceptable performance drop. The approaches are well motivated and theoretically verified, however the contribution scope of this work is kind of limited and the empirical study part can be further enriched by discussing and comparing with more related work.	2
Frankly, I think the paper was rushed out. The idea is sweet but clearly requires more effort to show its effectiveness. The current draft only comes with very preliminary experimental results without comprehensive comparisons against various baseline methods. The motivation of the proposed vector quantization method is also not clear. Overall, I do not think the current draft deserves to appear for a presentation at ICLR 2022.	2
This is a promising paper and discusses a relevant problem. Some detailed questions remain and some changes I believe are necessary. If the authors address my concerns I will improve my rating of the paper. 	3
Questions were raised during the review process. 	3
This is a solid work that will be certainly be helpful to the community, and I recommend it for publication.	4
"Correctness:
Most of the theoretical results are well supported by the proofs in the supplement material. It is unclear if the pros and cons of the methods summarized in this paper can be generalized to more sparse linear regression setups. 

Technical Novelty And Significance:
Sparse linear regression is a well-studied area. Although the authors have done a good job to deliver their theoretical results, I feel the technical novelty is limited. This paper focused on some specific settings. I also hold a conservative opinion towards its significance.

Empirical Novelty And Significance:
All the methods compared in the simulation study have mature algorithms to implement. The simulation is done under a low-dimensional setting (n=1000, p=300). The data generating processes are quite simple. I feel the novelty and significance of empirical studies are limited.

Flag For Ethics Review:
I do not foresee this research having any ethical issues.
"	2
"This paper is well-written.
Theoretical analyses for the case of n<p are needed in the paper. "	3
"Given the complexity of calculations as mentioned in the _technical novelty_ section, it isn't clear that more realistic settings can be addressed, i.e., larger blocks, more complicated correlation models, etc., with analytic phase curves. 71 pages of proofs suggest that small increases in model complexity will quickly result in prohibitively complex calculations. Although I believe the analysis of this idealized model is of sufficient interest to researchers in variable selection methods, it would be nice to know that the results can reasonably be used by a wider audience.

I would like to know that the simplified block correlated model is in fact a reasonable approximation for a number of real examples or that phase diagrams for more realistic models are within striking distance. For now, I'd say the paper is marginally below the acceptance threshold and ask for the authors to further motivate the simplified model.


====== REVIEW UPDATE ========
Based on the authors comments, I am improving my recommendation from a 5 to a 6 provided the authors incorporate their response into the paper (I still haven't seen a revision). "	3
"Overall, I think this is a nice paper with strong theoretical results. It provides insights to the theoretical study of standard sampling algorithms in federated learning. The paper is also well-written.
"	3
The paper lacks theoretical novelty but I liked the idea of introducing LD in federated setting. I may change my decision after discussion with other reviewers.	2
The paper has given extensive theoretical convergence analysis for FA-LD algorithm via 2-Wasserstein. However, empirical studies to verify theoretical findings are missing in the main paper. 	2
"The delcaimed contribution include FA-LD and its theoretical guarantees.
However, the analysis technique is commonly used in litureature, while no empirical expeirments to illustrate the effectiveness of FA-LD.
Besides, there are some presentation issues and ambiguity on motivation.
I don't think this paper is ready for publication. 
"	2
"
This work is mainly of theoretical interest, as it considers randomized perturbations in a federated averaging setting. However, its technical scope precludes the most useful instances of it in practice, and it is missing experiments. Therefore, it seems of limited usefulness in my view."	2
Novelty is slightly weak but empirical performance is strong and the method might be practically useful	3
"I am giving a borderline reject for now due to the novelty concerns in light of a very similar paper in my opinion. I know that there are a lot of devils in the detail between the two works, and it would have been an obvious accept if this works outperforms SceneTransformer significantly.

In the rebuttal, may I ask the authors to further highlight the difference between their approach to SceneTransformer? And where do they see that their method is intuitively better than SceneTransformer?

POST REBUTTAL: The authors pointed out that SceneTransformer should be considered concurrent work. If that is the case, I have no issues to accept this paper."	2
The proposed architecture is tailored for the task of multi-agent trajectory prediction task and presented to be effective in the benchmarks. However, we do not know if it is due to the superior performance of the underlying computation units (i.e., transformer blocks) or the proposed extensions (i.e., discrete latent variable/learnable seed, set assumption). I also think that the presentation could be improved. 	2
This paper compiles an interesting model for multi-agent motion forecasting. Although the transformer module is similar to Axial Transformer [Ho 2019] and the latent variable formulation and optimization are similar to MFP [Tang 2019], it is the first to successfully apply those two module together for motion forecasting task, with clean and concise code released. Thus I would vote for weak accept.	3
"* Seems like a solid contribution to steerable network design.

* Paper is well written.

* I would expected to see the method tested on some equivariant task as well.

* The applicability of the method to 3D classification is not clear as it is compared to other alternatives.
"	3
"The paper presents interesting theoretical ideas that may give a practical solution to the problem of designing equivariant steerable kernels to a large number of groups and arbitrary signal domains. The experiments are done reasonably well and provide some evidence for the utility of the method. The authors should try to improve the motivation of the paper and if possible, present more possible application areas where they see these ideas being most useful. Overall, I am inclining towards acceptance of the paper.

UPDATE AFTER AUTHOR RESPONSE:

I thank the authors for a clear response to my questions as well as the explanations. Overall, I am not convinced about the motivation of this paper and its practical significance. If, as the authors say, only some of the subgroups are of O(3) are ""occasionally practically relevant"", then the ideas at the moment are not very practically impactful. They may be in the future, but I still don't see why designing equivariance for the subgroups of interest for an application directly is really more difficult. I am sticking to my original rating, but the paper does have theoretically interesting ideas which may be useful to the field. "	3
This paper presents an elegant and general technique for constructing equivariant kernels for subgroups of E(3). The paper is generally well-written and the authors intend on releasing code along with the paper to help researchers build their own kernels. However, the current exposition is quite difficult to approach and it seems like one would really need to read the long (40 page) appendix to get the most out of the bulk of the paper. While the authors present a wide range of experiments, they don't really provide compelling evidence that this general construction of exotic subgroups of E(3) will lead to tangible benefits. Overall, I support this paper since it seems correct and there might be researchers who will benefit from being able to construct these CNNs, but I do think the paper has issues that limit its impact as written.	3
To summarize, the contribution of this paper is solid. However, an evaluation of the practical performance compared to recent works and a better organization of the paper structure (especially the length and the complexity) would benefit the paper.	3
The paper is good enough and it is supported by experimental results.	3
"1) Their algorithms need $O(\epsilon^4)$ gradient queries. It would be great if the authors can clarify that is this tight bound or can we achieve better complexity and if we can what would be a proper approach.

2)The setting is novel and interesting from a practical and theoretical point of view.

3)Author motivates the subject from a theoretical and practical point of view. 

4)In the introduction, the author reviewed the previous works carefully.

5)The authors successfully provide simulation results for their algorithm.

6)The paper is well-organized and well-written.

7)the code is included.
"	3
Overall, I like this paper. I hope the authors can deal with the above comments. 	3
While there are some typos and restriction, I strongly appreciate the model design which consists the backbone of the paper. Shortly describing, I like the main message of the paper even if there are main details that should be clarified and corrected to express the exact result that can be derived from the paper.	3
It would be a successful work for the ICLR blog track but not the ICLR main conference. Many ideas are clearly of merit to inspire further research but not formal and supported enough as scientific work. I sincerely recommend submitting this work to the ICLR blog track this year, if the authors can sufficiently revise the writing style to suit a blog.	3
The paper explains the relation between self-attention and several previous algorithms and its implications, offering different perspectives on the self-attention module. However, the contribution is lacking in both justifying if self-attention and prior arts are functionally similar or just similar in the form, and exploring how the relations explained in the paper can help us better design self-attention or apply it in other areas.	3
I recommend this score since this paper is to consider an interesting problem, some suggestions are hard to implement in deep models on large-scale datasets (millions), and there are no experimental results to support the arguments.	2
I like the idea to simulate self-selection bias with POMDP. However, I have questions in the discussions of the comparative studies (ADS effects as the authors explained versus simple failures of convergence). I also question the proposal to completely block the gradients to discourage taking advantage of any changes in the environmental states. I gave counter examples in RL mountain car problem, goal-oriented online shopping, and even news recommendation when opinion biases do not stop it from providing useful information. The paper's proposal seems oversimplified and it needs to be validated in real-world scenarios to produce sufficient impacts for this venue.	2
I believe this paper is an empirical, meta-learning extension to Performative Prediction (2020). I believe the main findings of the paper are not attributed to meta-learning, but to the idea of repeated retraining for online learning, which is well explored in Performative Prediction (2020). In addition, this paper does not cite Performative Prediction (2020), and the writing (especially the use of notations and terminologies) needs work. Therefore, I incline to recommend rejection. 	2
I think the paper has some interesting ideas, but I think it needs to be improved before it is above the bar for acceptance at ICLR. 	2
"I think the incentives perspective offered by this paper on model-induced distribution shifts is interesting. Designing simple tests enables the authors to provide some interesting insights about the relative performance of algorithms. However I am not sure what the algorithm designer can learn from the results of these unit tests. My second concern is the treatment of related work. I don’t think you should claim the definition of this phenomena as a novel contribution. 

I am willing to increase my score if the authors provide a satisfactory solution of how to address the conceptual overlap with existing work and they can convince me about the usefulness of these unit tests to guide algorithm design."	3
"This paper proposes a general but tractable class of MDPs with exogenous distractors, and proposes a novel algorithm which provably obtains representations that exclude the spurious latent dimensions of the state. Given the technical significance of the work, I am in favour of acceptance.

-----

**Update:** the author response resolves the issue with visual gridworld experiments. I'm still in favour of acceptance."	3
While I appreciate the importance of the problem setting and think I understand the general thrust of the paper here, I overall left the paper with a lot of confusion. I'm hoping the authors can provide clarity here--both in their response and by revising the manuscript--during the discussion phase. Indeed, after some discussion and edits to the paper, I feel that the paper is much more clear and recommend acceptance.	2
In my opinion, the contribution of the paper is strong since it analyzes an important problem setting, presents an analysis of representations learned using inverse dynamics, and shows that it works in practice. The weaknesses, specifically little discussion of prior work and problems with the experiment section seem fixable, and under the condition that these points improve, I will recommend acceptance.	3
A well-argued paper that sets up an important problem and introduces a novel algorithm (PPE) to solve it. This is somewhat undercut by PPE only working in a restrictive setting. Empirical results would benefit from an additional baseline (bisimulation) and (optionally) a previously published benchmark. Some discussion of mutual information approaches should be made. But some reasonable attempt is made towards these improvements, I'd happily see this work accepted.	3
"I like the idea of prompting LLMs to decompose high-level instructions and extract actionable knowledge, which is a novel perspective for embodied agents. However, the execution of the idea and the experiments are not right there to support the claim. It would be great if the authors can show its effectiveness on ""actual"" embodied agents in different environments instead of text games. "	2
This paper shows promising results on extracting executable actions using pretrained language model. However, there are major concerns about how this relates and contributes to planning and grounding.	2
In addition to their impressive performance on several standard NLP tasks, large language models have been found useful for a wide variety of tasks when prompted appropriately. I believe understanding, via investigative work, what is the extent of the knowledge contained in those models to be important as it can better guide the research community towards what to explore next. The proposed paper is exactly about doing such an investigation. I found the paper well-motivated, and it includes the experiments to support the authors' analysis and discussion. For those reasons, I recommend this paper be accepted at ICLR.	3
While the authors explore an interesting approach to planning based on language models, the paper is weak in terms of technical novelty and experiments. 	2
"Overall, my opinions on this paper are fairly positive, because it tackles an interesting new direction with new simple methodology for generating actions from LMs. However, there are some issues with evaluation, that could greatly improve the paper if addressed (or at the least, discussed a bit more in the limitations section). 

--

Update post review: I am lowering my score from 8 -> 6. I still think this is a good paper and it should be accepted, but I am not very convinced by the evaluation. I think the evaluation issues I (and KhzN) have issues with could be improved between acceptance and publication though."	4
"The current version of the manuscript is not ready for publication. 
- The problem understudy is not well motivated.
- Incomplete literature review.
- Multiple incorrect or inaccurate statements.
- Several unclear statements and/or unsupported claims.
- and, further items discussed above."	2
The paper does not reference a large body of existing work on the Kalman fitler / LQG duality which the paper claims is their contribtion. The EnKF is also not well motivated for this setting and the experiments are very limited and seemingly not reinforcement learning tasks. 	2
Based on my above comments, I think the paper is interesting but not good enough to be accepted at this time.	1
Interesting idea but limited impact in real-world scenarios.	2
"Paper proposed novel attack, however have several serious drawbacks:
* assumptions about attacker knowledge might be too strong and need further discussion
* evaluation of the black box attacks on adversarially trained model does not seem to support one of the claims of the paper.
* overall paper is written as a collection of somewhat unrelated stories."	2
I find a few issues in this paper that push me towards rejecting this paper. Please see comments above for details.	2
While the new attack method seems interesting, it could be a hasty conclusion that the adversarially trained model is “more vulnerable” to black-box attacks. I suggest either making a weaker conclusion or strengthening the theoretical & experimental supports to the conclusion. Because of this concern, I’m a bit cautious to say that we should accept this paper.	3
"The paper makes strong claims with important and interesting implications for the field of machine learning and develops a novel attack mechanism defeating a proposed defense. However, the claims are not backed up properly with a major, obvious metric missing entirely from the evaluation. If the claims are backed up according to the success rate of the attacks – instead of the magnitude of the perturbation – and if evaluation is carried out on a more diverse set of models and datasets, this can be a very strong paper. 
"	3
"- please restate the contribution of the proposed method.

- should need more experiments. like in transfer learning. And need more analysis, like what is the performance of the best-performance group of labels."	2
"Overall, this paper proposed an interesting and important perspective on the fairness of distillation, which will inspire future works to pay more attention to the fairness of distillation. The writing quality is good. The contributions to empirical analysis are acknowledged by my side.

The weakness comes from the technical contributions. In particular, it seems that the proposed AdaMargin does not work well in practice. This limits the contributions of the methods part. The contribution of AdaAlpha is clear.

Considering both the strengths and weaknesses, I vote for acceptance at this stage. My initial score is between 6 and 8."	4
The paper has some interesting aspects and does a good deep-dive into some very specific aspect of knowledge distillation, but the focus is very narrow and the improvements obtained do not look impressive enough. The score would be around 4 if that existed.	2
Overall, I enjoyed reading this paper, which brings out an important issue in distillation. However, I would feel more confident in the reported results if a more detailed statistical analysis was provided, as well as some comparisons with more recent distillation approaches, which leaves me with many open questions. Furthermore, there is no discussion on why distillation brings out this behavior from a theoretical standpoint.	3
The experiments in the paper are on toy examples and the approach proposed has several limitations for it to translate to other tasks, for example, the complexity of the environment and the task specification. But the techniques proposed are simple and can be extended to more complex cases with further research.	3
Overall, I am not entirely convinced that the approach of simply grounding text descriptions onto image observations can serve as a reliable proxy for rewards. In addition to that, the paper, in its current format, has a lot of missing details. Therefore, my vote is to reject this paper	2
The idea of the paper is interesting. However, the final implementation doesn’t really need the pretrained vision-language model.	2
Overall the paper presents an interesting and creative solution to the important problem 0-shot language specification for robot manipulation from images. However, the proposed method seems to be task/environment/viewpoint specific, and the paper should further discuss these limitations. The paper can also improve its experimental domains, baseline comparisons, and coverage of related work. 	3
Overall, I believe that this paper could be a meaningful add to the theories behind the distributed optimization. Numerical results also look solid enough to support the theoretical results.	3
Overall, the proposed work is important and interesting, and the paper deserves publication. The compressed gradient averaging technique with error feedback and its related convergence discussions could shed lights on other related works. 	3
I believe the paper has its value in extending the adaptive optimization framework to distributed approach with compressed gradient averaging. I mainly have concerns about the assumption on convexity and the impact on the performance brought by data heterogeneity.	3
"An interesting idea but a difficult paper to read and understand.
"	3
I give marginally above as the experimental setup seems rigorous. The authors are quite transparent about the lack of theoretical novelty when they state they are not proposing a new equivariant neural network design and instead, they build upon previous work to demonstrate working in new domains with unspecified group actions. I will revisit the score depending on other reviews. I can not comment on the utility of the idea only in MDP structured space but in general, the problem is relevent.	1
see above	2
A lot is packed in to the paper, making the message a bit confusing, and the main contribution not so easy to tease out. Further, it isn't clear to me what the theoretical guarantees are and it seems that a fair amount has to be known to apply the methods and to effect the equivariant transitions. To me it seems that simpler solutions do exist, at least for some cases, but perhaps I am missing something here. I also found the symmetric embedding steps in the Meta-Architecture to be somewhat engineered. The authors are indeed grappling with some interesting and very relevant questions, but separating the theory from what is engineered is not so easy (for this reviewer at least).	2
"The concluding statements of this submission are that ""the AP patterns could be the closest approximation to a network’s perception and this study’s results confirm this"" and that the proposed tools ""validate[] the assumption that we can reliably take these patterns as the learned content by a trained network"", but the existence of these data points is not sufficient to claim that they universally represent the learned content of a trained network. Adversarial example literature has shown that adversarial examples are so ubiquituous that they can be found both with and without semantic features, in almost any desired shape and constraint."	2
In general, I believe that the work isn't yet mature to be accepted as a conference paper, and I would encourage the author to submit it to a suitable workshop. That said, I do believe the questions considered in the paper hold a lot of promise. I would also encourage the author to support the claims by a strong set of experiments. The author currently experiments on MNIST & Fashion MNIST, which isn't sufficient to support (or validate) the hypothesis proposed in the paper.	2
"The results of the paper are not original enough to warrant publication. It also misunderstands what are adversarial examples. Hence, I strongly recommend rejection. I also did not see enough evidence that the Author is familiar with existing literature about the topic - the reference section is quite short, which is usually not a good sign.

The paper reports an interesting result: small ANN models trained on simple image datasets can be shown to simply memorise the average element for each class. This is suggests that perhaps we should stop using MNIST and F-MNIST datasets to analyse how neural networks learn."	2
The authors observe some interesting and trackable patterns in adversarial examples. However, I believe the current version still has a lot of room for improvement.	2
"The motivation of the article is good, but I have some problems in terms of contribution and theoretical analysis. Experiment and analysis are okay.
"	2
This paper studies an important open problem,  but is lack of rigorous mathematical demonstrations and has limited empirical studies.	2
Good research problem, but the presentation is not good enough to make the key idea clear.	2
The paper works on an interesting and valuable problem. However, the analysis results are not strong enough to support the claimed contributions.	2
"This paper lacks proper justification for the design choices of the presented method and is thus not ready for acceptance. 
"	2
"The paper makes interesting contributions, but the clarity could be improved. At this point, my score is a reflection of my uncertainty about the above issues. 
"	3
The paper presents an interesting idea and has some promising results. However, there are a number of clarity issues (including lack of details on an important design), claims that seem unsupported, and limitations in the experiment (in particular, not comparing to certain baselines and evaluating on tasks that may not be challenging enough) that make me currently believe that the paper is not ready.	3
The paper proposes an original way to frame reward shaping. However, the presentation is lacking clarity and many of the choices/details are not well motivated. Despite my negative review I think there is potential in the approach, but I believe that a major rewriting is necessary as well as more qualitative empirical analysis. The quantitive empirical results are not especially favourable either.	3
Primarily due to my concerns above, I am initially recommending a weak accept of this paper. I am happy to engage in discussion with the authors and other reviewers in order to reach a more confident final recommendation.	3
The paper is strong empirically but lacks theoretical rigor.	2
"The paper is well-written and I enjoyed reading it. I cannot 100% verify the novelty of the paper since I am not so familiar with the related work but given what the authors wrote in their related work section and their results it seems novel. 

Overall, I didn't have any major complaints about the paper. "	3
The paper offers some good theoretical insight into the general problem and introduces a simple method that clearly outperforms the baselines. The problem statement is interesting and very relevant to the community. The evaluation is extensive and clearly demonstrates the strengths of this paper. While the theory does not cover all aspects (eg temperature rescaling), I liked the paper overall. 	3
This paper proposes a novel method, Average Thresholded Confidence (ATC) to learn a threshold on the model's confidence. It is well-written and well-motivated. The proposed idea is simple and technically sound. The claims are well-supported by theoretical analyses and extensive experimental results.	3
"I appreciate the authors' work from an interesting point of view. However, I think the statement of ""ViTs are insensitive to patch transformations"" does not hold, and the contribution of the contrastive loss is not clear. These two points make the current paper not reliable."	2
"In summary, I enjoyed reading the paper. The authors perform thorough analysis of the hypothesis as well as their proposed approach. The weaknesses I mention are not minor and I would like to see them addressed. My biggest worry is the statistical analysis of the results, as  the entire paper relies heavily on experimentation.
Nevertheless, it did not dampen my positivity about the paper and therefore I feel comfortable recommending acceptance. 
"	4
"My concerns are mainly focused on the experiments.

1. This paper only conducts experiments on the ViT while there are other SOTA vision transformer structures, e.g. swin-transformer [a]. The authors should conduct experiments with more transformer structures to demonstrate the generalization of the exploration and the proposed negative augmentation strategy.
[a] Swin transformer: Hierarchical vision transformer using shifted windows, ICCV2021

2. The authors only conduct experiments on one dataset, ImageNet. Although ImageNet is large enough, I wonder if the experiments’ results can be reproduced on the dataset of CIFAR-10-C and CIFAR-100-C which are utilized in AugMix.

3. The authors have tried with the transformation of patch-wise shuffle, rotation, and infill. How about other commonly utilized transformations in data augmentation? E.g., horizontal and vertical flip.
"	4
This paper provide some interesting findings on ViTs and propose patch-level negative augmentation. The authors also conduct experiments to validate the contribution of the augmentation and losses in details. However, the approach is only validated on the naive ViT-B/16 architecture and not convincingly enough. Moreover, additional analysis on losses and different architectures are expected.	3
I think the method is sound, and up until the experimental section the paper is well laid out and straight-forward to read. My understanding began to breakdown in the experimental section, where information was either referenced in the Appendix or completely in other papers. This made it hard to read and slowly down reviewing a lot. In most of the experiments, comparisons to other methods in the literature are missing and it is therefore hard to gauge the significance of this work.	2
"While I like aspects of this paper (e.g. I think the FE formulation for constraints on unstructured grids can be useful), in its current form it's not ready for publication. Many of the more interesting contributions (e.g. higher-order basis functions, hard constraints, irregular grids) don't really work that well and aren't used in the main experiments: most results are on square, regular domains, with soft constraints and linear or radial basis functions, which is quite similar to what a lot of physics prediction papers are already doing with auxiliary losses (and for which there aren't any baseline comparisons). 
I'd encourage the authors to work further on the method (e.g. make hard constraints work for nontrivial systems), and choose good experiments to showcase the strengths of constraints on irregular grids. I also think constraints for adversarial methods that the paper touches on could be worth exploring further."	2
"The paper proposes a very interesting method to enforce differential / Integral and differential constraints on an interpolation function derived from the learn operator that fits the data. 
The experiments are well conducted but the method’s presentation lacks clarity (see main review) notably on the estimation of the interpolation coefficients and their link to the learned solution function? 

If my concerns are addressed, I will be glad to increase my score.
"	3
"Overall, I think this submission would benefit from further revisions. It is definintely an interesting direction, but in its current state the paper applies existing methods from regular grids to Lagrangian discretizations. This is a good idea, but not a very fundamental step forward. In combination, the results do not contain many surprises, and the focus on GANs in a way detracts from the main goals of the paper. The presentation of the work could also be further improved. So overall, I'm leaning towards the negative side given the current state of the submission.
"	2
This paper solves an important problem and is novel, though I am not fully convinced by the claims due to some potential misunderstanding.	3
The work is technically sound however I find the title/introduction overstating the significance of the results to some extent and not sure if the motivation is strong enough.  	2
The paper is written very well and has nice complementary results. I am however uncertain about whether the main result would be effective in explaining generalization error using VC dimension in reasonable scenarios. 	3
Overall, the paper proposes a convenient and principled method to an interesting problem. The empirical evaluation is generally convincing, but could be somewhat more thorough in its analysis, especially given that the paper is a full page under the page limit.	3
I am overall impressed by the idea of geometrical algebra-based rotation equivariance. The experimental results are also impressive. However, from my viewpoint, the paper still has several weaknesses in terms of clarity and experimental validation. I would like to hear more during the rebuttal phase. Thus, currently, I vote for weakly accept.	3
I am not an expert on this topic. But, I think this paper proposes a novel formulation to provide rotation and permutation equivariance for deep learning models for small point clouds which are useful in many applications. They also show improvements over existing approaches in three different applications. Therefore, I suggest accepting this paper.	3
This manuscript has several issues to be accepted as raised in the weaknesses.	3
The idea is simple and easy to implement and people can try to fork this in whatever they're doing. However, the improvements shown aren't significant to strongly push for accepting it.	2
"This paper proposes a simple method to improve the performance of image-based RL. My main concern is about the novelty as proposed method is commonly used in CV methods. More further discussion and study will improve the quality of this paper.

"	2
The authors should provide an explanation regarding the discrepancies in results highlighted in this paper vs the RAD paper for the same benchmark tasks. The experimental validation and baselines need more work before this paper is ready for publication. 	2
It's an interesting new usage of data augmentations that is nicely demonstrated	2
The paper provides insights on the optimal representation under covariate shifts, which is a potentially important problem. I think the authors did concrete work to develop the theory and set up the numerical experiments, though there are also limitations discussed in the main review. 	3
In summary, the paper theoretically analyzes the conditions under which domain generalization would be possible. However, the writing is not clear at places and the empirical results are not very convincing.	3
"The problem is fundamental to machine learning, the theory developed by the authors is stimulating and to the best of my knowledge new, the empirical analysis is informative, and the writing throughout is very strong. I vote to accept.
"	3
"# Additional experiments and visualizations is necessary 
* Besides the points stated above, I am also interested in seeing the statistical significance of the result. Can the author report a standard error for all experiments for k trials (at least k >2)? Please show hidden representation with later layers during the before phase, forget/reinitialize phase and relearn/pruning phase. 
* Additionally, the effect of sparsity on model performance, how to ensure signals or later features are sparse enough for the model to generalize better. Having theory to back these claims is ideal, given current work is heavily experimental, I would encourage authors to add more insight and results to back their claims. "	2
The paper opens a new and potentially immensely useful perspective on training deep neural networks. However, making claims this strong requires proportionally strong and versatile supoort of experiments and/or theoretical results. Therefore, while this paper has promise, I find its current form too raw and unpolished for publication in a major venue such as ICLR.	3
This paper offers a new hypothesis to explain the success of several recent DL methods. This hypothesis is well supported by experiments and offers a straightforward method to reduce overfitting, effective in image classification and language emergence problems. I believe it could positively animate the ML and DL debate as other proposed interpretations did in the past, such as the lottery ticket hypothesis. In my opinion, this article should be highlighted at the conference.	3
Despite the main idea being counterintuitive at first glance, the topic becomes clearer and more interesting with the reading and the experiments, nicely explaining previous works. Therefore, I believe that the ideas have merit, however there is no theoretical proof that proves the hypothesis. There are some impracticalities of this work, that I would appreciate if the authors can address. 	3
"Overall, I enjoyed reading this paper. However, I have two main concerns (see Weaknesses above for details): (i) the invertibility assumption used to obtain the theoretical results is at odds with the fact that latent correlation-based multi-view learning typically does not use invertible encoders. This is not made sufficiently clear, and the claims that the presented analysis explains or helps understand the success of current methods is not fully justified and should be toned down. (Such an explanation would also have to explain why these methods work without invertible encoders.) (ii) part of the theory appears weaker than results in (von Kügelgen et al., 2021) which are not compared to in sufficient detail; in particular, I encourage the authors to include a more complete comparison.

I believe the paper also provides valuable novel contributions, in particular, the finite sample analysis (Thm. 3), the new learning method (Sec. 4), and the thorough experimental evaluation (Sec. 6); I actually see these as the main contributions (Thms. 1 and 2 seem to follow quite directly from invertibility and independence) and think they could feature more prominently. Together with addressing (i) and (ii) above, this would make the paper a solid and well-rounded submission, and I remain open to increasing my score depending on the authors' response.

### Post-rebuttal updates: 
The authors have provided detailed responses to my comments and have made several modifications to the manuscript that satisfactorily address my two main concerns (see the discussion below for details). I have decided to increase my score as a result. "	3
The paper is generally easy to read and tackles an interesting problem with a novel solution (as far as I can tell). The issues I have pointed to can fairly easily be fixed. Hence the positive score (I would have given a 7 if that was an option).	3
This paper proposed a novel approach for multi-view learning problem. A theoretical analysis of the proposed model is provided. In experiments, most of the datasets are small scale and simple data format. More discussions and analysis about its potential real-world applications could be discussed.	3
Although the paper is interesting and well written, I have some concerns regarding the theoretical contributions. Therefore, I think the paper is slightly below the bar.	3
Overall I think this is a novel and interesting contribution, that solves an important problem and provides a framework that improves over previous work.	3
Though the idea of this paper is interesting, the weakness of the theoretical analysis reduces my criterion on it. I think this paper is a bit below the bar.	2
The authors address the problem of the biased updates of the $\alpha$-EigenGame in a simple yet effective and theoretically sound way. The proposed changes make the algorithm both highly data parallel and also very robust when using small batch sizes. While more work is definitely required to understand the underlying factors that affect convergence rates, I am confident that this work makes an important first step towards this goal.	3
Improving the efficiency and predictive performance of MPC-based techniques for machine learning is an important problem. It would be good to include experiments that demonstrate the efficiency over the benchmarks. 	3
"The MPC aspects need more work.
"	2
Unfortunately, it seems to me that this paper has a fundamental mistake and cannot be accepted.	1
To be honest, this paper is below acceptance standard because of innovation and lack of security analysis, although the writing of manuscript looks great. So, currently, I think I would not accept this paper and looking forward to see other reviewers’ comments to make final decision.	2
Although the proposed method looks promising and comparative evaluations show performance improvements compared to SNNs nearing the ANN performances, the efficiency aspect compared to SNNs have not been justified well and there are some other critical flaws as pointed by the other reviewers. Authors did not address these issues. Therefore my decision is reject.	3
"While the presented results show that the proposed approach of temporal dilated spiking convolutional network can be trained to recognize audio keywords with competitive accuracy, the main contribution of the paper is a system that combines ideas from multiple existing works. Some key experiments pertaining to the claims are missing, such as the benefit of manual time constant selection or how the manual approach generalizes to new tasks, how it detects the no keyword case in deployment etc. The experiment section is missing data about memory requirement reduction than alternative approaches and throughput comparisons on a standard neuromorphic processor.

The paper can be strengthened by the addition of the following: 
1) Time constant optimization instead of hand selection and its effect on accuracy on different tasks. In the current setup, it should analyze the  effect of varying the constant in Equation 8.

2)  Compare the memory requirement of the same WaveSense equivalent implemented using synaptic delays vs. WaveSense using time constants, to show the reduction in memory and computation requirements using synaptic time constant as mentioned in Section 2.1.

3) Experimental results to show robustness regarding Weakness 5.
 
If detailed experiments and analysis are presented on the above issues in the revised version, with justification of the related claims made, I will be inclined to reconsider my score to a marginal accept."	2
"Based on my concerns listed above, I recommend for reject.
"	2
Overall, the reviewer recommends not accepting the paper. The authors gave a neat idea on how to implement temporal convolution with spiking neurons efficiently. However, the overall quality of the paper does not reach the standards of the conference. First, additional experiments are needed to validate the effectiveness of the proposed method and demonstrate the advantage of the method compared to existing approaches. Second, the structure and writing of the paper need considerable improvements to make it more readable. More detailed problems of the paper are listed in the cons in the main review.	3
This paper needs to strictly rewritten and organized for proofreading, and thus, I will take the score of “rejection”.	2
In short, while I appreciate the significant effort that went into this paper, I found it comprised of many parts that fell just short of convincing execution. The model architecture presents only minor changes over an existing one; the quantitative evaluations show only minor improvements over the SOTA (and the authors rely heavily on the qualified claim that they are the best performing ‘mid sized model’ to distinguish themselves, a claim only somewhat supported, given the t5-small model); and present inconclusive and hard to interpret qualitative results. 	2
"## Strengths:
* very well written and clearly structured text
* good motivating example
* interesting approach in scope of the conference
* gives a concise intro to previous work and ties back throughout the paper
* achieves respectable performance at comparable small model size

## Weaknesses:
* underperforms in terms of the state of the art
* anecdotal examples offer very limited insights in current form
* approach mostly orchestrates existing architectural concepts with little actual novelty
"	2
"Overall, I think the paper has merits. Program repair in the wild using end-to-end neural network is inherently a difficult problem. The proposed architecture shows a good performance while using a much smaller model, and can spawn further discussion of algorithms using graph edits for program repair. The paper has also shown a pretty comprehensive evaluation. However, the paper will be in a much better shape if the pre-training task is designed more throughly. We would also hope there is a better performance gain from incorporating pre-training. I lean towards accepting the paper.
"	2
The work proposes two modeling contributions over prior work, neither of which is ablated quite carefully enough, primarily in terms of comparing models of more reasonable size. The eventual results are not particularly strong compared to many other baselines, which, while equipped with more parameters, also use models that scale better with larger parameter budgets.	2
The work is interesting as it addresses an important practical problem and proposes a principled solution to it. Experimental study can be made stronger by considering additional tasks and datasets. Also, practical usefulness of the proposed approach needs to be carefully reasoned as it appears to be critically dependent on most queries being easy for the edge model whereas in practice this might not be the case. 	3
I think the authors are thinking about a very interesting problem, but the work is at an early stage and not ready for an ICLR publication. The main reason is the lack of comparison with similar work. For more details, see the cons above.	2
"The paper proposes an interesting idea to combine the edge and cloud inference together, but as mentioned above, it is not practical to use MACs as a metric when the main concern of the paper is to maintain the low latency of edge inference while improving the accuracy. The experiments should be redesigned to make the proposed method more convincing.
"	3
The overall contribution is not significant as the hybird framework is straight forward and the model determination is decoupled and directly from published work.	3
"Overall I think that the paper is well written and has potential but is not ready to be published in its current stage with all the weaknesses mentioned.


====Rebuttal Update====
As written out in the rebuttal response comment below, I am relatively satisfied by the changes the authors have made and recommend that this paper be accepted."	3
Although the paper is not technically innovative, I do think AMR is a better form of intermediate representation. But my other worry is how to get a good AMR parsing tool, because there is relatively little annotated AMR-text parallel data, and I wonder if the trained AMR tools are ready for use. Overall, this work has a certain contribution. Therefore, I suggest accepting it.	3
Overall the paper is interesting and mostly clear, and it shows improvement over most baselines, with a fairly convincing argument that it is a more efficient learner with smaller amounts of training data. However, I have some lingering questions about what exactly has been demonstrated in terms of importance of the imagination module, and why certain inputs were included in the verbalization module -- and I would have liked to see more analysis discussion on exactly why the SKGs are improving performance, and how we can think about the way this compares to strong alternative models.	3
"Overall I believe this is a good paper, to me it is certainly above the bar. While I think the paper can be further improved (see my questions above), I would suggest to accept this paper. 

But of course, correct me if I misunderstood the paper. "	3
Overall this is a strong paper with sufficient novelty in its model architecture in the context of motion prediction for autonomous driving. The experiment results on standard prediction tasks on popular benchmarks are convincing. Despite some minor issues in presentation, the paper is above the acceptance bar.	4
I have some issues with the novelty of the work presented, the accuracy of the claims made, and the somewhat lacking comparison with the related work and key ablation experiments. See main review, above. 	2
"Without the required analysis as requested in the revised version, the present work lacks research rigor, technical novelty, and is simply an impressive engineering effort. If no revision is made, I would defer it to a workshop publication.

If authors provide **strong** analysis on challenging, rare, and interesting examples as provided in WOMD, I may recommend acceptance (pending additional review of the provided analysis).

============= POST AUTHOR-DISCUSSION =============================

The authors have made good faith attempt in addressing my concern and engaging in discussion. 

While I wouldn't fight for it's acceptance, I have no problems if it is published in ICLR. I have updated my score accordingly. "	2
"Overall, I think this is a very thought-provoking paper with many interesting novel ideas and impressive results. The performance of the marginal prediction is a bit questionable, and there is also room for some writing improvement. I recommend this paper to be accepted, but I hope the authors can clarify my questions in the final version of the paper.
"	3
While there is work on diverse counterfactual explanations and different VAEs (DIP-VAE, Beta-VAE, etc), and work algorithmic recourse, this work seems to marry the two ideas of of providing multiple counterfactuals based on disentangles latent variables. This can be quite helpful in practise. 	2
I am borderline for this paper in the current stage and I may increase my score if the author can solve the above concerns.	3
Overall, I would recommend an acceptance. The field of interpretability is becoming increasingly more important. The authors introduce a novel method, called DISSECT, which brings a meaningful contribution to the aforementioned field.  The method is novel (bringing a fresh perspective) and performs well empirically. I did not give an 8 as I still had some reservations about some aspects of the paper (as highlighted in the section before).	3
While I do think that this is overall a good paper, I do have some minor issues that I would like to see clarified. First, I am worried about the dependency on a strong generative model that can successfully disentangle concepts. Second, I’m not sure that the fact that this method does not depend on predefined user concepts is a good thing. Thirdly, I think that the paper might benefit from a deeper discussion of the importance of generating realistic counterfactuals from a causal perspective. Lastly, I think that the technical contribution might be a bit too thin, given prior work that this paper builds on. 	2
Very good paper with correct methodology. relevant results and a concerning overlap with a recent ICLR paper	3
"Given the relevance and experimental results of the proposed model, I think that this paper may be valuable to the audience of ICLR. Nonetheless, some substantial limitations about the nature of the heteroscedasticity contribution and unclear points in the paper prevent me from giving a firm positive score, thus choosing a score marginally below the acceptance threshold.

I believe that this paper may be improved in the course of the discussion phase and would be pleased to raise my score, should my concerns be addressed during this time.

### Post-Rebuttal Update

I acknowledge the authors' response and thank them for their detailed answer. As explained in my follow-up response, the rebuttal partly addressed my concerns. While I still find that it could be improved on some aspects, I believe that this paper may be accepted to the conference. Therefore, before discussing with the other reviewers, I choose to raise my score from 5 to 6."	3
"This is a paper that can have an impact in real world applications, and as such I think it should be accepted. However, I did not find the paper easy to follow in this form.
For this, for now I have set ""marginally above the acceptance threshold"", but I would be happy to revise the score upwards if I see that the authors improve the exposition in the paper."	3
Given the limited novelty, some unclarities, and lacks of some baselines, I give 5 to this paper now. If the authors can clarify and add these baseline results, I may consider increasing my score.	3
"The paper may have some new results but is better fitting for a forum for theoretical analysis and
preferably a journal where more space is available to fully elaborate on
the detailed definitions, assumptions, claims and their implications.
"	2
Please refer to my main review.	3
This paper represents the development of an important tool for understanding tree ensembles. 	3
This paper proposed a new learning objective that enhance AlphaZero and FDFPN to solve a game. However, without a sufficient discussion about the significance of the studied problem, I do not recommend acceptance.	2
A well-written paper, with no major issues. Some minor issues but I expect these should be relatively easy to clear up or resolve.	3
"I am generally positive about this paper. There are a few small issues to correct, and some things which could be expanded, but the basic pieces of a solid submission are there. Howeve, I do think that the readability of the paper could be substantially improved through some reorganisation and rewriting of some sections.

---  Thoughts after revisions and discussion ---
I think the readability of the paper could has improved. Even if it could be further improved, I think the paper is in a reasonable state for publication.
------"	3
It is straightforward that this paper makes AlphaZero faster by modifying the target and using existing heuristic knowledge.	2
While some of the proposed techniques seem to be interesting, the paper is very difficult to follow and the experimental results are not convincing. 	2
"This is a good work which tries to integrate ideas from both conventional time series forecasting models and more recently DNN models. There are some novel ideas which are interesting and are worth studying further. Nevertheless, as mentioned above, the significance of the proposed model for multivariate time series anomaly detection is unknown due to the limitations of the empirical study.
"	4
"·	To summarize, this paper proposed a novel interpretable outlier detection algorithm with stack residual blocks, regularized model complexity selection and novel CUMSUM detection algorithm. The proposed method is technically sound. However, the selecting datasets are solely in univariate setting and are shown to be flawed in previous study, which makes the empirical evaluation less credible. In addition, technical justification and empirical analysis of residual framework is missing, making it hard to evaluate the contribution of interpretable components. I believe some more modifications will certainly improve the quality of paper to meet the ICLR standard. Therefore, I vote for marginally below the acceptance threshold."	3
The authors provide the very first second-order convergence rate results for nonconvex-strongly-concave minimax problems. But I believe that the style of the current version is not very suitable for the machine learning community and the ICLR conference as it does not focus on the large scale setting and does not care about practical performance. 	4
I think the paper addresses an interesting problem, but its algorithm development is not very novel. Moreover, the authors need to better explain why achieving such second-order stationary point should be of interest in the context of minimax optimization. 	2
"Pros:
1. The studied problem is interesting and the proposed method is novel.

Cons:
1. Theoretical analysis is on the weak side.
2. No empirical results.

Overall, I recommend to reject this paper."	2
"Overall, this paper is nicely written and contains solid theoretical contributions to nonconvex minimax optimization. Given the emerging and growing interests in minimax optimization in the general machine learning community, I recommend accepting this work.

-----------------------------
After reading the other reviews, it seems that the present version indeed consists of a number of issues and is not yet ready for publication. But overall, I feel the paper contributes to analysis and algorithm design for minimax optimization. I have updated my score."	4
"Pros:
- New method combining elements from first and second order methods
- The method only approximates the solution of the max problem
- New convergence guarantees for the min-max problem
- Well-written mathematically and in general

Cons:
- Results are not surprising, and somewhat a direct implication, considering the techniques used and the current literature.

Overall: I recommend to accept the paper."	3
"This is an interesting idea. This paper provided detailed experiments to test different advantage of this idea. However, the discussion on why it work seems not quite convincing. Also, part of the experiments missed some details. Some conclusions may not correct.
We expect the author provided more convincing discussion and more comprehensive comparison detail."	2
This paper proposed a solution to a new problem of transferring 2D weights to 3D. The solution works well on many dataset pairs. Though some claims made in the paper are questionable, this paper deserves to be published at the conference.	3
"Although the experiments in the paper show that the proposed model can improve the performance and data efficiency compared to training from scratch, the overall experiments results are not strong enough. In addition, it's still unclear why this kind of 2D to 3D transfer is reasonable. I am inclined to reject.

Update: After reading the feedbacks from the author, I raise my score to 6."	2
Overall the paper has interesting experimental result on transfer learning between image and point cloud data. The authors did a good job explaining the idea, concepts, procedures and experiments, showing the effectiveness of the transferred model on performance. data efficiency and usefulness on few-shot setting. The experiment itself could be further extended though.	1
It seems to the reviewer that this is the first paper to use 2D image pertained models for 3D point cloud understanding. Although the idea is simple, the work is interesting and inspiring. Therefore, I am recommending the acceptance.	4
"The proposed Bayesian formulation for gradient leakage is reasonable, but has little practical significance because it neither provides an easier way to break sophisticated defense mechanisms nor does it lead to better reconstructions. The design of attacks against recent defense mechanisms (Soteria, ATS, and Precode) may be a useful contribution, but does not merit a publication in itself. 

Since the authors have managed to show that the proposed attacks can be derived as approximations of the Bayes optimal adversary and are not purely custom designed, I have updated the rating."	2
"I have a positive outlook on this submission. Classifying and unifying previous attacks in a single framework and clarifying the optimal choice of regularizer and objective is a helpful contribution to the community. 

I do have several questions regarding the experimental evaluation that I would like for the authors to answer and I would be interested in a clarification why the inclusion of Sec. 5 should be part of this submission and its connection to the overall Bayesian framework discussed in this work.

--------------------------------------------- After Author Responses: --------------------------------------------

The authors have adressed my concerns during the response period. This lead me to increase my score from 6 to 8."	3
This paper is overall well-written. The proposed method is simple and empirically effective.	3
"- Even though the assumptions seem to be correct, the contribution of the paper is extermley simple and straightforward. 
- The experimental results seem to be insufficient as it is only compared to two other methods. The experiment section is written poorly and the importance of each section is not justified. "	3
In general, the paper is written clearly, connects previous work in a good way, so it is valuable in that sense. I am willing to chance my vote towards accept if the authors could clarify my concerns and convince me their work has sufficient contributions to be published.	2
"My recommendation of this paper is based on the novelty with respect to previous works along this line of work, also considering the writing and experiment quality. I am not a domain expert and I didn't verify the correctness of equations and proofs.

My main concerns are 1) the proposed approach is not explained clearly (or am I missing the context?) and 2) the experiments included in the main paper seem shallow and repetitive. "	3
In its current form, the structure of the paper makes it difficult to assess the novelty. Also, there are previous approaches in the literature that seem to overlap with the aim of the paper. 	3
Overall, this seems like a strong paper with good theoretical justification and experimental results. The one possible weakness is perhaps lack of comparison baselines (though not sure if it is possible or not)	3
This is a nice paper with good contribution for the (ML based) pipeline for COVID vaccine design.	2
Considering the lack of theoretical novelty and biological accuracy, it is difficult for me to see the impact of the work as it is currently presented.	3
"Strenght: The paper offers an interesting and apparently unexplored variant of set cover that can model a vaccin design problem. The evaluation seem to show good performance. 
Weaknesses: there seems to be something unclear in the pseudocode of the greedy algorithm that prevents me from fully assess its behaviour; the importance of the weights in the experiments is not clearly discussed (without this, the problem reduces to known Multi-set multi cover)"	3
The submission is built with a clear application in mind. It provides two reasonable approaches for a well-modeled problem. There are little theoretical contributions, but I found the practical contribution well-executed and convincing. 	3
The paper proposes a new decentralized AC framework which combines existing decentralized techniques including distributed averaging and TD learning. The idea is interesting but not sophisticated. The NAC part is probably the first decentralized NAC algorithm. Combining existing tools may lead to new analysis challenges, which needs to be further clarified. Thus I think the paper is on the board line. 	2
Two new decentralized AC algorithms have been developed with guaranteed  sample and communication complexities. However, some claims are not justified and the implementation is not realistic. 	2
Overall, I think the current form of the paper still requires substantial work to make it technically solid and sound. In particular, the numerical results are not very convincing, and there are numerous confusions in the analysis, which have weakened the theoretical contributions. I hope my comments can help out.	3
The paper is professionally written and well-organized. This is the first work that derives sample complexity bounds for decentralized AC-type algorithms, and the obtained sample complexities match the existing ones of centralized AC and NAC. Moreover, the corresponding communication complexities are orderwise lower. I think the developed decentralized AC and NAC algorithms are good complements to the existing multi-agent RL literature, and hence I recommend accepting this work.	4
This paper proposes an reverse KL term to match the marginal distributions in domain adaptation. The results are promising. The main concern is the implementation is inconsistent with the new generalization bound.	3
Overall, I consider this work to have some interesting ideas however I consider it needs a major rewriting and clarity on the contribution of the theoretical results. In my opinion, they look more as a way to motivate an algorithm which could be done following the results from either [1,2,3,4] rather than a new generalization bound as claimed. The writing should also be more careful when comparing and criticizing previous work. I also believe comparison to recent DA algorithms/methods is need it, Similarly, more experimental analysis in order to validate the effectivity of the proposed algorithm is also need it.	2
"All-in-all, I find the paper of a great potential contribution to the field. I only have two issues (i) the sudden strange simplifications in equations 15 and 16, and (ii) selective experiments besides the weird results in Table 1.

Update:

I read the other reviews and the authors' responses, I'm fine with the authors' reply. So I am keeping my score.

"	3
The paper is easy to follow, and the idea is easy and effective. However, some assumptions seem too strong and some datasets are missed in the experiment part.	2
"Overall, this works lacks complete analysis, which is not usual in a bandit paper, and the relation between the background and the discussions is not clear. Hence, the contribution of the work is not clear. The detailed comments are as above.



================================
I appreciate the efforts of authors to add on theoretical results during the rebuttal period, and decide to raise the ranking from 3 to 5."	3
TL;DR: The paper is a bit hard to follow for the non-expert in compression that I am (arguably like many of the bandit community) so I am not sure of my evaluation and I hope to be able to clarify it during the rebuttal / discussion phase. I cannot make a strict recommendation for now but I will update my score after the rebuttal phase. 	3
"A solid work overall, but with some questions on the novelty and assumptions. Will be open to author responses and adjust the score accordingly. 

======

Post-rebuttal update: I thank the authors for addressing my concerns on the availability of state information to the decision-maker versus the agent. I hope that the authors add these details, especially the factory example, to the paper. My other reviews remain the same."	3
"The paper is well-written and well motivated. The technical discussions are concise and clear. The analyses for numerical results are comprehensive and thorough.

Weaknesses: the paper would be much stronger if it includes theoretical guarantees for the clustering scheme, and also results/discussions/experiments on how the cluster scheme’s performance decays with the number of states.
"	3
"Although mostly an empirical study, the research topic is interesting. I think the paper undersells a bit its contribution actually, as some variants considered are not trivial. It also provides interesting future directions. On the other hand, it is as bit hard to read, requires more proof-reading and the scale of the experiments is not big. Thus I believe it is a bit below the acceptance limit.

*After Rebuttal*: Thank you for your response. It seems that few changes have been made to the manuscript. I will keep my score unchanged. I wish the authors the best to improve the manuscript."	2
"I am leaning towards a recommendation to reject the paper since the empirically findings in the paper are inconclusive and not significant as mentioned in the Main Review section. 

---
After Rebuttal: Thank you for the response. My concerns are not addressed and I think the paper is not ready for publication. I hope the authors can keep improving the paper since it is an interesting topic. I will keep my original score and evaluation. "	2
"Although this work raised an interesting question, this paper combines the existing algorithm and sampling metrics, which lack both novelty and empirical significance.


-------------------- Post Rebuttal ---------------------

Thanks for the response. I read the response carefully, and I maintain my original score. I hope the authors to further improve this work in the future."	1
"Overall this paper asks an interesting question, but fails in its execution for the reasons I have listed above. I believe it would be more suitable for publication at a workshop.

Based on this I recommend rejection."	1
I give an initial recommendation of score 6 mainly for appreciating this novel view to mainstream meta-learners, and solid discussions supporting the points. However, to meet and standard of ICLR and demonstrate a clear contribution to the research of meta-learning, I believe further efforts on comprehensive discussions are highly expected. 	3
"The paper proposes an interesting problem, and based on theoretical and empirical analysis provides a neat solution. Issues stem from the perhaps overly concise writing; many aspects of the paper are in need of elaboration. These include the motivation, problem setups for the various benchmarks, and the reasoning behind certain conclusions. Currently I am scoring the paper as if my understanding is correct and authors’ intended arguments are simply wrong, but if these issues can be clarified I’d be happy to raise my score. 

POST-DISCUSSION:

Authors mostly address the above issues, in discussion and in the revised manuscript. I have some lingering concerns, shared with reviewer q98z, regarding the lack of _demonstrated_ practical benefit, and the fact that accuracy gains in the more “real-world” benchmarks shrink substantially relative to the motivating XOR problem. However, the revised paper is much improved, conceptual novelty remains high, and on the whole the paper is of sufficient quality for acceptance. "	3
As stated in the main reviews, the empirical results aren’t particularly impressive, but overall the paper does provide some interesting analysis that may encourage new ways of thinking about few-shot learning problems. As such, I think this paper may be of interest to the ICLR community.	3
The implementation details and related work of the proposed paper should be further added.	2
The main idea in this paper is simple, easy to implement and the empirical results are good. However, (1) this very idea is not that novel since ego-nets have been used in the past,  (2) most theoretical results are similar to prior work and do not make a particularly compelling case, while other works on ego-nets have gone further in that respect and (3) the multitude of engineering choices obscures the root causes for the improved empirical performance. Therefore, I am leaning towards rejection, but I would encourage the authors to clarify their position w.r.t. the above and make their statements more clear since it seems that ego-nets have good potential in terms of practical performance	2
"The strengths of the paper outlined above generally outweigh the weaknesses, so I am therefore recommending the paper for weak acceptance for now. I am looking forward to the discussion period and I am willing to change my score once my points are addressed. 

EDIT: I have raised my score to 8. "	3
This paper puts forward a novel framework that can easily plug different kinds of GNN and improve the expressiveness. The proofs and experimental results well support the proposed method. Some concerns are listed above.	3
"The paper proposes a simple and meaningful architecture to enhance the expressive power of most standard GNN architecture. 
The experimental results are promising, however, the authors only consider a limited set of benchmark datasets. 

However, there are some problems with the theoretical contributions, see above. "	3
"The paper has solid contributions by presenting an adversarial parameter perturbation based model regularization scheme that is successfully applied to recurrent SNNs. However there are important methodological comparisons/discussions missing with respect to similar methods that were recently proposed. My major concerns are listed in the above reviews, which concludes that I find this work currently to be below borderline.
"	2
Techniques in this paper appear to be novel modifications to neural network training that combine projected gradient ascent with a loss specifically designed to promote robustness to small perturbations in network weights (or any similarly learned parameter).  The authors provide evidence to the strengths of this technique when compared with other mitigation measures for noise in the parameter space (either adversarial or as a byproduct of physical properties).  This research provides an interesting path toward making imperfect analog computing devices practical for large scale use and deployment.  Though I am not an expert in the specifics of this adversarial and noise related network training methods, the paper layout and methodology appear sound.	3
"This paper proposed a simple yet effective solution for parameter mismatch with gradient-based adversarial training method.
However, since I am not familiar with the prior works on the parameter mismatch, I am not very confident about my assessment."	3
This paper has looked into the existing solution approaches to a relevant and interesting problem and focused on understanding how those solution approaches work. This analysis provided some insight which, in turn, were exploited to introduce a new architecture that can match/outperform the existing methods with much simpler network architecture. The idea is interesting, the experiments are mostly well carried out, and the paper is reasonably well-written. However, as explained in the *main review*, some aspects should be further improved to increase the concreteness and overall clarity of the paper.	3
The paper presents a thorough and well written analysis of inductive biases in HNNs. 	3
"I think this paper deserves acceptance. In my opinion, it clearly adds new knowledge to the body of work on HNNs, is written exceptionally well, and introduces a useful framework (NODE + SO) for modeling dynamics in non-trivial environments that has the potential to benefit the model-based reinforcement learning (MBRL) community. A few minor requests are provided to strengthen the paper.

==========================================================================

Update after rebuttal: I will be maintaining my initial score and hope that the authors will update their paper for the final camera-ready version to include the promised improvements."	3
Overall this submission is well motivated, thoroughly executed and well written, hence my recommendation for acceptance.	4
"The idea is simple and works on the problems tested. The larger applicability and constraints of the approach are not clear. Explanation of the approach can be improved.
"	3
This paper identifies a specific problem of distribution drift in offline Meta-RL and proposes an interesting idea of using unsupervised data collection to alleviate this problem. This paper is generally well-written. However, the proposed approach has limited technical contributions and makes strong implicit assumptions. The experimental evaluation also needs to be improved by including more settings.  	2
I believe this work tackles an interesting and promising research direction (i.e. alleviate the distributional shift issue in offline meta-RL) and proposes a convincing framework to address it. The experimental results verify the effectiveness of the proposed framework. Therefore, I am leaning toward accepting this paper with some concerns on the assumptions made by this work and its applicability.	3
The online training process with generated rewards needs more clarification. And it will be better if the proposed method is evaluated on standard offline RL benchmarks.	2
	4
While I believe the objective of the paper is worth research: a neural network that works well with data missingness out of the box, I believe the method is a little complicated to be a practical tool and for other research to build upon it. Additionally, I want to note that, while I understand the algorithm well, I never worked with data missingness before.	3
Handling missing data is important in real machine learning problems and it is good to see that imputation-free method can also work well under this issue. Since there are some unanswered questions in the paper, I would like to give a slightly positive rating.	4
"I raise several questions about the motivation, clarity, and experiments of the paper. I believe the current version should be significantly improved to reach the acceptance of ICLR.

**post discussion with the authors**

I thank the authors for this productive reviewing process. I increased my score because my major concern has been addressed. "	3
My main concern is the applicability of the insights to more standard settings (W1). The submission also needs major improvements in clarity (W3).	3
As the problem of the role of implicit biases in the systematic generalization seems very interesting, some incorporated constructions and problem designs seem artificial, and the conclusions lack significance, especially in the light of their limitation to the linear case study. 	2
I find that the paper tackles some interesting questions but does not provide enough value to be accepted. The presentation is lacking. The figures could be improved and the language could be more precise in places. I also find that the work fails to connect with early work on (systematic) generalisation in connectionist models.	2
"Overall, the reported results are interesting, but the findings are too few and too narrow.
This paper does provide a benchmark and preliminary findings, that could be interesting to build on, but I think this paper does not have enough substance yet to be published at a conference. "	2
"Detailed comments:
- ""classification problems without inherent uncertainty"", I am not sure that such problem exist. Please rephrase or elaborate further.
- Minor: make sure that all notation is correct (e.g., scalars are not bolded), both in the figures and in the text.
- ""of just 75%"", can you please clarify how?
- It is not fully clear why MSE_p is declared gold-standard, please clarify why.
- For Fig 4 results, more details are needed. E.g., early stopping is not well explained.
- I suggest moving Fig 5 much later, it seems introduced too early.
- For the kernel function, it is not clear whether the r-window is in the input or output space.
- In Section 6, uniform is not really uniform? Since it still increases with z.
- It is unclear which models were actually used for different data sets. This should be explained to a greater detail.
- In the last paragraph in Section 6, please explain better how each scenario aligns with the data sets, it is unclear as is. In fact, currently the entire section feels hand-wavy.
- Also, the last sentence doesn’t parse well, please rephrase.
- I recommend removing the last sentence in the Conclusion section."	3
The paper lacks a clear problem statement, sufficient technical novelty, and a consistent logical flow. Under these conditions, I am not able to recommend an accept. 	1
Overall, the authors proposed a simple and effective method for improving the uncertainty estimation for binary classification. Several drawbacks include: the reason why training directly on the training will not lead to over-confidence is unclear; performance is worse than some baselines in some dataset; the method seems limited to binary classification and it would be interesting to see at least its performance on multi-class classification. 	2
The work is clear and concise. The authors propose to tackle an interesting problem. However, it would benefit from further justifications of the utility and contextualisation with other domains of machine learning.	3
I am okay to accept the submission. But it can be improved as mentioned in the above review (e.g., including ablation studies, more tasks, comparisons with previous models, and analysis).	2
"This is an empirical work that aims at improving the processing of long-range dependencies in sequential data. 
I've updated my score  recommendation is to accept the paper given that the additional experiments and standard deviations are finalized and included in the paper. I would recommend to further include a word-level tokenization of the PG-19 dataset (and maybe others) to make them comparable to previous work [Rae et al.] and more challenging to the model. This would allow readers understand whether such benefits are indeed beyond the compressive transformer or other work. Also, as the authors are using character-level language modeling, it would be much better to use the bits-per-character metric instead of perplexity. 

My recommendation is to reject this work given the poor quality of the experimental section (see above and below). Also, there will be some challenges to reproduce this work with respect to the datasets that should be addressed as well. 

Questions:
- Please, could you clarify how did you choose the hyperparameters and justify them?
- What is being used as search in k-NN? The output of the layer connected to the memory? Are the keys used in the search or both key/values?
- Why did you choose k=128 in k-NN? What would happen if these value changes?
- Why do you use one layer of memory, if two improve the results?
- Could you add std dev. values to all your experiments? 
- Please explain why does the arXiv dataset have such a low perplexity?
- How many sequences are used for training, validation and testing?

==================================

Update:

I've updated my score based on the rebuttal discussion with the authors, and the additional elements presented. The new score is based on the existing merit, however, the contributions are limited and the experimental results require further work. The additional experiments could strongly benefit the understandings of the benefits of this method. For example, finalizing the experiment with 2K vs 500 + 1.5K memory with transformer and transformer-XL, would add lots of value. The presented results are difficult to compare with previous work (other than the included transformer-XL) due to the selected tokenizer. Therefore, including other models, e.g., compressive transformer, is missing. This could have been avoided by reproducing the experiments from those works. "	2
This work is highly related to those scenarios, such as document-level QA/retrieval and code completion, where the models should consider and leverage long inputs. The key contribution, based on my understanding, is the joint KNN/local attention part, where both external context and local context are combined. In general, the paper is well written and all details are clear to me. One thing could be further improved is that I suggest to include more downstream tasks to verify the effectiveness of the proposed method.	2
This paper introduces a new method for extending the transformer model with a simple datastore. The insert and query operations into this datastore do not have to be trained which is a big advantage for efficiency and simplifies the implementation. Results are shown on multiple datasets, with the model showing big gains on one and smaller gains on the others. I believe the idea presented is reproducible, and extensible and I’m looking forward to seeing how this research direction develops. The analysis presented is insightful and could guide further research. I believe this paper should be accepted. 	4
In summary, the paper presentation requires improvements and the motivation for designing such a model is not clear. There are several methods that the authors did not use for comparison. 	2
Good paper but some discussion of highly related work is missing 	3
"In the light of the current state-of-the-art, the paper falls short severely (W1). It's also overly limited in the type of rules that can be generated and how they can be combined (W2).
"	2
"This is a well-written paper having some interesting results. A few minor revision are list below.
* The idea of using column generation techniques to add new rules is interesting. However, the methods and theoretical innovations are still not sufficient. The author(s) must dig deeper into the innovation points before the article can be accepted.
* The literature review is weak. Related work has not been investigated sufficiently before just presenting the author’s model.  "	3
The active learning approach proposed by the paper builds incrementally on existing ideas of uncertainty and diversity sampling. They provide decent empirical evidence to suggest that their methods are at least as good as existing methods, and sometimes better. However, I hesitate to recommend this paper for acceptance as-is, due to misleading claims, incomplete discussions, and a number of clarity issues. Many of the design choices seemed arbitrary and require more thorough justification. I would be inclined to raise my score if the paper's current weaknesses and issues are properly addressed.	3
"Overall the approach and results look interesting, but the limited experiments make it difficult to fully capture the value of the methods proposed. I would be happy to increase my score if the authors can further expand numerically on where the improvements are coming from.
"	3
I appreciate the effort of authors towards a unified framework for sample selection in active learning. However, as detailed in the Cons section above, the importance of many elements of the proposed method are not adequately justified through theoretical or empirical results, and comparisons to prior work are lacking in number of baselines as well as range of tasks. I believe additional experiments/analysis addressing these concerns are needed to support the major claims of the paper.	2
The proposed method achieves slight improvement over a classic method (k-center) by introducing new objective and constraints. The paper lacks deep analysis and solid experiments that explain why the new components are better. The contribution is somewhat incremental. 	2
The algorithmic and empirical contributions of the paper, as well as the theoretical grounding, largely justify in my opinion its acceptance at the conference.	4
"‌‌
"	4
"I believe that the meta-learning community will find this paper interesting. It provides new insights into formulating meta-learning models and gives examples of such insights being applied to obtain empirical gains. The paper is well-written and features exemplary empirical execution.

POST-REBUTTAL: I have updated my score to 10 and confidence to 5. I think this paper should get an oral, if not best paper. It is the best in my batch and is arguably the best I have read all year. It may be the best paper I have ever peer-reviewed."	3
Overall I believe this paper proposed an insightful and novel approach to addressing the stated limitations of typical MG approaches.  The authors do a good job of motivating the key innovations proposed and, given the significant research interest in MG’s in the past few years, it is reasonable to assume that the methodologies presented will be of broad interest.  Moreover, the experimental results are impressive and clearly demonstrate the improved performance of BMG and analyse where this comes from.  I do believe that the experimental results would be better served with more detailed descriptions of the problem settings and methodologies, however given the overall level of detail presented in the appendices I do not doubt there validity or significance.  Even so, for the stated reasons of novelty, interest and potential impact, I believe the paper is suitable for acceptance in the current form.	4
This paper introduces a new method for high-resolution video prediction, however, the experiments are not sufficient, I believe more experiments are needed to justify the effectiveness of the proposed method.	3
"
In general, I found the proposed method effective in generating visually good video prediction results. However, I found the technical contribution of the paper somewhat limited. Each technical component is not new and the way of combining them has also been explored before, which limit the novelty value of the paper. Please see the previous box for more strengths and weaknesses details."	2
"The main advantage of the paper is its simplicity, and that is what distinguishes it from other approaches, i.e. it is simpler, but achieves better or close to SOTA results. The demonstration that such a simple approach can give (near) top results is definitely interesting.
However, the novelty value compared to previous approaches is very limited. All in all, I'm leaning towards a rejection because I'm not convinced that the results of the paper warrant a full conference publication and might be better suited for a workshop, but I'm curious to see what the other reviewers think.
"	1
"Considering the two important limitations described above, I find this paper to be below the acceptance threshold. Given the limited technical novelty, the empirical evaluation should be stronger for this paper to be accepted. I believe that such an improvement could make it ready for publication.

I understand that the suggested improvements are significant and it may be difficult for the authors to properly address them during the rebuttal. Nonetheless, I would be pleased to increase my score, should my concerns or misunderstandings be clarified during the discussion.

### Post-Rebuttal Update

I acknowledge the authors' response and would like to thank them for their answers. As stated by the authors themselves, the necessary improvements could not have been implemented during the rebuttal, which is understandable. Therefore, I choose to maintain my score."	2
My initial rating is reject. The technical novelty of the paper is arguably only the VQ-GAN encoder. The authors should show more empirical evidence that this component is crucial for their results on video prediction. This can be done via ablations and comparisons with prior work (as described in my weaknesses section)	2
The paper provides a  novel and clever experiment design to show that the limit of the current explanation approaches but shows limited potential to be used as a good protocol for large-scale real data tasks.	3
My major concern is that this paper does not have any technical contributions. And the proposed dataset is not very interesting or distinct compared to the existing. The dataset building or description is also not very rigorous.	2
The authors reveal an interesting research question and provide with a user study design. The results from the user study seem solid. However, the dataset and the questions are not general and proper to assess concept-based or attribution-based explanations. The authors should first improve or clarify this issue. Otherwise, the dataset may not be very useful.	2
"Overall, I think more work like this should appear at ICLR ---
explanability is an important topic in machine learning, and, while a
user study is perhaps less conventionally presented at ICLR, I think
it should be. Furthermore, I admire the experimental practices
presented here, e.g., preregistration. I think my two big critiques
(toy-ness not applying to more common image clf problems, and
non-binary tasks) can be addressed via presentation updates in
revision."	3
I give marginally above for now but I will revisit it based on other reviews and discussions. The problem of learning manifold transformation is well motivated. Experiments seem ok. The idea to use pre-trained classifier to fit MAE latent space traversal is sensible. 	2
The proposed work is interesting and addresses an important challenge in representation learning. I'm not convinced the baselines or datasets used in the empirical analysis are most appropriate for this work.	3
In fact, I found the proposed method is quite interesting.  A better explanation of why the proposed ill-posed decomposition can provide the desired solution would make the idea more convincing.	2
"A good theoretical approach with results to back it up.
"	4
The theoretical contribution is not much as most of the method part can be put into appendix and very well-known. The experiments are proof of concept type, and the paper is essentially using well-known basis for functions of manifold, without giving any recipe for choosing frequency or maximum bandwidth L. The contribution is pretty limited as it's essentially using well-known basis in several applications.	2
The theoretical basis of this paper is well supported, but to what degree this method is of practical needs further justification. The whole model of the paper with the proposed positional encoding method is yet to be well stated. The link with the NTK of the proposed method is not evident to readers and may not be the truth as the authors mentioned.	2
The paper makes a contribution towards a more general positional encoding scheme on manifolds. There needs to be more clarification on the applicability of the method on arbitrary manifolds, as well as the computational cost.	3
"
Strength: 
1. This paper presents an interesting idea attempting to extend positional encoding to non-Euclidean manifolds.
2.   This paper provides experiments for three selected manifolds.

Weakness: 
The motivation of the proposed work is unclear (see comments above). Moreover, some further arguments are in order. 
Experiment provided only show that the selected base functions have similar properties of Euclidean encoding, but that doesn't explain why the selection of base functions can help extract details. Are they really better than  using sinusoids ? Is it really important to design base functions based on the principle? Take sinusoids as example, if we multiply a constant before sine bases and keep cosine bases still, we break the constant-norm property, but the network can still learn from these inputs and mitigate the spectral bias. The increase of performance is not very obvious compared with Euclidean encoding (0.5-1 PSNR increase). Experiments of how basis elements (such as max frequency) affect the performance is not found.

"	3
Overall, this is a good paper with solid analysis. It should be of interest to the area of Bayesian inference. But I think more practical validation should be addressed.	3
A great paper, well written. The topic is nice. Some things can be improved (taken a step forward) to make the paper truly valuable also for practical uses.	3
"First of all I am not an expert on SVGD thus I tend to be conservative in evaluating the paper. 
Overall I think the paper is written very well & I have enjoyed much going through it. The problem studied in the paper, the variance collapse of SVGD, seems to be important and interesting in my perspective. I think the authors have made sharp observation towards understanding this issue & how to mitigate it. The main pitfall of this work, in my perspective, is the theory part; as explained above, I think the presented theory has quite some gaps from the experimental findings. 

I am leaning to weak acceptance for now. Authors feedbacks are welcome as I may have missed some important points."	3
Overall, this paper is written clearly and easy to follow. It also has significant theoretical contributions about the SVGD dynamics under the proportion limit. Although I am a little confused about the motivation of introducing MMD-descent, this paper has more merits than its drawbacks.  	3
"- A major issue here is lack of consistency and adequate comparison with prior work, especially for Li et al., 2019, where the authors proposed and studied reward relabeling, multi-task distillation and use the triplet loss. It becomes a bit unclear how significant the technical contributions. This also affects the contribution on empirical results: it is hard to evaluate their significance without a proper comparison to prior methods. I recommend the authors try to design other analysis or ablations to showcase why the proposed design is a good choice. And to further investigate some of the interesting observations such as the performance drop issue after training BAIL for a long time. 
- Another major issue is the clarify of the paper. Authors should go through the paper slowly and refine the writing, this is more than just minor issues on grammar and typos, the writing in the paper should be very clear and make sense, any claims should be made with supportive evidence, etc. 
- The paper proposes some interesting ideas, and the multi-task batch RL setting is relatively not well explored, but due to the large number of issues, it's hard to believe the paper is ready for publication in its current state.  "	2
Recommending acceptance subject to convincing experimental analysis improvement and improved description of the novelty of hte approach	3
Although the paper showed that MBAIL consistently outperforms BAIL under the multi-task setting, I struggle to recommend acceptance because 1. The algorithm is closely tied to one specific offline RL algorithm, which limits its use cases 2. There could be more baseline comparisons in the experiments section 3. The main technical contribution is specific to tasks where they all share the same transition function, which further limits its use cases. 	1
This paper proposes two algorithms for the multi-batch RL problem, and experimental results demonstrate their advantage to some extent. However, the writing of this paper needs lots of improvements, the technical contribution is limited, and the final results are not solid enough to support the claims, I feel this work is incomplete and recommend rejection.	2
The paper is tackling an interesting problem -- explanation of graph. However, some results are a bit counter-intuitive. 	3
"The technical contribution of this paper is limited. The proposed method is somewhat incremental compared with the existing method PGExplainer.  The paper lacks rigorous empirical analysis, especially in terms of studying the interpretability aspect. 
"	3
I think this work is overall good, however, some technical details need to be clarified and the proposed model can only be necessarily applied to a limited range of explainers.	3
The main concerns are about the setup of the explainer and the evaluation.	2
"I think this work as merits.
I'm giving a weak reject because I would like to see more motivation for this work and how it could be adapted to use cases.
I'm currently keeping my confidence score somewhat low because the theoretical contribution could be significant enough to alleviate the need for thorough motivations/experiments.

======= POST REBUTTAL ==============

Thanks for all the explanations and clarification.
Some of my concerns have been resolved.
I'll increase my score to 6.
I will not go higher because I still think some experiments should be added in a realistic lifelong RL benchmark, i.e., where the tasks won't actually obey the theoretical task algebra. Upon acceptance, I still encourage the authors to add such an experiment.

Minor detail: please add the provided intuitions for Theorem 2 in the manuscript "	3
"This is a good paper with a sound method and good results. The author just need to work on a few points to award a clear acceptance.
Some claims about the efficiency of the algorithm with the number of goals need to be removed. A important step of the background is missing. Finally, some short discussion about limitations would be welcome."	2
Overall, I think the paper can be a nice contribution to the field of lifelong RL, however, due to my concerns detailed above, I am currently voting for “marginally above the acceptance threshold”. Out of all my concerns 1a, 2a, 2b and 3a matters the most and I am willing to raise my score to accept if they are properly addressed. If, in addition, the authors can address the remaining points, this will increase the confidence in my voting to accept. 	3
The targeted problem - a lack of multi-task and lifelong learning capabilities of most RL algorithms - is an extremely important one, as it resembles real-life applications a lot more closely. Additionally, the idea of finding a decomposition of multiple tasks via boolean logic is highly appealing, as it allows structural abstraction in RL, another important but underexplored question. Unfortunately, the paper's contributions are a little underwhelming. The logical decomposition is nice but fairly straightforward. The theorems provide an interesting, but also not groundbreaking insight. And finally, the experiments are unclear in execution, only on one domain, and therefore unconvincing. Overall I really like the idea, but the paper needs substantial work to become a full contribution.	2
This paper is fascinating and well-motivated.  However, a few notation and clarity issues made it difficult to follow Sections 2-3, and I am also concerned about the statistical significance of the authors’ claims in 4.1.	4
For the reasons described above, especially concerning clarity and novelty, I am recommending a weak reject at this stage. However, after getting a better understanding of the theoretical contributions, I may be willing to raise my score.	3
See above. 	2
This is an interesting paper that has both conceptual and technical novelties. However, I also see some technical issues in the paper, and I'd like to accept the paper if those issues are addressed properly.	4
The paper provides a simple and practical approach to obtain a clustering solution from some predictor labels. A rigorous analysis of the algorithm has been provided. I would recommend the paper to be accepted. For further improvement, the authors need to address, at least empirically, if not theoretically (1) robustness of the algorithm to different level of label error, i.e., the performance of the algorithm when the assumption fails (2) robustness of the algorithm to different $k$, the number of components. 	2
"This paper starts from an interesting loss family as PloyLoss, but the final version is quite simple and trivial. Considering both the experiments, motivation, and the final method, I would like to rank this paper as 5. I am positive about this paper, but more experiments and clarification are expected.

After rebuttal, most of my concerns are addressed. Thus, I raise the rank to 6."	3
I like this paper. The idea is new, and the discussion is deep. I have no comments on how to improve the paper. Although partial experimental improvements are marginal, I think the novelty is promised. It deserves to expose to the community to inspire future works.	3
"The paper is interesting, but it is unclear to me whether replacing losses with their polynomial expansions while bringing only small performance improvements to existing methods would suffice for a publication in ICLR. 

After rebuttal, most of my concerns have been addressed. I am therefore increasing my rating to 6."	2
In general, I am positive about the paper, though I need to be convinced about the significance of the results and the importance of the work.	3
Overall, this paper presents s simple, effective approach to improve NAR machine translation. Their claim is supported by their experiments. I still have some concern described above. 	2
The paper is well written, presents well motivated improvements for improving NAR models. The empirical evidence is convincing, and proper ablation experiments have been presented. I have provided some suggestions for experiments and technical corrections to the authors. I believe this paper presents interesting findings which will be relevant to the machine translation community. I recommend acceptance for the paper. 	3
"This paper shows interesting observation points to understand why the multi-modality problem occurs in non-autoregressive MT systems, and proposes a few ways to mitigate the issue. The modifications seem effective and improve both raw and distilled performances. More impressively, the gains are bigger in the raw setting. 

I also find that it is nice to remove the restriction of non-autoregressive models that rely heavily on autoregressive models."	4
Overall, although the paper presented impressive results, it missed some meaningful discussions, such as the importance of indistinguishability and why its correction loss works better than SMART. In addition, it needs to polish the draft.	2
While the paper is interesting and offers fresh point of view, I feel the paper lacks justification of important parts : such as why do we need a similarity between two NNs.	2
While the approach is novel and interesting, I think the experiments are not convincing enough and too hand-waving to definitely validate the procedure. Since the approach is purely experimental, I think the work is too preliminary for publication.	2
"The contribution to the proposal and validation of a more effective framework for comparing trained NNs is acknowledged. On the other hand, the novelty of the idea is low and its applicability is questionable.

[Post rebuttal comment] I appreciate the author's responses. The author's perception was clear to me, but I think it still needs to be improved with additional validation, so I did not change my grade."	2
The motivation and the problem studied in this paper is interesting. The authors present a novel and effective approach to represent neural networks in a topological way and propose a similarity measure using PD discretization, which is supported by the experimental results. However, the discussion part could be more focused on the discretization’s ability to separate the experiment groups, rather than sensitivity to parameter changes. 	3
I evaluated this submission primarily as a theory paper; in my opinion the current theoretical contribution is below the acceptance bar, for reasons listed above. I am happy to adjust my score if the authors can address my concerns. 	2
"The paper is not well-written and theoretical results are not well-stated. The paper also igonres standard results in the statistical results, bias-variance decomposition vs the four regimes and the non-parametric rates. The reviewer also find out that the author lack of knowledge in basic statistical machine learning theory. Reviewer suggests the author write down the model considered (random feature, neural tangent kernel regime NN?) rigorously and state all the notation and theorem in detail. 
The empirical experiment in the paper is interesting and meets the bar of a top conference, however the ""theory"" part is confusing, not novel and even something can be wrong."	3
Overall, the paper studies a very interesting phenomenon. It's quite interesting to see such power law with respect to width and data set sizes with the other being fixed. Such good combination of interesting phenomenon with theoretical explanation is a great fit for ICLR. Hence, an accept has been recommended.  However, authors need to clarify a few concerns.	3
Overall, I think the paper should be accepted provided that the authors make more of an effort to relate their results to learning. 	3
"It was not possible for me to fully assess the correctness of the claims. The ideas and claims are of interest for the area, however the present form of the presentation is below the bar of acceptance, in my opinion. 

[Following the authors' reply] I have read and appreciated some of the authors updates and clarifications, following their reply to my questions. Admittedly, the paper needed significant explanations from the authors. I have updated my score which is now less negative but still only marginally above the acceptance. "	3
"Overall this is a nice paper though I think there is still room for improvements for some parts. Some results are slightly weak and I don’t really see any application for most results. Perhaps the authors should discuss more about the application parts.
"	2
This paper introduces a specification of sign matrix representation to tournaments, with some interesting ideas, but the significance and improvements over Alon et al. are not made clear enough to warrant acceptance.	3
The paper is lacking in terms of performance on domain adaptation tasks as compared to state-of-the-art methods. Some more clarification about the method will increase the readability of the paper.	2
A uncertainty calibration method under domain shift with a distributionally robust learning framework that show competitive performance on comprehensive experiments but lacks enough novelty.	2
"The research question is important and useful but the writing of the paper may overclaim the usage of the proposed method. Also, it may be hard to conclude how the proposed method works under the current evidence. 
"	2
This paper derives a new scaling for predictive uncertainty in unsupervised domain adaptation. Motivated by Distributionally Robust Learning theory, the scaling, density ratios, are verified against human frequency scores. Results show better calibration under similar (slightly better) predictive performance. 	2
I am sorry to say that I have to reject the paper. The paper is still in its early stage and requires more careful presentation especially in the beginning to make the contribution more clear and crisp 	3
"Despite investigating the application of a promising set of techniques
to an interesting problem, this paper fails to sufficiently support
its claims. The main conclusion rests upon the correlation of a
proposed measure of change in deep nets during training with
validation set accuracy, but no effort has been made to control for
possible confounds that could explain such correlation
trivially. Moreover, multiple important passages in the paper are
extremely hard (or indeed impossible as far as I'm concerned) to
follow, and noticeable gaps are present in the references to related
literature.
"	1
"While I am very excited about topology-based approaches that aim to
understand the training or testing behaviour of neural networks,
I cannot endorse this paper for publication yet.

The current write-up is suffering from several issues, which need to be
rectified in a **major revision** before reaching the quality standards
of ICLR. These issues include:

1. Lack of clarity: an improved introduction to topological concepts is
   required and the proposed method needs to be compared more with
   existing topology-based approaches (in particular since it does not
   meet the requirements of a metric in the mathematical sense, an
   analysis of approximation guarantees is crucial).

2. Lack of experimental depth and delineation to existing work: since
   the express goal of this paper is to analyse generalisation
   properties of neural networks based on their topological properties,
   a comparison with existing work (Rieck et al., 2019b) and an improved
   experimental suite (containing previous work *and* non-topological
   baselines) is critical for corroborating the claims of this paper.

**Updated after rebuttal**: A lot has been discussed during this rebuttal period. I would strongly
recommend to pick up some of the suggestions of reviewers in order to improve the paper. A recent paper
by [Birdal et al.](https://papers.nips.cc/paper/2021/hash/35a12c43227f217207d4e06ffefe39d3-Abstract.html)
demonstrates how to successfully assess generalisation performance (disclaimer: I am *not* one of the authors
of said paper). I hope that this may serve as a partial inspiration."	3
Though I appreciate the ambitions of the work, I feel that the empirical results are, unfortunately, not convincing. The absence of theoretical results and the very strong similarities with the concurrent submitted paper diminishes it's impact as well. 	3
Introduction: without a motivation; Related work: not informative; Method: no detail & incomplete; Experiment: missing. Reference: bad formation. So I would recommend a rejection.	1
"1, too simple idea, even not a novel one
2, no experiments"	1
Overall, the paper critically lacks any experimental rigour so it is impossible to evaluate any of the technical claims made.	2
I would recommend to reject this paper as the theoretical novelty is insufficient, and there are no empirical results.	1
The paper investigates an interesting learning protocol, proposes an intuitive assisted learning framework, but lacks sufficient theoretical justification and empirical support for the significance of the solution.	2
The paper considers a novel problem setting and has a promising direction. The theoretical part can be enhanced.	3
The paper proposed a simple and straightforward assisted learning framework for a learner with limited and imbalanced data. It seems that the paper can be improved by presenting specific examples of scenarios that require the proposed framework and supplementing some parts in the experimental part.	2
"My score is mainly based on TRAKR not evaluated on harder classification tasks, lack of comparison with state-of-the-art DL models, and unclear technical novelty.

Post-revision summary
The authors addressed some of my comments. However, the motivation as written remains to be faster runtime for real world applications, but accuracy is lower than MLP and runtime is not that much shorter. A little surprising, but would expect the state-of-the-art to be some form of transformers as opposed to MLP. Given MLP provides higher performance, I will keep my score."	2
The comparison with other methods seems rather incomplete.	2
While the manuscript is well-written and the ideas presented are fairly straightforward, the authors need to address the primary competitor to TRAKR, deep nets, head-on. First, it is likely that deep nets are on par if not better-performing than TRAKR, which the authors readily admit.  However, the authors also need to perform analysis of inference speed with deep networks.  If deep nets can run almost as quickly as TRAKR, then this complicates a central pillar of the paper.  	3
Overall, I believe the paper needs a lot more work in order to be accepted. The authors reasoning for proposing a generative model that does not depend on ordering of is not well motivated and the contributions of this work are not strong. Also, the results of the experiments the authors have performed are far from convincing, specially because they have not properly compared their method to more relevant prior works. Finally, the paper is not written well.	1
"The paper proposes an interesting solution to the issue of formulating priors over set-structured latent variables. However, in its current state, it does not offer clear enough evidence that this solution is actually beneficial compared to previous approaches. In particular, its (potentially) beneficial properties should be more clearly demonstrated using quantitative metrics.

Post rebuttal: I have increased my score to a 6, as the authors have added additional results supporting the argument that LOMs improve the unconditional sample quality of the underlying VAE."	3
The overall ideas proposed are novel and a potentially useful contribution to the space of image VAEs. The model proposed is shown to work well on well-known multi-object datasets, and a large set of ablation studie disentangle the contributions of the different parts of their method. I'd like to see more discussions about the conceptual limitations of their ideas, as well as references to related work in finding canonical orders for set-based tasks. The authors should also address the clarity issues in the text and figures mentioned above.	3
This paper considers an important problem in object-centric scene representation, that is, modeling and inferring relationships among objects in a scene. However, both modeling and inference methods have heavy overlaps to existing works and, the main contribution of the paper, inserting a deterministic layer between scene-level random variable $s$ and slot-level random variables $z_{1:K}$ seems unable to block dependences among slots. The proposed method has not been compared to closely relevant methods, and the evaluation metrics for decomposition and reconstruction performance are also not fully investigated. The overall scene generation performance of the proposed method is inferior to GENESISv2.  	2
The paper introduces an interesting perspective on guaranteed convergence for DQN. However, this is not the first algorithm that is doing so, and the experimental results show improvement only in specific selected, somewhat unusual circumstances.	3
"The paper is excellently written. The method presented could be groundbreaking. However, it is not explored whether the proposed method is generally applicable (see ""Main Review"" for details)."	3
The noverlty and significance of the proposed algorithm is limited.	2
"Overall, I do not feel like this paper is above the bar for acceptance because the core idea is not terribly exciting and has a lot of technical issues. 
The author should rewrite the paper and provide all other details mention in the weakness section.

"	2
The manuscript is heavy on analysis and seems to reflect work-in-progress results rather than solid and generalizable findings.	3
While there is novelty in the proposed method,I find it quite limited. Moreover, at this point, I have several doubts regarding the quantitative evaluation, i.e. Sections 3 and 4, of the proposed method (please see my review). Perhaps it is a problem of clarity, but is something to be addressed if the manuscript is to be published as a solid piece of scientific work.	2
"A very nice idea probably a bit incremental.
An experimentation not sound in some aspects."	3
This paper proposed a novel target-label-agnostic reverse engineering method for reverse engineering in recovering trojaned triggers on clean images. he proposed diversity loss and topological prior significantly improved the performance in finding trojaned triggers and the quality of the found triggers compared to other existing methods. The proposed method is novel, the experiments were well conducted, and analysis was well performed. 	4
"1. The setup does not seem practical.
2. The comparison with existing works is unfair.
3. There is no comparison with a state-of-the-art approach.
4. The paper is not evaluated on adaptive attacks.
5. It is not clear about the contribution of different components.
6. The presentation of the paper needs improvements."	3
"This work introduces a novel approach for the detection of backdoor neural networks.
The performance of the approach seems to be promising.
However, there are some speculations around the use of topological loss for detection as the compactness of the triggers is not a necessity in backdoor attacks.
If the authors can elaborate on this assumption, I would be happy to increase my score."	3
This paper is well-written and proposes a pretty novel trigger hunting method. However, the limitation of this method is also very obvious, not good enough for real threats. I will increase my score if the authors can provide more insights under more general settings of trojan attacks except for only TrojAI benchmarks.	3
The paper has some limitation on the knowledge assumptions and need for relaxation, but the contribution is enough in my opinion.	3
This paper provides interesting contributions to the line of work on adversarial attacks. The paper contributes a novel attack method and shows that it outperforms the SOTA baselines. 	3
The paper is possibly on an interesting topic yet the problem studied in the paper is contrived and the results are of limited interest. 	2
In general, the paper is well-structured, and the problem studied here leads to a robust RL agent, which fuels the applicability of RL in real-world situations. However, some claims in the paper are not accurate. But these claims do not affect the main results. Also, there is still room to improve the writing.	3
Overall, the idea of this paper looks very promising, and the authors also achieve better theoretical results in terms of lower Jaccard estimation variance. However, the experiments and the related work are a lit weak. Such limitations justify my initial rating.	3
The method and theoretical analysis are interesting. However, the paper lacks a clear justification of the benefits of C-MINHASH and a through comparison with related works.	3
"This paper is well-written.
It needs to compare the proposed approach to the previous approaches. "	3
"Personally I like the paper and I hope that the paper will be accepted. ~However, I strongly feel that this paper should probably be merged with a separate paper by the same authors on reducing two permutations to one to tell a full story. I had a look at the arXiv version (https://arxiv.org/pdf/2109.04595.pdf) of the other paper and the main proof is just 4 pages. There is no compelling reason to split them into two papers. ~
"	4
"The idea in this paper is interesting, and seems promising for an ensemble-learning method interpolating between (or extrapolating from) a collection of neural network base model to a stronger combined meta-model. However, it is not presented or evaluated as such. Instead, this work tries to pose it as a way to learn a ""model manifold,"" and in this context the results are rather superficial and lack clear insights or benefits. Therefore, I would consider it as a borderline paper. It has some potential to be above the acceptance threshold, if it is refocused appropriately and compared to other ensemble methods, but this type of revision might not be sufficiently simple to complete within the tight review schedule for this conference."	2
"I find this work novel and interesting. However, I have concerns about the 2 main contributions of the paper, which result in a minor contribution of the paper to the community overall. Therefore, I tend to reject the paper in its current state. However, if I missed something which made these contributions more relevant, please let me know and I'll be more than happy increase my score.
"	4
"Overall, this paper presents an intriguing idea of exploring the space of learnt networks. The simplicity and generality of the approach are encouraging. The paper could be significantly improved with an extensive empirical evaluation and comparison, and potentially with formal guarantees.
"	2
I think that the approach is smart and potentially interesting, but the empirical evidence used to demonstrate its usefulness is not fully convincing. In all cases defining which  hidden states should be compared  is  easy (or even trivial, if the architecture is really the same), but in these conditions other approaches for comparing the representations can be used as well. 	3
This is a good initial attempt to attack the neural temporal logic programming. However, the steps in the proposal --- temporal compression, predicate modeling, composite event prediction, combinatorial inference, and rule reduction --- requires more clarity and refinement to reach a publication state. Overall, the proposal has little element of neural networks. To me it is more on matrix formulation of ILP problem. It is Okay to me but please consider renaming the paper title. 	2
The paper is clearly written and easy to follow. I like the idea around the proposed differentiable operators used to predict the temporal relations among atomic events. Also, the experimental results are interesting and promising. However, they are preliminary. I would like to see more in-depth analysis on the correcteness of the time intervals for atomic events, the scalability of the proposed strategy and a comparison against recent structured neural approaches for spatio-temporal reasoning. Furthermore, the authors should discuss and relate their work against existing literature in the field of video question answering.	3
Although there is some novelty in the proposed approach, the empirical results are too weak to validate the effectiveness of the approach. I do not think the paper in its current form is good enough to be published in ICLR.	2
Overall, I thought this is an interesting contribution to the ICLR conference. I do have some doubts about the completeness and validity of the experiments.	3
This paper deals with a decades old question in neural networks research, of how we can produce high-quality outputs using reconstruction errors.  Their solution is logical but I have some questions about the details.  Nonetheless I think it represents progress on this problem and should be accepted.  I also found the samples to be impressive.  I believe that this paper will have very high impact.  	3
"This paper discusses an interesting and long-standing topic with a perspective somewhat different from existing ones (i.e., whether information is perceptible), with statement supported by a two-stage VAE scheme. The contribution, both in insights or methods, however appears limited in terms of adding to existing knowledge. The paper can also strengthen discussion and comparison with related works that have examined the same topic.
"	2
While this paper studies an interesting problem in a well-presented way, I have doubts about both the novelty and the motivation of the proposed approach.	2
While I do find the proposed method interesting and thought-provoking, I don't think it's evaluated with sufficient rigour. I also find the rest of the story in the paper to not be sufficiently novel or well-connected to prior work. In the end, I consider the paper to be below the bar for acceptance.	3
Overall I find the paper interesting but in its current form it is more a workshop paper. Many important experiments are missing. The main claim is not justified and may be wrong. I encourage the authors to improve the paper, to better study the reasons that cause poor generation and to resubmit.	3
The paper reads well and has a very extensive demo page. I believe it furthers the state of the art in this area.	3
"Simple method suggested by the author to fix some known artifacts from GAN audio vocoders.
"	4
"Overall, I think this paper gives several insights to design a conditional waveform synthesis model by combining AR and non-AR TTS model.
However, I think it slightly lacks novelty in that there have been several models designing an autoregressive model with chunk-level output. Also, I think it is a little weak in supporting its claim and there is room for improving the writing. As a result, I will give a score of 5 for this work."	2
"Overall, I feel the paper may need some additional work to better connect different technical points, and provide discussions on the necessities of the sophisticated methodology. 

===================================
After reading the responses and other reviewers’ comments, I will keep my score. "	2
"The proposed framework is novel and addresses a relevant problem of FL.
The formulation of the paper seems ad-hoc with respect to the proposed application, as well as the related experiments. There are also  concerns on the privacy leaked during the sharing of the local representations."	2
The paper is a combination between dynamic graph neural networks and federated settings. The proposed method is a practical solution for learning a model in a distributed system with topology information. However, the alignment of federated settings is unconvincing, and the contribution from graph neural networks is also very limited. 	1
The paper considers a less addressed scenario where a datum consists of multiple parts, each of which belongs to a separate owner and proposes a local-model framework to perform “federated inference” in this scenario.  The main technical idea is the isolated learning of representation of local datasets and then aligning them to make the prediction. But this learning paradigm may heavily hurt the privacy of federated learning, since the new representations may leak the local data, and alignment of multiple representations may also lead to inference attack of these data. Furthermore, the validation and justification of the proposed methods are still not so mature for a top-tier conference. 	3
Overall, I think this work represents a solid contribution to the field of robustness certification and RL, against what some may consider a more practical adversary than in evasion attack settings. There are a number of subtle algorithmic and experimental points that I would like the authors to clarify before raising my score, such as the complexity of COPA-search, and a more in-depth analysis of robustness to poisoning critical / bottleneck states over non-bottleneck states. I feel like this is a key point to address as it will provide stronger evidence of the deltas between the different aggregation protocols.	4
Solid work with minor issues that can be fixed.	3
This paper studies an interesting and important topic, and the proposed method makes intuitive sense. However there are many flaws in terms of writing, algorithm assumption, and empirical results. So I do no think this paper can be accepted before a major modification is made.	2
Overall, I think the paper studied a very important and interesting problem, but the paper needs to be improved in writing, related work, and discussions on potential weakness of the COPA framework.	3
Though this paper provides some theoretical analysis for the LSR, the proposed TSLA builds on some guesses like “training one-hot label could be “easier” than training smoothed label”. Further, much more experiments should be added to prove its effectiveness.	2
For the label smoothing strategy, this paper theoretically analyzes the convergence behaviors and proposes the TSLA. Experiments on several small datasets demonstrate the effectiveness. More experiments are needed to verify its generalization.	3
This paper is overall an interesting paper. Although I still have some additional concerns regarding the threshold of $\delta$ in Theorem 3, and empirical aspects of $T_1$ and $\theta$ in the proposed TSLA. I am willing to increase the score if the authors well address my concerns.	3
The proposed theoretical explanation for LSR is meaningful, however, the experiment in this paper needs to be further enhanced. 	3
I feel like there is a plenty room of improvement. Some key aspects of PER is not discussed in the paper, I'm not convinced the proposed solution solved the issue, and the theory is not very technical either. Thus I don't recommend acceptance. 	2
I feel like the paper does not indicate (1) why we should build on PER (2) in my opinion, the algorithm is flawed and may not scale otherwise to problems where dynamics models are inaccurate (3) the empirical results are not convincing. So, I am going for a reject.	2
The paper provides strong motivation and empirical results supporting their new method Dyna-TD.  However, given their method is a simple extension of HC-Dyna and Dyna-Frequency from Pan et al, 2020, where they replace the hill-climbing objective with a TD-error objective, it seems important to include more analysis comparing and contrasting TD-Dyna to HC-Dyna, as well as a deeper understanding of Dyna-TD's learning dynamics. For these reasons, while I find the method itself promising, I recommend this paper as being marginally below the acceptance threshold.	3
"I recommend rejection of the paper.
In the review above, I have done my best to give the authors detailed info on why I believe the paper in its current state is too shallow. The idea of using a Langevin dynamic to draw states according to their TD error seems like an interesting basis to me but the current paper needs more work to be accepted for publication.
To summarize my main arguments:
- Lack of connection to the litterature on MDP solving and non-uniform sampling in SGD.
- No actual justification of why PER is (or not) sound.
- Not clear enough on what the contribution is.
- The algorithmic contribution is not studied enough, both theoretically and empirically."	2
"The paper is clear and interesting, but suffers from two major weaknesses:
1. It seems to me the big 'insight' is Turing completeness of the model, but this is (mostly) done in previous papers. Once this is established the remaining reductions are much less surprising.
2. I'm not sure the paper is a good fit for ICLR. It is mainly a complexity/computability paper where the models are related to ML. I'm skeptical that its insights are interesting for most of the ICLR audience  (If I'm wrong I'm  happy to be corrected)."	3
This paper develops an interesting point of view on machine learning and computability. The proofs seem to be correct. I support accepting this paper once it is revised to take into account concerns raised above.	3
I am not an expert in this area so I'm not sure to what extent the field was already aware of issues of computability. Superficially the connection between EC and the halting problem does seem quite straightforward, but if this was not observed or studied in any depth previously in the EBMs literature, I think the submission is a worthy conceptual contribution that should be accepted.	2
The paper considers an important problem, is technically sound (at least from what I can tell), and well-written. Although the practical implications are not immediately clear, I am convinced that it is actually relevant to understand the theoretical limitation of considering energy-based models with ever-increasing expressiveness. 	4
"This paper pushes forward work on large scale multi-task transfer learning, with a huge number and variety of tasks (107 for training) with many test suits and research questions explored. It is well written and convincing and I would like to see it published in ICLR. 
"	3
"In general, I think it is important to carefully study multi-task scaling in the post pre-training era. There are many papers in a similar vein recently and to be useful the paper should concentrate more on asking meaningful questions and rigorously analyzing the results to provide recommendations for future work. I hope the authors take the criticism constructively to improve their work further, concentrating on more analysis rather than more downstream benchmark results.
"	2
"The idea of leveraging multi-task learning for NLP has been introduced before as mentioned in the paper and not referred to as [3, 4]. It is great to revisit and extensively test this idea at scale at the paradigm of pre-training methods. It also offers potentials for future works on how existing labeled datasets can be used to further improve NLP models. 

[3] McCann, Bryan, et al. ""The natural language decathlon: Multitask learning as question answering."" arXiv preprint arXiv:1806.08730 (2018).

[4] Subramanian, Sandeep, et al. ""Learning general purpose distributed sentence representations via large scale multi-task learning."" arXiv preprint arXiv:1804.00079 (2018).
"	2
Overall, the paper fairly convincingly argues that mixing supervised and self-supervised signals during pre-training is extremely beneficial to downstream performance and sample-efficiency. However, there is limited methodological novelty and the benefits of scale and incorporating supervision may not be particularly surprising.	2
It is a bit hard to justify the significance as it is not an apples to apples comparison as both communication and accuracy are different. 	4
The paper is well-written and the proposed idea is novel. Discussion and comparison with related works are adequate. However, my major concern is the correctness of the per-parameter compression. If the authors can clarify this point, I am happy to further increase my score.	3
Overall, I like this paper and recommend it be accepted.	3
"I think the most interesting part of this paper is that it leverages the randomness in the sampling process to achieve differential privacy. My major concerns are that (1) the recovered model updates at the server side (even without clipping for privacy) are biased wrt the true model updates; (2) the algorithm doesn't support selecting multiple clients at each round, and (3) experiments can be improved.


=== update ===

After reading the authors' responses, some of my concerns have been addressed. Therefore, I increase my score to 6."	3
The proposed approach of combining OOD detection and classifier through joint training is an interesting approach. However, some parts of the paper lack clarity and thus, it is hard to evaluate the contributions. Authors are encouraged to address the comments and I will be willing to reconsider my decision.	2
"I recommend weak rejection for this review. I believe the evaluation is somewhat questionable and also needs some rigorous discussion with related works. I still believe the idea is sensible, so I carefully request the authors to respond to my weakness part during the rebuttal.

-------
**POST REBUTTAL:** After the response, I am slightly above the threshold as the proposed method seems to be a scalable work as it stabilizes the hardness of training robust out-of-distribution (OOD) detector.
"	2
I vote to accept this paper because its contributions are clear, novel, and significant.	3
"Although this paper proposes a principled method ProoD for robust OOD detection and has some interesting theoretical results, the experiments conducted are not enough to show the effectiveness of the proposed method. As I mentioned, the authors should be more careful in building the baselines and evaluating the methods. Also, they should give more details about the experiments like the attack objectives used. Thus, I think this paper is not ready for publication. 


***[Post Rebuttal]***

I think the proposed method ProoD is not very novel since it simply combines previous techniques, and the performance of ProoD is not stable across different in-distribution and OOD datasets. Also, in practice, it might be hard to use ProoD since under the False Positive Rate at 95% true positive rate metric, ProoD doesn't have good performance (it might be hard to select a suitable threshold for ProoD). Thus, I keep my original score and think the paper is not ready for publication."	2
"Currently, it appears to us that the proposed solution does not provide adversarial robustness on the **in-distribution data**. If this is the case, we believe that this drawback makes the method much less valuable than previously published competing approaches. Suppose the authors clarify the text and prove that the proposed solution, like ACET and GOOD, provides adversarial robustness on the **in-distribution data**. In that case, we may improve our score mainly if the other concerns are also addressed. 
"	3
Mostly, I consider the contribution made in this paper to be meaningful to the clustering community, but it could be improved (at least in writing). It has a valid theoretical framework, but as a review from the broader clustering community, I find it hard to judge how significant these findings are.	3
This paper improves upon the literature for the k-medians clustering problem in the general metric setting as well as in the differentially private case. The approximation guarantees improve the best known results in this case. The experiments demonstrate the validity of the theoretical contributions as well.	3
This paper considers initialization methods for $k$-median clustering in both the standard setting and the differentially private setting.  The paper gives theoretical bounds for their methods in both settings and backs this up with an empirical study.  Given the misleading presentation of some of the results, a lack of discussion/comparison to prior work on k-median in tree metrics, and a lack of running time/iteration count comparison in the experiments, I am not okay with accepting this paper unless these points are addressed.	3
"In summary, the paper neither introduces new techniques nor obtains significant improvement over past results. My suggestion for improving the paper would be to add a discussion on why the suggested algorithm should be the right ""initialisation"" algorithm."	2
"Overall, whereas the ideas are easy to follow and the illustrations are well done and the experimental results demonstrate a match between boundary graph GNN outputs and compute heavy simulations (Figs 4 - 6) the contribution of this paper in terms of new methods or representations or representation learning are modest with regard to where the bar is, in my view, for ICLR. I do not doubt that the method has practical value, and clearly a lot of work went into organizing the experiments, conducting them, and putting together the results. Perhaps this paper would be a better fit to a simulation conference or one with a more engineering oriented focus. The idea to dynamically alter graph structure - and as far as I could tell there are not too many other methodological innovations - is a bit thin with regard to novel content. I'm also puzzled by the choice of title - the choice ""3D simulations"" appears to be quite general, and it is not adequately supported by the paper's content (the focus on granular flows)."	2
Based on the strengths and weaknesses of the paper listed above, I tend to take a slightly negative stance.	2
"The paper proposal for how to create an input graph for a particle simulation interacting with complex geometries is plausible. However, considering how large overlap with prior work there is, except for the ""graph encoder"" part of the model, I am not sure if this contribution on its own is large enough to meet the ICLR bar. Specifically, I am not sure the paper will bring many new insights to the average ICLR attendee, beyond seeing it as an implementation detail for existing models. In this sense I am borderline between weak accept and weak reject.

I would be much more inclined towards a stronger accept if the focus was a bit more applied:
1. Here's an actual application, with realistic data, that engineers really care about, and also here's a specific set of generalization settings that engineers really care about.
2. Here's how you adapt an existing method (e.g. one of Li et al, Ummenhofer et al, Sanchez-Gonzalez et al, Pfaff et al.) to this problem including changes to the encoder to avoid wasting compute on large regions consisting only of boundaries, which ideally really matters if you want to get good running times in that application.
3. Here's a detailed study of why engineers in those fields should prefer the learned model over other state of the art non-learned model. Including plausibility of generating a training dataset of the required size, limits of generalization, running time, trade offs, etc.

Of course, it may be that a different venue could be more appropriate for a paper like that. Although as an ML conference attendee, I would personally be interested on the specifics of any evidence of the promise of end-to-end learned models replacing traditional simulators in any domain."	2
"Pros: 
1.- A very interesting contribution to GNN applied to 3D simulation.
2.- It could be a first step towards using GNNs in complex mechanical and biomechanical simulation.
3.- The increase in speed without affecting too much the precision is a very interesting contribution.

Cons:
1.- The model must be explained better for easier understanding (check the main review).
2.- It is not clear the differences between the base GNN model (without the dynamic boundary nodes/edges addition) and the one proposed by Sanchez-Gonzalez 2020. (Maybe because cons 1).
3.- Lack the comparison with other similar approximations."	2
"In general, I am a little bit concerned about the significancy of the paper's contributions. The method and theory part shows a lot of similarity with Tripuraneni et al. (2020). However, bringing it and the analysis to zero shot domain adaptation is valuable in my opinion. 

"	2
I have not found flaws in the proofs or reasoning of the paper, and find the theoretical contributions particularly interesting. While I feel the experimental part is less impressive, I feel that this paper deserves acceptance. As noted in my confidence score, I am not overconfident about this, and would recommend weighing my opinion accordingly.	4
Unless I see results on more challenging datasets such as WILDS, I will remain unconvinced of the usefulness of this work. It is not just the number of source domains T that matter. 	2
Although I have some doubts about the applicability of the method to practical applications, I don't see this as a reason for rejection. Moreover, the paper definitely has significant novelty and is technically sound. I would like to see an improved experimental section to increase my score.	3
Despite limited novelty and significance, I found the work interesting and recommend acceptance.	2
It's an interesting study that aims to propose a solution to a common challenge in the visual transformer model. However, I can't support its acceptance considering the presented results. 	3
My main concern is about the insufficient experiments. Please see the above comments!	3
I think this paper proposes an interesting conclusion from its derivation of the theoretical bound on self-attention modules. The amount of contribution on the theoretical part seems substantial. But the verification part is weak. I don't feel I am confident to eat the takeaway from what I see in the experiment section.	3
"The paper presents a theoretical analysis of the inductive bias of self-attention models, showing that the transformer architecture can learn sparse functions with some modifications. 

"	4
I like the contribution of generalization bounds for Attention and Transformer models. But there are some weaknesses. Therefore, my recommendation is marginal above the borderline.	3
This paper is well-written, correct, and provides results that are significant and advance our understanding of attention mechanisms. I recommend publication. The reason I do not give it the highest score is because of two slight weaknesses: (1) lack of interpretation of the obtained bounds with respect to choices of Norm operation (2) Simplistic and only mildly convincing experiments.	4
Given the limited method novelty and empirical findings, I'd recommend a reject.	2
The author does a great job explaining the idea, concepts, procedures and experiments.	3
"My research experience is on design hashing for computer vision tasks and I am familiar with work on LSH.
The paper is overall well organized. The technical contribution is the derivation of the ranking efficiency measurement, which is lower than the bar of ICLR. There are some flaws as identified in the weaknesses."	2
In summary, this work looks interesting, but the concern about the contribution (details in Cons 1,2,3) justifies my current rating. I am glad to increase my rating if the authors can solve my questions. Thanks. 	2
"This paper try to provide a defense against backdoor attacks through outlier detection. I find the direction of providing provable guarantees for outlier detection is interesting. However, the assumptions in the paper are very strong which makes the threat model of the paper narrow. These assumptions are not just about the data distribution, rather, they are about the properties of the distribution generated by adversary that can be easily violated by adversary.

I find the presentation of paper a bit confusing. The introduction talks about backdoor attacks but the technical parts of the paper has nothing to do with backdoor attacks.

I also have concerns about some of technical steps of the proof that I mentioned in my comments. I also have a hard time understanding the main theorem that there seems to be an undefined term (delta). Also, I don't see any connection to alpha in alpha-compatibility, which is one of the assumptions of the theorem.

"	2
I will vote for accepting this paper. The idea of separating training set into self-expanding (homogeneous) sets is novel and interesting. The use of a set of weak learners and proposed boosting algorithm are technically sound. The experimental part demonstrates the performance of proposed defense over other existing work.	4
I feel the major shortcoming is the lack of an adaptive adversary, possibly overestimating the efficacy of the overall defense and giving a false sense of security. 	3
Good attempt, but the assumptions are too strong, making the entire theoretical analysis less convincing, problematic in several places, cherry-picked results,  missing many details about the overall training procedure, strategy for hyper-parameter choosing, missing experiments on many other attacks and datasets (only experiment with the simplest BadNets attack and CIFAR-10 datasets is given). Overall, I don't believe the proposed defense approach is good or simple enough to be useful in practice. 	2
I like this paper. It discusses the weakness of some noisy-label learning methods. The paper also conducts a solid experiment (except missing one critical comparison -- MentorMix). MentorMix is also a sample selection-based approach. It has demonstrated superior performance in dealing with the noisy labels. Because MentorMix uses mixup to enrich data, it should be robust against data imbalance. I would like to see a comparison of MentorMix and the proposed method. 	3
The authors propose a novel algorithm, justified theoretically and validated empirically to improve the sample selection in the case of noisy labels training.  	4
The proposed method is a novel method concerning an interesting yet under studied direction with a potentially significant impact to many applications (e.g., worst-group generalization, fairness), which the paper does a good job brining attention to. Results are overall promising, in particular showing very good results dealing with label noise + extreme class imbalance (Figure 2). Some concerns should be addressed during open discussion with authors.	3
"From my perspective, this work studies an important problem in previous sample selection criteria in learning with noisy labels. And the proposed method is clearly explained and theoretical analyses have been provided. Sufficient experiment results validate the effectiveness of this approach, although the results in semi-supervised learning seem not very convincing. Overall, I tend to accept this paper.


################# After rebuttal ##################\
Thanks for the responses. I still tend to accept this paper and keep my score."	3
Although some minor issues are better to be further addressed, CNLCU is indeed novel and technically sound. The experimental results show the potential superior performance of CNLCU. Thereby, I rate this paper with marginally above the acceptance threshold.	4
While this work is an interesting way to explore the power of large pre-trained language models, I am not sure it is compared properly to methods (also starting with no parallel data) that may have access to the same kind of datasets used to train these language models. I am also not sure how this work compares to supervised and/or semi-supervised techniques using the same model sizes, or what is the state of the art for using transformer LMs with prompting for NMT (where parallel data is available). Discussion of these points would be enlightening and may lead to updating my recommendation.	2
"- The paper employed huge LM/prompting/backtranslation techniques to construct bidirectional translation models. The technique of model construction seems remarkable, and there is some concern about data.
- The paper is somewhat hard to read due to syntactic/semantic issues, and I guess I possibly drops some context to assess the paper correctly.
- I am not fully familiar with recent advances in so-called ""unsupervised translation"" and huge LMs, and I didn't judge if the proposed method is novel or not, and not provided comparison with existing studies."	3
This paper is easy to follow, and the idea is very interesting and novel. The proposed method achieves the new start-of-the-art unsupervised translation performance on the WMT14 English-French benchmark. It is better to conduct additional experiments on other language pairs to make the experiment results more reliable.	3
"In summary, I don’t believe this work in its current shape is of the adequate quality to be published at ICLR. The biggest flaw is that it is unclear whether anything presented in the work is truly novel or unknown to the machine translation community. The work can be jointly summarized as “language modeling can yield an initial translation model whose performance can boosted tremendously by back-translation”, but no single aspect of this is new, nor is the success of the combination of the various pieces at all surprising to anyone who has been following the unsupervised translation literature over the past two years.

Coupled with the limitations of only studying a single non-English language, no low-resource language pairs, as well no detailed discussion/experiments on the few aspects which distinguish it from other works (e.g. the overwhelming English-centric bias of GPT-3), I cannot recommend acceptance. 
"	1
The paper is well written, the idea is solid and novel, supported with massive and strong experimental results. Overall, the is a strong submission and I would recommend accepting the paper. 	4
"Overall, I like the idea of Perceiver IO considering it is a good way of modeling arbitrary numbers of input and output tokens. Based on the experimental results on various downstream tasks, it indeed shows some superiorities over the strong baselines. The experiments are solid enough to justify the main claim. However, as discussed above, my main question is that whether we want to develop a unified read-process-write architecture for all these tasks. What are the main merits of this act? I will hold my score to the borderline and wait for more feedback from the authors.

[Post-rebuttal comments]

After reading the authors' feedbacks to all reviewers including myself, I think they had addressed most of the concerns and questions. All reviewers think this work is novel and appreciate its solid contributions. As such, I raised my rating to ""accept"", and look forward to the furture work of applying it to learn from multiple tasks."	3
"This paper provides very a strong empirical contribution to a unified architecture for various input domains. Not only showing flexibility, but Perceiver IO also shows powerful performances in various domains. Although I have a small concerns that ImageNet results (a reader can mislead the results) I think this paper is a good paper and indeed recommend acceptance.

--------------------
Post-rebuttal comments.

The authors addressed my concerns very well in the rebuttal. I will keep my decision as ""8: accept, good paper""."	3
Perceiver IO extends the Perceiver architecture to scale to arbitrary outputs and output lengths. The proposed method is evaluated on a wide variety of tasks and achieves good results comparable to SOTA in most of them. I recommend the current score, with minor reservations over added novelty over Perceiver and also I would encourage the authors to include more analysis and comparison with other SOTA multimodal models.	3
"The paper seeks to develop invariance and equivariant graph neural
 networks. The proposed approach makes strong assumptions. As such, it
 has very limited applicability. The evaluation is also very limited."	2
Overall, there are certain merits for the proposed architectures, but both the originality and the experimental significance are insufficient. I initially suggest weak rejection. 	2
The paper is well written and the proposed solution is simple and computationally efficient. The claims are sound and the empirical evaluation is partly adequate but partly needs work.	3
Overall, I think the current version of this paper is slightly above the acceptance threshold since its motivation and proposed methods are technically sound and convincing.	3
The dataset and metrics presented by the authors are valuable contributions, but the paper has significant problems with framing, presentation, quantitative evaluation, and language/formatting. Due to these quality deficiencies, I cannot recommend acceptance of this paper in its current form.	2
As I mentioned in my review, this is a great paper proposing a new dataset that is well constructed and evaluated and is high-resolution. However, I don't feel that ICLR is the right venue for this work. 	3
This paper should be rejected. Its main contribution is the dataset, RainNet, and benchmark results on RainNet. However the paper does not adequately describe the dataset, and the benchmark results are not reported with measures of uncertainty, which makes comparisons impossible.	3
Overall well written. As the authors mention Ml-readiness; its also important to think about how will an actual domain scientist use this work. This is where evaluating downstream applications and understanding uncertainty in the results is crucial. This part is missing, other than that its a clear and concise approach. 	3
This paper develops a data-driven offline optimization for identifying good hardware accelerator designs, and conducts sufficient ablation studies to validate their methods, while the evaluation against SOTA methods can be improved.	2
This work presents an effective framework PRIME to tackle the challenges of automating hardware design optimization. Though it seems to be an incremental medication of previously published efforts, the proposed method is clear and useful. The paper could be stronger with a better analysis as stated in the Main Review.	3
"The paper proposes an interesting approach to offline design space exploration, and demonstrates that it performs better than online approaches. Though I am not totally satisfied with the evaluation, it is a relatively low lift to bring it up to par with existing literature in the space. A more thorough evaluation would make more a much more compelling paper, but in my view this is not necessary for acceptance.

Weak accept, conditioned on the authors providing more details about the evaluation (specifically, all of the questions marked with (*) above). I would be willing to raise my score further if the authors address some of my other questions (evaluating the sensitivity of the approach to the validation set, discussion the tractability of optimizing with infeasible points, evaluating the amount of compute required to train and evaluate the surrogate, etc).
"	2
The paper is clear, describes its problem and solution well, clearly showcases its results across a variety of tests, and generally succeeds at convincing the reader that the approach works as advertised. There is room for improving the 'why', but the paper is solid nonetheless.	3
I am tending to a weak accept of this paper given two outstanding concerns described above. These concerns are based on extensive discussions between reviewers in a previous submission cycle, and I would like to better understand how the authors built on these remarks before I adjust my score.	3
Overall this is a strong submission. It clearly presents the problem with prior approaches and proposes a solution, and proves it indeed corrects the issue. However, because I am not as familiar with the specifics of ODE-RNN and could not verify the correctness of the proofs, I am not fully confident in my score.	4
While there are some relevant and potentially interesting numerical results, the theoretical results lack clarity and precision, and thus I cannot recommend publication in the current form.	2
This analysis is only of real interest if it is solving an issue not already solved by using an LSTM/GRU gate. If this is the case and can be explained and evidenced with experiment by the authors in the paper then I would opt for accept, for now I am leaving my review as a weak reject.  	3
The paper combines two well-known models, the math. sect. lacks a bit precision. Empirically, however, the model seems to perform superbly on irregular time series.	2
"**Strengths:**
1. Good problem setup
2. Interesting idea with good scoping 
3. Impressive experimentation suite 

**Critiques:**
*Content:*
1. The experimentation section mainly convinces me that this architecture is good for accuracy and convergence speed on sparse inputs, not that it's generally advantageous - but the latter seems to be the claim of the paper, and the former is less surprising given the nature of the architecture (though still interesting - it's compelling to see that the architecture works). 
2. The experimentation on general improvement is a bit weak. 

*Presentation*
1. Formulation is clear, but it's presented piecemeal in long paragraphs that make it hard to parse and put together. Since the formulation is simple (a good, elegant thing), it's frustrating to spend a lot of time hunting down all the pieces.
2. Excellent thoroughness in testbeds, but the results don't have to be presented in such a process-driven way. They can and should be claims/driven. 
3. The figures need to be more readable 
4. ""Specialist""/""neural module"" distinction is unclear (see main review)"	3
"The paper proposes an implementation of an interesting theory from cognitive science for more efficient information processing in networks, by making connections between entities sparser. While there is extensive evaluation on environments, most of these involve processing a small portion of the input. The paper will be made a lot stronger if the approach can be shown to scale to larger more unstructured datasets where transformers are known to work well. 
"	4
"The paper proposes a novel method for coordination between neural modules which is well motivated. The experiments are comprehensive although some details are missing. Addition of multi-agent tasks would make the paper stronger.

Edit after author response:
I read the author response and I would like to maintain my positive rating. "	4
Paper proposes a novel approach which produces commendable experimental results across a wide range of tasks in vision, language and reasoning. The impact of this work is very significant and is of high relevance to the community. While I am not entirely familiar with the cited prior work on key-query-value attention mechanism which forms the backbone of the proposed approach, the entire paper is very well-written and comprehensible. 	4
"This paper is well written, and clearly demonstrates the advantages of sparse training with rewiring, gradient updates for non-participating weights, and an induced exploitation step. The paper also includes very thorough and useful empirical results on sparse training and through ablation experiments, but lack comparisons to similar methods. Unfortunately, I currently recommend weak rejection since I don't believe that there are quite enough significant novel contributions. While the experiments and ablations shed some further light into how to effectively use these methods and provides some better understanding, this space and the concepts have been thoroughly explored in previous works.


===============================================================
Post-rebuttal
I thank the authors for their clarifications in the comments and changes in the paper. I believe much of it is clearer now. Due to this, I will be raising my recommendation to marginally below the acceptance threshold. Still believe that as other reviewers have mentioned, the paper would be significantly stronger if they compared their recommendations more directly with other recent work in the space since they claim to be providing extensive hyperparameter recommendations and comprehensive recommendations for how to train sparse models.


There are also minor changes which would make the figures much clearer. Figure 3 should include their method with the combination of  Fix, Reset, and Regularization. The trend with rewiring and resetting steps in Figure 2 would also be clearer if the graphs were slightly extended to demonstrate at what frequency resetting and rewiring is required."	2
"I think this is interesting research and highlights a general and contemporary challenge when training deep neural networks. The problem is well motivated and the implementation well described. The experiments are extensive. however, as this is not compared against any other model in the field I cannot assess whether this approach is truly useful. I would be able to change my mind if the authors provide such results, or argue why they don't provide it. Also, please make your code available in an anonymous repository, and publicly available at the end of the review session. If you have the compute, please provide confidence intervals on your results. Also, please elaborate on what it means for weights to ""decorrelate"". E.g. why is the threshold important."	2
As mentioned above the experimental results provided at the end of the paper (one of the most extensive set of experiments I have seen in a paper), would be a strong contribution alone. However, authors use half of the paper focusing on explaining why more connections are needed for better results, which I find not very novel and a bit confusing. Authors say that pruned connections are used in training for exploration and search very vaguely; which doesn't provide any new insights on the existing understanding of the training of sparse neural networks. I am willing to increase my score if my concerns are addressed and looking forward to read the next version of this work.	2
It is an interesting paper and the experiments are strong for me.	4
The framework is potentially interesting, but the experiments in the paper do not convince me. It would be helpful to see more difficult problems, as well as multilevel problems with n > 2. Furthermore, there are a lot of details on hyperparameter tuning that is missing, leaving the question open whether the stated performance improvements are due to the hyperparameters or the MPINN idea.	2
This paper introduces a new multilevel PINN for learning solutions of PDEs. The presented ideas are novel and of interest to the scientific machine learning community. One shortcoming of this paper is that the experiments appear to be too preliminary, i.e., no benchmarks are provided and the models are not carefully fine-tuned. This makes it difficult to judge the advantage of the proposed method as compared to other state-of-the-art PINNs. At this point I feel that the paper is slightly below the acceptance threshold, but I am willing to change my score if the authors can address my questions above. 	3
While there are several concerns and weak evidence to support the main claims, the idea is very novel and significant. I infer from the method and its idea that the additional evidence to support the main claims will come by sooner or later, either by this paper or other authors. There are so many incremental papers without any originality that have been published just because of large-scale experiments for demonstrations. In contrast, this paper has weak demonstrations but a good original idea. Whereas both types of papers have their own merits, I think that the latter type contributes to our community, science, and academia in a longer-term. Accordingly, I recommend this paper for acceptance, subject to minor revisions. 	4
"This paper proposed the multilevel physics informed neural networks (MPINNs), which is inspired by classical multigrid methods for the solution of linear systems arising from the discretization of PDEs. It showed some interesting results, but those results are lacking of explanation in depth. The authors are mostly just stating the observations, which is not enough to meet the standards of a good paper. 

In summary, I would not recommend its publication in ICLR until all my comments are addressed.
"	3
"There is no supporting motivation for this work.  The paper makes claims of debiasing ""beauty"" predictions. These claims are not supported. Two ethnicities are used, which is inherently biased. The hypotheses in the paper offer no new insight as the paper cites works that have shown similar before. If it is known the hypothesis is true, there is no need to test it."	1
The main contributions of this paper are AestheticNet and a new approach to bias-free machine learning tools. The results look sound but some details are not elaborated (See the main review for details). I think this paper is marginally above the acceptance threshold.	3
I'm rejecting the paper because of breach of anonymity. Moreover, the work lacks novelty. Please see 'ethics concern'  for more details.	1
Novelty is unclear. The paper just confirms existing findings and does not introduce any novel method.	1
"
- The paper is well written and easy to read.
- Theoretical results need clarification, while some assumptions need to be relaxed.
- Code for reproducing the results is provided together with links to publicly available real data sets used in the manuscript.
"	2
"While dimension reduction in hierarchical forecasting is an important problem, the paper lacks clarity and justifications for all the design choices. Major references are missing (dimension reduction for multivariate time series, probabilistic hierarchical forecasting, etc). The experimental setup is also questionable (loss function, etc). As a result, I do not think the paper is ready for publication.
"	3
"The paper is well written and gives great introduction for the subject. The 
suggested method’s strength is in its easy to implement building blocks, the 
ability to take as into account information from all hierarchy and to scale well 
for larger hierarchies, though it could be better if the issue of dealing with 
new time steps would be addressed. The datasets chosen for the 
experiments are showing good results against relevant baseline, although 
there are few more relevant methods that were not included in the baseline."	3
"Overall, I tend towards accept.  The work appears to be sound and the method is well-motivated and well-designed.  I feel the extension from MDP Homomorphic Networks is not so large.  I believe the symmetries considered here may also be essentially considered as symmetries in the setting of van der Pol, 2020.  I feel the experiments are also somewhat limited.  They consider only the group $C_4$, compare only to versions of the proposed model, and the domains seem a bit contrived.  It is thus hard to evaluate how useful the method would be in practice.      
"	3
"This paper is not well-written and very confusing. Refinement is needed.
"	2
Overall, the paper introduces an interesting observation on certain real-world MMDP tasks and proposes a novel network structure that performs well under two different tasks, one with real-world data and the other with simulations.	3
"This paper is in general well written and easy to follow. The method is well motivated. My major concern is that the paper lacks comparisons to existing algorithms that are either permutation invariant or not. And the information of training and execution is missing. 
"	4
Solid work on orthogonality of convolution, though there seem to be some overclaiming / imprecise statements in the intro/abstract that may be misleading. 	4
The proposed method achieves exact orthogonal convolutional layers through re-parametrization. The method is theoretically grounded and easy to understand. Numerical proofs are provided to show exact orthogonality is achieved by composing a sequence of learnable convolutions. 	3
The proposed framework is nice, and the initial results are promising. However, there are various unclear parts in the paper. In addition, some of the claims are not verified and experimental analyses are limited. Therefore, the paper should be improved with additional analyses and in detail revision for clear acceptance.	2
The paper is well-presented overall. However, better positioning of the work, and more convincing experimental results are needed.	3
As mentioned above, despite the contributions, the writing (if not the method itself) is often confusing. I would suggest the authors address these ambiguities in both the author response and the revised version.	3
Since the paper primarily focuses on the application of a learned sketch to an iteratively solved LS problem (both components well established elsewhere) the contribution seems marginal. I believe the paper is marginally below the acceptance threshold as is. The paper can be improved by addressing questions and comments above, in particular, evidence that the method is applicable to more general convex problems and a more comprehensive comparison for total time to solve compared to naive sketching.	2
The core idea of learning sketch matrices is interesting. The theory that provides a potential justification for why learned heavy row sketching might improve on the oblivious CountSketch is welcome. However, the “learned oracle” for the heavy row sketch seems very easy to break: Simply permuting rows would mean that it has to be relearned. Overall, it’s not clear how useful the learned sketch would be in practice for that reason. The experiments are all done on very small problems that don’t need sketching. Parts of the paper are also unclear as outlined above. For these reasons, I think it’s below the acceptance threshold. 	2
I feel this paper is well thought out, and the experimental results are highly interesting. However, I also feel that the studies taken here (adaptively select rows) can be taken one step deeper: in particular, it would be very interesting to see theoretical analyses of why such learned sketching give improved results (over oblivious sketches).	2
Two main concerns: limited technical novelty and limited value for neuroimaging. My recommendation is based on these limitations.	1
This paper presents a valuable methodological contribution to multi-subject fMRI data analyses. The merits of the proposed technique are well supported by the experiments and results. 	3
"Overall I think this is a very interesting and useful idea in the convergence of the fields of neuroimaging and learning representation. However, the lack of important experiments and explanations make me recommend only marginally above the acceptance threshold. I believe my questions can be mostly (if not all) addressed during a rebuttal process, after which I’d be happy to change my score as I think overall this should be shared among the ICLR community.

------------------ EDITED AFTER REBUTTAL PERIOD:
I followed the authors answers to my questions and other reviewer's questions. Although the authors didn't address all my questions (for example my comment titled ""Further discussion"") nor they uploaded a new pdf version to check corrections, I believe my main concerns were clarified. Assuming the authors introduce the written clarifications in the paper, I'm changing my review recommendation in two points:
- Changing ""Correctness"" from 2 to 3.
- Changing my ""Recommendation"" from 6 to 8."	3
I think this is a well-motivated approach and the early parts of the paper are a pleasure to read: it was easy to understand what each piece of the contribution is meant to do, and its motivation and relation to past work. When it comes to the empirical evaluation, however, the paper runs into a common problem with noisy fMRI data, which is that it's hard to convincingly beat linear methods, and it then overstates (to my read) the strength of its empirical contributions. The performance gains are modest and nonlinear functional alignment methods (e.g. from the hyperalignment family such as KHA/DHA) and more advanced linear methods (such as RSRM and MN-SRM) are not included in the comparison. The structure of the latter portions of the paper suffers as well: while the specific modeling choices are well-motivated, the experiments do not fully probe the novel contributions. As a result, while I think this line overall is worth pursuing, I would like to see stronger empirical results for this work to be a successful ICLR submission. 	3
"My recommendation (reject) is based on
- a missing justification of the architecture design
- a number of missing important details about the fitting procedure
- a poor quality in the reporting of results"	3
Although the proposed method builds mainly on top of previous techniques, the paper presents some interesting contributions. However, it is not clear how this new representation will work with noise, and I believe it would be beneficial to clarify this. However, if the other reviewers think the paper is ready to be accepted, I will not be against it.	2
The paper proposed a new method for implicit shape representation. The authors have done adequate experiments to show the effectiveness and transferability of the proposed method. However the method is an intuitive combination of traditional displacement mapping method and SIREN, lowering the novelty of this method slightly.	3
The proposed implicit displacement fields have the ability to represent detailed geometries in relatively small network sizes. Experiments are done thoroughly to demonstrate the effectiveness of each component of the method.  If shapes are pre-aligned properly, the implicit displacement also enables the shape manipulation application. Despite having some uncertainties (see weaknesses), the reviewer holds a positive opinion on this paper.	3
"While neither the SIREN architecutre nor the displacement field is a novel contribution, their combination is not trivial and shows exceptional results in terms of quality and model size. In addition, the paper presents some elegant solutions for efficiently evaluating the loss in this new setting, and using a feature based UV mapping that can be transferred to similar shapes. 
I think these are sufficient for publication. The quantitative evaluation is a bit lacking, and the final version of this paper should include an exact list of shapes that were used in the evaluation. op"	3
"Overall, the paper raises and simultaneously addresses an interesting
problem for RL algorithms that employ function approximation. The paper
is somewhat held back by the empirical analysis, specifically in the
lack of statistical significance in several of their experiments. I am
also not sure of the correctness of the intuition that the dot-products
result in aliasing in reinforcement learning. The paper is still above
the acceptance threshold, but I hope that my comments below contribute
to improving the paper.


Edit: After discussion with the author and reading the back-and-forth between the authors and reviewer 5pgu, I have increased my score from 5 -> 8.

"	4
Paper that investigates a simple regularization technique for off-policy value function learning in deep RL. The idea is quite well motivated by showing a few insightful values through learning (not only the score). The empirical results are quite convincing, even though they could be more complete.	3
The paper looks theoretically sound some questions remain regarding the experimental efficacy of the proposed regularizer claimed by the authors.	3
This is a good paper which studies an important problem from both empirical and theoretical aspects and brings in sufficient novel stuff. I appreciate the contributions made by the authors and recommend this paper.	3
The paper is well-written and interesting but limited in originality and novelty given works of Vilnis et al., Dasgupta el al. It is an interesting application of earlier findings to the domain of multi-label classification, but it is lacking in motivation for the precise model choices. The comparison with earlier works is limited without which the significance of the work is unclear, which led me to suggest a weak reject.	1
Multi-label classification is an important problem that has real world applications and is relevant to the ML/AI community. Enforcing taxonomic constraints in multi-label classification is an important subproblem. The proposed approach is a novel solution to this subproblem, and is well-framed in the literature and theoretically-grounded. the paper is relatively well-structured. Experiments and analysis seem sound, and experimental results are proming: MBM is improve taxonomic consistency while preserving or surpassing state-of-the-art predictive performance on several benchmark datasets compared to reasonable baselines.	3
"The paper is interesting and results seem to be quite good, but the justification of why availability of the label taxonomy should not be available is a bit lacking. The proposed method is quite a simple step from existing literature, and there are several shortcomings about the text highlighted above. The experiments would benefit form more baselines.
"	3
The paper shows an interesting idea, however to have full acceptance some revisions are needed (see full review). 	3
This paper proposes a set of data augmentation tricks that demonstrate empirical gain on GNN fairness. However, the presentation of analysis and motivation is not convincing and can be largely improved. 	3
The relevance of the problem setting, technical and empirical novelty, good presentation, and significant results warrant acceptance of the paper in my opinion.	4
Overall the proposed technique following the analysis on the theoretical upper bound, is simple and effective in improving fairness metrics. Experiments could be made stronger though, such as by including sensitivity analysis on the hyperparameters.	3
"1. The motivation of the paper is not clear enough.
2. Several claims are not well-demonstrated.
3. The experimental settings are not quite reasonable, and the experiments are not convincing enough."	3
I recommend to accept this paper because I think that the paper is relevant to the ICLR community and has some innovation supported by experiments (and some minor theory).  It also serves as a good survey for the community with many useful details for AI practicionaries in the field of metric learning, which are outlined in appendix. However, my concern is that the paper is not well organized and maybe better suited for a journal venue. 	3
The paper is overall very clearly written and easy to follow, the authors made a great effort to conduct the experiments in the paper. I am not an expert in this area and may have missed some related work in this area. My main concern is in terms of the error function in (10) as described in the main review. 	3
The authors propose a new and generic loss function in mixup scenario for deep metric learning. The experiments on popular datasets mostly support the claims in the paper with the needs of explaining some of ideas mentioned in the paper. 	3
Although the proposed approach seems to be novel (applying mixup to dml), the scope of the technique is limited to the dml applications only and the contributions are marginal. Also, showing some images of the retrieval, tsne plots would be helpful and can provide more insights into the problem.  	2
This paper is generally well written and easy to read. It provides sound technique details with theoretical analysis. Also it presents lots of experimental results to support its claim that the mixup of inputs and outputs improves the representation quality. However, the novelty is of a bit limited. Overall, it is an acceptable paper that will attract attention from relevant researchers.	2
"Due to the limited novelty of the proposed method, I need to re-evaluate my judgments after the rebuttal.

UPDATES: The authors have addressed all of my concerns, though experiments on 400M dataset are missed by uncontrollable reasons, as initial judgement, I tend to accept this paper. "	3
"The authors identify a new problem and propose a simple, elegant, and versatile solution to solve it, which has a consistent, incremental impact on the important task of zero-shot recognition. The insights and conclusions made by this paper are moderately significant, but not adequately supported, and not strong enough for acceptance.

UPDATE: after reading the authors' response, I have decided to raise my rating."	4
"Overall, I like the idea proposed in the paper to use OT to mine missing matches for language-vision pretraining. The paper is clearly written and makes intuitive sense for most part -- I'm overall positive on accepting this paper but still like to hear the authors address my concerns listed above. 
"	3
"The work introduces a valid approach for considering many-to-many correspondences in image-text datasets which is an important problem for vision-and-language learning.

Although the presented results show consistent improvements over alternative approaches, the approach does introduce a few new hyperparameters. Intuitively they might need to be tuned carefully for different datasets to be able to observe any improvement over simpler approaches. The ablation studies do suggest that within a given dataset the sensitivity to some of those hyperparameters is high.

Additionally, the work makes a side-by-side comparison with ZSL prior works, which, unlike this paper, do control the separation between seen and unseen classes/concepts in their training/test data. Such comparison and claims of exceeding ZSL their performance and reaching a new state-of-the-art might lead to misleading conclusions.

**Update (after the authors' response):**

Raised the recommendation to _6: marginally above the acceptance threshold_"	3
According to my main review, I believe this paper still has room to improve	3
"While this paper combines two complementary techniques in counterfactual explanation and CAVs, I would like to see CCE compared to other counterfactual explanation methods that are trained on natural images and provide semantic highlights or perturbations.  I also think that several important details have been omitted from the discussion (see Weaknesses section), and that in its present form, the work is not ready for publication.

Update: In response to the authors' extensive revisions to address my concerns (and those of the other reviewers), I have increased my score.  "	3
"The paper provides a novel way to explain a classifier's mistakes on a test example when given it’s label in terms of a known set of concepts. The paper is very well written and the controlled evaluation demonstrates that it is effective when the concepts can describe the mistakes perfectly. However, the evaluation lacks baselines and results when some of the assumptions fail (concepts don’t describe mistakes). Furthermore, it is hard to understand how the “in-the-wild” evaluation shows the effectiveness of the method. Finally, the method requires knowing the label of the example and thus is more of a debugging approach rather than an explanation method.

In its current form, I think the paper is borderline accept*, however, if the authors can provide answers to some of the weaknesses I presented, I am more than willing to move the score up to an accept as I think the ideas and the problem are extremely interesting. 

"	4
Overall an okay paper with some very interesting ideas, although I have some concerns regarding the evaluation of the method (see main review).	3
"While the proposed method is interesting, the supporting experiments need fundamental changes in order to be impactful.  Specifically, the analysis should not start with the assumption that a set of ""OOD images"" has been identified and the experiments should include more concepts that are relevant to the predictive task.  

# Update after discussion with the authors

I'd like to thank the authors for a very productive discussion and for taking the time to run additional experiments to address my concerns.  At this point, I think that the setup, results, and analysis for the settings where the model is trained on a deliberately biased distribution are convincing.  As a result, I've increased my score to a 5.  

However, I am still concerned that using CCE in a less controlled setting will be difficult due to ""false positives"" for relevant concepts:
-  These appear in Figure 4 in the form of ""water"" and ""ashcan"".  In this setting, it's easy to label these as false positives because they are unrelated to the task;  but this seems like more of a reason to exclude those concepts than an actual fix to the problem.  
-  As an example of where it is more difficult to determine if something is a ""false positive"" consider Figure 6.  While it seems likely that concepts like ""building"" and ""flag"" are false positives, its much less clear  if ""cow"" is.  If we consider adding a literal ""cow"" to the image, it seems unlikely that that would help the model detect the zebra.  But with a more subjective interpretation of emphasizing the ""cow shape"" of the zebra, then it seems more likely.  Either way, this seems like something that should be tested directly.  

One way to address this would be to:
-  Train a model on the entirety of the MetaDataset
-   Find misclassified points and run the batch CCE on them
-   For each of the top concepts, translate the explanation into a hypothesis about about the model's performance on real images (eg, snow is relevant for dog, so the model will perform better on images of dogs with snow)
-  Measure how often those hypotheses are correct

"	3
See above.	2
This paper is novel in the sense that it cooperates Huber loss with value evaluation to robustify value evaluation. It also conducts experiments to justify this improvement. Therefore, I think this paper is marginally above the acceptance threshold.	2
This is a nice paper that makes a novel connection between the secondary variable update in GTD2/TDC with the conjugate formulation of Bellman errors involving an auxiliary state function (both of which involve predicting the residual error with a separate model). While the technical contributions in Section 3 (e.g. Theorem 3.2) are not particularly significant, I believe the main value in the paper is the conceptual linking of two different lines of work to derive an improved algorithm over well motivated baselines. The empirical evaluation is well motivated and quite thorough, even if only for a limited set of benchmarks.  	3
"The major contribution is to develop a new type of loss, which is not sufficient according to the ICLR standards. 

My main concern is that the contribution is not sufficient. I think the main contribution of this paper is the introduction of a robust loss. The conjugate reformulation is standard. And the bound developed in Theorem 3.2 is not surprising.

The writing is also not clear. For example, MHBE and MABE are never defined."	2
A nice natural idea for doing RL with access to text instructions, experiments on RTFM impressive but only 1 setting is a negative. Poor readability is the biggest negative.	3
Limited novelty and improvements but significant gains in performance---although unclear because the writing isn't clear.	2
The FRL model shows impressive results on Messenger and RTFM. Still, there are a few improvements that the authors can make to improve the clarity, the analysis of results and also provide a more detailed discussion about cases of failure, especially on Messenger. One weakness of the model is how the data is sampled to train the manager, additional discussion on how this can be scaled to more complex environments is needed.	3
An interesting application of Feudal Reinforcement Learning (FRL) in text-guided RL environments (e.g., RTFM (Zhong et al. 2020)). Lack of details limits the significance of the paper. 	2
Overall, the paper proposed a method for propagating uncertainty through neural network that could potentially be useful. However, it seems some important baselines are missing, the introduction is somehow misleading, the proposed method only works in very limited cases, i.e., Gaussian+ReLU and Cauchy+ReLU (while theoretical results on other activation such as sigmoid/tanh/other ReLU variants are not provided).  	2
Overall, the strengths of the paper outweigh the weaknesses, although going beyond the ReLU activation (see Weaknesses above) could significantly improve the quality of the paper. Therefore, I am inclined to accept the paper.	2
"The proposed method is a simple and effective method to propagate distributions through a neural network. I am fine with the experimental results. However, I am not voting for accept right now since in my opinion there are some important details missing (as outlined in my review above) that are required to reproduce results and that would improve the quality of the paper. I am also missing a discussion of failure modes.

--- POST REBUTTAL ---

I have updated my rating from 5 to 6."	3
The main strength of the paper is that the proposed method is simple. The main weakness is that the method is only applicable to limited kinds of layers of neural networks, and the paper lacks more comprehensive experiments to demonstrate the method's performance.	3
This paper proposes to combine mean-field approximation with the permutation invariance idea in [Liu et al., 2019b], which is an interesting direction. Both theoretical analyses and numerical experiments are provided. However, the theoretical justification of the mean-field approximation is very unclear. Also, some closely related algorithms are not compared.	2
As for the moment, I find the contribution below the standard of a top-tier conference like ICLR. It is not clear what the exact theoretical contributions are and how they are related to the experimental part. 	3
The submission is well written and provides a good contribution to the MARL literature.	3
"I tend to vote for rejection of this paper. The effects described are similar to adversarial attacks and in most but not all cases the influence of a parameter is only investigated on the image where the saliency was computed. 
The evaluation of the proposed approach in terms of providing pixel wise interpretability section 3.4 is limited.

This review was done after the rebuttal period as an emergency review. I had no access to the discussion with the authors which might impact my understanding."	3
"
I am not an expert in this field and I did read carefully about this paper. I could not find any weakness in this paper. The contributions proposed in this paper are really interesting and are verified by the proposed experiments. "	4
The proposed method is new as a parameter-wise visualization, but it becomes somehow similar to the  parameter-space adversarial attacks. The authors would better claim the similarity and difference of these 2 topics. Besides, adversarial attack needs to be reviewed as the related work. 	3
The contribution might be not enough for iclr.	3
I preface this summary with an admission that I am not the most familiar with the OOD detection literature and global null hypothesis testing. However, I believe that formulation of OOD detection as a hypothesis testing problem is well thought out and beneficial for the ML community by rooting the problem in a concrete statistical framework. Furthermore, the proposed method seems sound (I skimmed through the math, albeit not thoroughly) and has significant speed-ups compared to existing methods, such that the method is far more likely to be deployed in real-world applications and edge devices. For these reasons, I believe this to be a valuable contribution to the ML community.	4
"The approach lacks novelty and experiments are limited to justify the efficacy of the proposed approach. 
"	2
"Hypothesis testing using the empirical cumulative distribution is not novel enough as a ""new framework"": existing works already include such spirit of it by using FPR95 of the in-distribution samples. The theoretical part is also not strong enough without considering the gap between the true and estimated cumulative distribution. 

The proposed method uses p-values from every layer and channel in CNN, but intuition and arguments of Fisher and Simes tests as layer and channel reduction methods are not fully presented. Lastly, the experimental evaluations can be further strengthened (by adding more recent baselines). "	2
"As the framework is novel and results shows good performance both in accuracy and computational complexity, I feel that the study deserves to be published. But the length might be issue.

I chose ""marginally above the acceptance threshold"" as my recommendation due to the length issue. If there would not be any limits, my recommendation would be ""accept, good paper""."	3
"The problem definition is clear and there are enough materials and descriptions to support their proposals. This paper has supplementary materials and those also support their proposals.
In federated learning data  hetegeneity is a most important issue. As for their performance advantages in comparison with other competitive FedAvg-variant methods, FedBABU is robust against data etegeneity and outperforms others in personalization as well.
"	3
"Overall, I vote for accepting. I like the logical flow that 1) capturing the root of degradation lies in the head, 2) prove the feasibility of using a shared random head, 3) compare the performance between the proposed scheme and the baseline. I think the authors find/tackle the problem in a clear manner. My remaining concern is if it were really the last layer(head) only that creates a degradation problem in a federated learning setting. If so, why? Hopefully, the authors can address my concern in the rebuttal period.

=====POST-REBUTTAL COMMENTS======== 
Thanks for the authors' response. The author`s responses address well my concern, and I recommend this paper be accepted.
"	2
Overall it's a nice paper, the results and the analyses are good. Nevertheless, there are some issues. First, it ignores [1] which uses a similar idea. Second, I am not entirely convinced that the empirical evolution of the baseline methods is fair. I am willing to reevaluate the paper based on the author's response and adjustments to the paper.	2
"
## Major Comments

* The paper makes some very strong claims which I don't find to be fully supported by the experiments. For example, ""we argue that training the head using shared data negatively impacts the personalization"". This is a strong, broad claim, but it is made on the basis of experiments on only a single, artifically-federated dataset (CIFAR100). In the absence of (i) stronger empirical evidence, or (ii) some theoretical motivation for this claim (either novel theory or via connections to existing theory), it is difficult to fully get on board with these claims, which form the foundation of the proposed algorithm. If there is empirical evidence from other works supporting the authors' conclusions here, please also cite it. (Another claim needed more support is that training the ""head"" during federated rounds degrades performance.)

* The claim that ""randomly fixed classifiers are acceptable..."" is not clearly connected to the authors' statements about orthogonality, which are relegated to the Appendix. Even on reading Appendix B, it's not clear to me how this discussion of initialization proves the authors' claim, or whether this also might imply that personalization of the head might also be unnecessary under certain conditions. Perhaps you could draw a more explicit connection, or link this to work on randomly-featured models, since I sense a connection.

* It's not clear to me why the authors opt to fine-tune the entire model, in order to accommodate FedAVG (mentioned at end of Section 5.2.2), when the results on FedBABU suggest that fine-tuning only the head is sufficient. This seems to require extra compute, and increase the risk of overfitting, while providing no benefit to the algorithm of interest, FedBABU. Please clarify. (I will note that, in T5 (Raffel et al 2020), the authors show that fine-tuning the entire model is preferable to fine-tuning only the later layers, if I remember correctly.)

* I feel that a key analysis missing from the paper is related to the degree of overfitting (or not) that happens during fine-tuning. It is unsurprising that fine-tuning a small amount improves performance, on the clients' data; it is important to know, however, if that results in a much poorer performance on out-of-distribution data (i.e. another client, or a more globally-representative dataset); I was surprised not to see such an analysis in the paper, particularly because this seems of both theoretical importance (the authors imply a kind of tradeoff between global and local model accuracy) and practical importance (it is likely that real-world client data will drift). I would suggest some empirical evidence to this end, or at the very least some discussion of it.

* The finding, in Sec 5.2.3, that FedAVG outperforms *all* preexisting personalization algorithms, except FedBABU, is to me very significant. If true, I think that this should be a ""main contribution"" of the paper and should be mentioned much earlier, since it suggests that this is the only current personalization algorithm that actually achieves its intended aim.

## Minor Comments

* The tables are very hard to read; I would recommend replacing them with bar graphs (with error bars) and optionally including the tables in the supplement.

* The switching nomenclature between body/extractor and head/classifier was quite confusing to me as a reader, particularly since ""classifier"" often refers to the *entire* model and I am inclined to read it that way. Please choose one terminology and use it consistently.

* Fig. 1 could be improved a great deal. It needs an illustration of ""head"" vs. ""body"", as this is a (the?) major difference from FedAvg that is not currently aptured. Along those lines, it would also be useful to note what is transmitted to the server in the rightmost column, since again, this differs between FedBABU and FedAvg. Also, note that the figure has room to be wider; perhaps you could add a fourth column characterizing the personalization step of FedBABU.

* It seems there is no explicit connection made to the ""pretrain -> fine-tune"" paradigm that is increasingly prevalent in the broader machine learning community (now also referred to as the use of ""foundation models""), although some works in this area are cited (e.g. BERT). An explicit connection would be useful here, and would help motivate the use of a similar approach in FL via FedBABU. Please clarify how, if at all, you believe this method to be different from this transfer learning paradigm.

* On p.5, the claim that global training pays attention to ""unnecessary and confusing information for personalization"" seems like an incorrect characterization. It may be ""irrelevant"" to a specific client at a given point in time, but we might reasonably believe that such data actually improves the generalization capabilities of the model (including if that clients' distribution changes in the future) -- does that make it ""unnecessary""?


* The authors reference multiple times using a learning rate of zero to not update the head layer (which implies that a no-op is performed, for every paramter, with learning rate zero)...why are these updated being performed at all, instead of only updating \theta_{ext}?

* The conclusion references ""regularization"" as reducing models' ability to personalize; I am not sure which part of the analysis/experiments this refers to. Please clarify.

## Typos etc.

* All of the ""i.e.""s in the abstract should be removed.

* P.1: ""popular networks have one linear layer...and ResNet"" -- this sentence is confusing; please revise.

* P.2 The phrase ""representation learning based on the same fixed criteria"" is not clear; a similar phrase is used in 5.2. Please clarify what a ""criteria on learning representations"" (5.2) is and what the ""intuition"" you are suggesting here may be.

P.3 ""When the bottom layers are matched with the body"" and ""When the top layers are matched with the head"" are unclear.

* P4 ""decayed 0.1 times"" --> decayed by a factor of 0.1

* Why abbreviate (B), (F) in Table 1? I would suggest ""Body Updates"" vs. ""Head + Body Updates"" or similar.

## Update after revisions

I thank the authors for their response. Having read the author response, reviewed the updated manuscript, and considered the other reviews, I am keeping my score the same. While the authors addressed several more minor points regarding the paper, I don't believe they addressed my main issue with the paper, which is that the empirical evidence is fairly weak and limited to only a single dataset; I think that either more empirical evidence, or some theoretical support for the proposed method, is needed before being able to argue for acceptance."	2
While the paper is generally clear and technically sound, I found several serious issue with it and recommend rejection at this time. Specifically, I am skeptical of the usefulness of the central question, worried that the theory is assuming away the interesting part of the problem, worried about novelty, and had a few more minor concerns about the experimental setup. 	3
"While some claims about offline RL doing better than BC as ""surprising"" are not necessarily something I agree with, overall it's an interesting paper and allows the community to make progress towards better understanding of the tradeoffs between different policy learning mechanisms and would recommend acceptance. 
"	3
"I am leaning towards a recommendation to reject the paper since I think the paper does not clearly answer the core question: whether should I run BC or offline RL for several reasons mentioned in the Main Review section. However, I am open to change my scores after author response. 

----
After the author response, most of my concerns were addressed. I think the paper provides good empirical and theoretical understandings of the comparison between BC and pessimistic VI, so I raised my score to a 6. "	2
Overall, I consider this paper to be a great contribution to the research community. The question investigated is important for the research in offline RL and practitioners. The theoretical analysis is well conducted and meaningful insights can be drawn from it. The experimental results are adequately reported and support the findings.	4
"The paper is well written. The literature review is extensive - even though there can be several additions (see the specific comments in the main review). The work extends the scope of existing work in AMT significantly. The experiments are thorough and the authors declare that they will present the necessary code, experimental setup, and results in the camera-ready version so that the work may be reproducible.

Given the work's strong points as described above, I would like to recommend the paper for publication in ICLR 2022."	3
"Overall, the paper is well-written, and I vote for accepting the paper. The authors proposed dataset split, model, and some evaluation metric for somewhat new task.
"	3
"This paper achieved a great milestone in AMT. The demo examples are very impressive and show great potential as a musical tool or a data generator for symbolic music modeling. The main weakness is that the main contribution is designed by hands, in other words, the technical novelty is low. Also, vocal tracks are not included in the experiment. But, this seems to be a matter of dataset availability.







 "	2
Very solid work. A few more details about the experiment and the model would be great. 	4
The paper proposes CCWM, a learning via retracing method, based on cycle-consistency constraint. It leads to better sample efficiency and final performance by learning better state representations. Need more clarification about its novel contribution relative to a recent work (VirtualPlay), which considers cycle-consistency learning in model-free RL setting.	3
The motivation for the contribution is not very clear and there are a few unclear elements in the formalization (e.g. sudden change in the value function for the prediction of the irreversibility) .	3
The proposed representation learning method for RL is interesting, which involves addtional retracing samples for training. However, more explanations are needed to make it clear. I expect to see more elaborations.  	3
Overall, I am for accepting. I like the idea and I think it's a novel approach for state representations in RL. My major concern is about the clarity of the paper, as written above. Hopefully the authors can address my concern in the rebuttal period.	3
"Overall, while there are relatively few technical novelties in the paper, the given results demonstrate that this simple scheme is able to improve performance over baseline methods in their evaluation.
For me, generalization to unseen OOD-sets is still a very important part of any novelty detection method and should be evaluated. 
As the method heavily relies on implicit regularization of disagreement between models in the ensemble, I think there could be more experiments in the paper that show that there is sufficient disagreement even on close OOD tasks and how this changes during fine-tuning. 
Also, it would be nice to clearly demonstrate that the improvements over the baselines methods are caused by both the new disagreement score and the early-stopping-based training. "	2
Some confusion exists both empirically and theoretically. I recommend weak-reject at this stage.	3
A reasonable, simple, and interesting approach for anomaly detection with ensembles. The main weaknesses are the reliability on clean ID data and the lack of a discussion and analysis of the approach limitations.	2
The proposed method have some empirical results on the Hard/Near OOD. However, this relies heavily on the carefully selected  and high-quality validation sets.  Also, the experimental comparison is not fair.	2
"Strengths
- The proposed method is intuitively making sense. 
- The main idea ""Introducing the randomness to multiple models using different label assignments to the unlabeled datasets"" would be promising to discover the samples which are not presented in training set.
- The experimental results are clear and concrete. The baselines are somewhat complete and the comparisons seem fair.

Weakness
- We need another training procedure for different testing set which is less practical. 
- Experimental settings should be more diverse. More ablation studies are needed. 
- Robustness of the model training is one of the biggest concerns about this paper. Relying on the validation error for early stopping to avoid overfitting to the ID unlabeled samples seem somewhat heuristic.

--------------------------------------------------

I am appreciate to get the detailed responses from the authors in this rebuttal.
I carefully read all the reviews from other reviewers and authors response.
However, there are still remaining concerns.

1. Practicality of the settings
- Not only me, other reviewers also concerned on the setting of this paper.
- Also, the authors did not answer about re-training problem with new test set. Retraining again and again for new test set would be very painful.

2. Fundamental limitations of the methods
- The robustness of the proposed method is highly related to the diversity of the submodels for the ensemble.
- However, this diversity is significantly limited if the number of unlabeled data is small or the problem is the binary classification.
- Even though the authors provide some interesting results with this setting, the fundamental limitations of the model itself remain concerns.

3. Early stopping
- As I mentioned above, the best validation performance would be the model ""before the fine-tuning"" because there is no distribution mismatch between training and validation. 
- It is unclear how fundamentally address this problem even after reading the rebuttals.

Therefore, even though the authors provide some additional experiments (thank you), I am going to stay on my score (6) and cannot provide more than the score (6).
"	3
"This paper provides a new perspective into SSL. But the designs are not fully explained and some remains unjustified, e.g. the prior distribution for all classes and the number of novel classes.
"	2
Overall, I like this work especially the proposed setting. Although the novelty is not very strong, this is the first step for open-world learning. Also, there are some concerns and missing experiments that should be addressed during the rebuttal. I think this is a promising and interesting task so that the first work should design a well-considered setting and provide extensive comparisons for the following works.	3
This paper is very well written and well motivated. I recommend accepting this paper for publication if the authors could address a few issues described above. 	2
Although the paper has some weaknesses such as comparison in the conventional SSL setting, the paper proposes a new problem that considers a more realistic situation and the proposed method seems to work properly for the designed setting. 	4
This work introduces a new semi-supervised learning problem that has more real-world applications. Even though the technical novelty of different components of the proposed method is not high, the solution is well motivated and achieves promising performance on multiple datasets. However, some of the points are not convincing and require further clarification to assess the significance of the work. I would appreciate it if the authors address these concerns in their response.	2
"To summarize my review, I think that while the high-level goal of the paper of exploring the necessity of intermediate supervision is interesting and important, the experimental evaluation is not rigorous enough to demonstrate it, missing along multiple key dimensions, which include: 
* Choosing a task that is too narrow
* Very narrow exploration of models and experimental settings within the chosen task
* Unjustified/unexplained choice of an unnaturally constrained model (1-layer 1-head transformer)
* Lack of exploration of other relevant models (e.g. recurrent networks)

For these reasons I unfortunately recommend rejecting the paper in its current form, but do encourage the authors to extend and explore it further in order to improve it!"	2
The proposed approach is promising, but the experiments are only performed on a very simple, limited task domain, and an important control experiment is missing.	2
As for the weaknesses, I have to recommend the rejection of the paper. There are flaws in multiple claims, and the authors seem to mix the availability of computation steps and supervision of them.	2
"The main idea of this submission is interesting but the contributions are not significant enough for ICLR. In ""Weaknesses"" I provide some additional questions that the authors could address to increase the amount of contribution / insights.  For the moment I recommend to reject this work but I strongly encourage the authors to continue working on it!"	2
The authors present a new method, introspection, that can be added on top of feedforward networks to improve accuracy and generalization of the networks.  While they have written a well-structured paper that provides both intuition and rigor, the empirical results are somewhat lacking in that there are no statistically significant results that support the claims of accuracy improvements that introspection can obtain.	3
The authors propose a very interesting modification to neural networks for object recognition and do a convincing job in showing that it improves generalization and calibration. However, the proposed method is only tested in CIFAR-10 and its use in larger networks and datasets is questionable. Furthermore, the explanation of the method is unnecessarily confusing and better controls could have been chosen to make a more convincing case.	3
"Overall, the paper presents an interesting approach with promising results for improving robustness, both in terms of accuracy and uncertainty estimation. I believe with a more clear presentation, more thorough discussion of related work, more extensive experimental evaluations, and ablations/evaluations of specific assumptions and design choices, this will be a valuable contribution.
"	3
"The core parts of the paper are not clear and do not allow a proper comprehension and evaluation of the scientific contribution of this work. 
The overall idea seems valuable and experimental results appear promising. Yet, with the current manuscript, I can only judge the potential of this work rather than the work itself, so at this stage my recommendation is a 3: reject, not good enough.
"	2
"**Review summary**

This is an interesting application paper in the domain of molecule optimization with some minor technical contributions. The preliminary results are interesting, but it would be more solid with more quantitative comparison with existing methods on existing benchmarks. 
"	2
I think the idea of using IUPAC names is intriguing, but neither the theoretical arguments not the empirical results (due to lack of benchmarks) are convincing. 	3
I personally feel good about this work, but unclear parts are main concerns and there are no comparison with other works. 	3
This paper studied an important problem, but the theoretical results are not convincing.	3
In short, the paper is technically sound and the developments are clear. The derived analysis under non-convex objectives seems to be a useful contribution to the literature, showing a modest improvement over the state of the art. ​However, the paper is still not novel enough based on several existing works and could be strengthened by demonstrating more significant results instead of incremental, such as weaker conditions. 	2
The paper studies an interesting problem. The proposed algorithm attains theoretical guarantees, and its performance is also empirically validated. However, there are still some questions requiring more discussion.	2
An incremental work, by applying compression on vertical federated learning algorithm. The theoretical analysis follows existing compression stochastic algorithms' analysis (where the compression error eventually goes to zero).	2
The experimental results are quite limited so that they are not enough to support the claims in the paper. 	2
I think this is a borderline paper that addressed an important question with reasonably good performance while lacking necessary elaboration and justification. As commented in my 'Main Review', my major concerns to recommend this paper lie in their framework, novelty compared with existing literature, and comparison studies. I am willing to upgrade if my concerns can be addressed during the rebuttal period.	3
Overall, the authors study an exciting topic causal identification in POMDP, which is a quite general, and challenging learning setting. My main concern with this paper is its novelty. First, the unbiased estimator in Eq. (4) is not surprising and follows immediately from the backdoor criterion. I am pretty sure many similar MLE estimators have been proposed. Second, the bound in Theorem 1 might be interesting, but appears to be a simple application from the bound in (Manski, 1989). It would be encouraged if the authors could elaborate how to combine these different methods to obtain a more accurate estimation of the target interventional distribution.	2
"The setting considered is very limiting as it assumes there are no latent confounders. See the paper for the tricky issues involved:
Shaking the foundations: delusions in sequence models for interaction and control
Pedro A. Ortega, Markus Kunesch, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Joel Veness, Jonas Buchli, Jonas Degrave, Bilal Piot, Julien Perolat, Tom Everitt, Corentin Tallec, Emilio Parisotto, Tom Erez, Yutian Chen, Scott Reed, Marcus Hutter, Nando de Freitas, Shane Legg

The experiments are not performed in any non-trivial settings. 

The writing makes the paper hard to read. For example, it references rule R2 without specifying where it is or first introducing it.

===
The authors have made their contributions and assumptions more clear, and will add  results comparing with related work. I am happy to upgrade my rating."	2
I find the proposed method to be novel and effective. the paper is clear and well-written. I have a few comments around modeling designs but I think the comments are addressable. 	4
Overall, this paper makes decent contributions in both technical and empirical aspects, and therefore I recommend accepting the paper provided that the authors correct the writing issues.	3
I am decidedly leaning towards acceptance. In my opinion, this is a strong contribution that deserves to appear at ICLR. I will most likely raise my score to an 8 after the authors will answer my questions and modify the text accordingly where needed.	3
Due to the novelty of this data-efficient grammar-based molecule generation method and its impressive performance, I recommend to accept this paper.	4
The remarks and empirical results are interesting. However, CUDS is a simple variant of CDS – the only difference is how to set the rewards. Therefore, I think CUDS needs additional meaningful theoretical analysis in addition to empirical results. 	2
The major issue of this paper is it seems to be an incremental study from existing works. I would expect the authors to address this issue and add more insights. 	2
"I recommend that this paper be below the acceptance threshold. 

1. This paper release the strong assumption with a reward relabelling function in [1], which archives data sharing between multi-tasks without reward. It seems like an extension with a minor revision of CDS in [1]. 
2. The intuition of the great performance is also straightforward since more data yields a better estimation of the transition kernel $P$ in MDP, even without rewards. 
3. Although in the examined experiments the proposed methods show competitive performances compared to CDS and other baselines, it is not convincing enough that without an estimation of the reward, CUDS/UDS can achieve great results in more general scenarios.


[1] Yu, Tianhe, et al. ""Conservative data sharing for multi-task offline reinforcement learning."" arXiv preprint arXiv:2109.08128 (2021)."	2
"The proposed attack seems interesting. I think the main problem with the paper is with respect to the clarity in writing, especially in Section-4. Further, the past methods which broke detection methods need to be discussed in detail to highlight the significance of the proposed method. 
"	3
"I would love to recommend an accept for this paper, considering its topic of research, effectiveness, and novelty.

In addition, I want to clarify that I served as a reviewer of this paper for its last submission. I am glad to see that the authors followed my suggestions and made several improvements on their writing. For the last submission, I give a weak accept score in the final stage. Considering that the paper has been improved on several aspects, I raise my score to accept."	3
The proposed attacks for creating adversarial samples are simple but effective, and the evaluation is sufficient.	3
"Overall, the two proposed techniques, though a relatively simple update to existing approaches, seem to perform well and provide an effective attack against state of the art adversarial attack detection techniques. However, the presentation of the paper is a bit confused in places and the motivation for the approach a bit unclearly argued. It is hard to argue against such convincing numerical results, so with some rework to address the issues raised I think my recommendation for the paper could increase.

Edit: Score increased after author response and revision."	2
"The current paper mixes the two connections (i) [block structure] -- [dominant datapoints] and (ii) [dominant datapoints] -- [results].
To make the paper rigid, these two connections need to be distinguished.
The results in the current paper implies the connection (ii).
However, the validity of the connection (i) remains questionable.
I suspect it can be an artifact induced by the use of KCA with linear kernel for the similarity measure.
If this is the case, the paper needs to be reorganized with its main focus on (ii) but not (i).
If the authors are willing to connect (i) and (ii), it would be essential to confirm that the reported phenomena are not artifact induced by the use of KCA with linear kernel, and demonstrate they are ubiquitous irrelevant to the selection of similarity measures."	2
Interesting paper following up on Nguyen et al 2020. Some nice insights and overall a well-written paper, but probably impact is low. My recommendation is still leaning towards accepting the paper in the conference.	2
This paper provides a reasonable explanation to the block structure phenomenon. It is a useful empirical contribution. 	3
"The authors consider the problem of the block structure phenomenon uncovered in previous works but consider large-capacity networks trained on relatively small datasets. They try to answer the question: ""is the block structure a sign of overfitting to idiosyncrasies of the data and training process, or does it pick up meaningful signals?"" They tried to eliminate the block structure by a novel principal component regularization method, or alternative existing training mechanisms such as transfer learning and Shake-Shake regularization. In my opinion, the novelty of this paper is too small for the conference ICLR."	2
The paper does not deeply examine the phenomenon at hand, it makes assumptions that are not explained, and does not explore implications well. 	2
Overall this is an interesting theoretical paper with its limitation. Currently, I tend to accept it.	3
The current paper is misleading and needs a significant revision. Based on my current understanding, the paper is less interesting as suggested by the title. I cannot recommend acceptance given the current version. 	2
"To the best of my understanding, the theoretical results obtained by the authors (optimal stopping time control + resulting risk bounds) are new and provide more refined insights that the existing literature with respect to the impact of sample size and dimensionality, despite the limiting Gaussian assumptions. In addition, while the experimental section could use a lot more exposition, from what I can tell the design is straightforward and natural considering the theoretical results the authors want to verify. My overall take on this paper is that there are some results of value here, but the paper itself needs some work, so I am left on the borderline with this work, tending to accept, but with the caveat that I am not very familiar with the developments in this line of work since Ali et al.
"	3
Overall, I think the theoretical contribution is rather limited given the work of Nakkiran et al., 2020b, and the empirical part should be improved. Therefore, I don’t recommend the paper for acceptance at this stage.	2
"The authors proposed a simple technique to improve clean accuracy. However, the method is based on intuition, which in my opinion, is flawed: not all large perturbations should be labelled with its adversarial label. The authors should provide a theoretical justification for their intuition. Besides that, the experimental comparison is outdated with few recent defenses missing, which improve upon TRADES defense.

### Update after the author's response
I sincerely thank the authors for addressing the majority of my comments and concerns. The experimental results are undeniable and clearly demonstrate the advantages of the proposed technique. Based on the new results for the medium perturbation $\epsilon = 12$ and additional experiments with other attacks, I tend to overlook my doubts about the paper's approach.

I recommend accepting the revised version of the manuscript."	3
"Overall I think the paper is well written. It shows a significant boost as compared to existing art and has some minor issues at present.
If the concerns are properly addressed I am willing to increase my score."	3
Paper enjoyable to read with extensive experiments supporting a clear and novel idea leading to improved results. The authors also propose great analytical tools to investigate their hypothesis. Hence, I vouch for acceptance.	4
Strong empirical evaluations and promising improvements, but the method itself is somewhat ad-hoc and not very principled.	2
The paper properly motivates the study of expressivity, complexity and unpredictability in the framework of deep learning based language games as a way to gain further insights into natural languages. The paper also studies the usage of contrastive loss in referential games and this loss helps alleviate the collapse of message types in language games. The key contributions of the paper is predicated on the definition and closed form expressions for unpredictability and complexity. However, I do not believe that those expressions are sound. Although, there are extensive experiments, some conclusions are not well supported. Therefore, as it stands, I would not recommend the paper to be accepted.	3
Overall, it seems like a strong paper with a clear hypothesis and brings in a novel perspective which could be a worthwhile contribution in the subarea of emergent languages. I see no obvious issues with the paper.  	2
Some interesting ideas in this paper (measuring the effect of the environment on the effectiveness of the language protocol, introducing a new and sensible definition of effectiveness), but unfortunately I think there are issues outlined in the main review which preclude acceptance in the current state. 	3
"This paper studies ""expressivity"" of emergent language, which is empirically the transferring ability of the language. The paper also defines ""context complexity"" and ""unpredictability"" of the underlying language game. The study shows that these two factors both contribute to the language expressivity, but they are contradictory on the candidate set size $|C|$ so a trade-off is needed. Generally, the analysis is novel and interesting. Although I have some concerns about the scope and significancy, the work still provides useful insights for language emergence study. "	3
Overall, it is useful to have an additional operator/policy that is somewhat similar to softmax but with its own strengths and weaknesses. The form of the operator is a little ad hoc and too specific, but all things considered I lean towards acceptance here.	3
"The new resmax operator proposed in this work seems interesting and could be a more robust choice for Boltzmann-type policies. The paper contains straightforward reasoning supporting its candidacy, but falls short in really proving the claims that resmax outperforms softmax in terms of (1) exploration, (2) convergence to the optimal policy, and (3) deep learning practice. Nonetheless the method seems interesting and worth trying more widely.


### After responses

After reading the other reviews and considering the authors' responses I am leaning towards rejecting this paper. The results are potentially interesting but not compelling, and I do not find that the analysis in this work significantly illuminates the problem otherwise."	3
"I do not think this paper has enough novelty for acceptance. The proposed operator is indeed novel, but I can see several drawbacks of the operator, mainly since it can depend on very poorly estiamted value functions. Morever, the paper claims to provide an alternative to the softmax operator - there are quite some related works discusisng issues with softmax, and proposes alternative operators which can help with faster convergence, or solving the counter-examples of PG divergence with softmax policies. The softmax operator is well used in the policy gradient literature - and it is surprising that the paper completely ignores any issues or discusses anything about the effectiveness of the resmax operator in PG learning context. 


The lack of discussions to related works (both theoretical and empirical) is a major red flag for me, since I do not know how reliable this resmax operator can be. Empirical experiments are not enough - and by that I do not mean large empirical studies showing effectinvess of resmax; but there are several counter-examples that exists in the literature (e.g from Mei et al) that discusses issues of the softmax operator. It would have been helpful to see why the resmax operator can be considered as an alternative to softmax (e.g like the Escort Transform proposed by Mei et al) in the context of those experiments. 
"	2
"A novel non-expansive and simple exploration policy called ResMax.
I read this simple and direct paper with pleasure and it wakened a few questions in my head. This is a reason for me to support the paper.
On the other hand, due to the lack of experiments comparisons, I am still wondering if the proposed method is a clear improvement against mellowmax and the host of tuned-softmax variants that come to mind.
"	3
This paper is enjoyable to read and provides good insight to the problem of large computation cost of AdvProp. The proposed solution Fast AdvProp demonstrates good performance with less computation cost and can potentially be applied to various vision tasks.	4
Overall, I would recommend to accept this submission into ICLR 2022. The paper is well-written and the results are good. I would be happy to see some of the points, that I stated in my review, would be implemented in a future version of the paper.	3
"I am ambivalent based on what I mentioned above. The paper provides empirical insights, but IMO not huge enough for ICLR, and is well written. However, I am wondering whether there is any new scientific discovery here -- it will be nice for those ""tricks"" mentioned in the paper, the authors can contrast clearly why they are novel as it seems like they have been previously proposed. I also question the AdvProp vs Fast AdvProp results where we have to significantly reduce AdvProp to have the Fast version beat it. I will be interested to see what the other reviewers think and I am opened to discussions and changing my feedback."	2
Overall, I think the paper is interesting but has some technical details to be addressed. In addition, the novelty of the paper is increamental. 	2
In this paper, the insight from the theoretical results can match the empirical observations pretty well. Their results certainly deepens our understanding on the learning dynamics of modern neural network.	3
"I welcome the direction of the present study, trying to understand the dynamics
of deep neural networks via simpler models. However, I cannot recommend
acceptance of the present paper for three reasons: the implications of the
studied dynamics on learning or representations in neural network remain unclear
to me; the results derived from the Ornstein-Uhlenbeck process do not go beyond
the previous work to an extent that would warrant the publication at ICLR in my
opinion; and the unclear presentation of the results which doesn't always
clearly separate previous work from the presented results, or theoretical
predictions from numerical measurements (Fig. 6).

**Edit Nov 14th** I thank the authors for the lengthy replies to the points raised in this review. After careful consideration of their comments, and also taking into account the valuable points made by other referees, I keep my original score for now. I have increased my confidence from 4 to 5."	2
"The paper presents several concepts in a very clear and readable manner. However, the novelties presented are limited/incremental (certainly not as revolutionary as suggested by the title), and some of the results are either not new (anomalous diffusion in DNNs has been known for years now), not relevant (the lack of detailed balance in results relying on a stationary state distribution), or not well-supported (space filling curves instead of limit cycles). On the other hand, I do find some value in some of the results, such as the characterization of diffusion and displacement as a function of hyperparameters, and showing that under certain conditions the modified loss is isotropic.

The authors should reduce their claims regarding what is new in their work and adapt them to what is the real state of the art of the understanding of DNNs, and remove disputable statements such as ""The major limitation of the stationary dynamics approach to analyzing SGD is its implicit assumption that the system is in detailed balance"".

"	2
"The paper presents an interesting result about the presence of sub/super diffusive behaviour at the end of training. Although the result may be of limited practical interest, it is a new finding and would be interesting to understand causes and consequences of that (not discussed in the paper). 
The paper has a large number of flaws that I believe can be amended to get a publishable result.  "	2
"This paper proposes a novel idea that combines style transfer with RL to improve the
generalization performance. The experimental results show the proposed approach is competitive with the existing baselines. The results could be better improved by adding more runs and more ablations that approved the effectiveness of ARPO. More theoretical studies are needed to understand whether the algorithm converges to optimal policy. "	2
"I like the innovative point that policy is not only learned to optimize return but also robustness against style transfer of state observation. Compared to data augmentation methods, the proposed methods showed promising methodology and empirical advantage.
Issues in the above section need to be addressed however.
."	3
This method represents an incremental improvement gained by developing a novel architecture. This paper — and the success of the proposed methods — was not surprising. But of course the authors of course should receive credit for getting things to work so well. The quality of results generally ranges from acceptable to quite good, although the quality of ablations falls far behind some comparable papers and can be improved.  Overall, a good paper. 	3
"While I think the idea motivating this paper is interesting and potentially useful, I think the execution of the idea in this paper, the lack of strong or varied empirical results, the overselling of the effectiveness of the method, and the incomplete discussion of related works makes me recommend the paper is rejected. Several of these criticisms are relatively easy to address, but overall I'm unlikely to recommend acceptance unless the authors add encouraging empirical results on a different benchmark, or vastly improve the results they currently have.

EDIT: after seeing the response from the authors I have raised my score from 3 to 5, and my correctness score from 2 to 3. I believe the improvements are worthwhile but don't make the paper worthy of acceptance: crucially, I think better comparisons with baselines (such as UCB-DrAC, and a more performant SAC implementation), and combining the method with an off-policy method (such as SAC or QT-Opt) would make the empirical justification of the idea more robust and make me more likely to accept the paper."	3
"This paper successfully attacks SOTA multimodal contrastive learning, which reveals the security threat from unfiltered data. Personally, I appreciate the authors' effort to demonstrate the urgency and importance of addressing data security issues.  However, I am afraid that this paper does not meet the ICLR's novelty requirements. 

If the authors could convince me of the issue of novelty, I would reconsider the rating."	2
Paper shows an extensive evaluation of both poisoning and backdoor attacks on CLIP with a very demanding task. Authors demonstrate vulnerability with just a few samples. I suggest an accept. 	3
The paper is one of the first to illustrate the vulnerability of contrastive learning to poisoning / backdoor attacks. However, the experiments only involve a single dataset and it is unclear how well the findings can generalize to other datasets.	4
The paper is well-written and conducts very detailed experiments, I recommend acceptance.	3
**Overall assessment:** The paper is well-written and easy to follow. While the main idea of learning domain invariant features is simple, its use in the context of robustness against adversarial attacks seems to lead to a significant performance boost. The experiments and, in particular, the ablation study section is insightful and aligned with the paper's claims. I think the paper is above average, and therefore I would like to vote for its acceptance.	2
"+ The paper is easy to follow and the idea is straightforward.
- The experiment section is not comprehensive. Only a few methods are included in the comparison. More recent SOTA methods are missing."	2
I think the paper conducts extensive experiments to demonstrate the effectiveness of the proposed method, including some interesting ones, e.g., robustness against unseen perturbations, transfer learning (I like them). However, the novelty of this paper is insufficient, and using the domain adaptation principle and learning invariant representation has been widely studied. Therefore, I vote for rejection.	2
Overall, this paper proposes a simple and effective adversarial learning method DIAL for robust representation learning. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method and provide solid results. However, the novelty of the paper is not significant as similar methodology exists in domain adaptation. 	3
This paper is overall well-written and easy to follow, but the ablation study and discussions may be inadequate to validate that the performances, properties, and novelties of the proposed modules. 	2
The current version of paper may need experiments to support claims shown in the paper. 	2
"This paper has a clear description of the found three problems, but the ablation studies are not sufficient. For details, please see weakness part in the main review section.
 
From the experiment, it can be seen that the stability of the model is poor. And the consideration of the performance reproducibility of training complex models with huge parameters is also very important."	3
"While some aspects of the paper are nice, such as theoretical framework and lower bounds, etc., the framework's value addition is not clear due to multiple reasons.

1. Authors do not discuss the space complexity of oracles and time ( or number of passes) to train oracle.  This essentially makes me question the comparison in table 1 as indicative of valuable contribution.
2. The experimental results look promising. But again, due to the lack of inclusion of memory required for oracle, it is not clear if the new method actually beats the old techniques in terms of space-complexity. 
Also, the predictors that are used might require more auxiliary memory (see points 4,5 above). 

Overall, I believe the paper fails to justify the value of the new methods, and including a discussion on the lines of points raised in the review above should help strengthen the paper.  I would be happy to revise my score in case the authors address my concerns. 

Update: I have updated my score after the revision provided by the authors. There are some good theoretical contributions in this paper which I believe will be useful. The idea of using machine learning oracles with sketching algorithms is, in general, a established idea which deserves exploration in different problem. For this particular problem of counting, I am still a bit reserved on applicability of this framework to the specific problem at hand when accounting for all the meta costs (such as oracle learning etc). However, I am still of the opinion that this paper is worth the acceptance for its overall value to the community.




"	3
Overall this a well-written paper, with clear contributions to an important problem.  	3
"In terms of the theoretical contributions, the heavy edge oracle and the information used from that seems to be a good amount of information in the model. Would be worthwhile to check if there is simpler version of the oracle.  

In terms of the empirical contributions, the graphs used in the experiments look small in size and space used in terms of the number of edges is more as compared to the other (random edge and random walk based) models. Thus, it would be great to compare the results with the random walk and u.a.r. edge based models with all the algorithms using similar number of edges."	4
This work introduces a new scenario where current methods for learning robust features are not capable of leveraging all available data. It also provides a solution in the form of a method called TOFU which has few requirements in terms of additional data annotations compared to existing methods. The authors recognize that bias is a human defined concept, and could vary from dataset to dataset, so it is best to have a method that can identify unstable features in an automated way. TOFU is an extension of an existing work, and has some theoretical motivations, but the underlying assumptions may be too strong. TOFU outperforms all other methods considered by a large margin, and the authors investigate a potential source of its success by confirming that it is indeed able to cluster data well according to unstable features: a requirement for later on doing group DRO. Sufficient details are provided to enable reproducibility, and many baselines are compared against, making the experiments comprehensive. However, the method is not particularly novel, nor is it clear if the scenario presented occurs often in real life. Limitations of the method are not considered, even though there are a few clear ones both technical and empirical.	2
This paper proposes a two-stage method for OOD generalization and transfer learning problems. However, there exist many weaknesses, including method, problem setting, theoretical analysis and experiments. 	2
Please see main review	2
This paper addresses the spurious correlation by transferring knowledge from source tasks. Although intuitions are provided and empirical effectiveness is illustrated accordingly, the method is restrictive due to the strong assumption on the transferability of unstable features. Several important related work and baselines are also missing.	2
"Pros:
+ A new setup that transfers the knowledge of spurious patterns to a target task.
+ The writing is clear. Figures and tables are beautifully produced.
+ The results are demonstrated in multiple datasets and the number is convincing compared to several recent baselines. The ablation study of group numbers is great.

Cons:
- The hypotheses in my opinion are a bit too strong. It's unclear in what real-world settings this method will work unless in the contrived setup in this paper. Including a limitation in the paper can strengthen the paper.
- The multi-stage approach of training could be brittle.

I am leaning towards the borderline with marginal acceptance. The experiments are complete with multiple baselines, clear writing and a good ablation study. I feel the assumption is too strong which would need some justifications. And if the authors can show a robust study under different mildly violated assumptions could further strengthen this paper."	2
"This paper draws too many conclusions based on very small networks, datasets, and for a pruning regime where the network is simply trainable and has nowhere near the performance of the unpruned model. Overall I'm not convinced that ""effective pruning"", and extreme pruning gives us a enough insight to justify accepting the paper.
"	1
Overall, I believe that this is a well-written paper, with a clear message, which also has some space for improvement.	3
"The paper is well written, very clear, and well presented. The authors raise and clearly document the discrepancy between raw sparsity and the actual effective sparsity of a model. An interesting and compelling new baseline is presented.
However, the results are shown on small datasets without comparison with many modern architectures. 
Replacing the MNIST/LeNet/VGG results with more modern architectures would clearly increase the impact of the paper."	2
"The paper eloquently presents and quantifies --- in the small scale --- the known issue of disconnections as affecting parameter count and pruning methods [Tenaka et al]. The paper can further engage with practical implications. 
I agree that counting effective rather than direct parameters is methogologically good practice. However, the limited scale and demonstration  limits the insight applicability in practice --- especially, as SOTA methods and architectures have structural characteristics which call into question the the level of discrepancy (missing discussion/dimensions of exploration).
"	1
The paper studies one of the sources of bias (data imbalance) in machine learning applications. Given the nature of current datasets and the bias with the current machine learning techniques, this is an important topic to explore. Although the proposed technique is currently applied/evaluated under restricted settings, this is one of the first work to do so for deep metric learning. 	3
"Overall, the paper addresses an important problem, i.e., fairness in deep metric learning. Although the method does not always improve fairness compared to the baseline and the technical novelty seems weak, I think this work could motivate research on fairness for deep metric learning.

---
After authors' feedback:  
I appreciate the extensive experiments and detailed answers. Most of my concerns are addressed. "	2
This paper is well-written and the proposed problem is intrersting. The supplementary materials provided by the author are very sufficient, and the comparative experiment is very detailed. The innovation of the method is general, but it systematically studies a new problem, so it is recommended to accept it.	3
"Since the paper has weaknesses in terms of the novelty of PARADE and missing experiments of bias mitigation in the downstream classification task, I recommend “marginally below the acceptance threshold” to this paper. I would like to increase my rating if the authors address my concerns.

============

After Authors' Response:

I appreciate the authors’ feedback, which addressed my concerns. I will increase my rating to “6: marginally above the acceptance threshold.”
"	3
"Strengths:
1. Topical: Looks into a relatively new problem of generalization during cross play after training using self-play.
2. Strong conceptual overview and motivation with examples.
3. Use of attention based embedding for contextualized policy prediction seems to be new in POMDPs, although it has been used in vision and NLP applications before.

Weaknesses:
1. Weak literature review with very general coverage of the problem space.
2. Lack of strong baselines - The related work mentions Bayesian/causal reasoning based approaches that are considered too expensive. It might help to show the performance-accuracy tradeoff of those methods vis-a-vis currently proposed approach.
3. The quantitative evaluations are not well explained / justified. E.g., 
  (a) why does the baseline DQN (simplest model) perform remarkably well for the same hand condition, while the proposed approach has significantly worse accuracy? 
  (b) Why does the proposed approach always converge into two clusters of related cross-plays?
  (c) How is the diversity of the learnt policies measured? 

Overall, although the problem itself is novel, the treatment is not convincing enough and does not meet the bar for an ICLR acceptance yet.
"	2
In summary, the problem tackled in the paper sounds interesting and the idea and method are reasonable and easy to follow. While more explanations and analysis are preferred for the experiments, my current rating leans to acceptance.	3
"Though I'm not an expert in multi-agent RL, I think the work will be interesting to the field. 
The paper offers an interesting study showing that action and observation information should both be leveraged to form interesting communication / coordination strategy.
However, the paper is limited by the features are explicitly available, making strategy generation less impressive."	3
The proposed approach works for this bespoke game. It would be good to have more evidence that the approach is generally useful.	3
"The work is well structured, with a good theoretical basis to support the proposed methodology. The empirical results are very promising, although the small amount of datasets combined with the lack of confidence intervals does not allow for meaningful conclusions to be drawn.
The only major doubt concerns the use of test data in the training phase, which may have compromised the whole experiment."	4
This paper provides both theoretical and empirical results for a boosting method for graph structured data. The results appear to advance the state of the art and the submission seems to have valuable contributions.	3
Overall, I like the problem the paper aims to address - how to better combine GNN with boosting methods for learning on tabular data. The paper proposes a novel way to address this problem, which is based on a principled meta-loss. Empirical results show the effectiveness of the results. I feel the paper can be improved more by iterating on the formulations in Sec 3.	3
Overall, the approach seems sound and principled, although the scope is a bit narrow. Hence, I will give the weak accept. I would also like the authors to address my comments/questions.	2
"Although this work is well-motivated, I have two main concerns leading me to a « weak reject » notation. 
- First, on the theoretical side, the problem of delayed reward is not tackled frontally, but rather implicitly through the predictron. As such, delay is not formalized, although intrinsic to the problem considered. It weakens this paper’s contribution in my opinion, as one could question whether PDQN would necessarily work in a different type of domain (e.g., healthcare, as mentioned in Sec. 1), where the reward function has other dynamics. 
- Second, delayed reward and delay in general (delayed execution; delayed observations), have been addressed in RL. This span of relevant works is never mentioned nor discussed. I list below a few references that are welcome to be discussed/compared with PDQN.

*Relevant references (all of them are missing in this paper)*

[1] Konstantinos V Katsikopoulos and Sascha E Engelbrecht. Markov decision processes with delays
and asynchronous cost collection. IEEE transactions on automatic control, 48(4):568–574, 2003.

[2] Derman, Esther, Gal Dalal, and Shie Mannor. ""Acting in Delayed Environments with Non-Stationary Markov Policies."" ICLR 2021.

[3] Jeffrey S Campbell, Sidney N Givigi, and Howard M Schwartz. Multiple model q-learning for stochastic asynchronous rewards. Journal of Intelligent & Robotic Systems, 81(3-4):407–422, 2016.

[4] Thomas J Walsh, Ali Nouri, Lihong Li, and Michael L Littman. Learning and planning in environments with delayed feedback. Autonomous Agents and Multi-Agent Systems, 18(1):83, 2009."	2
This paper tackles an interesting and promising industrial application of RL. To address the long reward delay issue, a combination of DQN and predictron (for estimating targets) is proposed to train the agent, which makes sense. Empirical results are presented to show the advantage of the proposed method over common heuristics and vanilla DQN. The paper needs significant improvement in formal problem/objective definitions and the choice of baselines in benchmarking for a more convincing case. The simulation environment is not a standard one, so some calibration results should be presented to ensure the validity of the results.	2
From a methodological perspective, I found the evaluation to not be rigorous enough. As an application perspective, there are critical application-specific issues that are not addressed.	2
"
*Summary Of The Review
I am worried about the novelty of the algorithm, and the claims are not supported by experiments.
"	1
Though the experiments could include more baselines and further analysis of how design decisions for NodePiece affect task performance, the current set of results seems to sufficiently demonstrate the utility and versatility of the method.	4
The paper is inspired by the use of subwords in NLP and aims to leverage similar techniques for GNNs. Evidently, the proposed method is not behaving like subwords, as NodePiece could potentially degrade the performance a lot. There are missing experiments, and the paper has weaknesses in its analysis of the performance of NodePiece, i.e., we don't know when it works and when it does not.	2
"A promising direction with mixed results.

After author response: I am satisfied with the answers to all my questions."	3
The paper presents a novel approach for embedding large knowledge graphs using a small number of parameters. The empirical results strongly support the usefulness of the proposed approach and I believe that even though this won't be the final word on creating compositional representations of KG vertices, but it will be appreciated by ICLR readers.	3
The idea is novel, the proposed approach is simple and elegant and the paper is well written with most of the technical details with strong experimental results. 	4
"I think this paper proposed a novel idea to reformulate object detection to a sequence generation problem. It provides us with a new perspective to think about conventional vision tasks. However, from the paper, I do see several drawbacks of the proposed method. Without seeing strong proofs, it is hard to determine whether the proposed method is a good one for generic object detection. As such, I have a general concern that this paper can bring us a new viewpoint but not new insight to solve object detection problems.
"	3
The proposed ideas are novel and, from my perspective, valuable enough for their publication despite not obtaining new state of the art results for the task. There is already a significant body of works that have addressed the analysis of images as a sequential process in which a predicted token coditions that posterior predictions. Pix2Wav simplifies DETR and improves its performance a bit. However, there are still some doubts listed in the weaknesses that should be clarified before making a final decision.	4
The submission carries an important message for the object detection problem and the framework results are reasonably good, But the paper presentation should be substantially improved to be a reflective of the contribution.  Limitation of this formulation should be clearly elaborated and the ablation study should contain insightful results for the contribution of each proposed module (formulation, architecture or loss). To this end, I rate this submission borderline 	2
In general, the proposed method is novel and gets good results. I believe it will get a huge attention and lead to a big change in computer vision community. However, there are a few concerns as mentioned above, especially inference time. I hope the authors resolve my concerns.	4
Drawing connection with steerable network, this work attempts to rectify a missing link in prior art for equivariant network. Although, the technical contribution is minimal, the paper would have stand out if the empirical results were strong. I suggest that authors improve upon their results and further demonstrate wide applicability.	2
Overall, I enjoy reading this paper, but I have minor concerns. I'm definitely willing to raise my score if my concerns are well addressed.	4
Although parts of the paper were difficult to understand and some of the contributions are questionable, the core contribution, making equivariant GNNs in the message passing flavor is a useful step and together with considering higher-dimensional state spaces seems to result in improved performance.  Thus I tend towards accept, but I would prefer revisions. 	3
I agonized over the rating to give this paper. On one hand, it reports a clear and unambiguous benefit of incorporating covariant information in the node and edge attributes. It is also well-motivated and well written and is definitely valuable. However, the technical contribution of the paper at this point (in how the equivariant neural network literature is progressing) is relatively minor. The paper also seems to overclaim its contributions, one case being in the sense of how it relates to steerable graph networks (although this happens because they are positioning themselves in contrast to Satorras et al., Thomas et al., etc). While I am a big fan of simple methods that just work well, I feel conflicted about this paper. I am currently rating the paper a 6 (which is mostly due to their experimental results), I would be willing to adjust my rating if the authors can present arguments showing that my concerns are unfounded or not rational. 	2
I generally think the proposed general steerable GNN is meaningful. Yet, given certain unclarities related to the novelty and experimental significance, I initially suggest that it is just marginally abve the acceptance bar. 	3
A very strong paper that provides an elegant framework for building E(3) equivariant graph networks. The proposal is very general and well-motivated; it is also well-situated within the literature. The exposition is generally high quality, and the experiments are rigorous and compelling. The supplementary material gives thorough background and the code seems pretty clean. A few comments / questions about some details of the paper remain outstanding.	4
This work introduces a dual algorithm to solve the infinite-dimensional mean-field training problem. The update step is solved approximation by estimating an integral term in every iteration. The new technique is interesting and the new linear convergence rate improves the prior result.	3
The paper outlines an algorithm and carefully analyzes it. The algorithm appears to be effective in a simple synthetic example. However, the assumption of efficient sampling of the parameter distribution requires substantial additional justification. 	3
The paper proposes an interesting variant of dual coordinate ascent that achieves improvements in the convergence rate. There is not much to criticize since the paper has optimized the complexity to a good extent, except for the inherent difficulty with the approach that depends crucially on $\lambda_2$.	3
"The paper proposes and studies an interesting optimization method for problems in the space of probability density functions. The main application considered are mean-field neural networks, but  I can see how this algorithm will be very useful also for other problems over probability densities which are ubiquitous in ML.  

There are several smaller issues mostly regarding the technical clarity and motivation in the paper (see main review). If these are clarified/addressed in a revision, I am also willing to increase my score. "	4
Paper is borderline, could be improved significantly by improving the exposition. I've included several suggestions for this.	2
- The technical contribution of the paper is very simple (adding few L2 normalization layers). While showing some good empirical evidences, the overall contribution of the paper is marginal and does not extend state-of-the-art. Also, while proposing many small techniques, they are not fully verified in experiments (please see my comment above).	2
"Overall, the paper presents some good empirical results of OOD detection on benchmarks. This paper is well motivated and solid, but the writing needs to be improved. However, it is somehow weak in terms of technical novelty. Therefore, I vote this paper marginally below the acceptance threshold. 
Given that I am not very familiar with OOD, I would like to check other reviewers' opinions."	2
On one hand the paper has good contributions, provides insights into the problem that are supported by empirical evidence; on the other hand is poorly organized and written: most of the key results are supported in the appendix, which should be optional - not critical - to the reading of the paper.	4
While the motivation and method makes sense, many of the claims are unsubstantiated, weakly substantiated, or ambiguously substantiated (see Weakness 1). Because the focus of the paper is mainly on demonstrating the empirical utility of the proposed method, significant improvements are needed to make the paper acceptable.	2
Overall, the authors attack an important problem of interest to the ICLR community. While the idea of using options for better exploration in deep Q networks is interesting, the proposed method does not currently provide clear performance improvements over existing approaches in the experimental settings shown, and the experimental settings themselves were relatively limited in terms of the diversity of baselines considered. In addition, some claims made in the paper are not fully backed by experimental evidence. Thus, I recommend the current version of the paper be rejected.	2
"I think the ideas presented in this paper are interesting and promising. However, I would like to see them validated on more than 6 ALE games -- currently the results are somewhat mixed (helps on 4/6 to some extend) and there isn't an empirical result that very clearly demonstrates a new capability enabled by this approach. 

The paper already needs to be more clearer written and framed w.r.t a rich literature on semi-MDPs and hierarchical RL. "	2
This paper introduces a new agent architecture that has several moving pieces and it doesn’t do a good job at evaluating their impact. In the end it is hard to claim that the value of the main idea (to use an intrinsic reward signal without having to linearly combine it to the extrinsic reward) was backed up with clear experiments. There are several confounders. For example, it could still be that the observed empirical performance is due to some sort of temporally-extended exploration, and that’s all. I don’t see data to rule this out. Finally, the paper makes several claims that are not precise or that are not supported by data, including some of its claimed contributions.	2
This paper has proposed a relatively new idea on training face recognition models by using multiple binary classification losses. The proposed method has convincing motivation and the technical design is reasonable. Experimental results ascertain the superiority of the proposed method as a unified solution. However, some statements of the paper are misleading or vague. Moreover, as the proposed method comprises multiple effective components, it is also unclear that whether the performance improvements come from the new formulation or from one or two incremental components.	3
Overall, this paper presents a novel face recognition model, called SphereFace2, with interesting insights and persuasive explanations. The proposed model outperforms the baselines on popular 1-1 verification and 1-N open-set evaluation. Despite few minor issues, I lean towards accepting this paper.	3
The idea of using binary classification learning to cope with face recognition is well-motivated and reasonable. The framework and loss function design are also clearly explained and further demonstrated by experiments on benchmark datasets. Although there may be some minor errors, I think this is a good paper, and would vote for accept.	3
The authors present evidence that BatchNorm leads to a preference for parameters that minimize a data dependent norm, and demonstrate a simple student-teacher training method to correct for these effects.  There are a few alterations I'd really like to see, though I still believe that, given the required corrections, there is enough value in this particular insight and the experiments to see it published.  I would have dearly loved to see more emphasis on other measures of uncertainty for the networks (cf. https://github.com/uncertainty-toolbox/uncertainty-toolbox) rather than looking strictly at accuracy.  If my other criticisms can be addressed, I will increase my score.	3
"The technique is simple, well-described, and seems to be effective on the considered toy datasets. What I am missing right now is compelling evidence that it actually works in practice, and in practically relevant settings. For this, several key results are missing:

1. Full scale ImageNet-C (and -R) evaluation with relevant larger models (ResNet50, ResNext101 etc, following prior work). This will make the paper much more interesting for a wider audience interested in corruption robustness. In addition, I suggest runnign on ObjectNet --- BatchNorm adaptation does not work on this dataset, and it would be interesting to see whether CT is effective at mitigating this issue.
2. Comparision to more recent domain generalization techniques, and especially reporting results on more datasets beyond VLCS. To make a claim that the method improves domain generalization, the authors should consider the DomainBed evaluation setting, or a similar, more comprehensive experiment protocol that avoids the issues discussed by Gulrajani & Lopez-Paz (2020). Alternatively, the authors might drop this claim, and move the table as a proof-of-concept to the supplement.
3. I am missing a statement how hyperparameters were tuned. The supplement simply states a few values, but not how the authors arrived at them. In particular, they should be picked according to the *validation error on the training domain*. Ablations over a few crucial parameters will help here (for example, the learning rate). It is unclear why these were not added, given that the results are obtained on fairly small scale datasets.

I think it is feasible and reasonable to expect these kinds of results in an ICLR paper. Point (1) only requires to train models on ImageNet (3 seeds, ideally) *once*, getting scores for the requested setups is easily doable afterwards.  I find it possible that on full ImageNet-C, more recent techniques like DeepAugment+Augmix on larger models (ResNet50) will outperform the proposed technique. Even then, I agree with the authors comment (cf. Table 2) that a huge advantage of the method is the removed need for well-designed data augmentations. However, I feel that this might come at the expense of only being effective in settings where batch norm adaptation is working. Since the technique is effective on corruptions, a great test would be the ObjectNet dataset, where batch norm adaptation fails to work.

Regarding (2), this is not a strong requirement, the authors could equally decide to drop the claim regarding domain generalization.

Point (3) is again crucial---especially Table 2 indicates that the baselines in the paper were not sufficiently tuned.

If the authors manage to add these results (minimally 1, comment on 2, and clarifications on 3, along with addressing the detailed comment in my review), and show a realistic comparisions of tradeoffs to state of the art augmentation based models, I will increase my score. Right now, in my opinion the paper does not meet the bar for ICLR, as the number of truly relevant datasets, and comparisions to the state of the art is limited.

I am happy to address questions/clarify the exact scope of points 1-3 above if this is needed."	3
"
Overall the paper is clearly written, the arguments are sound and the empirical evidence is solid."	3
I think the simplicity and effectiveness of the proposed approach out-weights the limitations. This approach has potential for further improvements because it does not rely on access to the target data distribution.	3
The paper is well-written and the conclusions are reasonable. But I still have some concerns about the too strong assumptions of the main theorem, which might limit the usage of the conclusion.	4
"The proofs in the paper appear correct, and at first glance, they appear relevant to generative models research community.
However, upon closer look, despite the commendable theoretical work, the paper seems to amount to proving the *existence* of a network of certain complexity, in the invertible case, and the *existence* of a generative subnetwork that only allows inference networks with pathological (exponentially growing) complexity. Without any guidance of how one could construct the networks under the said conditions, or measure how far we are from meeting those conditions, I am worried that the paper boils down into a mathematical exercise without considerable impact in this line of research.

I also had concerns about the sloppy wording of the claims in the introductory section, and the structure of the paper in terms of the order of how things are presented, and the lack of clear bottom line.

In combination, I lean towards rejection, but I may improve my score if the authors can convince me of the significance of the theorems, and if they manage a considerable rewrite of the introduction section. Given the considerable theoretical weight and intellectual import of the paper, I would encourage the authors to do so."	2
The paper starts with a good inquiry, but the answer provided is unsatisfying. It relies on a conjecture from another field which, even if proven true, would not make it all that useful.	2
"The work addresses an important problem for monitoring models after deployment for harmful changes and derives principled hypothesis tests. Empirical study sufficiently validates the method on real datasets. My main concern is the presentation of the method. It lacks details on related to implementation and guarantees for the methods for readers who are unfamiliar with past work (Waudby-Smith and Ramdas 2021, and Howard et al. 2021). The work provides provides a strong contribution and I feel, subject to the response, the presentation can be improved.

---
Based on the response

The presentation of the methods has been substantially improved. In particular, the contributions paragraph at the end of Introduction gives a good overview of the work. I would encourage authors to more prominently list the confidence bound variants (betting-based, PM-H, PM-EB, CM-EB, Hoeffding) in Sec 2.2 to more clearly layout the options to implement the tests."	2
"I'd recommend reject at this point.

-----------------------------------------------------
Thank the authors for the response! The authors substantially improved the writing and the presentation in the updated version. They also added new content regarding comparison with other papers/other potential methods, which makes the contribution more clear. I read reviews from other reviewers as well as the corresponding responses. The answer clears most of my concerns. Although I think the theoretical contribution is not significant, adapting sequential estimation to the practical setting would be nice. "	2
I give the paper a 6 because I find the contribution novel and clear and would be happy to see it accepted in the conference; however, I would not champion pushing to accept given my belief that there is room for significant improvement.	2
Based on my knowledge and background research conducted while reviewing this work, I think the authors have proposed a novel framework to detect dataset shifts. It can potentially be used to improve several applications fraught with predictive models. The work is well supported with the help of illustrative examples. My only concern is their comparison with the existing works on dataset shift detection is not well explained/not supported.	4
"This paper proposes an effective and theoretically sound method for sequentially detecting harmful distribution drifts. Overall, this work is solid but still requires some improvements on the experiment.

## Update after Rebuttal

We thank the authors for their detailed responses and for updating the manuscripts. The authors have addressed my questions, but we tend to keep the current score based on the significance of the improvements made."	3
"Even though empirical study is suggesting potential benefits of the approach in applications with a pronounced data quality issues, the article haven't assured me that proposed modulation layer is more applicable/useful (or sufficiently distinct) from the attention layer. Studies exploring attention for handling missing data have already been conducted (eg. Wu, Richard, et al. ""Attention-based learning for missing data imputation in Holoclean."" Proceedings of Machine Learning and Systems 2 (2020): 307-325.), and comparison agains them (as a closely related approach) would be appropriate."	1
The contribution is based on intuitions that do not seem very solid and should be better studied. It does not really perform better than other approaches. 	2
"The paper has the good idea of adding a layer modulated on the data quality as a plug and play layer. It is evaluated in a variety of problem types, data sets and experimental settings against state of the art. However, the approach does not do much better that existing state of the art.
    "	2
Despite good motivation, the proposed method seems to have several theoretical and practical limitations and the current experimental results do not provide enough evidence on what are the benefits of the proposed method.	2
The paper does have contribution towards the development of FedPAGE and its analysis for convex and nonconvex setting. While the nonconvex result is shown to be better than existing ones, the convex result is not rigorous enough to me. Numerical experiments appear to not fully verify the theoretical results as they only contain nonconvex example.	2
Results seems promising. However there isn’t enough discussion of limitations, novelty, and comparison to some baselines.	3
 Reducing the communication cost  with multiple local updates by applying PAGE in federated optimization.	3
 My main concern of this work lies in the significancy of the results, and improper comparison with other algorithms.	2
Based on the above points, I would say the current version of the paper is slightly above the acceptance threshold. I would be willing to increase the paper's score based on the authors' responses/updates to my comments/concerns (mainly around novelty, assumptions, practicality, and experiments).  	3
The problem studied is important and not well-studied in the literature. Some theoretical results are developed, though with strong assumptions. In general, I think this paper is interesting, so I recommend a weak acceptance and would be curious to see other reviewers' opinions as well as the authors' rebuttal.	3
"Overall, I like the idea of gauging the robustness to changes in categorical features. However at this moment, I feel the setting is slightly artificial and the tightness of the gauge does not come across. (For example, can we estimate the value of $m_{\Omega}$ and $M_{\Omega}$?)

I feel the paper can be greatly improved by dropping the mutual information part. It's not contributing to the core ideas. 

Therefore, I'm giving a score of 3 for now. I hope the authors can address my questions. "	3
In summary, the paper is novel, and gives some insights to the adversarial robustness. 	3
The paper is a nice contribution to the theory of SGD in the quite fundamental linear regression setting. It is well written, contains original and relevant results, and provides good experimental verification. Therefore I recommend acceptance on ICLR without esitation.	3
"Although the paper is well written and points towards a relevant direction of research, I believe that the contributions are rather incremental with respect to previous literature that is not taken into account. Therefore, I think that at the present stage this work is not ready for publication.

*** Update after reading the authors response***

The authors have carefully addressed my concerns and improved their manuscript accordingly. I recommend the updated version for publication and I have therefore raised my score to 6."	3
Based on the cons listed in the Main Review section, I would like to give a rating of 5 for the current version of the submission.	2
This paper studies an interesting problem, but its contributions are unclear to me as similar models have been studied before. Rederiving the loss dynamics with different data structures makes the paper a bit incremental. Overall, this is a borderline paper and I'm inclined to vote for a reject. That being said, I'm happy to increase the score if the authors could reorganize the paper and emphasize their novel contributions. 	2
Though there are still some minor flaws in the paper, I think it is a good one and is nearly ready for publication.	4
I find the paper generally of high quality, but the lack of any timings is concerning. If this can be fixed, it would be a pretty good paper.	3
It is a good work with clear motivation and  promising performance, the reason of poor performance by directly applying the supernet-based NAS to optimize ViTs need to be discussed more.	3
"The motivation of this paper is not very convincing, need to provide more evidence to prove. For more details, pls see the weaknesses.
Also, there are some recent works on NAS+ViT missing in the related works, such as ""CATE: Computation-aware Neural Architecture Encoding with Transformers"", ""Vision Transformer Architecture Search"" and ""AutoFormer: Searching Transformers for Visual Recognition"". It is suggested to add them into comparison and discussion, especially for Autoformer which might be the first ViT+NAS work and has been published in ICCV21. I will re-consider the rate if all the concerns are well addressed or not. "	3
In general, the writing of this paper makes it difficult to follow the authors’ core thinking.	4
Overall, the proposed idea combines existing approaches on graphon estimation and spectral clustering to lead to the proposed clustering and testing approach. It is a neat idea and comes with theoretical guarantees, however, I find the setting of dense networks and the strict assumptions on degree sequence as required for graphon estimation, restrictive. The performance of the clustering algorithms and the testing approach when these assumptions fail and when the number of clusters is unknown, is not clear from the paper. 	3
"Overall the paper is well written. The proposed solutions are novel to me and address an extremely challenging problem related to the small sample size regime. However, a few negative aspects are present: there seem to be incorrect claims, the graph transformation appears not well-defined, and the experimental setup does not bring decisive conclusions.
Given the points raised in the main review section and unless it turns out I misunderstood those parts of the paper, I am afraid to say that the paper is ready for publication in its current state. However, I encourage the authors to carry on the research in this direction, because it appears promising and only few (yet, important) aspects need to be improved."	3
"

The paper is interesting and sound, while the result on itself is not yet very practical it could form a good first step.

"	4
"My recommendation is influenced by the following issues:
- The novelty of the contribution is not clear to me. I'd like to understand C3-C6 because overall it seems to me that epistemic uncertainty driven intrinsic rewards have been used before and here it's not compared thoroughly against such baselines (which are mentioned in related work)
- C7 - I don't understand how this section contributes towards understanding what type of uncertainty acetylcholine signals."	1
"(+) Nice and novel idea
(-) Too poor mathematical justifications and unclear description of the method.

I sincerely hope the paper will be improved for a further submission because I believe the idea has an high potential, that would be missed if it appears in its present slovenly form."	4
An impressive amount of work has been done; the writing is mostly easy to follow, and the results seem promising; however, provided that the proposed framework is closely related to the existing ones, further research towards practical results is needed. The better exploration, proposed here, is important, but it may or may not lead to better-performing agents. I, therefore, think that the paper is better suited for a conference workshop rather than for the main track. Thus, overall, I tend to recommend rejection.	2
As I stated in my main review, I suspect that auxiliary training is necessary for getting the better performance of the tiny network. I suspect that an equivalent sub-network of the larger net will have similar results in comparing to the performance of the tiny network solely training on the large datasets (ImageNet, Pascal VOC etc).	2
While there are lots of works studying how to improve the accuracy of large models, there are relatively fewer works focusing on the tiny network training. This work demonstrates that the tiny models suffer from under-fitting rather than over-fitting therefore requires different training strategies. The proposed method, NetAug, is effective and simple to implement. It also works well with other techniques such as knowledge distillation and pruning. There are existing works sharing a similar idea of NetAug for large model training, which slightly hurts the novelty of this work.	3
I think this is an interesting paper, but I still have a few concerns stated above, especially on the experimental part and some statements on knowledge distillation. 	3
"I think this paper's strengths are clear, but some important parts (explanation and baselines) are missing. Hence, my initial rating is weak reject. I will raise my rating if all the issues are resolved with a strong rebuttal in the discussion period.
"	2
"In summary, I recommend the acceptance of the paper. The idea is very simple and can be combined with efforts both on explainability and neurosymbolic methods that offer trust to the users.

The paper presents a strong combination of minimal information used to ground the explanations with learning and reasoning methods to provide adaptation and coherence. The use of planning allows to produce small explanations in the most critical parts.

If anything, the weakest point is that the notion of locality might be hard to control. In some cases, the reason for an action not being possible might be further away. Another round of interaction with the users might allow to discover for what concepts it might be useful to look further. For instance, a video game might feature a switch that should be open to achieve some effect. "	3
The paper is well-written and the methodology is novel. However, XAI is most needed in critical domains like recommendation systems, healthcare, and education. Would be good to see some discussion in these areas.	3
Overall, I think this is a good paper with the potential to spark more work in the area. However, the evaluation section can be made stronger to really convince the reader of the merit of the method. In its current draft, I am somewhat inclined for the paper to be accepted. 	3
"I am not convinced that this paper is ready for publication in ICRL. There are many good things to this paper -- such as a beautiful motivating example on the first page, and a detailed technical appendix -- but it still needs more work to be of interest to the broader ICRL community. The paper has significant presentation issues, which make it hard to evaluate the soundness of the claims, theoretical significance and novelty. 
The described model evaluation, and user study, are insufficient to support the claims. "	2
This paper proposes a new variant of QR-DQN with ensured monotonicity in the quantile function. Given that the quantile crossing issue is not new and the design of SPL-DQN closely follows NC-QR-DQN, the overall technical novelty is somewhat limited. Despite that SPL appears to outperform other variants of QR methods in some control tasks, the amount of improvement is not very significant compared to the benchmarking results. Therefore, I lean towards rejection for now but am willing to change my score if the authors address the concerns.	2
The paper proposes a novel algorithm with strong empirical results.	4
The paper makes a novel contribution in terms of using monotonic rational-quadratic splines to interpolate an inverse cdf function for distributional RL. The empirical result is very promising. I suggest weak acceptance.	3
"The method is a natural ""upgrade"" of NDQFN and thus its novelty is relatively low. 
The experimental results on continuous control environments with noise show better results in most environments.
However, it lacks a real analysis of the causes of this improvement.

For these reasons, I tend to vote for rejection in this current form.


**================== After rebuttal ==================**

Although I find the spline approach and the experimental results interesting, 
I still feel that the paper lacks theory to explain the improvement and, also, that novelty is relatively low.
My concern regarding the fixed quantile levels used for training has been addressed with the uniform sampling and the new experiments.

Therefore, I increase my score to 6."	2
My rating is due to concerns about reproducibility (no code and bunch of results on proprietary dataset), and lack of comparison with previously published results for the MQ-CNN model which this paper aims to improve over.	3
The proposed approach shows some improvement compared to existing work, but it’s technical technical novelty is a bit limited.	2
Given the significance of the three contributions, supported by the strong experimental setup (well established benchmarks and datasets) and empirical evidence (statistical significance), I am recommending to accept this paper.  	3
"This paper presents a region-aware deep learning framework for the point cloud registration task. It can achieve good performance
and is robust to noise. However, the authors failed to state the motivation of this approach clearly and the implementation details
cannot fulfill the claimed idea. This paper also has many flaws, such as suspicious implementations, insufficient experiments and
unclear descriptions. Therefore, I tend to reject this paper."	3
This paper is not straight-forward to follow. Many technical details are either not clear or not sound. So it's hard to understand and justify the contributions and novelty introduced by this paper. In addition, another issue is the correctness of the region segmentation module. I don't think it can be trained without any supervision. 	2
The major problem is it is hard to understand the underlying principle of why this network can encode the region feature and how is the region feature gets learned without any supervision. Overall, I feel this paper is not ready for submission and is below the level of this conference. But I hope the authors could address my questions, I want to make sure this is no misunderstanding.	2
While the method proposes an interesting idea, it is not yet ready for publication. First, the method description is unclear, especially in Sec. (2.2) and (2.6). Second, the experimental setup is not convincing, as identically sampled points are used for source and target shapes. Finally, several important related work from 2020 and 2021 is not discussed or compared against.	3
"The paper is not well-organized. There are many conflict symbols and illustrations which introduce confusion. 
The overall idea is just-so-so and it has less impact on the community from my point of view. The experiments settings are not clear, comparison algorithms are too old, and there is only one evaluation dataset. All those degrade the quality of this paper and I am not convinced by the results. "	2
The paper addresses the exploration-exploitation trade-off proposing the maximum entropy principle over the state-space as a solution to this problem. Although a promising avenue, many parts of the paper are confusing, using inside jargon, and decreasing the overall clarity. Some parts of the paper are not supported by results, and the main methods used are referred to the appendix. 	2
"# Final Remarks
* As evidenced by my main review, I have struggled to identify strengths of this paper, in large part because the precise nature of the technical contribution and how it improves upon prior work is unclear to me. 
* Still, the empirical results are at the level of a preliminary workshop paper and more experimentation seems warranted across a broader set of environments to confirm the utility of the proposed approach. 
* The clarity issues raised above suggest that the paper requires substantial revision in order to more clearly convey the contributions and algorithms to readers.
* For now, my score is quite low though I would be happy to raise it in light of suitable clarifications from the authors.

======= Post Rebuttal =======

I thank the authors for their response but the lack of clarity around the approach itself and originality as well as issues with the writing style point to substantial revisions that are needed before the submission is ready for publication."	1
Poor clarity makes the contributions hard to follow, along with weak empirical analysis puts the paper below the bar for acceptance. 	2
The paper considers an interesting and relevant question but the technical novelty and significance of the provided results does not seem to be sufficient. Algorithmic additions over prior work include using a combination of input and gradient noise and adding general multivariate Gaussian noise. The key analytic insight is to use Taylor approximation to estimate DP afforded by input noise. It is not clear how to set the hyperparameters of the proposed algorithm. Also formal presentation is somewhat lacking. Even though the approach beats some prior works empirically, I believe the work is marginally below acceptance threshold due to above issues.	2
This paper investigates how the random smoothing noise can be transformed into gradient perturbation, and then carefully compute the privacy loss, which seems an interesting method.	3
This paper mainly builds off the work by (Salman et al, 2020) [1]. Although DP analysis and tighter bound on DP guarantee are of some significance, the authors are suggested to 1) compare their proposed method with [1], 2) improve overall writing clarity, and 3) significant improvements over experiment settings.	2
"While I think this article studies an interesting problem, I do not think it presents its contributions convincingly enough. In particular, I have concerns about the technical quality and the novelty of the article that lead me to recommend its rejection.
"	2
Overall speaking, this is a decent and novel paper analyzing the training algorithm of EBMs from the perspective of probability measure. The paper could be strengthened if more theoretical analysis of convergence speed and more empirical results are included. 	3
This paper potentially has rich and interesting theoretical insights, but the results are very poorly described and insufficiently demonstrated in experiments. 	2
The proposed method is novel and promising for some useful applications. The paper is tightly presented although some reorganization can make the main contribution clearer. However, the theory justification, comparisons to the benchmark, and empirical evaluations are weak.	3
"I think this work has a lot of potential, and the theoretical quest set by the authors is important.
However, the theoretical result per se is not fully exploited, and while the authors state this is on their research agenda, the current paper is weak on the algorithmic (the suggested idea of an annealed p_R should be developed) and empirical sides."	3
This paper proposes a simple and effective numerical SDE integrator for score-based generative modeling based on SDEs. Paper can be stronger with a deeper theoretical understanding.	4
Good paper with interesting and essential ideas in improving the usability of diffusion models. However, the current experimental result and the method justification are weak.	2
"The major contribution of the paper seems to be an application of existing numerical solvers to sampling from SDE models. The novelty could be limited as the solver already exists (despite the tricks proposed to improve empirical results). Given that the theoretical contribution is limited, the major contribution would be on the empirical side. However, the empirical results are not impressive enough: when using a small number of sampling steps, the proposed method has much worse results compared to DDIM. As the goal of the paper is to efficiently sample from SDE models, having strong performance when the sampling steps are small is important.

The paper writing can be improved. There are some formatting issues."	2
"The algorithm seems to be empirically successful. However, the paper lacks justification for why their algorithm can be better. There is also much space for improvements in the experiments (especially, on the overall generation speed measured by seconds). In addition, the title is exaggerated. 

---------
Updates after the rebuttal

I appreciate that the authors agreed to use a more precise title. Some of my concerns are also addressed. Therefore, I will increase my score to 5. However, I still think the experiments lack some key components such as time spent and number of rejections, although they might strongly correlate to NFE. The empirical results are good but not impressive enough. I encourage the authors to further improve the method and make additional theoretical/empirical justifications. "	2
"The proposed algorithm in this paper sounds technically interesting. However, I have some concerns on some of the descriptions, model designs, and empirical results. Thus, for the current version, I would rate it as borderline paper, and I will consider raising the score if the main concerns are properly solved during the discussion period.
"	3
"This work formulates the graph structure learning problem as an inverse problem. Such a formulation is interesting. However, there are multiple concerns on the modeling, the setting, and the evaluation, which leave ample room for improvement before the paper can be published.
"	3
One fundamental question about the paper is not clearly stated.	2
I recommend weak acceptance because I feel like this paper presents an interesting, novel and well-grounded idea, with a decent level of evidence towards empirically ascertaining its contributions. However my confidence score is low as I lack expertise in this particular domain -- it is very much possible that I am unaware of all of the relevant baselines.	4
"The novelty of this work could be limited, and the experiments seem not enough
for validating the effectiveness of the proposed approach, so I recommend to reject.
=======================
After reading other reviews and the authors' clarification, I have increased my rating."	2
Overall an interesting read and a well written paper. My recommendation is to accept the paper.	3
"Overall, my impression is that the paper is not quite ready, yet. I believe that more experiments and theoretical analysis are needed to fully develop the proposed idea. I do appreciate the core idea, though, and believe that a future iteration of the paper may well be a good contribution to the field.

/edit The authors have resolved the most crucial points in their response. Provided that these crucial arguments are amended to the paper, I believe that it can be published."	2
"Correctness: 
- I believe the statements are correct, but I did not checked ever single line of all the proofs. 

Novelty:
- Many of the concepts are natural extensions of the sliced Wasserstein distances. However, there is sufficient novelty.

Empirical novelty:
- A very complete numerical examples set is presented."	3
"The authors introduce the ASWD a new method of SWDs and derive a set of theoretical properties and propose a numerical algorithm to approximate ASWD. The ASWD is a proper metric if and only if the original data samples embedding in a higher-dimensional space through a nonlinear injective mapping. In my opinion, the optimizing ASWD with respect to the injective mapping is strongly depending on the choice of the tuning parameter which guarantees that ASWD is a valid metric if it is greater than $1$. However, in practice, it is better to conduct experiments with values less than $1$. This creates some ""broken pieces"" between theoretical fundings and experiments."	3
The paper is well written and proposes a new sliced Wasserstein distance using the spatial Radon defined Transform defined as line integrals of a function along all hyperplances over a hyper-surface induced by a non-linear injective mapping. The new distance is supported by theoretical analysis and empirical experiments showing its effectiveness. Topological properties of the distance and some details on the empirical  evaluations remain to be clarified. 	3
This paper is well written and well motivated. The proposed method seems to have some originality, though the technique of using non-linear functions has been proposed. Various experiments show its significance over existing methods.	3
In summary, the authors addresses an interesting problem and proposes a novel method, and overall an interesting paper. The authors may need to improve its justification of the PGM omitting condition (ii), or find ways to make it consistent with the proposed 4 conditions of relational variable.	3
Overall the formulation seems interesting and results show that the general approach is on par with state of the art specialized methods for the tasks considered. However, the relationships in the experiments seem a bit forced and maybe simpler data augmentation methods could replace the relational augmentation methods used.	3
"I have some questions/concerns about the paper so recommend weak reject, but would be happy to bump up the score if my questions/concerns can be satisfactorily addressed.

------------------------------
The authors have satisfactorily addressed my concerns, and I bumped up the score accordingly."	3
Good paper help back by the presentation.	3
"Overall, I liked the idea of formulating the problem as ""learning relations from relations"". However, there are other approaches that are not mentioned here which try to find ""motifs"" in graphs or perform link prediction, where the objects (without their properties) are represented as nodes, relations are in the edges and new links are learned from the original graph.

From your example (sad-happy faces), it seems features of each image are being used to learn new relations. Why did you choose images for training? I would think that a tabular dataset (specially in areas such as health) could benefit much from your approach.

References are outdated. The last year for your refs is 2019. There has been a whole body of work regarding relational learning and methods developed in 2020 and 2021.

"	3
This work has several merits as pointed out in the main review. The key concern is the unfair comparison between Certify and CertifyADP presented in this work. Moreover, the writing of this paper can be vastly enhanced for better exposure. 	3
The underlying topic of constructing a certified robust classifier is important. The main idea of the paper is sound and is supported by extensive experiment investigations. Further, the authors proposed algorithms to reduce the computational cost, making their method not only workable but also practical. There also includes empirical studies to validate (to some extent) the assumptions in their theoretical arguments. Hence, I would like to support the publication of the paper.	2
"Certifying an ensemble of models is faster than certifying an individual model through the proposed two strategies. However, training an ensemble of models requires far more computational resources and time than training an individual model. Although the authors mainly leverage the pretrained models, but in practical applications this may be infeasible. I would suggest the authors discuss the issues of training an ensemble of models.   

The authors say that “…substantially increased certifiable radii for samples close to the decision boundary..” in abstract, but no results to validate this claim. I would like to see the results for those samples having a small margin.  

The authors assume that y_c is the clean part and y_p is the perturbation/noisy part. How to define y_p and y_c when evaluating the real image datasets? Or y_p and y_c are just for analysis? 

What’s the impact of $\alpha$ and $\beta$? 

In table 1, why ensemble is less effective than MACER when radius r is small? 

“Even when using only K = 2, we already obtain 81% and 70% of the ACR improvement obtainable by always evaluating the full ensembles (k = 10 and k = 50) “ => How do you calculate the numbers 81% and 70%? 

 In Table 3, what do SampleRF, KRC, and TimeRF mean or short for? 

My another concern is that the experimental results are only for certifying L2 perturbation.  There are several papers that design RS to certify $L_0$, $L_1$, $L_\infty$ perturbation.  For example, 

$L_0$: Wang et al. “Certified robustness of graph neural networks against adversarial structural perturbation via Randomized Smoothing”

$L_1$: Lee et al., “Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers”, NeurIPS 2019

$L_\infty$: Yang et al., Randomized Smoothing of All Shapes and Sizes

I am also really interested in whether the proposed ensemble is also effective for certifying $L_0$, $L_1$, or/and $L_\infty$ perturbation. 

The following papers also derive certified robustness based on randomized smoothing: 

Jia et al., “Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing” 

Zhang et al., “Black-Box Certification with Randomized Smoothing: A Functional Optimization Based Framework”

Mohapatra et al., “Higher-Order Certification for Randomized Smoothing”

Kumar et al., “Certifying Confidence via Randomized Smoothing”

Kumar et al., “Curse of Dimensionality on Randomized Smoothing for Certifiable Robustness”

Fischer  et al., “Certified Defense to Image Transformations via Randomized Smoothing”"	3
"The paper makes reasonable improvements to the sample complexity of InstaHide challenge. However, I have two main concerns:

1) Is the attack in the paper gives any significant improvement in real world application? I feel that it is not, and paper has no experiments. Given that InstaHide is rather new suggestion for privacy in ML and has not really been widely used, so I am not sure if this is a significant improvement in practice. I would like to know authors take on this.

2) My understand that finding W such that WW^T = M by applying Whitney's theorem seems to be the only difference compared to CSZ20 paper. Is this correct? It would be worthwhile to summarize technical improvements over CSZ20.
"	2
The authors provides a solid solution for a problem with limited scope. In the current form, I believe the value for the general readers are limited, therefore I would recommend for weak reject. It will be great if the authors can generalize the results for a larger impact. 	2
"First, the paper is well written and the theoretical results are sound.  Secondly, the paper indeed sheds light on the weaknesses of InstaHide. The authors have in addition shown the hardness of the involved regression problem with hidden signs, which is responsible for a large amount of time (theoretically speaking) needed for the suggested attack on InstaHide.

"	3
"Overall, I recommend accepting the paper. The paper presents a novel approach to finding large regions of adversarial examples, with strong experimental evidence that it scales well. The details provided would enable other researchers to reproduce the presented approach. Most importantly, this approach is likely to be something that other researchers can use and build upon.

Having said that, the paper has some issues with clarity. Details are provided in the main review, but I'd like to highlight in particular Section 5.3, which I found particularly hard to parse.

N.B. My current recommendation for this paper as-is is 6, but I'd be quite happy to upgrade the recommendation to 8 if the bulk of my concerns around clarity are addressed.

--- 

## After Paper Discussion Period
During the paper discussion, the authors addressed the bulk of my concerns around clarity, and I've upgraded my recommendation to 8 as a result."	4
Overall I incline to a weak rejection at this stage of the reviewing process but I am open to any discussions. The reasons that prevent me from giving higher scores are the insufficient descriptions of the motivations and the current way the experiment sections are written with, which I have mentioned in my main review.	3
See above.	2
My main concerns with the paper are around whether the RNN encodings are overfitting and picking up on spurious correlations in sequence data that say a convolutional encoder might already fix, which will mean that one does not really need symbolic encoders for the current task. This is an even larger concern in light of the fact that a general machine learning researcher might not have a lot of great intuitions about the neuroscience sequence data and that makes it hard to assess the impact of the work. For the rebuttal I would encourage the authors to address concerns marked with (*) in the main review.	3
This is an interesting and novel way to learn programs as symbolic encoders of the input. I have some nagging concerns about how incremental the contribution given how much this work relies on Shah et al.	2
This paper proposes a very interesting research direction, but the writing and organization of the proposed method make it hard to understand it. In addition, the current evaluation is way too simple and not interesting enough. I highly recommend the authors to add some experiments to show that the learned programs can help some down-stream tasks.	3
Although the concept and method are simple and new, and the paper itself is well-written, the significance in the problem setting and experimental results were unclear for me. Therefore, it is difficult for me to provide a higher rating at this stage. 	3
"The potential impact of this work is not clear as not enough is said about practical advantages.

Gaining some insights into the model behaviour would make the content of the paper stronger."	3
"- I agree with the theoretical significance of this work. 

- A few questions/comments on the perspective of practitioners are listed above. "	4
The topic is important and challenging. The results are novel, and the experimental section provides a nice illustration how the joint Shapley values can be used.	3
Well written and polished paper with novel angle on the active learning problem, with some theoretical guarantees and good experimental performance in low budget scenarios. Would like see some additional results, but this a good paper.	3
Overall a strong accept for tackling an important problem.	3
"This paper is well-written and motivated, with a theoretical guarantee.
Before seeing the other reviews, I am positive to accept the paper although including some concerns (e.g., scalability).
Please answer and clarify my concerns described above."	3
I think overall the approach makes sense, but I didn't find significantly novel contributions, either theoretically or empirically (maybe the authors could point out their novel contributions). As a result, I'll vote for a weak acceptance.	2
Overall, the manuscript looks like a good paper. I do hope reading the paper can be easier despite the page limit.	3
The theoretical novelties are limited, this paper mainly borrows the ideas from two papers DINO [Caron et al. 2021] and Swin transformer [Liu et al 2021]. However, the experimental results are very good in this paper, and the paper provides a lot of implementation details for others to reproduce the results. 	2
"I find the main idea of this paper convincing, the manuscript well written, and the evaluation thorough. The fact that the authors highlight some limitations of their loss term, in particular when applied to image segmentation, attests to their scientific scrutiny. I think the paper deserves a publication. I am willing to rate it 8/10, instead of 10/10, because I find the contribution incremental and the performance gain modest.

In this round of reviews, I lower my recommendation to 6, because I find the weakness (2), described above, acute. It misrepresents the effect of the contribution on performance in the teaser figure and in the first table with quantitative results. I am confident the authors can fix it in the next revision of the paper, which will prompt me to increase the rating to 8/10.

I encourage the authors to also address the weakness (3) in the next revision of the paper. The current transfer learning experiments (section 4.2) do not evaluate the specific contribution of the paper, but rather answer more general questions. I am confident that adding evaluation more focused on the use of $\mathcal{L}_R$ will vastly benefit this part of the paper. I nevertheless do not feel that I should request completing this task within the limited review period, and I will not lower my recommendation on this basis.

EDIT:
the authors addressed the weaknesses 2) and 3) listed in the main review and answered all my questions. I am therefore happy to endorse the paper. This is a solid work and deserves to be published."	3
The work extends the 1st order results in (LeCun et al., 1988) to include 2nd order information for feed forward neural nets, and experimentally compares the new 2nd order methods with other neural net training techniques. The Sifrian formulation is interesting and provides new neural net training methods.  There is a potential issue for the Sifrian to be hard to solve. However, the paper requires the existence of a peculiar regularizer, but does not discuss any statistical properties of this regularizer.	2
Given the above points 1, 2, and 3 I recommend rejection	1
Overall, I find the paper hard to read and follow, in particular due to the notations. The introduction of the Sifrian is not really justified (**Q1**). It seems to derive from a splitted problem with auxillary variables but this is never explicited. In particular, I believe there is a link with ADMM training that use such auxillary variable which should be explicited (**Q2**). Moreover, the results only apply to very specific networks and these constraints are not discussed (**Q3**) and the numerical experiments are not really convincing (**Q4**). For these reasons, I feel that the paper is not ready for publication and I recomend rejection.	2
The manuscript is poorly motivated and not clearly written, and the proposed algorithm has  neither strong numerical nor sound theoretical support.	1
"This paper studies generalizability of neural networks using four factors (interpolation, extrapolation, random, and composition). Previous works on better data augmentation policies uses similar factors to improve the generalization of visual recognition networks. So, the results of this paper are not surprising.


=================
Post rebuttal: I am convinced with author's response and I am leaning towards acceptance. "	2
This is a solid paper that contributes new empirical findings and a new benchmark on an important topic. I believe this work will stimulate future work on representation learning, disentanglement, and OOD generalization. I don't see any major errors and recommend accepting.	2
It is a well-written paper with great analysis and a very useful study to have. I have small concerns about the novelty of the conclusions, but the paper is still a great addition to the community. 	4
The study poignantly points out failures of current models in generalization through a large number of empirical studies. The work is well written and clear. It is not particularly novel to argue that models fail to generalize, but the current study provides quantitative and extensive benchmarks in well-controlled settings. Beyond pointing to limitations of current models, the study does not offer new paths towards new algorithms or solutions to those problems. However, sometimes, understanding what the challenges are is a good step towards generating momentum to find better solutions. 	3
"Reviewer believes the paper has some interesting novelties but more ablation studies are necessary to support the points made in the paper. I am looking forward to reading the rebuttal and will adjust my score accordingly. I am willing to update my score to accept if the rebuttal addresses my concerns.


----------------- Post Rebuttal -------------------------

Thanks for addressing most of my concerns. I increased my score to 6 with regard to the rebuttal. I think the paper could be stronger with more experiments including dataset size vs. accuracy plots which show where the accuracy plots saturate. 

Most of the points raised by the reviewers are just and important. However, I disagree with reviewer yR54 on the importance of using ""Adversarial Density Map"". As the reviewer mentioned, the problem of weakly supervised learning is typically ambiguous as the network might find to detect non-relevant features that still can decrease the final ranking loss. As this is also the case for other weakly supervised methods including object detection, I don't think the authors should be penalized for this. Instead, we need to focus on the novelties that can address some of these ambiguities. Adversarial density map loss put a meaningful constraint on the predictions to make them look like a mixture of gaussian among all the possible distributions that the network can generates. It is unlikely that it can solve all the ambiguities in the task but it should resolve some."	3
The idea of using count ranking is interesting. The annotation cost comparison is not convincing and the work is lack of technical contribution. 	2
This paper propose an interesting setting of pairwise ranking information as weak supervision to count objects. However, as I mentioned in the main review part, there are several limitations in both methodology and experiments. I hope the authors could address my concerns listed above and I think we could not accept this paper at current stage.	2
All in all, the paper had very nice ideas, but the writing and presentation is suboptimal. This means that it is not ready yet for publication.	3
The strength and weakness of the paper are very clear, as described above. I would give an overall score of marginally above the acceptance threshold based on its theoretical nature and serious study of the phenomenon.	3
"I am quite positive on the submission, however, there are two issues related to the proofs of the theorems. These issues might only be due to a misunderstanding on my part, but I am not sure. I hope the authors can clarify on this. Until then, I rate the submission as ""5: marginally below the acceptance threshold"".

**Post author response update**

The authors adressed the correctness issues.

Regarding 1. The authors confirmed, that Corollary 1, does *not* imply that NC1 ($\lim_{t\to\infty} \Sigma_{W,t} = 0$) and presented the remedy of replacing NC1 by a scale sensitive version, corresponding to the ratio of inner class variations to the between-class variations.  While this is a weaker result, it is still a wortwhile contribution and strongly connected to the observations by Papyan, Han and Donoho (2020).  
In fact, the updated definition of NC1, is precisely the quantity measured in the experiments by Papyan, Han and Donoho.

Regarding 2. The authors added the missing proof to the appendix, that the singular vectors of SNR indeed stay constant when subject to the dynamics from Eq. 5.

With these two major issues out of the way I will adapt my recommendation to **8, accept**.

Last but not least, I want to thank the authors for their detailed and clear responses. I really appreciate, that you took the time and effort to already updating the manuscript with proofs for my initial and follow-up questions."	3
I would recommend accepting the paper. The paper further extends our theoretical understanding of deep learning. The result is new and supported by experiments.	4
This submission investigates the interesting question of memory in language models, but the task formulation, data and experimental setup can be improved and therefore I don't recommend the paper for publication at this time.	2
Overall it is an interesting and well-written paper trying to explore an important question with some sound experiments. At the same time, it seems to have certain weaknesses in the experimental setup and design which diminishes the conclusiveness of some of their results. 	2
This paper thorough studies LSTMs and Transformers on a recall-based probing task. The work raises several interesting conclusions about LSTMs and Transformers, and they are well-supported by the experiments. However, it is somewhat unclear to me how these experiments might translate to other settings, and I'd like to hear more from the authors about this. Overall, I thought the paper was interesting and well-executed, if a bit narrow, and would be an interesting paper to the ICLR audience.	3
"Pros: 
A very important research question, a nice experimental setup, interesting results regarding GPT-2.

Cons:
Potentially inaccurate conclusions regarding the connection between LSTMs and transformers due to various confounding factors."	3
"By using verbatim extraction as a task, the authors reveal the difference of two popular autoregressive neural architectures: LSTM and Transformers. The authors also show that the phenomenon is related to the attention pattern, and have answered different questions including model depth, training size, intervening text impacts on the results.

However, many related works do not seem to be well compared or even discussed. These include at least attention in RNNs and copy mechanism, and maybe also related to linguistic structures that transformer might capture. These missings make some important research questions unanswered for this work. "	2
This paper tries to shed light on the optimization challenges of the popular WGAN formulation, and the simplified assumptions on discriminator/generator architecture look like valid ones to me. Although the authors use the previously established results in other works, gathering and presenting such results seems to be very important for less understood areas such as non-convex non-concave GAN scenarios, and I enjoyed reading through all theoretical parts of the paper.	3
This paper proposed for the first time the convex-concave game equivalence of 2-layer WGAN model training problem with general activation functions. Experiment results also favor the theoretic results. I would recommend accepting the paper. 	3
Although there are some results on WGAN, knowing these results does not lead to any insights on why WGAN really works as a good data generator method for many applications. 	2
This is a well-crafted paper, with both strong theory and adequate empirical evidence to support the claims. My suggestion is to refine the presentation to highlight both the intuitions and more practical aspects. 	4
In general, the paper is well-written and the experimental results look strong. However, the lack of clear motivation and explanation of the proposed method downgrades my rating to the paper. Furthermore, the theoretical part could be more detailed as the current one does not provide a convincing improvement over vanilla SGD method. I hope the authors will answer these questions in the future.	2
The proposed method of only storing critical gradients in a limited size memory buffer seems interesting. The experimental evaluation is comprehensive. However, the method only seems to improve in terms of gradient steps but not wall clock time to converge due to bookkeeping overheads.  	3
Overall good paper, some areas of improvements	3
"Overall, I like this paper as a simple to implement empirical improvement to other algorithm that just ""makes sense"": very large gradients should probably have an impact for a longer time BUT not all at once so as to not dominate fully.  It gives good but not overwhelming empirical evidence (difficult to compare to SotA due to architecture choice and different tuning) and some theoretical justification (albeit not applicable to the settings evaluated)."	3
"The paper proposes a new weighted training algorithm, TAWT, to learn the task-aware weights on tasks for better using the cross-task signals.  The proposed method is quite effective for transfer learning and multi-task learning on small target data. 
The weighted training is very important but there lacks good work in this direction. Therefore, this paper is great to give an attempt for task-aware weighted training."	4
An interesting algorithm, theory, and results. A few parts of the paper need clarification.	4
This is a good paper with novel ideas, but it can be further improved by adding more backgrounds, related works, and comparisons with other state-of-the-art methods.	3
A rare mix of well-motivated method, theoretical guarantees, solid experimental work and convincing results. There are minor presentation improvements to be done but I think this is a strong paper otherwise. As noted earlier, I am not able to review carefully Sec 3/Appendix A so this the caveat to my review.	4
In terms of convergence and stability, the proposed method is considered to be effective to a certain extent. Also, the idea of using contrastive learning for WAE is interesting. However, the explanation of the claim and the presentation of the results are insufficient.	2
The paper is interesting but has some shortcomings. I would like to see some results of the baselines to decide if the proposed regularizer does indeed improve results qualitatively. I do not believe that FID is a proper measure of quality (not just for this paper but for measurement of GAN sample quality, in general). I give the paper a slightly positive score based on the idea, but I am looking forward to some samples in the rebuttal to decide my final score.	3
The main idea of the paper is well motivated. However, I still find the results on image datasets such as CIFAR10 and CelebA hard to justify the superiority of the proposed method over baselines. I lean towards weak rejection but am willing to amend my score if my concerns are addressed.	3
"Overall I think the paper makes an interesting contribution to RL where the agent is able to access prior data in making accurate estimations of Q-values and through that produce a better policy for maximizing rewards from a given task. 

Some concerns related to the paper are as follows: Providing detailed pseudocode or code for replicating the results, ablation to understand the retrieval process, and detailed presentation of the proposed approach.
"	3
"The paper proposed a retrieval-based state augmentation approach for online and offline reinforcement learning.
The retriever is stateful and end-2-end differentiable.
The approach is evaluated in online RL using Atari benchmark and offline RL.
The experiments in offline RL missed the main baselines and could have been evaluated on a state-of-the-art benchmark like the D4RL benchmark.
The retrieval approach also could have been evaluated against a fully attention-based model over the dataset, like Performer or Reformer Transformer architecture for example.
In conclusion, while the paper is reasonably well written, the contribution seems somehow limited and more experiments are needed to assess the full benefit of the proposed approach."	2
The proposed method involves complicated design choices which are not well justified. Also, some important baselines are missing in the evaluation in the experiment part. 	2
"Overall the paper presents a nice idea and is well written, and introduces a novel offline multi-task benchmark. However I believe there are shortcomings in the experimental methodology, and furthermore despite the relatively large increase in complexity introduced by R2A, I don't believe the authors have enough convincing the reader that this increase is worth it.

I would therefore recommend at this point a weak reject, but believe if the authors could address the points made concerning experimental methodology and design choice explanations, I'd be willing to increase this score."	3
The paper proposes a novel DRE method which addresses some of the issues that the previous methods (e.g., TRE) had. The paper is easy to read but some of the arguments are weak and not sufficiently justified. More formal arguments/statements will help with the accuracy and readability of the paper. The paper is not theoretically strong, but offers a useful and practical approach which can be impactful.	3
"In summary, I like the idea of the paper, but most examples and experiments are on Gaussian distributions which may limit the scenarios of real applications. I would encourage the authors to include more results on more “malicious” densities, or at least explain why such suggested experiments do not make sense.
"	2
This paper attempts to contribute to solving an important problem in the context of DRE. While the attempt is novel and very interesting, there is insufficient comparison with existing methods and justification of the proposed method. Therefore, I vote for weak reject. However, depending on the rebuttal, there is a possibility to raise this rating.	3
This paper lacks several important analysis on the proposed methods and the writing was misleading and confusing. The paper requires a lot of work to improve. 	2
"See above
=====================
After rebuttal: The author response has addressed my concerns. I am happy to increase the overall rating."	2
The paper proposes an interesting extension of prior work that offers great potential for expansion in future work. I'm very positive about the framework and theoretical results, but slightly down on the empirical ones, since they do not speak to the particulars of the framework. 	3
The paper presents an interesting idea of viewing value decomposition by mapping in a different space after separating the reward functions. Although some parts of the paper still leave open questions, most is clear and very simply written. This framework appears to be useful at least in the Atari domain, but the specifics of the experiment and interpretation of results would benefit some more clarity. The paper contains interesting insights.	3
"The points highlighted under strengths and weaknesses form the basis of my rating. I am generally supportive of the paper and think that it addresses the test-time adaptation problem from a relatively constrained yet realistic standpoint. The paper is well-written and generally easy to follow. The most positive bit in support of the paper in my opinion is the lack of reliance on the entirety of target data or multiple passes on the same to adapt a model. Regarding weaknesses, since the paper relies heavily on the meta-source and meta-target split construction, I think it’s important to address the extent to which this reliance is true (in accordance with the suggested experiments in 1). The minor points in 2 are addressable, I think.

**Thoughts post author responses**

As stated in my follow-up reply to the authors, I think my primary concerns regarding domain-split construction were sufficiently addressed by the authors. Similar concerns were also shared by other reviewers and the authors provided sufficient experimental evidence to address the same. Additionally, concerns surrounding results with different backbones have also been sufficiently addressed (in my opinion) by the new experimental results. Given these and the fact that the paper makes an interesting contribution and studies a timely experimental setting, I continue to recommend acceptance of the paper (and would like to stick to my original rating)."	3
As the discussed above, due to weak performance, and lack of more empirical demonstration of the proposed method. I recommend marginal reject on this paper. I would like to adjust my rating after discussing with other reviewers and AC	3
"This paper addressed ""single test sample generalization"", which is an interesting and important problem in the field of domain generalization. The method is technically sound and novel. Although I have some concerns regards the method and experiments as indicated in the main review, I overall remain positive towards this paper. I hope the authors can address my concerns in the rebuttal.

After rebuttal: I have read the response and comments of other reviewers. All of my concerns have been addressed in the response. I recommend acceptance for this paper. I highly suggest that the authors should widely discuss the remaining weaknesses raised by the reviewers in the final version."	3
interesting idea, but confusing/inconsistent claims; unclear why a random split of source into meta-source and meta-target would create a distribution shift.	2
"Preliminary recommendation: unsure. The paper describes an elegant method with good empirical performance, but it does not provide intuition/theoretical explanations/empirical analysis why it should/does work. I feel that this paper brings little new scientific knowledge that future research could build on, hence its impact may be limited.

Requests/questions to the authors.
- Please address the negative points raised above.
- How are the source/target domains generated during training ? I understand they are generated at random, but since you have multiple domains in the datasets used (say, PACS) I imagine you do use the annotations of these domains at some point ?

---
**Final summary after discussion with other reviewers**

I am fine seeing this paper accepted given that this is the advice of the other reviewers. However I still see important weaknesses in this paper that the authors may want to fix in the final version and/or in future work.

1. There is no intuitive or strong theoretical support for adaptation from a single test example. The proposed method does produce non-trivial empirical results that means that it must rely on specific information/assumptions, which are not made clear. The reason it matters is that these assumptions must have limits of applicability. These **limitations** should be explored, discussed, or at the very least acknowledged in the paper. My impression is that the variational and meta learning aspects of the proposed implementation are eclipsing the more fundamental points on which the method relies.


2. The authors responded to the lack of **illustrations** by including fig. 1/2 but I don't find these satisfatory. Figure 1 provides almost no information regarding (1) above. As for Figure 2, it's not even clear what it represents (""architecture of our method""). Training time ? Test time ? The boxes are a mix of operations/layers, variables, and distributions. The colors are not defined. Etc.


3. I still think that calling of the approach ""**meta learning**"" is a stretch. Another reviewer made me realize that there is a precedent in the literature for using the term when merely mimicking the adaptation at training time, but I'm not sure that perpetuating a bad choice is a valid reason."	3
This is a piece of work of good quality, with detailed information. A rigorous and principled approach is taken by this work to address the second-order pruning issue. Experimental study demonstrates the overall competitive performance of the proposed method. At the same time, some parts of this work can be further clarified to make the argument more precise and clearer. The novelty and contribution with respect to the existing relevant methods can be better described. Also, the structure of the presentation can be improved a bit.	3
"1. This paper is an exciting work, and it would have a solid impact on the structured-based model pruning.

2. This submission also has a lot of minor issues. For example, the second-order approximation seems to be developed towards addressing particular two losses used in classification. It is not clear if the proposed approach would work for the other losses.

3. Some technical terms are not well explained.

4. The evaluation metrics used in this work should also include speed, but it is missing in the current submission."	3
Overall, I think this paper has its merits. However, as I am not an expert in this field but a general reader. I am lean to accept this paper.	3
Due to the limited potential performance improvement in practical benchmark database, my overall rating would be (weak) rejection	2
The idea is simple and generally applicable, but the lack of novelty and the marginal result improvement greatly limit the impact of the paper.	1
"The paper proposes a simple modification to recurrent language models that can potentially improve performance, but additional analysis and results are needed to establish the utility of the approach.
"	3
While I think this revisit to LSTM is interesting, the scope of the experiments are still limited. Comparison with transformers is not conducted, and the datasets are small scale.	2
Interesting method with but modest results.	3
"Generating relevant synthetic theorems is a nice and nontrivial
research direction. It is related to tasks such as lemma introduction
and theory exploration. The performance of the systems is good. 
The authors have done a decent amount of work."	3
"Due to the concern mentioned above, I don't recommend accepting this paper.

============
Post-rebuttal: My main concern is still the problem setup. I don't think it is a useful task to extract reusable patterns from existing proofs as stand-alone theorems and I can not see how a tool that can solve this problem could help the AI/TP community. So I decided to maintain my original rating of this paper."	2
"This is an interesting and simple (in a good way, i.e. clear) idea for turning the problem of lemma extraction into one of supervised learning on the nodes of a graph. The underlying assumptions are not entirely intuitive however (despite the claims in the submission). Moreover there are more direct ways of achieving the same result, but these older methods are not compared to.
"	2
Using a neural network on existing proofs, the authors show that sub-theorems can be extracted an reused to compress a proof library. This novel contribution is well documented and the authors provide strong motivation. However, aspects of the approach lack baselines or natural comparison points, making the work feel too isolated from current automated theorem proving literature and putting into question its value to the research community.	2
This paper proposes an interesting memory mechanism, and shows that it seems to help in some circumstances. However, questions about the quantitative evaluation, qualitative evaluation, and interpretation analyses leave me unsure about how much improvement the method really offers. 	3
The authors carried out adequate experiments to show the proposed module (MoCA) is can be used to improve few-shot image generation quality and they linked the module to the grandmother cell in the brain. However, the memory bank idea is not new and the novelty is limited.	2
A well-written paper with novel and significant contributions to an important research topic. The design is clearly explained and the experimental results are excellent. While there are a couple of minor issues, the overall quality is sufficiently high for an acceptance by ICLR.	3
I suggest the rejection of this paper in its current version. Although the authors have done comparisons of their framework with baselines, the proposed model is not totally novel.	1
Right now, I am more inclined not to accept the paper. However, I am happy to discuss and also change my evaluation if I missed anything important.	2
The method is trivial. The experimental results do not support publication of an methodologically boring paper. 	1
The proposed setting seems a bit misleading. The proposed model has no significant novelty. More standard benckmark datasets should be used.	2
"Concerns:
1. The introduction could have been more concise, I feel the authors could have avoided explaining the technicalities of the knowledge graph in the introduction. Rather, it would help the reader to be better situated in your research space if you discuss the fundamental difference between some of the related models and yours at an early stage of the paper. For example, the authors could have mentioned Wang et al. 2016 and ML-GCN  (Chen et al.) in the introduction itself. and explained why their work is different from the others. 

2. Are the comparisons fair? The GraphConv( ) step uses additional parameters, especially when the number of layers (iteration) increases. Could you include the number of parameters for all the baselines and your method?

3. The tasks are very specific to multilabel classification. What happens when objectives are different for each task? How would MetalLink adapt to that?

minor: In the paragraph: ""Results on biochemical datasets."", Table 5 ---> Table 1.
"	3
The paper is sound, introducing and validating an interesting heterogeneous graph design focused on each task and data sample as a different node. The work also offers an approach for the common labeling limitation in multi-task problems (by working with sparse labels in a dynamic graph architecture ). I recommend acceptance.	3
In general, I believe the paper is innovative and descent. The backed their proposal with thorough experiments. I think this work will be useful for researchers working in the area of multi-tasking.	3
The proposed approach is a novel combination of existing methods that is well motivated. Experiments are comprehensive, including an ablation study.	3
"The new architecture is interesting, proposes novel ideas that solve important issues of previous solutions and shows good empirical results. However, the paper has clarity issues and doesn't seem completely self-contained. Also the reduced number of random initializations goes below usual quality standards in published RL work. 

Still, I believe these points could be fixed through the rebuttal, so I am open to revising my score if the authors submit an updated version that tackles these points.

----------
Authors correctly addressed my concerns in the updated version"	3
"My evaluation largely depends on whether the chosen beaseline is consistent with the stated main contribution of this work: would AUC of other methods for RL would also be lower at earlier cut-off points. I see two potential possibilities:
(a) The chosen baseline is actually good for data efficiency -> the main claim of the paper holds, all's good
(b) The baseline was not good for data efficiency -> the paper should not claim data efficiency, but rather just go with performance increase (1425%, while baseline is 1202%).
I am looking forward to the discussion to gain clarity on this point."	2
"For an excellent version of this paper, I would recommend the following:
- Evaluate the models on more real world datasets with larger graphs - 100+ and 1000+ nodes and break down the performance of GNP based on the number of nodes in the graph. It is often the case that GNN architectures don't scale well on larger graphs.
- Add an ablation study with GNP+ with and without the additional linear layer."	2
"The GNP idea proposed in the paper is interesting and works well on small tasks. However, the non-negative constraint of the inputs may be too strong and can cause information loss. Without state-of-the-art large-scale real-world experimental results, I am not fully convinced by the effectiveness of GNP.
"	2
I carefully read the maintext and quickly go through the proof in the appendix. I am not sure about the correctness of the theoretical proof, though most of them appear to be correct. It is possible that I miss some details.	1
The GNP idea seems promising. Additional clarifications and experiments may strengthen the manuscript.	2
"It occurs to me that the main contribution is some new convergence analysis of SGD(M) with non-monotone stepsize in a multistage setting. The convergence rate of SGDM improves the state-of-the-art result of momentum methods. I could be wrong, however, whether the perturbation is cosine, triangular or any cyclic rule does not really matter, which makes this work less precise in characterizing the real effectiveness of each specific cyclic rule.  
Since the stepsize is decreasing, the influence of bandwidth is also decreasing, which makes the dynamic of this algorithm quite different from standard bandwidth algorithm, and more close to standard SGD. Although I agree with the author on the technical novelty in handling the non-monotoniticity, it was somewhat misleading given the title only mention Bandwidth stepsize. For these reasons, I think the paper still needs some restructure and improvement."	2
"The authors make some improvements over the existing literature for SGD and SGD-M algorithms, using a class of bandwidth-based step-sizes that is specified by stage lengths. The main weakness of this paper is the bounded assumption on the expected output of the problem. 

Since this assumption is quite strong, it is not quite comparable with the existing works for SGD without this condition. The novel aspect of this paper is also moderate. For these reasons, I recommend rejection. "	2
Although the bandwidth step size has been previously introduced, the theoretical results presented in this paper are interesting and novel. Additional discussions to some relevant works mentioned above should be included. The empirical contribution is somewhat weaker, but overall I am leaning towards accepting the paper.	3
The overall paper is well orgainzed and easy to follow. The theoretical results improves the current convergence rate than previous work. The experiments show the performance of bandwidth SGD. The connection between theorems and experiments could be stronger. Suggest marginally accept.	3
The authors empirically show improvements on 3 public LTR dataset using the BAR method over a neural baseline. To clearly support their research questions, they should continue experimenting with a few more baseline neural models to understand the effectiveness of BAR. Additionally, the ablations and design choices for transformation & loss functions should be done on more than one dataset to be able to conclusively make the claims made in the paper. The biggest gap is however, the lack of theoretical justification on why the specific choices in BAR are needed. The paper can just have empirical evidence to show one transformation that works well across datasets. However claiming that BAR is a novel listwise distillation framework with theoretical justification is far-fetched.	2
The paper explores an important area in learning to rank but the proposed approach lacks novelty and makes multiple ad hoc choices that need further justification.	2
I believe this paper is well written with clear statement of their research questions, extensive experiments, and rigorous arguments. I suggest to accept this paper to ICLR. 	3
As mentioned in the main review, I am inclined to weakly reject this paper due to the missing hyper-parameters experiments, unsatisfactory presentation quality. 	3
"The paper has clearly several weaknesses, but it is undeniable that it has brought forward an interesting topic of point cloud upsampling with temporal cues, and developed two relevant tasks that are useful in real life. I thus opt for ""marginally above the acceptance threshold"" as the initial rating and hope the authors could address my concerns during the rebuttal period."	2
"The proposed methods are well-investigated from different aspects in two different datasets. The paper is well-structured and well-written; most of the claims and hypotheses are supported by good arguments. However, it remains the question about the novelty's significance. If the authors provide reasonable explanations, I am ready to recommend the paper for acceptance.
 
Please, let me add a comment that I’m not an expert in the field of models for point clouds and admit that I could understand something wrong, but I've spent a lot of time reviewing the literature. 
"	2
The paper presents an interesting problem. The proposed method is effective. However, the novelty seems a bit limited. Some experiments and related works are missing. 	2
Please provide a short summary justifying your recommendation of the paper.  TPU-GAN is well designed to learn the underlying temporal coherence from a point cloud sequence, but additional experiments and additional explanations about the model and modules are necessary.  	3
I think this paper provides a nice contribution to the topic of representation learning in RL. The authors provided theoretical results for their FR and demonstrated its benefits on 4 different tasks. Although it would be nice to confirm these findings on harder tasks e.g., in the case of section 4.1, I believe the variety of tasks considered by the authors still shows some promise which is why I recommend an accept for this paper.	2
The paper gives a thorough description of a variant of the successor representation and its potential application in a few different settings. Its main issue is that it frames itself as a sort of preliminary description of an idea, with a somewhat limited evaluation in any one setting, but very similar ideas have already been shown to scale to the types of problems commonly studied in contemporary deep RL. Including a comparison to these ideas (both experimentally and also in the framing) would improve the paper and situate it better in context.	2
I find the proposed novel representation highly original, and ICLR is the right venue for its publication. It is a clear advance of the state of the art in computing intermediate representations that cache the system dynamics in a format that is more suitable for policy evaluation than the basic transfer function of the MDP. Its advantages over successor representations for the case of non-Markovian rewards are clearly explained and supported by empirical evaluations. For these reasons, I recommend acceptance. It is also apparent that the proposed approach has some limitations, and the authors readily acknowledge this. 	4
The paper presents a simple idea which is supported by ample theoretical analysis and (some) experiments. Comparison to prior work, while not exhaustive, is convincing. I vote to accept the paper but strongly urge the authors to improve the empirical analysis for the overall impact of the paper.	4
"This is a very well-written paper about an important topic which clears up some confusion caused by bugs in prior work. The arguments are clear and simple and likely to be useful to other researchers. Improvements and additions are certainly imaginable but inessential. I therefore recommend acceptance.
"	3
"I feel that the results technically are incremental given the literature (e.g., Allen Zhu et al ""A convergence theory for deep learning via overparameterization."" and other papers). Nevertheless, my main issue is the way the abstract is written (effectively overselling the result), since it is not widely believed (indeed there is a highly cited paper that claims what is ""widely believed"") but those researchers that have read Fact 7.2 in Allen Zhu et al do not believe this claim. Distortion on Manifolds and higher moment analysis makes the paper weak accept if my comments are addressed."	2
The paper is well organized, adds more nuance to our understanding of the expressiveness of neural networks, but also leaves a taste of the expected length after training being an obvious follow-up paper which could have fit here as well. I would have been happier to see the whole affair discussed as a single and stronger paper, but I believe that the contributions here are enough to cross the acceptance line.	3
This paper provides a novel set of theorems on distortion of length and volume. It is well written and precise.	4
"Introduction 

- I would remove the sentence “as well as can cater to the idiosyncrasies that arise with the usage of neural networks”. It is so unclear that you have to explain it right after with the id est. Stick to  what you do, this is good enough: I would replace the entire paragraph with something like “We develop a theoretical analysis of double descent in the finite width framework and study how this phenomenon is impacted by changes in the loss function” or something similar
- “by utilizing the tool of influence functions” —> ”by relying on influence functions which are commonly used in robust statistics” or if you really want “by using the notion of influence functions from ..” 
-  In the last paragraph, “starkly” might be a bit strong. I would replace with “clearly” or “which has a clear influence”

Section 2.1.

- When you use not only at the beginning of a paragraph, you should invert : “Not only they have useful properties” —> “Not only do they have useful properties” 

Section 2.2. 

- Why would you need regularity conditions to exchange the order of the expectation and the derivatives. This is not clear to me. Expand or remove. 

Section 3

- The sentence “the change in loss and which we leverage” should be changed to “the change in loss which we leverage”
- The sentence “this quantity is inconsequential ” is not very clear. Why not replace the remark with a clear explanation of the form : “Although the first term on the RHS of (1)  is not the exact training loss, it only deviates from that loss by a negligible quantity related to the difference of the training losses for two distinct training sets. When training for sufficiently long, both losses vanish and the difference thus becomes negligible. ” or something similar


Section 4

- A reference on the decomposition of the Hessian into The outer product Hessian and the functional Hessain would be good
- I would replace the constant $A_{\theta_0}$ and $B_{\theta_0}$ with simpler symbols such as lower case letter or $c_0$ and $c_1$ or 
- Do we really use the term external regularization to denote the absence of any regularization?

Section 4.2.

- The key result (to me) which formalizes the double descent phenomenon is really corollary 5. I think it would be good to have that result appear somewhere earlier. For example in an informal form in the introduction. 

Section 4.3.

- You provide a lower bound on the population loss, however, from the plot you give in section 4.3. it seems that one could also derive an upper bound (although probably not at the interpolation threshold). I.e. Would such an upper bound more difficult to derive given your proof technique?
- page 7, “we can be thoroughly assure of the accuracy” should be replace by “thoroughly assured” or “we “can thoroughly assure”

Section 5

- The first plot in Figure 3 is unclear. What do you mean by Test error in this case, how does it vary with respect to the population loss as highlighted by the double descent curve shown in red ? 
- The explanation of the binary cross entropy loss is not very clear (double check)
- What do you mean by “Note, this is not the case that networks were not trained long enough — rather, we run them for 40, 000 epochs” ? It would be more clear to replace the sentence with something like “Note that the networks were trained for long enough”

Section 6

- In the first paragraph, the sentence “which avoids the need to retrain training set size many models” is unclear. What do you mean ?
- In the statement of Theorem 7, I think you forgot the sentence “Consider the particular case of the ordinary least-squares ” which appears in the appendix. it would be more clear to add in section 6 as well. 
- Generally speaking, it is not clear how section 6 relates to the rest of the paper. Is there a direct implication w.r.t double descent ? If I understand well we are not discussing the over parametrization regime as we consider inverting X^TX ? Although we still have a bound in the inverse of the Hessian. What prevents a bound similar to 
- Last paragraph : “raises an important aspect”. You don’t really raise an aspect. I would replace with “which highlights an important downside of first order influence functions” or even simpler “which confirms the suboptimality of first order influence functions when used in the leave one out framework”. 


A couple of additional comments and/or typos:

- In the introduction “frequently employed deep networksand” —> “networks and”
- In the introduction: “And while the connection … provides interesting parallels, but ultimately its applicability … is unclear.” —>  I would remove the but ultimately
- third paragraph of the introduction, I would remove the “strive” which is not really appropriate here and replace it with the simpler ""we develop” (why complicate things when they can be simple?)
- I would the term “function output” this makes the statements less clear. I would stick with “the function $f$”
- Statement of Corollary 5: “For MSE loss” —> “For the MSE loss”

"	3
This paper presents a completely new approach to understand the phenomenon of double-descent, for very general models / loss functions (including finite-width neural networks). I have a few concerns about the assumptions being used in the paper though (as explained further above).	4
"The paper provides a novel analysis tool for the double descent phenomena, that is particularly useful in the case of finite width neural networks.  While the work can be of high interest to the community, it has some weaknesses, in some of the theoretical results and in the empirical validation that can limit its impact.This is why a recommend acceptance, but I think the paper can be improved.

Post rebuttal: The authors addressed most of my concerns, as well as most if the other reviewers' comments and questions. The resulting paper is substantially improved, and I therefore recommend accepting it. "	4
"An interesting exploration into the low eigenvalues of the Hessian and their links with generalization in finite DNNs. Some underlying assumptions need more support, the bound seems non-tight and not clearly reflective of the actual test loss, and the relevance to real-world settings, where other effects come into play, is unclear. 

Following discussions with the authors I currently raise my score to 8"	3
"The topic and the goal are both very important. However, the theorems are crucially limited and incremental as explained above. Moreover, the main claim to analyze the finite-width neural networks outside of the neural tangent kernel regime is not well supported because of the Assumption 5. 

"	1
The current paper proposes a novel and well-presented formulation of the causal discovery problem, and thus is of interest to the conference audience. Improved comparison with existing frameworks and expanded discussion regarding practical use cases are recommended to improve the paper.	4
"Good and inspiring paper on challenging subject, bit technical but clear accept.
"	4
"The paper tackles an interesting and well studied problem under a general setting, i.e. assuming uncounfondedness.
It is well written and proposes a nice method to discover the direction of causal relationship when considering pair of variables in isolation.
I have expected some more discussion about what to do in the case where the terms used in Algorithm 1 were not too different or extremely close. Furthermore, the results reported in Table 1 witness that the proposed methods achieves accuracy values similar to state of the art algorithms, while bringing more uncertainty about the true value of accuracy than other methods. Furthermore, for ANM the proposed method seems to be inferior to other state of the art algorithms."	3
The proposed method has potential, but the methods needs better explanations and justifications; and the results seems limited and mixed. 	2
The empirical improvement cannot be properly established on static and dynamic attention	2
In summary, I think the algorithm is this paper is simple yet effective, just swap some of operators in the orignal graph attention network. The impacts are broad. The differences of the dynamic attention and static attention can be further discussed.	3
"The proposed GATv2 model is simple but more expressive than the original GAT. The authors’ claim is justified both theoretically and empirically. I would recommend the acceptance since the principles and practices presented by this paper can be useful for future research to design attentional models. 
"	4
My major concern about this paper is its significance and novelty. Based on this concern, I would like to recommend rejection.	2
"1. Known results
2. incorrect proof
"	1
"I tend to vote for rejecting this paper. Despite generalizing the linear convergence result to mini-batch SGD and improving the width requirement to sub-quadratic, my major concern about this paper is its limited novelty, as explained in the ""Concerns"" section above."	2
"The main result of this paper is proving the subquadratic width scaling for SGD convergence in two-layer neural networks assuming the batch size grows with the number of training samples. I understand that the analysis requires new techniques to bound the distance of SGD iterates to its initialization, but I think the proof is conceptually the same as NTK analysis and am concerned about its novelty. Therefore, I think this paper is marginally below the acceptance threshold. I will be willing to raise my score if my concerns are addressed. 

------------------

Thanks for the response. After reading the response and other reviews, I decided to keep my original score. My major concern is still the novelty of the proof. "	2
The proposed approach is definitely promising, but I think that the work is too incremental for now to be accepted as is. 	2
"This paper deals with an interesting topic, viz. improving 2D and 3D
image tasks by incorporating more prior knowledge about topological
structures. While I consider this to be a highly relevant topic,
I cannot endorse this paper for publication at present, mainly because
of a lack of clarity, a lack of delineation to existing work, and issues
in the experimental setup.

All of these will have to be rectified before I will be able to endorse
the paper for publication; I feel particularly strongly about the
experimental setup, which currently does not enable a fair comparison of
the methods.

I realise that this is not the preferred outcome for the authors; at the
same time, I am confident that with additional rewriting and some
additional work on experiments, this paper can make a strong addition to
the ever-growing body of topology-based machine learning techniques.

**Updated after rebuttal**: I thank the authors for their efforts; while some concerns
where alleviated, I think the updates also showed that the current method does not
substantially improve upon the state of the art in the 2D case, whereas for the 3D case,
additional comparison partners would be required. This is not the only way to improve
the paper, though: another potential direction for future improvement could be to invest
in a more theoretical analysis of the proposed method. If either one of these directions
would be followed, it would serve to substantially improve the quality."	2
The paper is addresses an important problem of topology-aware segmentation learning. The proposed framework extends previously existing PH diagram techniques by adding positional emebedding, this add locality to the framework. The experimental evaluation shows improvement upon global methods. Some lack of comparison with other localized methods.	3
The paper is well-written and showcases an interesting application of TDA in the context of image processing supported by various numerical experiments. However, it contains some inaccuracies regarding TDA concepts/literature that must be corrected/discussed.	2
I think this paper could be modified to 1) better motivate the work and 2) consider alternative simulation assumptions to show the method is robust.  Otherwise, it addresses an interesting problem in a new way.	3
Overall speaking, this paper studies an interesting problem of estimating preference shifts induced by recommender systems. Nevertheless, the proposed method and experimental setup are based on too many assumptions, which may do not hold in the real world. Therefore, I recommend rejection.	2
"Although an interesting study, the paper has limitations (please see ""weaknesses"" section above). I would say that the current version of the paper is marginally below the acceptance threshold, but I am looking forward to the authors addressing my concerns above in their rebuttal.

## After Author Response
The authors' response has addressed most of my concerns and I choose to raise the score to 6. 

Also, I expect the authors to format their released experiment code, such that followers can better understand the model and simulation."	3
The paper addresses a real-world problem and provides tools to deal with it. I vote for weak acceptance. 	3
The proposed method seems to be a simple extension of an existing method (FastRNN) and thus has limited novelty.	1
An interesting model with nice properties that does relatively well empirically, but more baselines (eg, orthogonal RNNs) should be included before the paper is accepted.	3
"The method proposed in the paper has limited novelty. The theory section in the paper has a meaningful contribution.
The experiments are not convincing. 
Overall, the paper is not appropriate to publish on ICLR."	1
This paper presents a nice, simple and well-motivated idea. However, it fails at demonstrating how impactful it could be to the community with experiments that are not supporting fully the claims at scale or in realistic conditions. 	3
"(1) I'm a little bit confused with Assumption 4. Would it be enough to simply assume that $\mathbb{E}\Vert\dot{f}(\theta,X)\Vert_{2}^{1+\alpha}$ is finite for any $\theta$? Because it seems that if so, one can always find a function $G(\theta)$ to upper bound this? Or your assumption is stronger than it seems? I understand that $G(\theta)$ appears in (2) and Assumption 5. But it is not clear to me why you need this in Assumption 4, and am also wondering in mathematics, is there some name for the smallest upper semi-continuous function that upper bounds? Another comment I have is that in Assumption 4, you already taken the expectation. So my guess is that most likely, $\mathbb{E}\Vert\dot{f}(\theta,X)\Vert_{2}^{1+\alpha}$ is not just upper-semi-continuous but actually continuous. Do you know any commonly used example where this is not true? If so, why don't you assume this is upper-semi-continuous and use this function instead of $G(\theta)$ in later discussions?

(2) Some discussions on the intuitions behind Assumption 5, and connecting it to some assumptions that the readers are more familiar with would be very helpful. For example, the authors mentioned that Assumption 5 is satisfied for $\theta\in\mathbb{R}$ for $\exp(\theta^{2})$, $\exp(\theta)$, $\theta^{r}$ etc. But those examples are all for one-dimensional $\theta$. How about in high-dimensions?

(3) I don't understand why the assumptions in Section 3 are called Property 1-Property 4 instead of say Assumption 1'-Assumption 4' or Assumption 6-Assumption 9. They seem to be assumptions instead of properties.

(4) In Theorem 3 (and maybe in Theorem 2 as well), can you say something about the properties of $F_{\lim}$? For example, is it $L_{p}$ for some $p$?

(5) In Theorem 3, when you have stability, can your proof techniques also provide some non-asymptotic convergence guarantees instead of just some asymptotic results? That can strengthen the paper by a lot.

(6) In the proof of Theorem 2, it's better to write ""imply"" instead of ""supply"" and in the first line of Section 4.2., maybe you can write ""provides"" instead of ""supplies"". 

"	3
I found the manuscript to be clearly written and technically sound. Although it has some weakness, I think it still worth a publication. 	3
The convergence and stability analysis of SGD in the general assumption seems interesting. The authors also introduce some techniques in the analysis.	3
All in all, I think this is an interesting paper, and the method could be applied in other settings as well. In fact, I would like to hear from the authors if they have other ideas for future extensions, because ModelNet is more of a simpler toy setting. I'm currently giving a Weak Accept rating, but I'm very interested in reading the response of the authors and the other reviews.	3
Overall, I believe the paper is below the bar	2
The authors propose powerful and novel ideas. They are clearly supported by the performance shown in the extensive experiments and ablation studies. I strongly suggest the authors to work on the writing, especially in the results section.	3
The method seems to be working well, but is not very novel and lacks ablation studies to disentangle which components contribute to the performance improvements and how.	2
The authors present an encouraging algorithm that appears to perform well. However, section 2 seems to consist almost entirely of known results. The new theoretical results included however are often shown without proof and are lacking in rigour.	2
"This is a technically significant paper because it shows the connection between $\infty$-Wasserstein distributional robustness and standard adversarial training PGD. All obtained theoretical results are meaningful and have clear impacts. However, the presentation and writing of the paper need to be improved to make it a more solid paper. 
- How to associate the label $y$ to the formulation?
-  Appendix needs to be organized better and all proofs need to be consistent with the main paper.

The experiments are still humble which need a comparison with more baselines using more advanced architectures. Moreover, the robust accuracy for black-box attacks and Auto-Attack should be reported.

It is unclear why the proposed method is faster than PGD even a single adversarial particle is searched using the Langevin algorithm."	3
In summary, although this paper show certain equivalences between adversarial training and DRO, it is mostly a straightforward result under the infinity norm, and the overall contribution of this paper seems to be limited.	1
"Overall, I find this work relevant and well executed, although some details of how novel it is and details of its implementation might deserve to be improved. Results are good on toy domains and acceptable on COCO, but it is unclear how well it would cope with more complex situations. For now, I’d qualify this as borderline and tend to accept given it was an interesting read, but I am not extremely familiar with the literature and related work.
"	3
"The paper presents an interesting idea for object localization using RL. My main concern is that the current experimental designs are weak, because of 1) less sufficient studies on the effect of individual components, and 2) non-standard object detection protocol. Therefore my initial rating is “weak reject”.

After seeing the authors' response, I decided to increase my rating from 5 to 6 since the added experiments addressed my concerns."	3
The paper has enough novelty and importance but the rewriting needs a lot of work to make more precise and cohesive. With a solid improvement to clarity I would support acceptance.	3
This paper presents a novel method for solving query object localization as an RL task. The authors propose to learn an embedding space that respects the IoU ordering of bounding boxes and then use the distance in the embedding space as reward signal instead of raw IoU scores. The writing and presentation of the paper is clear. However, it is not easy to understand the similarities and differences between the proposed method and the baselines it compared with in the transferring task. It would better help the audience understand the significance of the results if the baseline methods are explained with more details. 	2
The paper proposes a simple extension of disentangled variational encoder improving the continuity in latent space. The approach is simple and logical but has little novelty and experiments do not showcase the utility in merging datasets.	2
"The idea is interesting and reasonable. 
However, the experiments on real data is suggested.
"	3
In summary, the minor modification of the architecture proposed in previous work and the limited experimental comparison are not sufficient to pass the acceptance level for the paper. Thus, my current recommendation is 'Reject'. 	2
Overall the paper seems very incremental on top of a single baseline, and the evaluation is very sparse. I don't see the novelty or significance here.	2
"Good core idea and intend, but the paper is incomplete.
"	3
"There are multiple issues in the paper, due to which the reviewer feels that the paper is not ready in its current form. 

1. How much message entropy can be communicated? In source coding, we have the entropy of message that is communicated through a certain message length. Similarly, when communicating through a MDP, it is important to know message length communicated for a given finite T length MDP. 
2. Since the optimal policy is deterministic, for a given state, there is a unique optimal action, it is unclear how the policy can be the same and still contain the message. This is because deterministic action has no entropy. It seems that the authors are assuming that even if message is sent o(T) of the time, policy will be the same - and thus o(T) times, actions can be chosen - so \tilde{O}(T\log |A| ) length message can be communicated without any change in average policy. Not sure, if the authors are able to get better - and how to quantitatively say that. 
3. In general systems, the model of MDP may not be known between sender and receiver. How can these issues be handled?
4. Channel coding and source coding are separate problems, and the authors seem to write the two as same in the text. 
5. Two of the special cases are given. How does the results here give same/improved communication results in the two domains. The result of message communications need to be at least the same as in those areas, with results in the general setup. "	2
"While the paper demonstrates some nice empirical results, my major concern is its lack of novelty in the proposed problem setting and the learning algorithm. Therefore, I think additional studies are needed in order to  improve the technical and empirical contribution of the work.
"	2
"A new theoretical game called Markov Coding Game and an algorithm to solve it. But I found the formalization the problem too muddled.
"	2
While this paper demonstrates good results with the proposed simple framework, there is no theoretical analysis of why such simple method is working. Moreover, it is also unclear to the reader whether the proposed framework generalize to other attack methods and other few shot approaches. 	2
To sum up, this is a mediocre submission and I would rank it below the acceptance bar of ICLR.	2
My main concern of this paper is the comparison maybe not fair i.e., this work uses a pretrained stage. It is also difficult for us to characterize if any overlap information between $X_b$ and $X_n$. Overall, I think it should compare all methods in the same condition. The ablation of base training and novel training is necessary to characterize the importance of each stage for robust few-shot learning.	2
While the theoretical analysis of the proposed spatial smoothing is interesting and valuable, the experimental evaluations are relatively weak or too some extent insufficient. Furthermore, the paper is not well self-contained, which highly relies on the analysis, experiments in the appendices.	3
This is a paper with simple designs but a lot of analysis and studies theoretically and empirically. The authors presented the proposed technique motivated in the context of Bayesian neural networks, which is not very strongly supported. The technique is clearly helpful in some settings and not very much in other settings. It needs further discussion on the limitation of the proposed technique and related works. I am slightly lean to reject this paper but may change my rating later.	2
"The work presents a novel method for reducing the computational cost when using BNNs in real-world image tasks.
Although I am not an expert in the area, I judge the work as being worth publishing since the theoretical analysis seems relevant and experimental results convincing."	3
"I believe the content and ideas to be of interest but the paper requires a major revision and proper thought put into reducing the content without the appendix. 
The language of the paper should also be improved. 
Acronyms are not defined throughout, assuming the reader knows what they stand for. 
The references are mostly from arXiv - I do not find this appropriate. 
I am also not convinced the ideas are novel. The authors do not provide enough evidence to prove this. "	3
In this paper, an efficient transformer structure is proposed to improve the efficiency of the transformer model. However, some empirical evaluations are missing and comparisons with prior art are not convincing.	2
"Overall, the reviewer thinks the contributions are small. Grouped FFN and grouped convolution are not new. The paper lacks a comparison on MLM perplexity and downstream tasks with stronger baselines, such as evolved transformer and Primer where grouped ops are used. With respect to evaluation, only SQuAD is evaluated, which shows limited impact on generation tasks. The reviewer would suggest to evaluate on a wider set of down stream tasks such as GLUE and superGlue. 

A few more detailed comments:
- Try to add more stronger baselines of efficient transformers (Reformer, Linformer, Evolved Transformer, Synthesizer, etc.). 
- Add more tasks (GLUE or superGLUE)
- Target general accelerators such as GPUs. Then analyze training convergence time on a general processor. 

"	1
The idea is well-motivated and easy to follow. It would be better to report more solid comparisons on popular GLUE datasets. 	2
This paper introduces GroupBERT, as an efficient architecture for pre-trained models. It designs grouped transformation and grouped convolution for efficient training. However, the experiments of this paper still lack convincing. Most of the experiments are only analyzed on MLM loss in the pre-training stage. However, the downstream task performance is an important metric to measure the quality of the pre-trained models, and this paper only selects SQuAD v1.1 to report performance, which lacks convincing. And the comparisons on downstream tasks only choose BERT as the baseline. 	2
"I am in favour of the paper mainly due to the compelling empirical performance, which should be of interest to the community, although some additional baselines and experiments could have been included.  It is clearly written with figures and illustrations to aid the reader. The main idea is not novel, but it has not been adapted to handling sequential data before, and it seemingly has many benefits in this context.  My criticism is regarding some discussions in the paper, and that some properties of the model seem to be overstated.

POST-REBUTTAL: Given the comprehensive answers given by the authors, and the implemented changes in the paper, I am inclined to raise my rating to an 8 from the previous 6."	3
"The contribution is original and could have a large impact both because it leads to better performances in several important situations (including long range dependencies) and because it could inspire new designs. The proposed method is thoroughly evaluated.
The submission is also clear and well written."	3
I am in favor of acceptance as the concept of CKConv provides enough contribution, although the experimental evaluation would have been stronger with standard irregularly time-series datasets (PhysioNet, MIMIC-III)	3
The described idea is a nice transfer from existing work of implicit neural network representations and continuous kernel formulation. Showing the importance of sine activation functions is very helpful to direct attention to the role of activation functions, depending on the actual modeling task. The empirical validation should be larger, though.	3
"I found the presentation deficient and my impression is that despite the results seeming promising, this paper requires substantial improvements to be ready for another review process. 

This paper seems correct in methodology however I did not check everything, in part of difficulties related to the presentation. "	2
Overall, I think this paper just has too many problems for me to recommend acceptance. I'm not fully convinced by the approach to meta-learn in function space-- I did not see a convincing argument explain how exactly this is necessary (e.g. a toy experiment to demonstrate its necessity would be nice)-- and the ablation results seem to be explainable by just having a higher capacity and therefore higher performing network. Lastly, I am a little confused whether this is a scene flow or a point cloud registration paper-- it would appear to be the former though the title and intro seem to indicate the former.	3
The overall idea is interesting. However, the experiments are much lacking behind to support the claims, and the current experiments have not compared with the state-of-the-art registration methods. I think this work is ongoing work and need to do more investigation.	3
"1. The proposed idea is new.
2. Many related works are missing.
3. Experiments are not solid to support the idea."	2
Technical content of the paper is overall pretty strong and idea seems pretty significant and novel. However, overall the paper is hard to follow. It would benefit and stronger with better organization (as pointed out in the weakness in the above). 	3
This paper proposes a new framework aiming to increase generic and personalized performance simultaneously in the imbalanced federated learning setting, the proposed algorithm is simple yet efficient according to the experimental results. Theoretical analysis are somehow insufficient, and some minor fix or explanations are needed to better understand the experiments. 	2
The idea of treating the non-IIDness on clients in FL as class imbalance is interesting, however, the paper can benefit from addressing the above raised concerns.	3
The use of ML to address complex problems in information theory is appealing. The authors consider the problem of obtaining upper and lower bounds for the rate distortion curve for source distributions that may be continuous, discrete, or mixed, and may not be known (only data are available). Some theoretical results are provided. Two major issues: insufficient details about the learning architectures are provided, so that an informed reader will not be able to reproduce the results in the paper. Second, there is very little discussion of the complexity of the algorithm. 	3
Although the proposed method has limitations, I find some of the contributions valuable. 	3
"Due to the weaknesses in the ""significance"" part of the main review I am marking the score as a 5."	3
"Mathematical reasoning in the paper is doubtful, and what kind of situations can be handled by the proposed
algorithm is unclear, see ""main review"" for the detail."	2
This paper presents a nice idea for improving compositional generalization of neural semantic parsers. The results on GEO_{TMCD} outperforms the currently best approach. However, there are issues with both experimentation and the methodology. 	3
In general I found the method interesting and the paper overall clear. However, I have some remaining questions about certain aspects of the method and datasets/tasks, as well as some potential concerns about the impact of the contribution, which I would like to see addressed before I can strongly endorse this paper.	3
"I feel a bit borderline about this paper, as the method seems a bit limited and heuristic -- not being clearly designed for compositional generalization or showing convincing results on the hardest tests of compositional generalization. But, it does seem to show consistent (if sometimes small) improvements on a couple models and several datasets, the methodology seems sound, and the paper is very clearly written. I've put an overall score of 5 for now, but I look forward to discussion.

---

Update after the response: Thanks to the authors for their thorough response to my comments! The explanations and new ablation results helped convince me that the choices made in designing the method were reasonable. I also appreciate the standard deviations, which make me confident that the improvements are real. I'm in favor of accepting this paper, and have updated my score to a 6 (from a 5)."	2
My score is justified by the doubts I have about the novelty, comparison with previous work, and clarity of the writing.	2
See review	3
"The idea of the paper is interesting, but there are many things that the authors should clarify, both in regards to limitations of the methods and evaluation (metrics, domains, comparison against other baselines).
For these reasons, I vote for its rejection."	3
The paper seems to provide an interesting idea for training a VAE that allows an analytical solution and therefore does not need sampling, but the empirical evaluation of the method is somewhat limited and presentation of the paper could be significantly improved. Therefore, I am voting for the weak acceptance as I believe the idea is worth to be known by the community but the existing drawbacks stop me from giving a higher score.	4
"
Overall,  I think the proposed method is similar to the moment matching method. It is marginal novel and limited to the Gaussian case. Thus, in my opinion,  this paper is marginally below the acceptance threshold."	2
Overall, I vote for rejecting. I like the idea that analytical approximate ELBO would help the computation. The finding on posterior collapse is also interesting. However, the method and evaluation are not adequately demonstrated. The writing can be largely improved too.	2
"The paper demonstrates the usefulness of a Taylor approximation compared to a sampling-based approach. However, that in itself is of limited novelty, and the paper lacks a proper comparison to other methods and the broader literature to classify it properly.
"	2
"The paper introduces a novel setting for VFL with decentralized labels, which is very well motivated, and proposes novel protocol for this setting, which is ingenious and shows significant advantages compared to existing methods.
"	3
There are several writing issues in the paper. Additionally, the paper suffers from a security flaw.	1
The problem addressed in this paper is of practical importance for many real-world applications. Thus, I would like to see its presentation at the conference. 	2
Overall, I think the paper can be significantly improved in writing. The paper does not convince me about the importance of the distributed label setting. Moreover, I do not fully understand how the algorithm work. Please refer to my main reviews.	2
Interesting paper, which however can be further improved.	3
"The idea of the paper is clear and interesting. It appears that the proposed approach only supports a binary protected attribute. The empirical evaluation could be strengthened by providing results for all the density functions.
"	3
"Pros:
1. The goal of this work is very well motivated and much needed.
2. The authors provide a very nice illustrative example clarifying and motivating the need for these methods.
3. In general the authors do aim to motivate every algorithmic choice with some backing from theory which is a very nice structure/writing style.
4. Overall I do think the general direction the authors aimed to go in is very exciting and has a lot of promise. However in this current iteration it is lacking technical strength (both experimentally and theoretically).

Cons:

1.Unfortunately much of the theory the authors use to motivate design choices seem to have mistakes (Proof of Theorem 5.1 is incorrect, the usage of Pinsker’s inequality also seems wrong)

2. Because of the aforementioned error with Pinsker’s inequality  (it bounds TV not MMD) it's not clear if the KL based objective (as a surrogate to the statistical distance) is well motivated.

3. Not very safe heuristics are used in proofs (e.g. equating infinitesimals) ; this is prone to mistakes.

4. The use of normalising flows seems to be only used in theorem 5.3 ? 

5. Finally the claimed theoretical results are “estimation guarantees” the way the introduction and abstract are worded seem to indicate that the theoretical results guarantee fairness for a given family of models, that is the way this work is selling itself seems to claim a bound on the true statistical distance. However that is far from the case, instead this work tries to prove that said distance can be estimated “very well” empirically. Thus all experiments in this work should be estimating this quantity as any success statements regarding fairness rely on such empirical estimates and there are really no theoretical guarantees on the fairness aspect of the method itself, it's more like there are theoretical guarantees on the evaluation of the method.
"	2
"Much fuss is made about controlling the statistical distance between the latent representations $z_0$ and $z_1$. This is justified because controlling the statistical distance amounts to controlling the maximum adversarial accuracy and additionally gives guarantees on demographic parity and equalised odds for any downstream classifier $h$ that uses $z$. 

However what is actually implemented in Algorithm 1 has no clear connection to controlling the statistical distance between $z_0$ and $z_1$. This feels like a sleight of hand to me. Certainly none of the theory supports Algorithm 1's ability to control the statistical distance. Furthermore no careful thinking is given to the tradeoff value of $\gamma$ in Algorithm 1. "	2
The paper makes significant contributions by proposing a fair representation learning technique with theoretical bounds. The paper can also be improved by better explaining the accuracy results and adding some more details of its techniques and experiments.	4
Although I am not an expert on this topic, the paper is well-written and the novelty of the idea seems to be clear after careful reading and thinking.	3
Establishing training guarantees of two-layer ReLU networks based on convex formulation of Pilanci, Ergen 2020 is sufficiently novel and interesting.	4
The paper contains some interesting results which may be of interest to the community, but the presentation and flow of the paper can be improved.	3
Overall, I really like this paper. It was easy to ready and nicely written. Moreover, the proposed model outperforms previous methods on the scene classification task on multiple datasets and on the part segmentation task. While the proposed model is marginally better than previous models, I really like the concept of utilizing wavelets for 3D shape modelling thus I vote for accepting this paper.	4
"In summary, this paper proposes a method for processing point cloud using a lifting scheme. The proposed method is novel, and with some bells and whistles, it achieves the state-of-art performance on point cloud segmentation and classification. However, there's some major issues blocking me from rating this paper above the acceptance bar. First of all, it is not super clear to the reader why one would like to incorporate the lifting scheme for point cloud processing. I don't think it is just for a few points gain in the performance. Secondly, there's some issues in the evaluation due to the choice of the performer vs plain transformer.

I'm willing to raise my score if the authors could address those concerns. 

------------------------

After reading the response from the authors, I would like to raise my score from 5 to 6. The authors have addressed my concerns, and I think this paper meets the bar of ICLR. "	3
Although the proposed method is theoretically sound and the framework design makes sense, the performance gain is hard to convince me the superiority of such a different learning framework.  	4
In general, the proposed method is novel in that it combines wavelet transform into 3D representation learning (I'm not an expert in wavelet transform, so I'm not quite sure how challenging this is). Experiments and ablation study are comprehensive, and the final quantitative numbers are competitive. My biggest concern is that the improvement over prior works are quite limited (no more than 0.2%), given all the complex schemes of the method and its required usage of Performer. I understand these benchmark results are somewhat saturated and hard to improve a lot. It's ok, but the paper will be more convincing if the authors can analyze the features learned by their wavelet transform, and show how the learned features are better or different from prior works that do not use wavelet transform.	3
"### Summary
* A novel formulation and a sound approach to generate novel scenes 
* Weak evaluation -- very simple synthetic dataset, unconvincing attack 
* Weak baselines -- no comparison against s.o.t.a scene layout generation methods "	2
I am not experienced in this domain but it feels to me that this should be more closely investigated from the view point of graph-based neural networks and graph-based image representations and rendering. The tree-based representation feels very restrictive to me. Also, I'm not sure if generating adversarial traffic scenes is the best way to evaluate this approach.	2
The T-VAE and knowledge-guided generation proposed in this paper looks new, and the attack looks interesting. However, the current version is a little bit hard to follow, and some contributions seem incremental, for example, the proposed T-VAE compared with VAE. I can reconsider my rating if the authors point anything I was missing.	3
The approach seems reasonably novel, and the paper's writing could be improved. The experiments and applications are not very exciting.	3
I would make a recommendation for rejecting this paper.	2
This is a well-written theoretical paper on understanding the sample complexity of learning shallow ReLU neural nets from corrupted samples; however, both the approach and the obtained results are somewhat narrowly applicable to the case of Gaussian inputs and do not seem like they can generalize to more challenging settings.	3
The error bounds do not seem tight and there are some concerning issues with their simulations.	2
This is a well-defined problem and the authors make an improvement on previous works on learning the parameters in generative models, which all (to my knowledge) avoided the existence of outliers.  At a technical level, the argument has nearly-exact parallels with the procedure of Wu et al., with a small modification for accoutning for outliers that involves swapping the gradient estimate the median and trimmed mean.  It appears to me that the current result does not recover the Wu et al. result as the probability p of uncorruption tends to 1, which is weird (see below; please correct me if I'm wrong).  The presentation also had some issues making it difficult to determine how accurate the parameter estimation really is.  These are the main reasons I'm leaning towards rejection.	2
"This paper proposes a cross-layer parameter sharing scheme for CNNs that demonstrates some benefits on image classification tasks.  However, the proposed method is similar to filter factorization approaches in several previous published works, for which experimental comparison is missing or incomplete.

Update (after rebuttal):
The author response addresses some of the experimental concerns from my initial review.
"	2
The idea to learn a joint subspace view to share weights across layers in a CNN is interesting especially with filter groups. They show good results on a variety of datasets against a solid number of baselines. Despite this, it is to my feeling that the idea is not entirely novel as i) low rank decomposition of convolution kernels has already been performed and ii) the idea to share filter basis/weights across layers has also been proposed. Further, the motivation/pilot experiment for the insight to sharing shared coefficients over layers needs to be worked on. I therefore propose 6 for now.	2
The idea of coefficient sharing across different settings seem interesting, and a ok experimental results. However, there exist some weaknesses in this work (see main comments above). Therefore, my current rating is borderline accept.  	2
Overall, the paper is hard to follow the logic because of unusual organization. However, the novelty of the proposed method is fully convinced with supported experiments and their explanations.	3
I recommend the borderline rejection due to the limited novelty and unsurprising experimental results.	2
For the reasons listed in the previous section, I think this paper is at the borderline, and I currently tend to recommend rejection. 	2
While studying inductive bias of gradient descent optimization and their role to generalization property is an important subject and natural gradient descent provides an interesting tool to investigate and isolate certain optimization artifacts, I believe the current paper’s direction of analyzing these may be flawed. Without proper justification of their methodology, the reviewer suggest that the paper shouldn’t be accepted to ICLR.	3
Overall this is an interesting discussion of the different inductive biases of EGD and NGF.	3
In summary, I think this work is very well written, clear, and addresses an important question (how to understand the role of parameterization in generalization) through an original lens, the study of natural gradient descent/flow.  However, the authors did not convince me that this perspective could lead to new insights that were not already well known.	2
"This paper in general is well-written and easy to read.  Some arguments made in this work seem to be reasonable. However, there are issues that weaken the arguments. Please address the issues in the main review. I am happy to give a higher rating.
"	3
"Recommendation: Reject. While I agree the idea is appealing, it is not too novel and some missing discussions would lead to a huge change in the original paper in my opinion.
"	2
The theoretical claims look solid, but their implications are unclear.  The experimental settings and results could be improved.	3
This paper achieves small improvements on large datasets and significant improvements on either low dimensional data or where the mixing rate alpha is very large.  The improvements are small but the idea is simple and logical, so I weakly recommend acceptance.  	2
"This work seems well-motivated, clearly explained with globally convincing experiments. My only concern is on how to choose the hyper-parameters k and alpha, which do not seem to always have the intended effect. Overall, I tend to recommend acceptance.
"	3
I tend to vote for rejecting this paper. Despite being clearly written, the justification for their proposed method is somewhat unclear, and the performance gain seems not significant and consistent enough. Therefore, I think more work needs to be done to provide enough justifications for $k$-mixup (perhaps from the theoretical side) and to further improve the empirical performances.	3
The paper provies new convergence results for momentum-SGD and AdaGrad, showing that momentum-SGD converges faster than SGD and AdaGrad's estimate sequence converges. The momentum-SGD proof is beneficial for understanding why momentum works in nonconvex objectives, which is still a not well understood topic.	3
In summary, the issues mentioned above must be resolved. In my opinion, there are obvious mistakes in the proof. 	3
The paper contains solid contribution in terms of theoretical analysis, especially for mSGD. The convergence results are on last iterate different from existing results. However, given the currents state of the submission, the clarity of the paper needs to be significantly improved. Adding proof outline and providing more explanations for key steps definitely helps to improve the quality of the paper.	3
The approach is overly simplistic, as it does not account for ambiguities. Methodologically, I'm not convinced that the model generalizes, as there may be significant overlap between the train and test examples.	2
"The paper has critical issues with respect to novelty and technical significance. The paper should cite the missing papers and compare with those models (and Sato.) Thus, I wouldn’t recommend publishing the paper in its current form. 
"	1
"Summary of the review
The paper studies a novel problem called semantic type detection, which has potential applications over tabular data. However, the paper should include pre-trained models as baselines to show the proposed DCoM outperforms the state-of-the-art methods by a significant margin. 
"	2
This paper presents two policy update frameworks (WPO and SPO) which relax the restriction to parametric policy distribution in the standard setting. Theoretical guarantees are provided. The numerical results also suggest that proposed policy optimization methods outperform the standard TRPO and PPO with better performance and faster convergence. 	3
This paper provides a new way of optimising the policy distribution relying on the Wasserstein and Sinkhorn distances. The methods is theoretically grounded so I would recommend an accept but I believe the authors would need to evaluate their method on more domains to make the paper stronger.	2
The main feature of this paper is considering a non-parametric policy class for Wasserstein policy optimization. However, the theoretical contribution adds marginal to the existing literature, and the numerical findings are not convincing to demonstrate the advantages of the new framework.	2
"Overall, this is not the most ambitious paper, but generally a nice contribution.  
"	2
An interesting result that seems to rehashing a significant amount of existing work without citation, is hampered by a poor presentation, and is not careful to place itself in the context of existing results.  	2
"This is an interesting paper but I hope the authors can improve their presentations of their main results by removing some proofs to the supplementary materials and include more intuitions and novelty compared to previous works like Koehler’19. Also it is reasonable to include a study on the convergence rate of generalized BP, either theoretical or experimental. 
"	2
The main result of the paper, Theorem 1, is dependent on Lemma 2. Therefore, I would like to vote for rejection unless every single step of the proof of Lemma 2 is made clear.	1
"The paper generalizes the findings of [koehler] to one specific and restricted setting. The technical results are derived analogously to [koehler] and it is not clear how (or if) the findings would generalize; neither to more general potentials nor to more general versions of generalized belief propagation.
Moreover, the paper has sever issues regarding clarity and deviates from the classical literature in its clarity. Also, the related work is not cited adequately. "	2
"I recommend acceptance of the paper. This is a very interesting and novel method, that is shown to perform very well on several different datasets. 
There are many quantitative experiments but showing qualitative comparisons would be very beneficial for gaining insights into how different methods compare. "	4
Overall, I vote for accepting. The paper is well written. I like the idea of transforming to control problem. The implicit recognition model also interesting. This would be useful to learn neural dynamics from large population recordings. Hopefully the authors can address my concern in the rebuttal period. 	4
I'd lean towards accepting this paper, due to its novelty and thoroughness. I'd be willing to argue even more strongly for it if the authors could address some of the points above (especially the more complex toy example).	3
The paper proposes a useful Schroding bridge framework for generative modeling that is simpler than previous counterparts. However, there are concerns on efficiency issues, writing errors, and fairness in experimental comparison. It is also unclear how much difference is this method compared to the original formulation of score SDEs.	3
"The paper introduces a new framework for likelihood-based training of the forward and backward SDEs that are inspired by the Schrodinger bridge problem, which is quite novel. But the paper is not very well written and the practical limitations of the proposed method are not sufficiently addressed. Therefore I do not vote for acceptance given its current form. 
"	4
The paper presents a novel framework for likelihood training of Schrödinger bridge (SB)-based generative models. The technical contribution of this paper is significant: it generalizes the SBGM framework and provides a practical algorithm for likelihood training of SB models. Although the paper has some clarity issues, I believe these can be fixed in the revision. Overall, I think this is a good paper and I recommend acceptance.	4
"To conclude I think that the idea of using FBSDE to derive the Schrodinger bridge
is original. However the method has a lot of overlap with the
works of [3,4] which is not acknowledged by the authors. The authors do not
precise which loss function they use in practice and because they do not give
access to the code (or at least to a detailed pseudo code) it is hard to
determine what is the original methodological contribution of the paper.
I think that the paper is not mature enough and that a true justification of Algorithm 2 with FBSDE is still missing. Experimentally speaking the authors did not compare their work with existing SB methods.
Based on these comments I recommend the rejection of the paper.
"	3
I find the paper in this current form to be borderline due to the fact that it is hard to parse and to understand why the framework is instantiated in the way it is. It is still an interesting contributions, and empirical results are promising. Upon such clarifications to be made (along with potentially visual examples providing intuitions), I would be considering raising my score to a weak accept.	3
"This work is interesting and shows promise, and if it were to work, would be a useful addition to the literature.  However, I am unconvinced by the method itself.  The method is quite involved, with lots of moving parts.  Furthermore, the presentation of the core of the method (the advisor) is remarkably poor, considering that it is _the_ contribution and that the rest of the paper is so well written.  The empirical validation is also a little lacklustre, both in terms of range and actual performance.  I also have more subjective issues with aspects of the paper (see comments above) that, while it would be unfair to reject solely on those issues, dissuade me from giving the authors the benefit of the doubt here.  Therefore, I recommend that this paper is not accepted for inclusion in this review cycle;  but note that with some reworking this paper could make a good submission to a good conference in the near future.  

Good luck."	3
Some mathematical representations still need some improvement. The experiment is not convincing enough. I would recommend a weak reject for the paper. 	2
I like the topic of this paper: devising better tools for evaluating differentiable NAS. And I have not seen the ideas in this paper presented in quite this way, which could be impactful in the community. However, there are unfortunately the weaknesses of only using NAS-Bench-201 and being a bit incremental. I recommend rejection.	3
"1. Somewhat interesting studies for NAS by dividing the search into two-stages
2. Lack of insights;
3. Limited experiments."	1
"This paper is a purely empirical evaluation paper, putting several advanced differentiable NAS methods together and study their joint effects.
However, the impact can be restricted as all studies are performed on a toy space. Additionally, I think the writing of this paper has a large room to improve, the authors should add some conclusions and discussions rather than just showing the tables."	2
Although this paper tackles an interesting problem, it is currently unconvincing. Many parts of the paper need to be clarified, the results need to be discussed, the proposed score does not serve the desired purpose, and the discussion of the related work should be improved.	2
see above	2
Overall, I find that this paper does not meet the bar for ICLR. I suggest the authors work on improving the methodology, adding more evaluations and improving the presentation. 	2
I like the idea of this work, but obviously the authors do not elaborate or justify their idea in either theoretical or empirical aspects. The results is not significant comparing with existing work, either. Besides, the submission seems not a complement paper due to its writing. I lean to reject it.	2
"The idea is sound and it should definitely be further studied. The experimental section begs for improvements. Not just the exposition, but the entire study. More datasets should be included. Inception-v3 is a rather old architecture and thus more modern architectures should be considered. ResNet-50 is included which is positive. 

Minor remark: The hyper parameters are listed however there is no word how they were selected. Usually with BN gradient clipping is not needed so it is surprising that it is used. 
I also do not see a discussion on computational times. "	2
The motivation for the proposed method is weak. Although the overall insights are interesting. Improving the motivation, especially with regards to the practical aspects (as detailed above) will help me to improve the ranking of the paper. 	3
"The paper is well-written and provides solid empirical justification for specific choices of normalization (set norm) and residual connection design (""clean path principle"") for neural networks that operate on sets. While ""set norm"" is somewhat novel, I am unsure about whether the ""clean path principle"" applies specifically to these set-based neural networks, or whether there is something specific about that design for equivariant networks. Overall, a solid empirical paper, but I hesitate to say that it is truly ""novel."" That said, I am open to increasing my score based on authors' replies."	3
The contributions of the draft are empirical and the experimental evaluation lacks an analysis of statistical significance, so the contributions cannot be evaluated conclusively. At least one contribution has questionable novelty.	3
This paper proposes interesting design choices for permutation-invariant networks, but I cannot tell whether the empirical results are statistically significant. Additionally, the paper does not show the effect of their components on the original shallow networks. I'd be happy to increase my score if these issues are addressed.	3
"* Well written and easy to follow
* The list of requirements identified by the authors from normalization equivariant layers needs a major refinement. 
* Lack of novelty and the benefit of the set norm is unclear
* Missing discussion on the relation to graph neural networks
* The experiments do not test properly the effect of depth on learning
"	2
This paper provided a lower bound in the binary domain adaptation classification problem, the conclusion is interesting and technically sound. However, this reviewer has concerns of significance and empirical validations. Based on this, I currently recommend a weakly negative score.	2
"This paper investigates an interesting and important problem: namely, the fundamental limits of transfer learning in the context of binary classification.
The authors derive a novel lower bound on the generalization error, based on which they present useful insights regarding the efficacy and limitations of transfer learning, especially as a function of source/target sample size and the relatedness between the domains/tasks.
As the presented results only make minimal assumptions and do not require full knowledge of the source/target domain data distributions, the results in this current work may be relatively easily applied to various real-world transfer learning problems (for binary classification). 
However, the current study overlooked closely relevant prior works on optimal transfer learning for classification (and other tasks), where relevant discussions on the efficacy and limitations of transfer learning have been made.
Furthermore, additional experimental results (e.g. based on synthetic examples) would be needed to clearly demonstrate the usefulness/accuracy of the derived lower bound and to further validate the insights presented in the paper.
"	3
"The initial recommendation of the paper. is ""reject, not good enough."""	2
"Overall, I recommend weak acceptance of this work: the authors introduce a natural notion of distance and give a clean lower bound, but the evidence of tightness is weak and the discussion of estimating transfer distance needs to be corrected.

EDIT: I have read the authors' response and my review remains largely unchanged. I believe with the addition of either a theoretical upper bound or more convincing empirical verification would lead to a very strong paper, but I can only recommend weak acceptance of its current incarnation. It could also be helpful to include experiments on estimating transfer distance in reasonable settings, since the assumption that $Q[h^*_S(x_T) \neq \hat{h}(x_T)]$ is small does potentially limit applicability to scenarios without any major shifts in the underlying distributional weights."	3
Based on the above review, the paper is slightly above the acceptance bound.	2
Given all the above, I am currently leaning towards recommending acceptance. If the authors address the above concerns (adding MiniGrid experiments, ADR comparisons, analysis of how the performance scales with the simulator's accuracy, and a discussion of the method's limitations), I plan to recommend acceptance (unless the other reviewers change my mind or realize I have missed some crucial details). 	3
The method is intuitive and it seems like it is capable of learning the chosen simulator parameters well, potentially leading to novel methods in combining system identification and offline RL. However, it is unclear how necessary the more complex sampling method is (compared to design deviation), and applicability to real world system identification problems seems limited. The paper would be strengthened by examining how well the method works in adapting 'realistic' simulators from real world offline data, where there is more likely to be interactions between multiple simulator parameters and the domain gap is larger. 	3
In general, this paper proposes a very interesting direction, but important analyses and experiments are missing in the current version. I tend to reject this paper before the discussion based on my concerns in the main review. I may raise the score if the authors can provide reasonable answers to my questions.	2
"The paper presents an important approach to address offline RL training by tuning a parameterized simulator to produce training data for a policy. This has great potential for further investigation and comparison/combination with other offline RL approaches. The empirical results are weak in the current form, leaving me less certain the work is mature enough for publication. I incline toward seeing this as sufficient contribution to merit acceptance.

My score would improve if the authors can provide clear statistical evidence of the superiority of the new method."	3
"This is a good showcase of the power of reservoir computing in signature methods, but there is room for improvement.
"	1
In summary, I believe this paper is under the acceptance threshold because of its limited contributions as well as some other issues, as I have pointed from W1 to W4. 	2
Using Signature theory for time series analysis is a promising direction with some theoretical guarantees. The paper however is not self-contained and some details are lacking. Please help to address the above comments.	3
"This paper would represent the best reference to the idea of ""randomised signature"" features if accepted. However, it is currently lacking in that it does not adequately reference prior work, and it has theorems that are slightly too vague. If these things can be rectified, I would consider raising my score. "	2
The proposed objective is straight-forward and can be of use to the MBRL community. However, I do think adding additional experiments to put the contribution in better context will make the work more useful for members of the community -- in both, understanding the significance of the work and potentially, improving value-aware objectives for MBRL. 	3
"The reviewer leans towards recommending the submission for acceptance. However, while the idea and the insights are sound, the experimental results could be strengthened by adding relevant baselines and testing on more environments. Addressing the outlined concerns might increase the overall score.
"	4
"The motivation and presentation of the proposed method are major strengths of the paper. The experiments presented on the simple (InvertedPendulum) and complex (Hopper) environments are insightful.

The paper has potential to be a strong contribution if the empirical analysis can be expanded based on the weaknesses I had mentioned in my review. 
"	3
A well written and motivated paper needs a bit of clarification on the motivation and execution of experiments. The motivation makes me lean towards accept, but I would appreciate if the authors fixed or clarified some of the limitations.	3
Although the theoretical results are flawed, I think most of the errors should be fixable.  I am open to increasing my score if the authors could fix the problems and make the analysis rigorous. The topic of combining EI with contextual bandits is novel and could be interesting to many audiences, even without the claimed regret improvement. 	3
As mentioned above, some important references are not cited. To show the technical novelty, detailed comparison with these references is needed. In addition, since the uniform constants in the proof do not have clear dependence on the important parameter $\delta$ and there are so many typos, it is a bit hard for me to evaluate the correctness and significance of the main results.	2
"For the reasons above, I am leaning towards weak acceptance.

========== Post-reponses =================
Thanks to the authors for the responses, I am staying with the current score.

"	3
This paper is well written and shows novel contextual bandit algorithms using expected improvement with new regret analysis.   This could be a nice contribution to the bandit community.	4
"The paper is okay but the contribution to the study of bandit problems does not seem to be significant. I think it would be better if the authors can highlight their contributions pertaining to what I suggested above.


=======================
Thanks for response from the authors. I am increasing my score from 5 to 6."	3
I very much enjoyed reading this paper. The motivation is stated clearly; the problem of having a biased gradient estimation of MLE in training EBMs is certainly a reasonable problem. The proposed approach certainly makes sense and I think is a very reasonable approach to this problem. While the work is very close to [1] which may raise some concerns regarding novelty, I think it can also be seen as a strength given  that it is showing a very good application of method for posterior inference in training EBMs. Furthermore, the addition of energy-based correction is I think, while trivial, is a very neat trick that enables one to avoid computing the determinant jacobian when running HMC unlike NeuTra. The experiments I think could use some improvements, but overall demonstrate the case that the chains in the proposed approach mix better than SGLD chains. The proposed model does have a small disadvantage in the sense that it requires to train a flow model in advance. However, as the authors show in Section 4.4, even using a small flow results in a significant improvement. Overall, I am happy to recommend acceptance. 	3
See above.	3
The paper is clear and easy to understanding, but there are some questions to be addressed.	2
I like the paper, and think it is well-written in general. The main contribution is a novel combination of existing methodologies. I am not completely familiar with the literature, but if this combination is novel I believe this to be a solid contribution. However, my evaluation is dependent on adequate responses from the authors.	3
Weighing the strengths and concerns as listed above, I am currently recommending the paper for a rejection.	1
"This paper puts together an approach to apply SFs to some robotic control tasks. However, the approaches are not particularly novel, the empirical results are not compelling and key details regarding training of the actor are not included.
"	2
Although the paper is well motivated and presents interesting implementation details, the proposed method misses the discussion of concrete similar work. This related work makes the proposed method seem fairly incremental and only marginally novel. Furthermore, the evaluation setup presents some flaws that make it hard to validate the method and its limitations. Therefore, my perspective is that this paper does not meet the requirements for acceptance.	2
The authors proposed a learning framework to learn object-centric abstractions for high-level planning, yet it does not solve the fundamental limitation of the manually designed planning domain, and it is more of an autonomous labeling tool that partially alleviates human efforts. 	2
Despite the above-mentioned limitations, I think that this paper carries sufficient novelty and is an interesting, significant extension of prior work in this area, which opens up interesting follow-up questions and opportunities for future work. Overall, I recommend “weak accept”.	3
"This is a well-written paper that introduces several techniques into an existing framework for learning symbolic representations for planning. The proposed contributions are interesting and the improvement over prior work is clearly demonstrated. 

While the final approach itself is still limited, in the sense that it relies on several strong assumptions, it advances what symbolic approaches are currently capable of. Further, by highlighting the trade-off between creating object types and problem-specific grounding it contributes an interesting problem that may not yet be on everyone's radar.

While I would like to see this paper accepted I also think it can still be improved. The presented approach is quite complex, and although the authors do a good job overall, the presentation for the grounding is lacking and it is not made sufficiently clear how this is achieved and what knowledge is assumed to arrive at the presented results. I also suggest that the authors consider creating a figure to accompany figure 1 that specifically visualizes step 1 (and step 5). In order to make the paper accessible to a wider audience, an introduction to PPDDL notation would be helpful to include. "	3
In this paper, the authors fine-tune BERT to find affective lexicons and show high correlations between estimated values and dictionary values. However, the paper is filled with a lot of drawbacks from methodology, style of writing, and results. 	1
This paper contains too much background knowledge up front, while the experimental part is too little and the analysis is too superficial.	2
There are clear gaps in both the proposed ideas and the writing/presentation of results.	2
The idea is interesting and results show the effectiveness. However, something is not clear and some ablation studies are missing.	2
The motivation of this paper is not well-defined. The goal of this paper is to improve the OOD performance of a full-finetuning model, however, it didn't compare with any method on OOD generalization / domain adaptation. The proposed method also makes parameter-efficient tuning method not attractive any more.	1
"Overall, I rate this paper as marginally above the acceptance threshold, mainly because the idea is interesting and somewhat novel.
However, there are some weaknesses esp. in the experimental setup and results that make this paper a borderline paper. "	3
I generally like the idea of this paper and the paper is perfect up to the place where the experiments are introduced. Discussions of this paper overlook major phenomena in the results, making the discussions and, probably, the conclusions are, in part, wrong.	4
"This paper combines adapter-finetuning and full-finetuning to handle both in-domain data and out-of-domain data and the idea is interesting. It's also well written and clearly motivated. Further, it provides a theoretical analysis of multi-class logistic regression to explain why it works. However, I think the authors overclaimed their contribution. Although this paper's title is ""Ensembles and Cocktails: Robust Finetuning for Natural Language Generation"", it lacks machine translation results, one of the most important tasks in natural language generation. In addition, the authors mention that ""ensembling is one of the methods that we propose as well"", which I don't agree with. I would recommend declining this paper."	3
The proposed algorithm and analysis/implementation are interesting but the experiments are not convincing. The paper also has some issues with the presentation (but these can be easily fixed).	3
The proposed method is interesting, while some points listed above need to be clarified.	3
This paper introduces a new efficient DR scheme for discrete OT. The problem is important as it's widely used in ML models. The new splitting Eq.(7) is interesting, and the authors have spent an extensive effort to make the implementation efficient and robust.	3
Overall, I believe this paper has useful ideas for improving the performance of OT methods, especially those currently in use, but it falls a bit short of proposing something that significantly advances the state-of-art for OT.	3
I'm leaning towards accepting this paper because it conducts an extensive analysis with existing approaches towards elegant explanations of adversarial robustness,	2
"Investigating the robustness of deep networks using the geometry of decision boundaries and adversarial subspaces is a well motivated and interesting idea. My main reservation about this paper is that the proposed interpretability metrics are demonstrated in a sanity-check style manner on only a small set of models. In my view, the utility of such interpretability metrics can be really demonstrated when they are applied against several models of similar types (e.g., an array of standard deep network models, and an array of adversarially robust models), and when it is shown that the empirical claims with the proposed metrics hold for a large array of standard models/training methods. 
"	3
This paper raises more questions than answers. It is hard to say the findings are reliable, accurate, or novel. The observations for adversarially trained models are more like a side effect of boundary smoothing, which is what adv training is designed for. Min-max optimization gives us a better explanation [8]. 	3
I think that the paper is well-written, continues an interesting line of research and improves on it. The Authors clarified the technical questions. While I'm not sure how applicable the results are to common ANN architectures (because of the smoothness requirements), I recommend acceptance.	3
I am not convinced that the current paper is bringing anything new to what is already known about the `spectral bias’ in the NTK regime, even though I believe the technical contribution to be non-trivial. For that reason, my recommendation is marginally below the acceptance threshold. If the authors can convince me that this damped equation can be used in more interesting ways (take target function in a different RKHS or use $K^\infty$ that is not the NTK etc.) or that I misunderstood or missed some important conceptual/technical contributions, I will raise my recommendation!	2
"A new function-space view of gradient-flow on neural networks. Establishes that in function space, dynamics of training (when projected along the most important eigendirections of the associated kernel integral operator, is very similary to NTK dynamics, even in under-parametrized regime.


After authors' response
---

My concerns have been fully addressed by the reviewers. I'm raising my score from 6 to 8, and therefore strongly recommending the paper for acceptance."	3
As a non-specialist of the NTK regime, I may underestimate the technical contribution of the present paper, but I was not surprised by the results and see the submission as a detailed and nice technical improvement of known results in the NTK regime. Hence, I am between neutral and unsatisfied by the paper.	3
I found this work very interesting. The proposed theoretical tool of “damped deviations” is useful, and the outcome (the “characterization”, see Summary of The Paper) is a nontrivial step for the important task of going beyond infinite overparameterization.	3
"The HWD is a novel discrepancy for distributions on heterogeneous spaces, based on slicing and embeddings, and in this sense, is not very original. That being said, it is faster to compute than the traditional Gromov-Wasserstein and its approximations; and HWD provides convincing results as a discrepancy in generative models. Nevertheless, I think that the present paper lacks of justifications on (i) how this discrepancy behaves, (ii) the formulation of the problem, (iii) the choice of the hyperparameter $d$, (iv) the role played by the sets of constraint $M_{C_\phi}$ and $M_{C_\psi}$.
"	3
It seems that the contribution is quite incremental (e.g., using embedding approach in Alaya et al. 2020 for distributions supported in different spaces to combine with distributional sliced-Wasserstein in Nguyen et al., 2020 to solve the Wasserstein in 1d-projected space).	2
In my opinion, the contribution of the paper is limited as it is a direct extension of [1]. Both the formulation of HWD and its theoretical results do not appear to be novel.	1
An interesting paper provides a good finding.	3
Overall, this work does not provide very new insights for improving adamw or showing the better generalization of adamw over adam-l2. Moreover, it also does not have new proof techniques/frameworks.   	1
I recommend to reject the paper.	2
I lean to accept the paper. The paper analyze the AdamW optimizer well, and the experiments on small dataset support their conclusions. It will be more convincing to conduct experiments in more realistic scenarios such as transformer-based architectures trained on ImageNet.	4
"The paper tackles an important challenge with thoughtful methodology and reasonable analyses. The paper is also well written, presenting several findings about the knowledge captured in BERT. 
The proposed methodology however has problems which limit the contribution of the paper.  "	2
I think this is a solid and interesting paper. My major problem is that this paper spends a lot of writing on development details that could be simplified to make place for more in-depth analysis. I am willing to further increase my score if the authors can present a more in-depth analysis. 	3
In summary, I find the main paper lacking in sufficient disclosure on methods, and I do not believe the techniques or findings of the paper to be sufficiently novel, though I can see that the dataset would be useful. I still vote reject for ICLR 2022.	2
Interesting topic and substantial empirical effort. Methodology (clustering, dataset annotation, etc.) has potential problems. Reporting needs improvements. The generalizability and novelty of the overall finding are limited. 	3
While the proposed approach is novel and interesting, various motivational aspects (e.g., comparison with prior work) are unclear or lacking. Moreover, the method is crucially limited by its incompatibility with federated averaging. Thus, the paper does not meet the bar for acceptance in its current form. However, I am willing to increase my score if my concerns are addressed.	4
Several aspects of the proposed method should be clarified, and I think that the extension of the MMDM with fairness constraints to the federate learning is marginally significant. 	2
All in all very nice and elegant looking contribution. However quite a few questions remain, I would be happy to see this paper accepted in case the authors can make those small modifications (that I have listed) and also address my questions.	4
I think the paper is missing discussion on important previous and related work as well as not including them in the results as baselines. Baselines should be added along with discussion around some of the results which does not seem to be promising. More details can be found under weaknesses discussed above.	3
The pipeline of the paper can work, but the impact of the method and the novelty is not very clear as it depends heavily on Chen et al. and to some extent like DECA’s principle in rendering. As mentioned in the main review, the dependency of the Chen et al. needs to be critically evaluated as an initialization. Also, the choice of underlying 3D face model and impact of it in the outcome is also needed to understand the method’s utility in the comparisons. In the rendering, it seems the high frequency is getting lost. A few more results with more wrinkles in face would be better to judge the effectiveness of the rendering given a good initialisation from Chen et al. 	2
"While the authors design two GANs to generate the detail map and final image along with several loss terms, the authors still need to do a thorough and fair ablation study (how important is the “weakly supervise” mentioned in both Section 3.2.1 and Conclusion? How important is AugW and DSL separately?) and compare with fair previous works (How about comparing with models using the same input?).
"	3
The novelty and scope appear limited and the evaluation is incomplete. The paper could be improved by comparison against state-of-the-art and the use of more standard-metrics. 	2
"This paper proposed a pipeline FaceDet3D that can generate plausible geometric facial details from a single image given any desired target expression. Novel losses were proposed and the model was trained and evaluated to show its effectiveness. 
However, I think this paper is still under the bar of acceptance as the experiments are not very convincing. Also, the readability of the paper could still be improved. "	3
"
As it stands, the paper seems to benefit from the gains of data aggregation among random subsets of clients, without adequately grappling with the increased privacy risk. For this and the other reasons mentioned above, in my opinion this paper should not be accepted as it stands. "	2
"In general, the technical and theoretical results of the paper are quite marginal. A clear discussion demonstrating the effect of daisy-chaining aggregate is missing. For instance, by using such method, how faster a federated learning algorithm e.g. FedAvg would converge.

Another concern is the assumption of iid data which makes the applicability of the proposed algorithm limited. "	2
"The research question itself is an interesting topic to explore. The authors develop an algorithm to solve this question by exchanging model parameters across clients rather than exchanging data. This is good preliminary work in this direction. More work needs to be done in designing a novel coordinator, discussing the communication cost, and explorations on non-iid data settings.
"	2
The paper presents a novel procedure for learning federated learning systems. I think that the paper misses important related works and that the empirical evaluation is not sufficient. Therefore, currently, I recommend rejecting the paper. I am willing to reevaluate the paper based on the author's response to my concerns.	3
I will not recommend accepting the paper. Mainly because of the limited contribution. I will vote for strong reject if I find similar references demonstrating similar conclusions in one-shot detection scenarios.	2
"- Overall, I think the discovery is well-made, it shines light on one perspective, that is, using more diverse training data can be beneficial for generalisation,  however, it misses the other element on, how can we further boost the generalisation from an architecture or model design perspective.
"	2
On the one hand, this paper is well written and produces sota performance. However, I am concerned about the novelty and the root cause for the improvements. I am leaning towards reject for now and would like to see the response from authors.	2
Considering that the contribution of this article does not meet the requirements of ICLR, I do not recommend to accept it.	2
The paper takes a positive step forward in constructing generative models over molecules by including graphical and geometrical information in a single model. However, some of the empirical evaluations muddle the overall story and could be improved/clarified.	3
GEN3D appears to be a competitive approach for molecular structure generation. The main weakness of this paper is, however, that it lacks methodological novelty. Methods, such as G-Schnet, MolGym, and ENF, are closely related to this work, and the originality of this work is, in my view, insufficient for this venue.	1
Although the proposed method seems an important research direction for molecule generation, the experiments did not show the usefulness of such approaches to compare with the previously studied methods. Some of the descriptions are insufficient to understand the model details as well.	2
My main concerns of the paper are its novelty and the way it introduces the generative model.	2
The model introduced in the paper is well-executed, and based on the generated data attached in the submission, the data quality appears to be good enough. However, in my opinion, there are more fundamental issues in the motivation of the problem and whether experiments successfully justified the usefulness of the generated data from the proposed model.	2
"The problem is really interesting. 


Correctness: 
There are problems with the experimental results. If the authors can add the suggested results and show that their results are statistically significant, I would be very happy to increase my score. 

Novelty: The approach also lacks novelty, only proposing an additional loss which is not clearly described. However, their application is very interesting."	2
"The paper addresses a critical problem in symbolic reasoning domains, and the
experimental results are promising. However, I think it's not ready for
publication because of lacking a discussion for practical settings to use the
proposed GAN-based data augmentation, an evidence to show its generality, and a
comparison with the previous work.
"	2
This is a strong paper overall: innovative, well-written, and potentially important. I think it will be of interest to anyone applying machine learning in data-sparse symbolic domains.	3
"This paper presents several interesting and novel ideas regarding the linear mode connectivity of the optimization landscape of neural networks. The findings are supported by extensive experiments. An interesting Conjecture is stated, which could be interesting for future studies. On the other hand, I think the writing of this paper could still be improved.

**Update after rebuttal:** The rebuttal addressed my main concerns mentioned under the Main Review section; thus, I increased my score. I particularly appreciated the empirical strength and the insights of this paper.

On the other hand, I agree with other reviewers that the authors need to clearly state the version of the conjecture that is already proved in prior works.

My other remaining concern is about the loss function used in the proposed conjecture. As in the current form, the conjecture is stated for the minimizers of generic loss landscapes of neural networks. However, it doesn't explicitly clarify whether there are any regularization penalties (e.g., $\ell_2$ or $\ell_1$ loss of weight matrices) in the loss function, which I believe is used in the experiments (e.g., weight decay). I think this aspect needs to be stated more carefully."	3
Overall, I think the conjecture presented by the authors is highly interesting and it is the first time I have seen it in writing. While the authors' empirical and theoretical results fall far short of verifying their conjecture in realistic settings, I think they are sufficient to recommend acceptance.	3
"I think the paper is a fairly thorough empirical evaluation and makes some valuable findings. However, I find it lacking for (what even the authors consider) a ""bold"" conjecture. I am a little confused as to the exact message of some of the experiments, and therefore am on the fence when it comes to recommending acceptance. "	3
The authors study the effect of width, depth, skip connections on the barriers (via lines, i.e., LMC) after having rotating one of the solutions with a suitable permutation. 	2
"Overall, I believe the paper is good in that it introduces a simple yet effective adaptation of adversarial training to make it effective against data poisoning. The presented empirical results are convincing. Though, I don’t quite understand the qualitative assessment of the introduced defense, so it would be great if the authors can improve the presentation there.
"	3
"1. There is no study on the impact of different poisoning rates.
2. The method is only effective against $L_\infty$ but not $L_0$ and it is not clarified in the introduction.
3. A baseline defense method is not evaluated in the paper.
4. The number of evaluated models and datasets is small and limited.
5. The writing needs improvement."	2
Overall, I am leaning towards acceptance. The proposed defense outperforms many existing defense strategies including differentially private SGD, adversarial training, various data augmentations, and filter defenses, providing a strong trade-off of robustness and accuracy.	4
This paper is clear and easy to understand. The idea is interesting and the arguments are convincing. However, since it is some kind of combination of two existing robust models, I give weak acceptance based on the novelty. 	3
The novelty and contribution of this work are marginal.	1
Many important comparisons and studies are missing. No technical novelty. 	1
I think the authors investigate a very interesting/relevant challenge that could be applicable to a wide range of machine learning applications, but the work as it is presented now -- the story, writing, and empirical analysis -- needs significant revision to present a compelling case for the utility, correctness, and applicability of their method. 	3
The setting is very appealing, and the paper is clearly written, but I'm concerned about the MADT-Online method (see above).	2
I really liked a lot about this paper, including the cool and quite practical use case that they consider for the first time. Another strong point is that they provide a data set to jump start research in this area, although its overall value to the community is probably not that high as it is from a trained model and not humans. It is also interesting that the authors show how transformer models can provide value in this setting. The trouble is just that it is unclear how the contributions of this paper are related to the literature on MARL and offline RL, making it necessary to provide more analysis to understand whether these gains are connected to the particular setting considered here or rather hold in general. It is great that this approach can improve on popular algorithms for offline single agent RL, but no algorithms from the multi-agent literature were compared to and there is very little theoretical understanding about the value of particular design choices within the algorithm.  As a result, I lean towards rejection at this time believing that this paper can make a much larger impact to the community following another significant round of revisions.   	2
This paper studies a new problem in MARL. However, its main algorithm is not novel and the presentation is not clear. Experimental evaluations are not sufficient, the comparison of MADT and BREMEN is needed. As BREMEN also studies the problem of online tuning RL with offline training. Thus I recommend the rejection.	2
The contribution is limited and the main idea has been observed in existing work. The current version cannot reach the borderline of ICLR.	1
Interesting and theoretically sound model. The set of experiments is quite modest, in addition to some minor presentation issues.	3
"Pros: This paper does a good job at providing a mechanism for inference in (parameter restricted) BMs with convergence guarantees, as well as an efficient method to learn the parameters of these BMs.

Cons: The proposed method cannot be applied in many settings in which BMs can (unsupervised learning, sampling, use of multimodal posteriors). Little experimental validation of the usefulness of the convergent inference.

So the settings in which this approach can be used is very limited, but within that setting, it provides the required details for efficient training and robust guarantees for inference."	3
This paper potentially includes an interesting technical contribution, while the significance is not convincing and the evaluation is weak.	4
This paper is well-written but its contributions are incremental with somewhat weak experimental results.	2
The paper introduces some novel and interesting ideas. However, the numerical experiments  provided  in this paper are neither complete nor  always relevant. In particular, comparison with state-of-the-art estimators are clearly missing, the synthetic datasets are too simplistic  to validate the proposed method, the regimes (in terms of samples) are not relevant and many other effects related to the choice and impact of the optimiser, batch size, etc... are missing. I cannot recommend the acceptance of this paper. 	2
The proposed idea is reasonable. However, some key information (including notation definitions and experimental setups) is not well described, which may make readers difficult to follow the main ideas and reproduce the results.  	2
Good contribution on an important topic.	1
In my opinion, this work is original and well-presented. The paper discussed the presented method in-depth and the experimental evaluation was well-constructed. I think, overall, this work is a useful and significant contribution.	3
"I recommend that the paper be rejected so that it may further develop its empirical setup & verify (or reject) the findings on a larger dataset with more substantial linguistic depth (more resource variation) and breadth (more languages). In its current form, the paper's larger-than-life claims are simply not sufficiently supported with the choice of one dataset and six resource-rich languages, even if the hypothesis and findings are indeed interesting.

Edit after first round of author discussion, 2021-11-16:

I am concerned that there is a disconnect between the stated contributions of the paper (which are expressed as rather broad and general in its narrative) on one side, and the setup of the empirical investigation on the other side. While I may view the insight as to the (ease of) availability of broad-coverage linguistic resources sympathetically, the fact still remains that the findings are closely tied with the UN parallel corpus and its six languages. This in itself would not disqualify the paper at face value. However, text e.g. under ""Summary of findings"" 1-5 and later in ""Conclusion"" comes across as masking this fact and painting a much broader picture.

I would be much more inclined to accept this (undoubtedly insightful) paper should it insert a clear Limitations section which would reflect on the preliminary nature of these findings given the dataset constraints.

As for the language resources, I would very much love to hear which of those e.g. in OPUS (https://opus.nlpl.eu/index.php) were not fitted to the experiment while only the UN corpus passed the filter. This would be an insightful addition to the limitations section, as the author response now seems to indicate massive limitations in all the other available datasets. Do share with the community!

Edit after 2021-11-18:

Upvoting the overall score after insight re: multi-parallel datasets available (see discussion)."	3
"Reasons for score:  
Frankly, if the figures and tables in this paper were significantly improved, this would be an 8 or higher. The main motivation of the paper is exciting, the methods are well controlled, and the findings are interesting. The core of the problem (as I discuss in ""Cons"") is that the claims made in the paper rely on the figures/tables for illustration as evidence, and the figures/tables are entirely unhelpful. In otherwords, the tables and figures detract from your arguments because they are unreadable. Once this can be fixed, I think its great work, and it would be interesting and directly useful to many people in NLP. 

EDIT - Wednesday, 17 November, 2021: In light of the authors rebuttal to my review, and their changes to v2 of the document, I am bumping my score up from 5 (marginally below threshold) to 8 (good paper, accept). My biggest problem with the paper before, were the figures, but I think that the authors addressed my comments to the best of their abilities by adding a lot more clarity to the textual descriptions of the images, in the paper and in the captions. The figures are still not great, but with their new textual descriptions, you can look at the general shapes of things, and visually understand the point, and before v2 I did not have this same level of visual understanding."	4
"This paper contains sound and controlled experiments that test the role of input representation in bringing disparity to language modeling results, which I believe will be useful for the larger community. At the same time, several claims in the paper are too strong or not grounded in practical use-cases, and some relevant literature is not referred to, leading me to be marginally inclined to accept this paper.

### Edit - 22 November, 2021
I spent some time reading the discussion between the authors and the other reviewers, as well as revisiting v0.2 of the paper. In light of these discussions, I think I am fairly comfortable accepting this paper as my main concerns have been addressed by the review and the comments of the authors. Hence, I revised my overall recommendation to 8, while adjusting a few other scoring parameters too."	2
"This paper studies the fairness of transformer models to model languages
of different morphological complexity. This work extends the previous
observation from Mielke et al. (2019) and show that similar results hold
for transformer-based conditional language models: morphological
complexity is not indicative of the difficulty for transformer to conditional
language model, and representation level matters more. This indicates
that transformers are **fair** towards the languages tested in this
work, in terms of conditional language modeling. They propose to use
conditional language models to make it easier to compare the difficulty
of modeling a specific target language, and the usage of statistical
tests for testing performance disparity among languages is a novel act.
However, experiment settings and findings/results resemble the work of
Mielke et al. (2019) to some degree. The part on statistical testings is
somewhat baffling and some of the statements are ambiguous. The figures
for the main experiments are not very easy to read. The title is also
somewhat not quite indicative of the content (while it does seem
relevant), mainly because of the term \""fairness in representation\"".

## Update on Nov 22
After a thorough and in-depth discussion with the authors, I upvote the score from 3 (reject, not good enough) to 5 (marginally below the acceptance threshold), meaning that I am still inclined to reject.
I change my score mainly because the authors have addressed many of my initial concerns, including the experiment settings, some odd wordings or claims, and the presentation problems.
But I still do not find the paper is good enough to be accepted. 
I am still not convinced by the authors' claim of their novelty; the novelties listed by the authors seem to be more of ""something that was not being done in prior works"", but I am not sure how those ""something"" can much advance our understanding on this topic.
Overall, I think this paper presents some nice and well-controlled experiments, but the interpretation of the results tends to be too high-level and might be overstating; I often have difficulty linking the quantitative results and qualitative interpretations.
I am not sure what I have learned from this paper.


## Edit again on Nov 22
I will upvote my score to 6. 
After going over the paper several times, I think there indeed are certain empirical contributions of the paper. 
I think varying the data size is an important experiment that has yet to be presented in any prior works and is valuable to be empirically verified, I do agree with the authors on this."	3
"Although I agree that the topic is important, and the study covers a
fair amount of experiments with sound methodology, I do not think we
learn a lot from the present paper: (1) the effectiveness of sub-word
units for morphologically rich languages is expected, and (2) the
domain of application (conditional LMs) is rather narrow. Given the
issue is studies using (non-conditional) LMs earlier (as the paper
cites), the benefits of narrowing it further down to conditional LMs
is not clear to me (after reading the paper). The issue with word
segmentation in Chinese is also studied before, and I do not see any
additional information the present paper provides."	2
The authors present a substantial experimental work that offers interesting insights on time series outlier detection. However, the paper needs further statistical analysis to better understand the significance of these results. Additional analyses would also help to understand which techniques to use in different contexts.	2
The technical contribution is incremental. Relevant existing non-RNN methods for outlier detection on time series are not included in the study.	2
Overall, the work seems to be a technical report that contains some preliminary results but has many key issues to be further addressed.	1
Preliminary results of an evaluation work for time-series anomaly detection. Motivation has to be reconsidered.	1
As I mentioned in the above weakness, this manuscript is not presented well, the technical novelty is not significant, and the experiments are not extensive. I suggest a strong rejection.	2
-	1
A good idea is presented concerning the data augmentation and the cam-based extensions for EEG data. Nonetheless, the short interval claim seems to be ambiguous. Several details regarding the mathematical background and more experiments should be conducted to test the proposal. In addition, the paper presentation needs to be enhanced.	2
In summary, I think the technical contribution of this paper is not significant, but it's also worth having a DP algorithm for the GNNs.	2
Although the proposed procedure seems to improve over existing approaches for some privacy regimes, I would have liked to see more analysis on when the improvement actually occurs (when epsilon > some value).  Furthermore, there are several parameters that need to be tuned, so it is not clear if the extra privacy budget expended for choosing these parameters would actually lead to a better privacy guarantee than the edge-level privacy.  I also do not see the novelty in the algorithm, since we could just pre-process each node to have at most K edges then run an edge-level private algorithm to get the same node-level privacy guarantee.	2
"This paper considers the node-level DP problem on graphs, which is the first work on this task as far as I can see.  The problem is formally defined, and guaranteed privacy is proved. 

The methodology is directly extended from (Abadi et al., (2015)) and Feldman et al. (2018), with the consideration that a subset of K neighbors could appear in each update step. However, the proof is incomplete as only 1-layer GNN is considered. Furthermore, none of existing DP GNNs are adopted as baseline approaches."	2
Proposed method seems an application of existed DP method for the graph data. So the novelty of the paper is limited. Experimental section is insufficient. I recommend rejection for this paper. 	2
I would like to see the acceptance of this paper if the authors can provide more comparisons and discussion on the performance / accuracy regarding to the size of layers.	3
I think that CoAE-MLSim seems like a good idea and could be a great method. However, the results are not yet strongly backing up the claims, especially that it is more accurate and generalizes better than the previous ML methods for this task. I think that a revision during the discussion phase could raise my score to 8/accept.	3
"The paper is lacking in clarity as mentioned above. Although technically might be novel and can benefit the scientific computing community, this is far from publishable from a perspective of readability. 

Update on Nov. 20th during rebuttal:
I think during the rebuttal period, I have gain much more understanding of the paper and willing to raise the score. My remaining concerns are clarity of the paper (including the fact that I still need to use 200% to see whats in the figures, but understandable given the page limit) and out-of-distribution generalizability.

The paper raised an interesting idea, using changing the simulation to the latent space by compression of autoencoders. However as the overall goal is still to simulate new designs, the generalizability of it due to autoencoders might still bottleneck its practicle impact.

"	3
The main issue is that their numerical experiments are not convincing enough to support their claims on small data, accuracy, and extrapolation. 	2
Overall I think the experiments of the paper are insightful and have merit, but the presentation, positioning, and claims could be significantly improved. Maybe if the paper’s focus, description, and claims were concentrated around “evaluating calibration of deterministic epistemic uncertainty models under distribution shift”, it would be more representative of the experiments, and it could become a more conclusive work. 	2
I believe that this is a good paper, with important conclusions and insights for the uncertainty quantification community, it is correctly evaluated, and should be accepted. There are no major issues to be dealt with.	4
Overall, this paper provides the evaluation for some determinist uncertainty estimation methods focusing on the robustness of uncertainty under distributional shifts. However, the paper should include more related methods in comparison with a discussion about the pros/cons and possible assumptions for each method. The experiment results should also be analyzed in detail. 	1
"
This paper compares experimentally a number of epistemic uncertainty estimators within a fairly limited but interesting class of methods. This could be useful to practitioners of the field. A broader set of comparisons could improve the paper."	1
(see above)	2
Overall, I rate this paper as marginally below the acceptance threshold (5). The problem is interesting, however, the proposed approach lacks a comprehensive evaluation with state-of-the embedding models to ensure the efficiency of the approach. For example, the authors can employ BERT embedding as a baseline to evaluate to which extent SERCNN approach is good. Further, I suggest the authors, clarify how do they perform sampling (e.g., 10 tweets) to assess their approach with little data.	1
Although the paper tackles an important problem, it does not have a sufficient level of novelty and technical contribution. SERCNN is a straightfoward combination of existing methods. SERCNN relies on pre-trained word embedding models. Pre-trained Transformer-based language models should be compared. 	1
The novelty is not enough, and many experiments are missing.	1
The paper does a good job analyzing poisoning attacks. It shows that the methods fail over time and they do not generalize to future attacks. It is in interesting paper and has value for the community to consider longer term security measures for facial recognition privacy concerns.	3
This paper has addressed an important problem that, data poisoning techniques are insufficient to protect people's privacy from modern development of face recognition models and it is of great importance to draw people's attention to the discussed topic, to potentially push any legislation actions instead of searching for technical solutions to protect users privacy. However, I think the technical contribution from this paper is limited as little new insight is provided.	2
This paper reveals the flaws of previous works that use poison attacks to protect user identity from face recognition systems. It has some good discussions and provides two methods to break those protections. There are still some concerns about the experiments and the significance of the previous works the paper is targeting. Hence, I give it a Borderline, slightly towards Acceptance.	3
Overall, this is a facial privacy analysis with insightful claims, but the presentation and the discussion is very confusing. Thus, we are not able to agree this argument whether is reasonable and solid. The final rating will depend on the authors’ feedback.	2
"It proposes an interesting way to use Transformers for learning a universal controller for different robot morphologies and presents an extensive set of experiments. However, the overall contributions to algorithms and network designs are limited.
"	3
This is a good paper with impressive empirical results. While transformers have been applied for incompatible multitask continuous control before, none of the works scaled the setting that much. Dynamic replay balancing proposed in the paper might be useful in multitask RL in general. I would give this paper a 7, but the system does not allow it. I am putting 6 for now and am willing to increase the score if the authors address my questions/comments during the discussion period.	3
"The paper presents compelling experimental results, but does not adequately differentiate MetaMorph from AMORPHEUS by Kurin et al., 2021, and does not include AMORPHEUS as a baseline in the experiments.

--- POST-DISCUSSION UPDATE ---

The authors have clarified the distinction between their work and that of Kurin et al., 2021, and have performed new experiments comparing MetaMorph to an ablated version (MetaMorph-AO) that approximates the more restricted morphological observations used by Amorpheus. 

Although MetaMorph and Amorpheus use differing sets of input features, they are still fundamentally the same architecture. (The ""morphology aware transformer"" used by MetaMorph is just a standard transformer encoder, like that of Amorpheus.) So in my view, there is little novelty in the Amorpheus architecture, and the work's value lies in the scope of its experiments. I have raised my evaluation accordingly."	2
I generally agree with the argument of the paper. As the authors mentioned in the Abstract, Deep learning research fields like vision, natural language, and audio actively leverage Transformers for large-scale pre-training followed by task-specific fine-tuning. The idea makes sense that it deals with robot morphology as a modality on which we can condition the output of a Transformer in the modular robot design setting.	3
The paper tackles an interesting problem in a sound way introducing some key technical contributions. Addressing some weaknesses in the presentation and in the experimental part, as well as being more open about limitations, could improve the quality of the submission.	3
"In general, I believe the proposed approach to be simple and elegant; the arguments about the data regime are well argued, and the evaluation is precise. However, my biggest concern has to do with the viability of obtaining the requisite auxiliary language instructions — I’m not convinced that this data can be easily obtained in the wild, which puts the value of this approach into question.

I sincerely hope that the authors can engage with me on this point!"	2
I think instruction generation as an auxiliary loss is an interesting approach and presents a new perspective that labeling subtasks with instructions is better than just collecting more demonstrations. I also think that some more baselines and clarifications are needed. 	2
"After reading the whole paper, I didn't find any interesting points that make me feel excited about this paper. I think the authors did good experiments to verify their thoughts, but this paper has no technical contribution and the results are not surprisingly good enough. 
This paper is easy to read and has no significant fault. "	1
Strong results that show instruction modelling help policy learning on two simpler tasks. Evaluation leaves much to be desired in terms of substantiating main claims.	3
"1> Difference between GAIN and the proposed method is not clear. What are the technical improvement over GAIN and how are they working? 

2> Experimental section is weak and could be improved as per suggestions. Also, there is no comparison with intrinsic XAI methods. Proper evaluation can make things more clear.
"	2
As discussed in the review comments, I think this manuscript lacks technique novelty, has potential limits to more challenging tasks, and has less practical usages. In addition, the overall manuscript is not clearly written. Therefore, I recommend rejecting the paper.	2
"The paper tackles an interesting problem of making the model more robust to spurios correlations. Although the reviewer finds the problem interesting and relevant for the ICLR community, the reviewer thinks that the paper in its current state is below the acceptance threshold. 
The methodological contribution is rather minor. The approach requires collections of binary masks that might not be available for all datasets. It is also a bit unclear what the community learns from this paper -- it is expected to have more object centered gradCAM activations by using object binary masks. Moreover, the presentation of the paper could be significantly improved. For details see main review."	2
"- The contributions over ""Bad‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌ Characters: Imperceptible NLP Attacks"" paper is not clear at all, the authors cite this work but have not evaluated as a baseline or discussed the differences of their work.

- Poorly written, makes reviewing painful, I would recommend revising, reorganizing and proofreading. For example, the sentence in the abstract, ""This  hypocrisy can disrupt normal AI services provided by the cloud companies."", here, I have never seen the word ""hypocrisy"" instead of the word ""attack"", it seems like an effort to paraphrase or use translation software from another language.

- No mention of easy and trivial countermeasures against this attack.

- Evaluating transferability of the attacks between different services would be useful. The authors complained about the API costs, what if the attacker crafts the adversarial example against a local sentiment classification model and submits it to one of the APIs? Currently, it is not clear how much each adversarial example costs and judging by the algorithm, it might be restrictive."	2
The technical novelty of the proposed method is limited. The empirical evaluations are not solid enough to demonstrate significant contributions.	1
"Detailed Comments:

1. The comparison with Boucher et al. is unfair and mostly incomplete. The authors need to better position their work w.r.t prior art to highlight their contributions. 
2. Work by Boucher et al. suggests that the proposed attack can be used for a variety of different tasks apart from sentiment detection. Can the authors try the same?
3. The authors do not evaluate any defense methodologies against these attacks. Can the authors comment on the impossibility of defenses? For example, what if one were to convert the string to remove all unicode characters (as a sanitization step)?
4. There’s no evaluation to suggest that the modified strings would fool a human. Was there any MTurk study performed to support these claims?
5. What are some other use cases of the purported attack? Can the authors comment on this? "	1
This paper considers an important topic for RL. However the proposed solution is heuristic and evaluations are not convincing given the simplicity of the tasks. Overall i think this paper does not offer enough contribution in its current form.	2
Given my comments in the above section, I think the paper could be improved if the authors can show the results of the method with a more realistic target score (e.g. R_max * T), provide the theoretical justifications, and evaluate the method on more realistic domains. I would vote for a weak reject given the current status.	3
As I described in Main Review, this paper seems to require extensive hyper-parameter search (proper $\alpha_{offline}$ and well-balanced $K_p$, $K_d$), and to propose the incremental algorithms on top of TD3+BC (and resemble Lee et al. 2021), combining Q-ensemble method proposed by Chen et al. (2021), downsampling for imbalanced replay buffer, and adaptive $\alpha_{online}$ tuning. Due to the lack of novelty and significant contributions to the ICLR (or RL) community, I vote for rejection.	2
The paper aims to solve an important problem: adaptive behavior policy constraint in offline-to-online learning. However, the proposed method is heuristic, and more theoretical analysis is expected.	2
The paper presents some interesting learnings via a diligent evaluation of the design choices used in Deep RL algorithms for continuous control settings. The paper presents an ensemble method using these learnings and posts some SOTA results. I believe the community would benefit from these learnings and the resulting ED2 approach. However, I do note that the novelty for the method is quite limited as ED2 is fundamentally an ensemble of previous method (SOP) with well-evaluated design parameters for the target task of Mujoco benchmarks. 	2
"I believe the paper should present a more careful analysis of these design elements and put more effort into justifying their choices used for comparison.
"	2
"Given that the algorithm is entirely empirically derived and the empirical results are not compelling, I have given this paper a reject. I appreciate that there are important ideas represented here, but it currently isn’t up to the ICLR standard. 

After author rebuttal, the score has moved from 3 to 5 (see comment for more information). 
"	2
"The paper’s strength is a straightforward performant policy optimization method and the insights developed through experiments. However, hyper-parameter search isn’t performed to substantiate the strong claims and it isn’t clear how much more computationally expensive ED2 is compared to its competitors.

*** updated ***

I will update later. "	3
This paper conducted an experimental study over a range of tricks that are often exploited to facilitate ensemble deep reinforcement learning. The corresponding empirical findings can be quite important to guide future design of more effective ensemble reinforce learning algorithms. Meanwhile, the technical novelty and theoretical depth of this paper may need to be strengthened.	2
"The work is technically sound, the core idea is creative, and involves a series of rigorous evaluations to substantiate the claims made. However, in line with some of the highly related previous works, I believe that the paper will benefit from evaluations on some additional NLP tasks that are considered more difficult (SUPERGLUE). Also, adding a few missing details and conducting a sensitivity analysis of the downstream performance with respect to the size hint would help the readers and practitioners who would use the pre-trained model for downstream tasks. I recommend a weak acceptance for the paper. 
"	3
The paper presents a straightforward idea to pre-train large scale LMs directly on HTML. The empirical analysis is thorough, the results look very good, and the writing flows very nicely. Overall a very nice and fun read.	4
Overall, I appreciate the authors' efforts to propose the first PLM trained with HTML-format data and make it work. The pretrained model also shows impressive performance on some tasks (e.g., zero-shot summarization). However, I have some concerns about the paper (e.g., weak technical contribution and novelty, dependence on plain text PLMs, overclaims in the comparisons with state-of-the-art plain text PLMs, missing important references). If the authors could address my concerns in the rebuttal, I'm willing to adjust my rating.	3
"This paper presents HTML-based large scale language model pretraining. The model architecture and pre-training method are both based on BART. The author presents the resulting model, HTLM, as a strong zero-shot learner while being on par with other large transformer language models on the GLUE fine tune tasks. 

While the authors present a lot of details in a linear fashion, there are still several gaps in the writing and would require clarification."	2
Though the paper does not propose any novel methodology, I believe that it is a solid step towards the use of real-world datasets for benchmarking unsupervised domain adaptation algorithms, and would be a valuable contribution to the conference. 	1
While I recognize the low novelty of the work, I believe the contribution of the extended WILDS data set as well as the insights provided in the empirical evaluation are of significant enough value to the machine learning community to warrant acceptance.	2
"Although the dataset itself is very useful and important, the analysis and bench marking needs some improvement. Nevertheless, this paper definitely has merit, and if the authors could clarify the questions raised, I would be happy to update the score. I haven't gone through the supplementary material in detail. If any of the questions above have a direct answer in the suppl. material, the authors can directly point to that and I would be happy to update my comments. 

Post Rebuttal 
****************

I thank the authors for answering all my queries patiently, which answered most of the questions I had regarding suitability of self-supervised and semi-supervised algorithms to the task. I would like to raise my score, and suggest the authors to include these discussions in the main paper."	3
"An impressive and exemplary dataset paper. The data collection and baseline implementation could be an important stepping stone for the ML community to address its biggest challenge yet: domain shift in the Wild. This paper highlights that we are not there yet, and that unlabelled data give an encouraging venue that has not reached maturity yet.
"	2
I would recommend this paper for acceptance as long as no other reviewers have major concerns. The paper conveys a good contribution. It's quite unfortunate that those two recent works (Kangle Deng and Yu et al. , both in July 2021) clash significantly with the two main novelties of this work, but as stated above, those papers are too close to the submission deadline to be considered double submission. In any case, I find the citation of the work of Kangle Deng. et al. a requirement for acceptance.	3
"The results shown in this work seem promising and advance towards single view 3D scene understanding. However, due to the mentioned weaknesses in the main review, I am leaning towards a ""marginally below the acceptance threshold"" score.

Update:

The authors have addressed the comments in my initial review and showed improved results on the Shapenet dataset. Therefore I am updating my recommendation.
"	2
A major disadvantage of using NeRFs is their need for very expensive GPUs. I like that the authors address this issue and propose a solution. The price that one has to pay for that (RGBD instead of RGB, providing the number of objects) can be justified, often. Furthermore, I suppose that this paper will trigger follow-up works which improve the weaknesses of this work.	4
"Summary
The authors use an encoder network to infer a set of slots for each object contained in the input image and then these features are used in Nerf to generate novel views. However, it lacks the motivation of leveraging nerf lack. The authors must make it clear. In addition, it lacks experiments to verify the proposed methods. Evaluation of advanced 3D object dataset such as Google Scanned Objects is necessary. Overall, the paper should be rejected.

Questions to authors

1) The paper seems to include some contributions that do not directly solve the problem. For example, can the decomposition work with traditional NeRF and ray marching instead of the proposed ray evaluation? 

2) Section 3.3 mentions that the ray marching in NeRF is biased, which is true and a well known phenomenon in volume rendering. 
Then the authors claimed that the proposed ray evaluation is unbiased. Could you provide a formal proof? 

3) The comparison does not include results that evaluates segmentation and depth separately using state-of-the-art methods for each aspect. How does this method fare compared to those SOTAs? 

4) Ablation study is lacking. Overall I can see that the proposed method outperforms a few baselines but there is not much analysis into the proposed method. Statistics about running time should be shown too as the proposed ray evaluation could be faster than traditional ray marching in NeRF. 
For contribution in Section 3.3., output quality compared to NeRF should also be shown. 

5) The experiments are done with synthetic datasets only. Since the proposed method is based on NeRF, how well does it work with real photographs? 

Detailed comments

- Abstract: the motivation of segmentation is not clear. Infering a set of NeRFs, one for each object, from a single input image is a good approach, but the connection to segmentation of the scene is left unexplained. 

- The derivations are sometimes quite lengthy and sometimes the motivation is not so clear. For example, Section 3.1, 3.2, 3.3 only provides background, and then Section 3.4 (which appears quite late in the text) actually tackles the problem. 

- The experiments are only reported with quantitative results. Having some visualizations of the segmentation and depth and some novel view renderings in the results would be nice. 

- When comparing segmentation results, please add IoU metric. 


Rating

The paper sounds weak in the sense that some of its contributions are on improving NeRFs while the decomposition part is rather marginal. The presentation is sometimes misleading. There are a few concerns about the experiments and baselines as well. Therefore, my current rating is below the acceptance threshold. I invite the authors to try considering the questions as commented. "	2
The paper introduces a simple way to perform image composition using slot attention and a GPT-based decoder in place of pixel-mixture decoders. The ablation study is promising, but I would like to see the inclusion of additional baselines and/or more ablation studies to properly contextualize this work.	2
Basically, the idea in the paper is interesting, but the experiments should be improved.	3
"My main concerns are that I cannot see how the proposed approach can generalize to more realistic domains, and I am confused about the main paper's story. Overall, I like this paper and its contribution, but I think the authors should highlight and show that their new model improves DALLE and not the Slot Attention model since DALLE could potentially benefit object-centric information for zero-shot image generation in real-life datasets. Furthermore, since DALLE has shown vulnerability to compositionality, I would expect the proposed author's approach to resolving it, and thus I feel this paper misses an excellent opportunity to do it. 

I am open to the authors' feedback and other reviewers' opinions.


After Rebuttal
-----------------------------

After reading the authors' feedback and other reviewers' opinions, I would like to thank the authors for their rebuttal. The rebuttal addresses most of my concerns. I am leaning towards acceptance of the paper. I vote for 6.

"	2
The paper is technically interesting, but the authors should carefully revise their experimental section to address the above concerns.	3
"The proposed method is not novel, in two contributions that authors claimed: greedy method and sharing weights

Decompositions such as with CPD and TC very often encounter degeneracy and cannot find the true model even with true ranks.

NNs and the considered dataset for the compression task is relatively simple and easy to compress. 
This does not confirm the greedy algorithm works well for other difficult scenarios."	2
This paper tackles an important problem and provides a handy algorithm. However, the impact of this study is not significantly presented in the machine learning context. The technical contributions are based on heuristics and not technically solid.	2
Based on my own understanding, the method is not well explained. It is ad-hoc and no theoretical guarantee of its convergence. I would not recommend it for publication.	2
I appreciate a systematic study of the reward hacking phenomenon where the parameters of the problem are manually varied. I think the related work overview should be extended to justify the “systematic” aspect of the paper. Besides, some information about the correlation between the true and the proxy rewards would be very beneficial to the reader without prior experience with the studied environments.	2
"The paper’s findings are clearly useful to the research community as the reward hacking problem has been a poorly understood but ubiquitous. Three specific findings about reward hacking (see above) had to my knowledge not been empirically demonstrated in RL environments or only mentioned in passing but not studied explicitly. Before this paper, we therefore did not know if the findings correspond to generally applicable phenomena and through this paper we can gain some confidence. However, some environments and reward functions could be made more realistic.
"	3
I believe this paper tackles a very important and timely problem through a purely empirical lens. Since, this is a purely empirical paper, it is important to have more comprehensive experiments that are reflective of real-world settings, and have better motivated design choices. The paper in its current form does not provide useful takeaways that are practically significant and would require significant revision. 	3
This paper does a great job at investigating an important problem in AI safety in a concrete and rigorous way, and to my knowledge is the first to do so. I am in favour of accepting this paper. 	3
The paper presents an interesting evolutionary framework, where mutation is replaced by bagged sub-gradient descent and lexicase selection is used to refresh the population. The proposed method builds on ideas of lexicase selection from related domains and applies it successfully to train deep neural networks on benchmark image classification tasks demonstrating improved performance. The work is novel and potentially valuable to the community.   	3
This paper is easy to follow. However, the current experimental evaluation is insufficient to clearly show the effectiveness of the gradient lexicase selection.	3
The performance improvements over baseline methods are too marginal, nor the method is novel enough, for the work to be considered above acceptance threshold. 	3
"1. Please justify where the proposed training strategy is more useful than ensemble models as they have similar complexity.
2. Please include experiments that justify the reason of the performance improvement (see above).
3. Please add comparison between ensemble model and the proposed method in terms of both accuracy and other metrics (Expected Calibration Error, Cross-entropy Loss, etc)
3. Algorithm 1 seems not quite helpful in this paper. Please consider to remove it and make more discussion to justify the claims.
"	2
CEN is a simple, elegant solution for disentangling controlled and normal effects, that is backed up by thorough empirical evaluation. However, it leaves a few open questions that I'm hoping the authors can answer in the rebuttal: 1) can CEN scale to domains with more rich and complex observations?, 2) can CEN handle first-person observations, and 3) does CEN improve exploration for more complex domains? Additional experiments and discussion on these areas would significantly strengthen the paper.	4
I like the general idea of the paper, but several empirical evaluations are missing to support / justify the effectiveness of the proposed method. Hence my evaluation remains closer to the rejection of the paper.	3
A clear and well motivated algorithm is let down by (1) experiments that fail to provide evidence to support the paper's claims. (2) An unstated reliance on an unrealistic behavior policy also limits this approaches applicability. In its current state, I must therefor recommend rejection. That said, I could definitely see accepting this with some significant revisions. To address (1), the experimental evidence must better match the motivation of the approach. You could perform new experiments that show CEN models more than just the agent whereas other approaches don't. You might also reconsider your framing; perhaps the agent/object distinction isn't relevant here and CEN should be better for a different reason.  To address (2), acknowledging the limitations (or correcting me if I'm mistaken) is enough. A first step towards a promising new direction is enough for acceptance; no need to solve everything in one paper, but being explicit about the state of things is a must.	3
In general, I think this paper studies an interesting and relevant topic (controllable factors), and brings interesting new insights (inverse models do not work well, and there may be better methods). There are good conceptual illustrations of the proposed solution, but I do think there are some problems with the equations in Sec 2.2 that underpin their ideas. Their proposed network architecture and experiments do make sense, and show improved performance over inverse dynamics models. If the authors correct (or further elaborate on in the rebuttal) some parts of Sec 2, then I think the paper would be a good contribution to ICLR. 	3
Overall, this work proposed some interesting ideas to analyze and improve the entity usage in long narrative generation task. However, the metrics need to be carefully designed, and the analysis needs to be improved.	2
Overall, the paper is well written and organized. The studied problem is very important which makes the paper to have many audiences at ICLR and the proposed techniques can benefit the field however after performing few more experiments to support them (see Weaknesses section Point 2 and 3).  	2
In summary the idea of using entity memory and computing cross attention between input and this memory to incorporate information from entities and their attributes looks promising. However, the metrics showing the coherence of the text is only limited to the location of the entities while for the text the overall coherence of the text should also be considered. Some sections are opaque but I think if authors can address those the paper can be effective for the community.	4
This paper tackles the problem of entity coherence and consistency in narrative generation and tackles the problem through the use of an entity memory network that augments the pretrained language model. This proposed approach is interesting and demonstrates the effectiveness of the proposed approach on two datasets. The clear gains of the proposed approach can only be seen on one of the datasets compared to a simple baseline. Further, the authors also propose using two new metrics for automated evaluation. These metrics show a low to moderate correlation to human judgments.	3
This paper proposed a novel algorithm to solve reinforcement learning problems with offline demonstration policy. The idea is interesting and nature for many real problems, especially for learning from demonstration problems. A very straightforward update rule is provided by using an estimation of KL divergence. This improves the computational efficiency of the proposed algorithm, which is very promising in real problems.  The Mujoco and TurtleBot experiments verify the performance of the algorithm with both simulation and real experiment. Overall, this paper is well written and the idea is novel and promising.	3
The paper is strong but missing some key experiments, which preclude inclusion at ICLR in the present form. I would like to see one or more additional relevant comparison algorithms included in the simulated experiments, as well as a sensitivity analysis for the learning schedule, as described above. I have not checked the mathematical proofs thoroughly. I have not reviewed the code included in the attached supplementary material.	3
In general, while the work has (at first glance) impressive empirical results as well as a novel formulation of the KL constraint between the RL policy and the behavior policy, there are quite a few concerns that would need to be addressed with direct justification and experimentation in order to recommend acceptance. Hence, I currently recommend rejection, but I am open to updating my score if all of my concerns can be addressed. 	3
I find the paper's goals clear and interesting and the execution faithful and technically sound. This work makes a curious step in the direction of mixing together online learning and pre-existing policy data and I think would be an interesting result for others who do research in this area. I wouldn't mind seeing this work presented at the conference.	2
"This paper studies an important problem of how to leverage demonstrations for reinforcement learning and proposes a novel method that is demonstrated to outperform a set of baselines in various domains. It makes the case for how distribution matching should be used instead of pure supervised learning of demonstrations and shows an unique advantage of doing so, which is the capability of dealing with partially observable states. I recommend acceptance of this paper but would love to see another baseline added to the experiments as I mentioned in my review above.
"	3
"This paper tackled an important issue of music generation. They proved the proposed contrastive loss by compared it to the non-contrastive loss. However, while the aim of the research is providing controllability or steerability of the model by musical features, the listening test seems to validate preference of the continuation. In other words, the listening test does not reflect the control by the musician subjects. The authors should clarify this during the discussion phase. 

"	3
I found this paper thought-provoking and creative, but I found the empirical analysis confusing and incomplete. I like the CSP setting outlined in Section 2, and the methods introduced in Section 3 are interesting. However, I found the writing in Sections 4 and 5 confusing to the point that it was difficult for me to understand the empirical results.	4
"There are interesting aspects in this paper (especially the comparison between bias-tuning and prefix-tuning in the compositional setting) but the overall contribution is not very substantial (only Eq. 5) and its effects not extensively discussed. The musical examples are not well presented (impossible to know which are the selected features, only one example per continuation for each model, not obvious to distinguish the generations from the prefixes, extracts are too short) and fail to convince.
The method could be of interest for a musically-focused conference, but it seems that the technical contribution is not strong enough for ICLR 2022. "	2
It seems like a nice contribution.  It's confusing in parts, but the issues should be easily fixable.	2
The paper has theoretically analyzed the effect of gradient clipping in local SGD updates in Federated Learning and FedAvg. To the best of my knowledge, the technical analysis and results are incrementally novel and improves the results of existing works. 	2
The paper provides theoretical analysis for local gradient clipping under the federated learning settings. However, the practical cases where the theoretical analysis can be applied is unclear. And, the experiments are based on homogeneous local data in classic distributed settings. The contribution is also incremental.	2
This paper has many things to be improved.	2
"To sum up, several parts of the proof should be corrected and the theory should be extended to support more general choices of $N$ and $I$. Moreover, several parts of the paper such as the comparison of the known complexity results with the derived ones and numerical experiments require improvements.

Unfortunately, the current version of the paper cannot be accepted to the conference. If the authors resolve the mentioned issues during the rebuttal, I will increase my score."	2
In sum, I think this is an interesting paper and the topic worth further investigating. But so far the paper is not ready enough for the bar of the top conference like ICLR. I would vote for rejection at this moment and encourage the authors to further improve on the idea and submit to another venue.	2
Primarily due to concerns about significance, I am initially recommending a weak accept of this paper. I am happy to engage in discussion with the authors and other reviewers in order to reach a more confident final recommendation.	3
"This paper brings renewed attention to the role of Bellman error in offline RL, beyond the more widely known double sampling issue. In my opinion, Bellman error (or relatedly, TD error) is commonly thought of as the equivalent of *loss function* in supervised learning but for offline RL, and such analysis is much needed for the community and a good step at understanding the empirical success of deep RL and its fundamental difference compared to supervised learning. I would be strongly in favor of acceptance if the authors can aptly discuss how this contribution relates to the past work provided above. 
"	3
"Despite the weakness mentioned above, I still think this paper is good. The message is important and the results are novel and interesting.

My current recommendation is (weak) accept."	3
Overall technical seems significant and somewhat new, but as mentioned above in the weakness, the paper is missing some comparison against stronger baselines, especially for previous methods in NAS. 	3
The authors extend an existing NAS method MiLeNAS on federated personalization. However, the novelty of the work is somewhat limited.  It is lacking clear support evidence and insights to support why MiLeNAS was chosen. To support the idea of NAS for federated learning, more other NAS methods are preferred to provide more insights and depict the challenges of this problem. 	3
None	2
"Overall, the proposed method to use gradient-based NAS for FL to improve personalization, shows good empirical performance. The complaint of mine, is that the innovation is on applying existing method (MileNAS, or DARTS-like gradient-based NAS), directly onto FL settings. There are very few modification on original method, other than the settings are different.

In all, I give this paper score 6, and I am willing to discuss with other reviewers and the authors in later rebuttal session.
"	2
Although this paper proposes a straightforward yet definitely useful method for speeding up distributed training, in my opinion the clarity of presentation both for method and results can be improved. I give a score of a weak accept for now, with possible improvements if authors address the issues outlined in the review.	2
The paper is well-written. The idea of 1-bit lamb is well-supported by the convergence analysis and good empirical results. In overall, I think this is a good paper. Some other experiments is recommended. The main issue is in the assumption of the theoretical analysis.	3
The paper propose a communication-efficient large-batch LAMB optimizer to accelerate distributed training. The experimental results looks promising, but there could be some improvements both empirically and theoretically.	2
The strengths of the paper (extensive experiments) hasn't been able to outweigh the weaknesses (limited novelty, lack of AGV baseline, a seemingly slow approach). 	2
This paper introduced an interesting set of black-box attacks and showed their utility through comprehensive experiments. It seems like a good paper. However, I am not very familiar with recent work on black-box attacks. 	2
The paper studies an important problem. However, the current evaluation in the paper is still lacking. More systematic evaluations on the evolutionary algorithms and stealthiness are necessary.	2
Although the attack looks indistinguishable from just a filtered image, it seems that the strength of the attack is questionable, in most cases images are mislabeled to a class which is very close to ground truth. Unfortunately, it undermines the whole purpose of the method, its practicality and effectiveness. Therefore, I tend to be on the rejection side. 	3
The idea in this paper is very interesting and novel to the adversarial machine learning community, however, issues on experiments and presentations prevent me from accepting it.	3
The paper is well-structured and the experiments seem generally sound, but they're too brief and at too high of a level to provide real insight into EC language structure and how we should identify and measure it.  However, I do call into question really how sound the ablation experiments are, since they are contrasted with such trivial baselines that it again hinders the paper from providing novel insights.  There evaluation measure is I think the closest thing to a novel contribution, and the authors seem correct in pointing out the shortcomings of existing measures, but the proposed solution doesn't seem a suitable replacement (it seems like it violates some core assumptions of what the community would want from an EC metric, and is also language and task specific).  Overall it seems like good preliminary work, and I hope to see a deeper exploration using these same methods and motivations.	2
Overall, I think this is a creative, technically sound, and very interesting paper with valuable contributions to the emergent communication literature (notably, the big question of how to make EC useful for the rest of the community), and the paper really made me think. It's a shame that there are some unanswered questions remaining in the paper (see Weaknesses). I hope the authors might consider some of these follow-up questions and experiments during the rebuttal phase, but I think even despite these flaws, the paper has enough contributions to stand on its own. If all of the issues in the weaknesses section are addressed I would consider raising my score to a 10.	4
I think this work addresses an interesting question, but I disagree with a few of their claims based off of the experimental evidence provided. The broader research question is important enough that I feel like even initial work in this direction is perhaps of interest to the ICLR community and worth accepting.	3
The paper presents a novel method and evaluation of using emergent communication for pre-training models on real NLP tasks.  This has the potential to help in low-resource settings, and demonstrates the value of emergent communication for NLP in addition to theoretical interest in language evolution.	3
"While the general motivation of this paper about offline reinforcement learning is spot on, there are issues with this paper as mentioned above. Happy to hear authors' answers to raised issues.


**update after rebuttal**

Considering authors' responses and comments, I've increased my score. "	2
In summary, I think the paper is well written and the idea is interesting. The empirical evaluation is significantly better than the existing baselines on challenging tasks like Ant Maze, and I thereby recommend accepting the paper. 	3
The idea is simple and straight forward, but more explanations are needed to provide strong insights. 	3
The paper provides a good-performing robust offline algorithm based on a very simple but novel idea. I recommend acceptance of paper based on the novelty of the idea and the clean presentation.	3
My recommendation is mainly based on the contribution of this work in theoretical results compared with existing work. 	3
The paper is clearly written and provide comprehensive comparison among relative works.  The summarization of different variants of DRO are simply instances of a finite-sum composite optimization is interesting. However all of them  seems already justified by other literatures. The proposed variance reduced algorithm mainly targets on solving a  finite-sum composite optimization. However, the object function involves a new ingredient  in which they also employ the variance reduction technique to solve it. As compared to the previous variance reduction work in composite optimization, the dealing with the new ingredient   may not be that difficult to handle. Therefore, I tend to reject this paper at this time.	2
In summary, the paper seems to be very interesting.  It regards DRO as a composite optimization problem and proposed a a novel mini-batch constraint sampling for handling heavily-constrained optimization problems. The efficiency of the proposed algorithm is sufficiently validated by experiments. The originality and novelty of the proof techniques are very limited. 	3
"The paper presents a fast novel kernel (SIKF) and relative positional embeddings for Linear Transformers (FastRPB). The results with SIKF are quite mixed, and the motivation not very clear. FastRPB shows good gains against other baselines on 2 of the 4 datasets presented. However for the other two, the results are quite mixed. The reasoning provided for why FastRPB is bad needs further investigation. Additionally, having results on an image dataset more difficult than MNIST would help draw better conclusions. 
"	2
Given the weak theoretical and experimental supports of the proposed method and duplication of previous work, I would recommend the rejection of the paper.	2
Overall, the improvement over kernel function and position bias is simple, but it hasn't led to significant improvement.  The performance is still far behind SOTA performance. Either SIKF or FastRPB is actually not leading to significant improvement over baselines on most of the tasks. 	2
This is an incremental study for two existing issues of the Transformer framework on the long sequence tasks.	3
Overall, I think, while the novelty is limited, the proposed method is complex with many non-clear approximations, which makes it not general. 	2
As stated in the previous section, I think the method this paper proposes is interesting, while there are still some perspectives that can be further improved. Hence currently I tend to vote a 5 for this paper. I am willing to raise the score if I have some misunderstandings or some of the concerns can be addressed.	3
"The paper has weak theoretical and experimental components. The theory doesn't obviously show anything new of sufficient generality. The data structure appears to be the same as in prior work on LSH. The experiments suggest that using the STORM estimator is only slightly better than returning the mean of your data.

I don't see the strengths of this paper anywhere. I got more and more confused about this paper the more I read.

It's well written, but I don't see the substance beneath the nice writing."	1
The paper contributes some new ideas to reduce the compuation complexity of ERM. The experimental evaluation on STORM is adequate, and the results convincingly support the computaion strategy  of locality sensitive hashing and sketches.	3
Overall I found the paper interesting and relevant. Some related literature is missing which I've highlighted below. The experimental results mostly correctly demonstrate the methodological contributions. In my opinion, the definitions and notation would need another pass of re-writing as in the current version it was quite difficult to follow with multiple read-throughs. I would be interested to read more justification of the assumed setting (in particular, on the distributional parameters, and how the method can be extended to nonlinear settings).	3
Interesting problem and solid results, but the assumptions (linear classifications) are very strong	3
Solid paper, important problem, interesting solution. Some caveats (in particular the assumption the classifier is linear in the input variables seems unrealistic for most real-world classifiers, potentially severely limiting the applicability of the COPA framework in practice), but overall clear accept.	4
Several important details and comparisons are missing in the paper, which largely weakens the contribution of the paper. Also, there are flaws in the statement, and the writing is not professional enough.	1
I think this is an important problem and the paper is reasonably well written. Unfortunately, it is a rather weak attempt. Essentially, it is a rehashing of existing ideas -- which are already uninspiring from an ML point of view. Further, as described above, the experimental section is rather misleading. For instance, there is no clear comparison of inference time with LoLa (does it improve the inference time at all?). 	1
The paper is marginal acceptable because the paper is addressing an important question and the method is effective. However, the improvement of the method is not impressing or suppressing. The entire is not well finished. Thus, this paper is regarded as a border line paper.	2
"The paper provides a well-argued application of the state of the art in homomorphic encryption.
"	3
In light of the above concerns pertaining to the proposed method's motivation, unclear arguments, and empirical methodology, I am recommending rejection at this time. I'm willing to raise my score should my questions and concerns (and potential misunderstandings) be adequately addressed.	2
In light of the issues I outlined above, especially in terms of the absence of theory for the tabular case, lack of experiments on truly hard-to-explore domains, and missing existing work, I am voting for rejection. That said, I certainly acknowledge issues that arise when using geometric discounting and I encourage authors to further explore this idea. A better discounting scheme may in theory have a high impact, but the current manuscript is not demonstrating to me (at least yet) that this is a good alternative.	3
"Although the premise of the paper is interesting, and the proposed approach is novel, to the extent of my knowledge. However, the proposed framework is introduced in a rather arbitrary manner; it is not clear how it relates with some existing work; the results are hard to interpret.
"	3
This paper is conceptually interesting but could be refined in its execution. A few small aspects of it are unclear (including, to me, the main definition: Equation 1). I believe the paper will be strengthened with a much more clear exposition of the central idea, and improved rigor in presentation of the empirical results. For these reasons I lean slightly toward rejection at the moment, but am willing to increase my score if the paper can be changed in these two ways. For clarity, they are: (1) Answer PQ.1 and provide further exposition in the paper to make the proposed criterion more clear, and (2) Include a variance measure in the first two sets of experiments, and hedge the statements made about the results in light of the observed variance.	3
"The proposed method is methodologically an interesting contribution, however the practical impacts on training efficiency and model efficiency are minimal and overstated, which limits my recommendation. 

"	3
The topic is interesting but the concerns listed above need to be cleared. The main concerns lie in the real-application value and real benefit of the methods.	3
This work is well-motivated, and the experimental results support claims well. However, major concerns lie in the practices; the proposed method additionally introduces hyperparameters and cannot help practitioners reduce inference time.	3
"The paper demonstrates SSN and NPAS and presents experimental results on various benchmarks.
The text could be improved for clarity and organization."	3
Overall this a great piece of work comparing tradeoffs between multiple performance objectives for time series forecasting models, including classical and deep models. Some interesting observations have been shown in the paper which are mostly well justified. The main weakness is that the choice of hyper-parameters has not been explained, raising the concern of whether better hyper-parameter choices can improve the baseline results. I am willing to increase my score once the concerns have been addressed.	2
The paper attempts a valuable contribution to the univariate discrete-time forecasting literature, but by attempting to cover too many things quite superficially, does not appear ready to recommend acceptance at ICLR in current form.	2
"This paper has conducted a thorough empirical study on forecasting models, but it still needs some major modifications on paper structures and clarifications on methodologies before being accepted.

## Update after Rebuttal

We thank the authors for their detailed response in addressing questions and updating their manuscripts. The analysis of dataset characteristics in Appendix F is interesting but it does not appear to provide useful information that can serve as an insight in the future: this is fine on my side since it is not easy to discover patterns on massive datasets. 
Although the authors have also addressed the minor questions, we still think the structure of the paper can be further improved to make the paper more readable. The authors need some non-trivial modifications before it can be accepted. We wish the authors good luck in future revisions."	2
"A very interesting paper that proposes an approach that I expect to be impactful for future research. 
I would like this paper to be part of the ICLR accepted works."	4
The paper’s contribution is non-trivial. Although the inefficiency may be a problem, I recommend accept.	4
Overall, this paper takes the important first steps towards a very promising research direction of bringing novel ideas from topological data analysis into GNN approaches to multi-variate time-series forecasting, and therefore could inspire and spawn several follow-up works. The paper is generally well-written. The experiments are extensive, cover 3 different application domains, and clearly demonstrate the potential of the proposed approach in particular, and this research direction in general. I have a few concerns regarding e.g. computational complexity that I would like to see more discussion on, but those do not prevent me from giving this paper a recommendation for acceptance. 	3
While the paper describes an intriguing phenomenon, the training setups the paper uses as baselines are exceedingly weak and underdeveloped. As such, it is not clear to me that the paper is presenting a real improvement to our training setup. 	3
A simple approach that nevertheless fixes GPT's stability issues and improves empirical performance.	3
In my opinion, the weaknesses outweigh the strengths of this paper. In particular, the lack of novelty is a major issue that is enough to justify rejecting the paper.	2
"This paper studies a straightforward pretraining strategy. It does not propose a novel method, and is not well-motivated enough as an empirical evaluation, in my opinion. Given prior work, results were not unexpected. At this point, I cannot clearly understand the paper's claims regarding ""stability"" and ""regularization"", and hence cannot recommend acceptance to ICLR (though I think it's fine as a workshop paper)."	2
In my opinion, this work is novel and meaningful. However, it is less satisfactory not to address the more interesting non-linear term N. I recommend borderline acceptance.	3
"The proposed method is interesting as it extends a recently introduced framework (neural operators). However, the main technical novelty seems not necessary in the context of previous work.
"	1
"A great work, and a great idea. 

However the notation needs a lot of work, the explanation needs a lot of work, the empirical study requires more work and explanation. "	3
My first main concern is that exp(x) can also be approximated by (1+(1/n)x)^{n}, the paper should take this as an baseline. Second is the nonlinear layer is not powerful enough to approximate the inverse.	3
This paper proposes an extension of GAIL that addresses longer-term imitation. I like the insight provided by this paper, but the overall technical novelty is somewhat limited. The empirical results look promising, but there are several places that would require better explanations. As a result, I lean towards rejection in its current form.	2
Although the premise of the paper is intriguing, it is not clear to me the relevance of the proposed approach, since the motivation is not convincing.	2
Novel issue in IRL with interesting solution to tackle it, although the proposition sounds a bit heuristics. A better explanation for it would have made the paper stronger.	3
"The paper is well written and the proposed method makes sense and is original. But it lacks a bit of theoretical motivation and I'm not convinced of its practical relevance, given the very marginal improvements observed in the experiments.
"	2
A method for partitioning the input space for a gradient boosting algorithm and choosing a different number of trees to be included in the ensemble in a partition-dependent way is presented. Improvements on 6 benchmark datasets relative to standard gradient boosting are reported. The method is intuitive and elegant but the number of benchmark datasets is not as large as it could be, and for that reason, the paper gets a 6 (marginal accept).	3
"Paper with a nice, elegant, and very simple idea for adaptively generating different number of ensemble sizes throughout the input space, in an attempt to generating better results.
The weak point is that the experimental analysis is not up to the standards that one would expect here, as I would have expected the method to be tested over dozens of datasets, with a proper analysis against other hyperparameter optimization approaches, and statistical tests to validate the significance of the results."	3
While the idea of using unsuccessful proof attempts with HER seems new and can benefit the learning-based theorem proving community, as it stands, the paper doesn't do a good job of showing the benefits of their overall theorem prover empirically. Combining these ideas with traditional ATPs and evaluating on published benchmarks is recommended.	2
Although the absolute performance of the proposed framework is not at a state-of-the-art level, the usage of HER (though not entirely novel) is justified and the overall algorithm is novel and well-designed. It is unclear how the framework compares to other learning-based systems. The paper is well-written.	2
"This is a nicely presented work with a novel approach to using clauses generated during proof search for training. The approach is somewhat ad hoc however, and could benefit from a principled RL scheme which learns an optimal policy, rather than fixing a heuristic search and combining that in an ad hoc way with a scoring rule.
"	2
"The paper demonstrates an experimental signal in support of using HER for bootstrapping ML-based ATPs without expert labeled data on TPTP dataset. The reliance of ML-based provers on expert data is a serious issue and efforts to alleviate that reliance are definitely encouraged. However just showing the effectiveness of a well-known technique such as HER, which perhaps is one of the most immediate remedies that comes to mind for sparse reward tasks, seems like an incremental contribution, especially since some critical baselines are missing which could’ve made the claims stronger. Note that incremental learning has been used in other shapes or forms before in this context and thus the main value proposition of the manuscript is the addition of HER to that scheme.
"	2
The authors propose a very compelling solution to the data scarcity problem in automated theorem proving: relabelling failed searches by the sub-goals reached. Several important details for getting this to work are laid out, and the system is able to outperform baselines and compete with the existing E prover. The main shortcoming of the paper is a lack of repeat experiments, and missing desirable ablations for high-level design decisions.	3
"Well written and interesting paper with strong theory and results, well conducted experiments. Recommending acceptation.
"	4
"While the ideas presented here are sound and backed by theoretical and empirical results, the lack of context with respect to prior approaches makes it hard to judge the impact and novelty of the work. The experiments, in particular the lifelong learning one, could benefit from more appropriate baselines. Furthermore, I have concerns about whether the assumptions and definitions are too restrictive, reducing the applicability of the approach more generally, especially compared to more recent work (Nemeck and Parr 2021). I am, of course, open to having this score upgraded based on future discussions and changes.

******************************
POST-REBUTTAL
******************************

The paper has been significantly strengthened during the rebuttal period, and as such I am increasing my score. One important point to clarify is the setting - the ""unsupervised RL"" setting is a bit ambiguous and could lead to confusion, since it could mean the reward-free RL (see Jin et al 2020) and is separate from the lifelong setting. Clarifying this in the paper is important. 

The main improvement that can still be made is extending it to the lifelong learning setting, where the agent doesn't get to pick its task/reward function. Such an agent would be fully autonomous and would be able to discover and store a library of policies on the fly. This would be much more in line with existing work, and make comparisons to other work (Abel et al, Nemeck and Parr) apples-to-apples. It would also make Algorithm 1 far more interesting than it currently stands. Nonetheless, I feel there is enough here as it is to vote for acceptance"	3
See above.	2
This paper provides an intuitive solution to move stylegan2-ada from an unconditional to a conditional approach and achieve the best performance on the FID metric, however more experiments, as listed above, should be given to help a better evaluation of all aspects of this work.	3
"The authors discover an interesting behavior of cGANs in limited data settings and propose a simple yet very effective method to solve the problem. The authors perform a comprehensive analysis of the problem followed by a comprehensive set of experiments and ablation analysis showing the efficacy of their method. Overall, the paper seems to make important observations and empirical contributions.
"	2
Overall, this is a good paper that addresses the limited data issue for conditional GANs. However, I have two major concerns: the transition design for the loss, and evaluation metrics. Please see Weakness section in the main review.	4
Overall, the authors work on an interesting topic and present promising results.  In my opinion, the solution seems reasonable, yet there are still many problems that should be further explored. I highly suggest the authors further improve this work. However, I may not champion the current version. 	2
"
This is a paper that clearly explains a straightforward but sensible approach. I judge it to be slightly below the acceptance bar mainly because (1) the method heavily depends on the success of two non-trivial preprocessing steps and has no way to recover from their mistakes, and (2) the complex dynamics among the expression parameters and deformation fields remains unexplored."	2
"My current recommendation is ""3: reject, not good enough"". There is limited novelty with important baseline missing (HyperNeRF). The synthesized videos are also blurry with artifacts. "	2
Although qualitative results of the paper seem convincing, the method itself is very similar to an existing one (NerFACE), while the main claimed novelty - the spatial ray prior - seems to be equivalent to training a separate background model with pre-computed segmentation masks. Thus I am leaning more towards rejecting this paper, but I am eager to reconsider my rating if authors address the concerns raised above.	2
While this paper add a controllability in the facial expression for a novel view synthesis by combining the techniques from previous works, the idea is not novel (which shows many overlaps with many existing NeRF based approaches), and the image synthesis quality is not convincing (low resolution without sufficient high-frequency details, notable jitters, and mismatch between the synthesized facial expression and conditioning ones) when compared to how existing methods have demonstrated. Although the proposed method performs better in several customized scenarios, but those testing examples do not cover diverse scenes that can truly validate the performance. Based on the current demonstration, the ability to control the facial expression is highly limited to a few number of mouth shapes (it's more like mouth control, not facial expression control). Considering the novelty and quality in the context of existing methods, the initial rating is reject.  	2
I appreciate the emerging topic of SSL in FL. The proposed personalized federated SSL is interesting. However, there are quite a few unclear details regarding the method and experimental settings. 	2
In FL, label deficiency and data heterogeneity are two of the main open research questions. Authors propose a natural idea to leverage SSL in FL with the support of testing, analysis, and experiments. However, I am not sure the novelty of this work is good for this conference. I might expect more novel technique ideas in this paper-increasing the FL field.	2
See Summary Of The Paper for detail.	1
I think the work tackles an important question, however the experimentation is inadequate to assess the usefulness of the approach.	3
The approach in the paper is very natural and I think the algorithm would be very easy to deploy in large-scale system. The proof is very simple and elegant, which I think is another big bonus. 	4
There are some issues with the writing and some questions regarding the validity of assumptions and the validity the claim of dimension independence in usual settings, as detailed in the main review. Thus my score of 5.	3
"As I mentioned, due to high time complexity (which is impractical), lack of comparison with SOTA in experiments and limited improvement on the upper bound. I tend to reject the paper. 
--------
After the rebuttal, the reviewer solved mots of my concerns instead of the time complexity. So I raise the score to 5. "	3
"All-in-all, the paper is interesting, but the practicality of the method is very problematic. 

I think I could increase my score if:

- The authors commented far more on the added complexity and parameter count
- Had comparisons to methods that have large num-classes
- Had comparisons on networks that were actually representative of mobile use-cases, or at least something that's more reasonably sized than a 150 layer network for CIFAR10
- Compared their method to multi-scale dense nets, and other mentioned methods"	3
"I think studying early-exit inference is a very promising research direction and I appreciate this work due its simplicity, which is important for achieving real world impact. As far as I am aware using nearest centroid classification (NCC) for early exit classification is novel and the experimental study provided can be useful for future research. However the potential impact of this experimental work is currently limited due to its choice of datasets/architectures and lacks in-depth investigations/ablations. Without these, I am concerned that the impact of the work would be limited. For example, it is not clear whether the method would scale to larger datasets and perform better than other alternatives. 
"	2
Please see the above review.	2
Although the main idea of the paper is intriguing, I think the empirical evaluation needs to be extended and the possible application scenarios should be more thoroughly discussed. In the current version, I cannot recommend accepting this paper for ICLR.	3
This paper is different from classical analysis paper in NLP. The authors claim to make progress on language model pre-training and it is essential to distinguish between the properties of specific model artifacts and those of the training procedures that generated them. I can buy this claim to some degree but still reserve my recommendation for acceptance.	3
The paper provides adequate models and a good estimation method for future researches on robustness.	3
"It's a sound paper, well overdue in the NLP literature, and reminiscent of other sober reflection papers in the vein of [Melis et al. 2017, Narang et al. 2021], though the latter concentrate mainly on testing the benefit architectural improvements.  Going beyond these papers however, the authors build and provide a bootstrap procedure to evaluate specific hypotheses.  

While I'm not sure that ICLR is the best venue for this work, it is clearly important and deserves to be showcased."	3
The paper presents an interesting approach, a novel contribution that might have some applications. From the methodological point of view, it is well-written, there is a good review of related works, a description of the MultiBERTs release, and an application to reduce a type of gender bias in coreference systems built on BERT. There are also comprehensive supplementary materials with proofs of theorems and lemmas. It seems to be a novel approach, it's hard to assess its significance but the presented application suggests that the impact might be important. Therefore, from the methodological point of view, the paper is marginally above the acceptance threshold, but if it is accepted, there are some minor typos that should be fixed before publication.	3
"Overall, while this paper is tackling an important question, its execution could be substantially improved. It was not clear what the contribution of the paper is from a scientific or engineering perspective. The analysis could be more informative about what the modules are actually learning. The central claim of the necessity of feature attention and multiple schema modules seems to be refuted from Figure 3c. The paper needs to either temper its claims or provide empirical evaluation for its claims. More details about these concerns are in the Main Review. It could be that I may have misunderstood something, in which case I would happily engage in discussion with the authors. 

== AFTER REBUTTAL ==
I thank the authors for their response and additional experiments. I have raised my score to a 5, and I especially appreciated the clarifications and the qualitative analysis. After reading the rebuttal and the revised paper, one concern I still have is that it is difficult to glean a single coherent empirical message from the paper: on some tasks FARM performs the baselines (Figure 3, 4), on other tasks FARM performs similarly (e.g. Figure 5b, 7, 8). Perhaps this could be something that could be fixed through the writing, but overall the case for the following questions could be made stronger: (1) Why these three types of generalization rather than some other types of generalization? What is the underlying property that all these three types of generalization have in common, and why is this property important for AI research? (2) What is the explanation for why the proposed method must be the method for tackling tasks with this underlying property?

Regarding (1), it would strengthen the paper's overall message to identify a single unifying characteristic of the problems considered, because at the moment the paper is written in a way that highlights the disparate nature of these generalization tasks, but without an underlying theme connecting these generalization tasks, they seem to be chosen in an ad hoc way. Regarding (2), this could be addressed with stronger empirical results."	2
This paper presents a method for learning modular representations for RL that allow for generalization in three regimes. The technical novelty is highly limited, but the empirical contribution is promising. At the same time, I would like to see a number of the presented results clarified and some additional analysis done to understand why the baselines perform so poorly and which parts of the proposed method contribute the most in different regimes.	2
"The paper develops an interesting suite of diagnostic tasks/environments to study generalization in deepRL agents. While the technical contributions and results are interesting, it is difficult to fully judge the merits of the proposed model and the main contribution of the feature attention mechanism if it is not referenced, discussed or evaluated in the context of related prior works [1, 2, 3, 4] that study the similar problems and propose similar solution methods. Further, the paper would greatly improve with more qualitative analysis into the CPS model’s components and their role in its generalization performance. For these reasons I’m giving a score of 5. I’m willing to increase the score if my concerns are addressed.     
"	2
In summary the proposed method has few strengths and weaknesses, it would be easy to understand the effectiveness of the proposed method, if the paper provides experiments showing the improvements when masking strategy is introduced to other transformer networks	2
The main concern of this paper is the usefulness of the locality mechanism due to the existence of Swin transformers which are more computationally efficient. 	2
"The advantage of the proposed masked attention over the existing literature is not properly discussed. In addition, paper writting is very bad, with substansive confusing descriptions and notations.
"	2
Based on the concerns listed above, I think at this moment this paper does not reach the status for publication in ICLR.	3
Overall, I think this work provides a neat and elegant analysis for AdaGrad and several variants. It shows that AdaGrad behaves similarly in terms of expectation convergence and with high probability convergence. However, I found the results still unsatisfactory in terms of I did not see how those results could explain the good performances achieved by AdaGrad and its variants. To my understanding, this is more like verifying and improving existing analysis to some extent but lack of providing new insights.	2
The paper considers and interesting and difficult problem. However, the results seem to fall short of realizing the ideal goals.	3
I think the contribution is meaningful and the writing is clear. If other reviewers are not doubtful about the novelty, then I'll give an accept.	3
The authors present an interesting hypothesis regarding modeling human exploration behavior. However, I am not convinced that the experimental setup captures general human exploration. The experimental setup were synthetically created to have repeatability and doesn't capture the general layouts (of indoor spaces or otherwise). Thus, I am not convinced that these hypotheses will hold true when tested on realistic environments. 	4
"I find the idea of the paper interesting, and I think the selected suite of experiments highlights the points the paper makes rather well. However, I cannot tell if the insights about map induction would extrapolate to more realistic cases, as the considered environments are very regular & specific in their repeating patterns, applicability might be limited. I also think the clarity of the model specification & the assumed probabilistic assumptions can be improved, I listed my main concerns in the main review.

"	2
The paper appears to be generally interesting and novel. Perhaps because of the page limit, not enough details that would be expected for a human behavioral modeling are provided, and I hope they can be added during revision. 	3
"In summary, the paper is generally well written and tackling and interesting question. The proposed task is interesting, but I am concerned that the paper does not provide a clear argument that the proposed methodology and analysis actually prove the hypothesis. 
- This paper is mainly a Psychology study (as the computational model for AI part of the claims is not really demonstrated), and ought to be reviewed by an expert in experimental psychology. 
- The analysis of the result does not provide clear evidence that the hypothesis is true
- The hypothesis should be framed more clearly with the existing literature, existing hypotheses of human spatial cognition and prior experimental evidence using established tasks. 
"	4
"This work opens a discussion around the problem of estimating the performance shift in commercial ML APIs (for classification). The paper defines a metric for the performance shift of such APIs (via the confusion matrix), and presents a method to achieve near optimal sampling rates.

The theoretical contributions of the paper are small but non-trivial. The experimental analysis is detailed and interesting, but could benefit from further ablation studies on the effect of the client-side difficulty gauge model. The problem is of interest to the ML community and the release of the annotated dataset used in this work would be useful to the community. 
"	3
API shift is an important problem that needs to be addressed in order to operationalising AI. This work proposes a way to assess these shifts using limited number of API calls and has a lot of practical importance. Most deployed ML models are also priced in a manner wherein each API calls has an associated cost and thus performing API shift assessment using limited calls is quite important even from a pricing stand point. 	3
I think that the problem of ML API result shift is real and important. I believe authors made interesting and useful contribution in evaluating such shifts. Although the paper has some weaknesses, I would recommend accepting it. 	3
"The paper makes atypical but important contributions to ML ethics. Although the proposed algorithm is not groundbreaking from a technical perspective, it does contribute significantly towards measuring and tracking changes in black-box APIs, and I think it is of high value to society. I am concerned that the idea of difficulty levels (K) is not fully developed, but I lean towards acceptance.
"	3
The paper describes a reasonable approach to generative modeling and demonstrates promising metric values. However, the approach itself is a combination of two well-known models. The main issue of the paper is that the particular choice of the generative models is not well-motivated.	2
This paper introduced a simple idea to train an energy-based model, which is very challenging in high-dimensions. The numerical result makes quite significant progress over state-of-the-art methods in terms of FID scores on natural images.  	3
The article not only introduces some innovations, but also conducts a large number of experiments to demonstrate the superiority of the algorithm.	3
"Overall, while the paper is interesting, the lack of justification for the authors’ motivation prevents me from recommending a clear acceptance for this paper. My current evaluation of this paper is borderline but can be changed after reading the author’s rebuttal.

***
Post-rebuttal: I appreciate the authors’ thorough responses to my questions. After reading the responses, I have tended to accept this work, thus raised the review score accordingly. However, this rating is only marginally above the borderline, thus I would not champion the paper for acceptance.

After the authors' follow-up response: I feel more comfortable accepting this work. I am sorry to keep my score as 6, mainly due to there is no review score of 7 in the system. Instead, I increased the significance score of this paper accordingly.
"	3
This paper is novel and techinical sounds. The proposed framework is not specially optimized for multi-label problems, but it is still a good paper to see in ICLR. Thus, I am incline that is paper is marginally above the acceptance threshold.	3
Overall, I really enjoyed reading the paper and it was refreshing to see such a practical application and benefit for the end-user when using the approach in terms of both predictive accuracy and reduced costs. Therefore, I recommend to accept the paper.	3
This paper addresses a practical problem to combine the different ML APIs for multi-label classification and provides sufficient theoretical and empirical analyses. Recommended for acceptance.	3
"1.The paper claims that ""FrugalML requires a large amount of training data and involves solving a non-convex optimization problem with complexity exponential in the number of distinct labels"". However, the paper does not provide the time cost of their proposed method and also do not report their time cost in the experiments. Since the paper aims to design an efficient ML API selection strategies for multi-label classification tasks, but the paper did no show any benifit of time of their proposed method from both thoery and experiments. This is a major flaw.
2,The paper just combines existing technique, but did not bring any new technique, or new theory or new insight.
3.The paper presents online algorithm. Then what is the regret bounds, i.e. bounding the regret of the proposed algorithm over the worst-case sequence?
4.There are too many efficient XMLC methods. What are the advantanges and disadvantages of the proposed methods and other existing MLC or XMLC methods?What is the different and connection between them?
5.Can the proposed method converge? Can you provide the theory and experiments guarantee?
6.Can you test your method on XMLC datasets? Can you compare your method with popular XMLC methods such as PPD-Sparse, SLEEC,SiameseXML,ECLARE,LightXML on Amazon-3M and WikiLSHTC-325K	? please refer to http://manikvarma.org/downloads/XC/XMLRepository.html"	1
The paper studies an important problem of improving the model robustness against adversarial attacks. Towards this paper aims to leverage the recent advancements in the area of generative models. The paper shows that utilizing the synthetic images generated from suitably chosen generative models can boost model robustness. The paper both theoretical and empirically studies various metrics to select the correct generative model for the underlying task.	3
The authors provide an in depth evaluation of using generative models to augment adversarial training, showing large improvements and interesting insights both empirical and analytical. I think the paper should be accepted.	3
The paper studies an important problem for adversarial robustness, and I like the theoretical analysis part. My concern is mainly on the experiments. 	3
The paper offers some interesting theoretical and practical results, but without the hole in the proof fixed I do not think it should be accepted. I will gladly raise my review if this is adressed.	3
The paper proposes and analyses a special case of the objective from [Titsias, 2019]. Unfortunately, the empirical study is not sufficient to make positive conclusions about the proposed technique.	3
While I like the motivation of the paper, I think the actual algorithm and the experimental details are somewhat opaque. This can be easily fixed by fixing the manuscript and making these things very clear. I'm also somewhat concerned about the practicality of the algorithm and I think this needs to be addressed by the authors in the manuscript. For these reasons, I lean towards a weak reject though I am more than happy to increase my scores if the authors address my concerns.	3
Overall I think the whole idea of providing a set of guiding principles for the design of objectives to tune MCMC proposals is interesting and relevant. However, I feel the presentation in this paper may be a bit too generic, and the derivation of the one concrete objective requires a decent amount of manual tuning. Thus, it is not extremely clear to me whether this framework, as presented, simplifies the development of new objectives significantly or not. In addition, I think the concrete objective proposed could be tested more thoroughly, specifically in some settings where samples from the target are not available (i.e. tuning its parameters online as most adaptive MCMC methods do).	3
The proposed Ab Initio objective seems to be an interesting addition to the current candidates of training objectives for MCMC proposals. However, given the heuristic formulation and inadequate empirical evidence, I am cautious about its general applicability.	2
Considering the limited novelty of the proposed network and inadequate experiments, I think this paper does not meet the standard of this conference.	2
I am not sure that the paper is innovative enough for acceptance to ICLR. The authors should also compare their approach with other state-of-the-art approaches, and evaluate on difficult real-life cases (pathological rhythms, different skin colors,…)	2
Overall as indicated above technically the algorithmic contributions are rather modest. The main novelty lies in the application described. If the paper is to be accepted it should be accepted due to the interesting application.	1
The work presented is of general interest and contributes with a methodology for a better understanding of learning representations for dynamical systems. The paper addresses several aspects such as the optimization of the loss function of neural network models to better estimate higher order dynamics, as well as the incorporation of the second derivative of the input frames to improve the performance of second order dynamics. Applying this to the beat wave characterization problem, the model is able to better estimate left ventricular ejection time intervals (LVET). The paper will be greatly improved if the revisions suggested in this review are taken into account, especially improving the comparative results with other models and better justifying the use of synthetic data.	3
In summary, the higher-order motion characteristics estimation from the video is an interesting topic while I have some concerns on the novelty and the experiment results.	2
The paper aims to provide some theoretical analysis for contrastive representation learning under weaker assumptions than prior work (like conditional independence) and has some interesting empirical findings about how performance of augmentations can be ranked using a metric that depends just on unlabeled data. While the general idea is nice, there are issues with the theoretical setup (as described in the main review), raising questions about the meaningful-ness of the assumptions and results. Furthermore the comparison to prior very relevant work is also inadequate. This leads me to assign a score of reject for the current version.	3
The authors expand our understanding of contrastive learning on top of the existing alignment and uniformity perspective, by studying the role of data augmentation.  They provide theoretical guarantees on downstream performance, and propose an interesting new metric that can be evaluated using only the given unsupervised data.  Overall I think this is a strong submission, and would recommend an accept.  	3
The paper provides a theoretical analysis on the interplay between alignment and augmentations. Empirical experiments nicely complement the theory, and lead to interesting metrics that reveal the properties of this interplay. Overall the paper is also nicely written. While there is one slightly incorrect claim (which I think is fixable) and some places that would need clarification, I think the findings in this paper are valuable to the field. Thus, I recommend acceptance.	3
The idea of understanding contrastive learning from the perspective of data augmentation for intra-class samples is interesting. However, 1) some key assumption for the analysis is too strong; 2) the analysis on the existing contrastive learning algorithms is preliminary and needs more work; and 3) the authors emphasized the importance of finding the sweet spot of data augmentation (ie, perfect overlapping). But how to achieve that in practice is not discussed.	2
The paper presents a useful dataset to evaluate word-superimposed image recognition models. The analysis on CLIP shows how it behaves for different categories of superimposed words. The writing can be improved, especially around the presentation of the task and some of the results. Further experiments on ImageNet-based CLIP would make the paper stronger.	3
This paper introduced a benchmark task for evaluating picture-word interference in visual-language joint learning models. The benchmark was tested with the latest model CLIP. The evaluation results brought novel insights to understand the recognition process of the CLIP image encoder, showing its distinction on visual representations of word forms and object images. This provides a new perspective for future studies to evaluate the interference of multi-modal processing in AI models. Some results and figures may need further explanation or clarification for better understanding of their findings.	3
"Work like this should is very important to understand how and why the state-of-the-art models perform well, what their limitations are, and how we can improve them.
"	3
"I think that the authors should continue to peruse this direction,
because I think it's potentially quite interesting: understanding
if/how CLIP can reason about ambiguous scenarios where more than one
answer is correct (in this case: transcription vs. object detection)
is cool. But: because the ""correct"" answer (or a task description)
isn't ever given to CLIP (as it was to the children in Rosinski's
work), I can't bring myself to be convinced by any of the experimental
claims.  My suggestion to the authors would either to i) consider
fine-tuning CLIP with using supervised data that specifies which of
the tasks should be undertaken (transcription vs. detection); or ii)
consider designing prompts that specify what should be predicted,
e.g., ""A photo of the word X written in red font over a picture of Y""."	3
"This is an interesting paper, but it is better to include more concrete theoretic/empirical analysis on why/when the suggested scheme works. Moreover, comparison with existing schemes is missing. 
"	2
Overall a nice attack that shows the keeping data distributed while only sharing model updates does not provide strong privacy guarantees.	3
"This paper proposed novel and effective data reconstruction attacks in federated learning. Overall, I recommend accepting this paper for its solid technical contribution and convincing results. 
"	4
The paper provides another piece of evidence that federated learning on its own is not sufficient to protect user privacy, and explains how even with only a minor malicious change the server can exactly recover user data. And the message is very clear and well exposed in the paper. However method is only a slight variation on ideas from previous work. 	2
The paper executes about the first thing you would try. This can be a strength and a weakness. I wish the authors provided more insight in their writing. Not only explaining what holds, but also why and putting in the context of existing work. The paper could be made stronger by investigating any or all of the directions suggested in (1)-(4) above.	2
"The theoretical findings of the paper are sound but it is hard to appreciate the novelties of this work over the existing techniques and analysis of GP-Bandits. I will be happy to increase the score if authors can precisely point the new challenges overcome by this work and what the one novel idea unique to this work.
"	2
I am positive about this submission, I think it is interesting, innovative and potentially very impactful. The results are impressive, though I think there is an opportunity to give a bit more insight as to the conceptual reasons for the more surprising aspects of the results.	4
This paper presents some interesting ideas and generalizes the setting to nonparametric policy class. I hope the authors could clarify their contribution w.r.t the SOTA rate. 	3
The proposed approach though technically simple relative to existing literature in neural and Gaussian process literature it is well motivated, technically sound and with comprehensive experimental results that support the improved predictive performance claimed by the authors.	2
I feel that the contribution is fairly incremental and I'm disappointed that they did not consider full normalizing flow models. However, I found the exposition engaging and the experiments thorough.	3
"I like that fact that the authors include multiple data sets that can show the effectiveness of modeling dependencies in the input (or the output). However, I think the novelty of the proposed methods is not that impressive. 

Minor comments

In the end of page 1: “CNPs model their respective outputs y_m and y_m’ independently, that is y_m \prep y_m’”. It would be better if the mathematical definition of the symbol “\prep” was included in the manuscript. 

The current version of the manuscript is not printable (on Windows 10). I tried to print the manuscript out from Adobe reader and Chrome several times but failed to do it. 
"	2
The paper tackles an interesting problem for RL, the proposed method is incremental but partially novel. On the other hand, the evaluation is limited and the results are not significant.	2
I find the paper clearly written and a direct application of epistemic uncertainty driven exploration in RL from preferences. Although simple, its novelty is marginal. Also, it lacks theoretical justification (e.g. bayesian RL intuitions, why uncertainty over the rewards might work better than uncertainty over dynamics, aleatoric vs epistemic uncertainty, etc.). Finally, the experiments, although they support the argument, there are experiments that don't contribute to the discussion (C2)	2
Exploration for preference-based RL is an interesting problem, but the current paper has multiple issues that should be addressed. I hope the authors can provide some clarification my questions above. 	3
The proposed method is of interest within the domain of preference-based reinforcement learning, as guiding exploration can reduce human operator workload. The novelty of the approach is limited and the claims have been shown before, but in a different framework. This limitation has to be overcome or leveled by a thorough evaluation, allowing generalized conclusions. 	2
"This paper discussed an approach to take priors into account in machine learning with a slightly different formula, which I have doubts on its correctness. In the experiments, they showed that performance in some tasks can be significantly improved by cleverly choosing class priors. The experiment results look good, but I do not think it fully justifies their algorithm because of the lack of baselines and inappropriate interpretations of an equation.
"	2
The proposed implicit generative model seems interesting and achieves good performance for a range of weakly-supervised tasks. However, I am slightly leaning towards the negative side due to the lack of clarity in the model design and missing comparisons in the experimental evaluation.   	3
I like the paper and its aim, particularly on the experimental front. There is room for improvement on the theoretical analysis and related work.	3
The idea is interesting and easy to follow. It has a very positive impact to the AI community. However, the paper writing and organization, experiment design and presentation still have room to improve. Therefore, I initially tend to marginally accept this paper.	3
I would vote for acceptance of this paper. I believe the idea is novel and practical, also theoretically non-trivial. The content in this paper is substantial. Still there could be some improvable weaknesses, see above.	3
"The overall assumption in the proposed bandit model is that the reward of an arm relies on the number of times it was played in the past. I recommend rejecting this paper in a large part because:
1. This setting cannot address the traffic routing problem which motivated the study.
2. Compared with existing bandit models which take history into consideration, the major difference in this paper is that it considers a short-term reset, which is not a significant modification.
3. The technical contribution is not substantial, as the overall structure of the algorithm and the corresponding regret analysis follows directly from other papers with limited new insights."	2
This paper proposed an interesting question, but the algorithms and techniques used in this paper lack enough novelty for being accepted by the conference. Thus, I tend to vote a reject. 	2
"This paper proposes a new theoretical bandit model called congested bandits. While there is some novelty in problem formulation, algorithm, and regret analysis, overall, the paper does not adequately address the problem of congestion in real-world routing problems. 

In online routing problems, congestion will depend on many other factors such as the exact time of recommendation, speed of the vehicle, etc. One way to relax the current assumption is to assume that the current reward depends on a weighted average of the number of times an arm is played in the past (with decaying weights). This paper proposes a niche theoretical bandit model; however, its practical significance is not clear. 

It is not clear how the non-increasing nature of f_cong affects the regret analysis. Does this property have any effect on regret bounds? Is it used as a part of regret analysis to prove sharp bounds? What happens when f_cong is not non-increasing? 

The form of the optimal policy when mean rewards are known deserves more discussion. Is the optimal policy an index policy? Is it a threshold-type policy? 

Please compare with other methods, like UCLR2 in experiments.
"	2
The paper is well written and the method proposed is marginally novel. An extra support section for optimal configuration and convergence analysis would be a great plus to justify the advantage of the method. 	3
Overall, this paper provides a new way to define the oversmoothing problem in GNN. But it lacks the novelty and solid experiments. 	2
Overall, based on the weakness mentioned above, I think this paper has severe problems with math derivations and the contribution of this paper is also unclear.	1
Based on the weaknesses of the paper pointed out above, I tend to reject this paper.	2
I would suggest a rejection of this paper given the current draft. 	2
Overall, I think it's not a presented in a straightforward way and it's lack of many details for readers to fully digest it. I feel the topic is interesting but in practice I don't know why it's important and author didn't motivate this. I will be considering to raise the score if the introduction is re-written in a motivating form.	1
"The paper suffers from a lack of problem that is demonstrably made headway on, i.e., where a proposed method improves over sensible baselines on sensible metrics, largely because data, baseline, and metrics are chosen and justified poorly in my opinion.

That is why I currently cannot see what anyone could get out of this paper as it is (other than remembering that arithmetic coding and k-d trees exist and possibly being tempted to think about volumes in latent spaces and the relation between overfitting and representational capacity a bit more, no thanks to the paper which does not ever talk about these issues itself).

Finally, I should disclose: I have reviewed this paper before (but do not know its authors or anything beyond that) and some more fundamental concerns are unchanged from my last review, though the paper has improved noticeably in my mind (thank you for these improvements, it really has gotten better). My review is thus focused on what I believe is possible to still improve about this paper rather than trying to reiterate previous concerns---that is why I have little to say about the evaluations performed themselves."	3
I have reviewed this paper last year, I can see the authors have reorganized the content significantly. I will give a weak acceptance this time and willing to discuss among reviewers.  ​	3
"This paper is novel mostly in that it utilizes a Gaussian process to generate an extra coarse prediction of the query image. However, an ablation study is lacking to prove the effectiveness of the dense GP module alone. 
"	2
many aspects are unclear	2
The idea of adopting dense Gaussian Process (GP) regression to learn the mapping from local deep image features to mask values, as well as considering uncertainty for the final segmentation is interesting. Although the proposed method seems like a dense setting of the existing methods, the paper is well written and the results achieved are very competitive. I'd like to enhance my rating if the authors address all my concerns.	3
The paper overall presents a novel idea with solid empirical validations. Except some minor concern I raised in previous parts. In total, I am inclined to accept this paper.  I am happy to raise the score if my concern is fully resolved.	3
The approach is promising, but is limited in terms of novelty, experiments and the comparison with existing approaches that makes it difficult to assess the true potential of this approach.	2
This study is definitely an interesting take on incorporating biologically-plausible components in an ANN design. However, the proposed model contains several limitations in terms of its expansibility to more challenging problems and some controls are missing to ensure the validity of the main claims, particularly those related to the robustness of the model. For these reasons, I cannot suggest accepting the paper in the current, but am willing to update the score if the authors address my concerns.  	2
"The paper has both valid strengths and weaknesses. The paper could be a clear 'accept' for me if the authors would at least present results with more standard MNIST architectures, like conv+pool layers and/or 3-4+ layers. Additionally, it would be nice to see results of the method at least on CIFAR-10.
"	3
The paper addresses an interesting topic. But it is not clear what its innovations are, and whether it can be implemented with a local learning rule.	2
"Some detailed comments:

* page 5 [Approximation for GIP kernel]: I get confused here because the authors first assume the feature vectors are normalized (which is OK but makes the GIP kernel defined in Equation (4) less interesting, which, in essence, only depends on the ""angle"" $\psi(x,x')$) and then further takes the nonlinear function $\zeta$ to a binary function: is there any evidence or previous results saying what type of kernel can one approximation with this particular choice of nonlinearity? At least, this is a much less general family of kernels than the GIP kernel defined in Equation (4).
* the theoretical results in Lemma 4.1, Theorem 4.2, and 4.3 are interesting: while it is stated in Lemma 4.1 that one needs at least $P = O(N)$ random features to well approximate the (whole) kernel matrix up to some $O(1)$ error, it turns out, in Theorem 4.2 that one only needs much less ($P = O(\log(N))$) to get similar training error in the distributed setting. Also, while the results for GIP and RF kernel in Lemma 4.1and Theorem 4.2 are somewhat similar (up to constants), the results in Theorem 4.3 for these two kernels are very different and one needs only $P = O(\sqrt{N})$ for random features but $P = O(n)$ for GIP kernel. Could the authors comment, or perhaps also provide some intuition on this (which may be of independent interest)?"	2
While the paper proposes some interesting methodologies for decentralized optimization over RKHS, it is difficult to assess whether in theory or practice they actually advance the state of the art. This is because they have not done a rigorous comparison of their convergence theory or contrastive numerical experimentation with competing approaches. These aspects need to be thoroughly addressed before I would consider this work to meet the bar for publication.	2
Overall, I think this paper is technically sound, has solid theoretical contributions and the paper is clearly written, but it doesn't position itself well in the related area and the experiment is weak. I vote for weak acceptance.	3
Overall, I think the paper presents a good contribution but I do have some minor concerns that I would like the authors to address as I have mentioned above.	3
The authors propose an interesting idea for constructing OOD classifiers starting from counterfactual invariance for symmetric transformation, an idea that is well-formulated in this paper. Despite the lack of real-world validation, I think the paper has a strong theoretical basis and is a decent contribution to the literature.	3
Interesting ideas with relatively toy empirical results.	3
"This is a very strong submission that contributes new theoretical insights to a hot topic in optimization for machine learning.
The lower-bound showing that random permutations are optimal in the general setting answers an open question about permutation-based variants of SGD, while the analysis of FlipFlop opens to the door to clever permutation schedules which improve convergence in specific settings.
I think, with additional empirical justification, FlipFlop could become another standard trick in stochastic optimization. 
Additionally, the analysis in the appendix develops many small, but novel results which may also have use for other problems in optimization; see ""Theory"" for an example.
In addition to the novelty of the theoretical results, the paper is polished and the writing is well executed. 

Given the above, I strongly advocate for this submission to be accepted. "	4
"The research is well motivated, and a simple intuitive idea (flipflopping) improves convergence for certain (admittedly limited) settings.
This reviewer thinks the paper could be a contribution to this conference."	3
Although the paper considers the optimality of permutation-based SGD only for strongly-convex functions, the theoretical results are interesting enough and the proposed trick FlipFlop seems to be a very promising approach to further accelerate random reshuffling. I am leaning towards accepting the paper, although additional experiments could further strengthen the paper.	4
In general, I recommend accepting the paper. Although the settings studied in the paper is limited, it provides a better understanding of permutation-based SGD and can motivate further studies.	3
The paper is generally well-written. Properties of the proposed class of divergences are well-investigated. A sufficient number of experiments are provided to demonstrate the usefulness of the proposed method.	3
Overall I think this is a very interesting paper. The new family of discrepancies is likely to find broad applications in machine learning and data science and can potentially inspire the development of other decision dependent discrepancy measures. 	4
"This paper makes a fundamental contribution to the family of divergences used for machine learning tasks and provides convincing evidence on the usefulness of H-divergence. However, there is still room for improvement related to the computation of H-divergence. Please describe the feasibility of computation of H-divergence for the experimental examples studied in the paper in your response.
"	3
Overall, it is a good paper with a complete analysis of the design of architecture, training/testing approach and extensive experiments. 	3
"The paper has a clarity issues and many notations are not introduced or defined.

--Edit-- The authors either explained or addressed the issues in the paper with other reviewers. I am happy to recommend the paper for publication."	2
Reasonable idea, which proposes a loss that improves stability; however, it has two disconnected contributions and is not so easy to read. Some of the important points are not discussed/analyzed experimentally.	3
"In my opinion, the paper brings significant advances to the field of neural PDE solvers. The proposed push forward trick, together with the temporal bundling trick, are valuable contributions as they impose favorably condition on autoregressive solvers (zero stability), improve the accuracy of other existing approaches as [Li et al 2020 a] and the inference time of the solver.
The approach is well evaluated experimentally,  showing accuracy and generalization results that are above the state of the art of neural solvers and beat classical solver schemes in inference time.
For these reasons I consider the manuscript to be accepted at ICLR 2022."	4
The novelty of the work is limited, there is no interesting distillation design that tailors to the unique characteristics of the dependency complication introduced by the graph. Besides, the literature discussion is not thorough, missing out on quite some important related works (KD on graph-structured data). In the experimental study, this simple distillation design didn't show competitive performance when the teacher model is more advanced GNN architecture. Overall, I recommend rejecting this paper.	1
I would like to recommend to accept this paper for its practical intuition, satisfying performance and comprehensive discussion on limitations.	3
Practical limitations of GNNs in industry setup are quite severe for existing GNN models and this approach offers an easy way to overcome them and leverage graph topology at the same time. 	4
"The paper proposes a novel approach for effectively training MLPs for node-wise classification tasks on graph structured data. The proposed approach allows to train architectures able to approach in most scenarios the performance of a reference GNN (SAGE) while requiring a fraction of the computational cost as no convolution is realized on the given graph. The experimental evaluation presented in the paper is generally sound and well highlights the benefits of the idea proposed in the manuscript.
"	3
"While this paper is working on a very attractive problem and provides a working solution, some technical decisions require further elaboration to make the paper technically stronger. 
1. The choice of the $b$ function and adjustment in eq.3 needs to be further discussed. Otherwise, Theorem 1 seems to be solving a self-defined problem.
2. While the methods used to ensure monotonicity and stability seems to work well in the current experiments, it is unclear whether it will require such complicated steps to solve the issue. I wonder if a regularisation term plus isotonic regression on the $Z^t_{k}$ would also solve the problem while always ensuring monotonicity. 
3. Improve model under distribution-shift is a well-established domain, I wonder if other reviewers with a stronger background in out-of-distribution prediction could enlighten if there are any good baseline to compare."	3
Lowering the score is mainly motivated by the issues that are discussed in the main review. Unless resolved, the general recommendation for the current work is that it is below the acceptance threshold.	2
This paper considers an new but important branch of truthworthy regression/forecasting, online learning with distribution drifts. The draft is clearly written but some concerns lie in the novelty and experimental setting.	3
I think this paper considers an interesting and important problem; the proposed approach is novel, but it not well justified in theoretically and empirically as mentioned in the main review. I'm willing to adjust my understanding and score as well.	3
There is some interesting idea of regularizing OOD action's value, but the significance of both theoretical and empirical contributions is vague.	2
Please see the main review. 	2
The idea of this paper is reasonable but to implement this idea, the problem of density estimation in high dimensional state-action space should be addressed first, which is unfortunately not convincing in this paper.	2
Overall, I believe that the contributions of the paper are only marginally significant. The arguments criticizing CQL are made but do not seem to be properly addressed by the proposed method. The results have shown that the performance has increased, but the introduced hyperparameter $\nu$ does not have a proper way of optimizing it, which is different from $\alpha$ of CQL that can be tuned to prevent the divergence of Q values. We usually expect to have a performance improvement from having another freedom on hyperparameter, and I cannot credit the performance improvement of this paper much.	2
I believe this is a solid work but with a limited scope of contribution. I'm not that familiar with some of the related work and used datasets, but I think additional comparisons are needed to assess the attractiveness of the proposed approach correctly. That is why in my opinion, this work is now marginally below the threshold.	2
"1, The paper claims that ""Cost-sensitive loss is hard to optimize since it is non-smooth and non-convex"". But how did the paper address this issue? what is the time cost of the proposed methods? The experiments also do not report the time cost the proposed method. Since the paper focuses on the time complexity issues, but the proposed method did not show any benefit of time both from theory and experiments. This is a main major issue.
2, The distributionally robust learning approach is uesed as the optimizer. But the time cost of DRO is also very high. How can you use DRO for fast optimization?what is the time cost？can you report the time in experiments？can the proposed method coverge？can you provide both theory and experiments support for convergency？
3, The data sets used in the paper is very small and trivial. Given these data sets, many existing cost-sensitive hierarchical classification methods can be efficiently optimized. Can you test your method on imagenet dataset and larg-scale NLP data sets？
4.There many methods aim to devide the cost-sensitive learning problem into some sub-problems. What is the advantage and disadvantage of the proposed method over existing methods? "	1
The paper needs to further motivate the significance, evaluate on truly large-scale settings, discuss related works and compare with more recent methods.	2
"In my opinion, incorporating ""learning to abstention""  to CSHC is somewhat interesting and new, in particular,  the authors provide the bijective correspondence between the hierarchical cost-sensitive loss and the set of abstaining losses to make the layer-by-layer efficient optimization possible, while the proposed LAM achieves better performance comparing DARTS despite of insufficiency."	2
Some parts of the paper are not so clear and require more clarification. Their approach doesn't seem to be novel enough. The experimental results and comparisons to other existing methods are inadequate.	2
"This work presents a new application of Koopman analysis for understanding neural sequence models.

However, it does not adequately present drawbacks of the current method (assuming globally linear dynamics), nor does it test the approach in settings where nonlinear computations are required."	2
"To summarize, the proposed method is interesting and could lead to several extensions. The English level is OK, and the writing is mostly clear. However, I believe that some of the descriptions (mainly in the results section) should be strengthened before the paper can be published. Moreover, I would expect a comparison to more baselines in the main text, like PCA or Kernel PCA. For these reasons, my recommendation is: marginally above the acceptance threshold.
I would be happy to raise my score if the authors could address the points I raised.
"	3
"Overall, I lean on the rejection side: the paper is not that novel techniques-wise in my opinion, and the insights on the applications need to be more developed and thorough.


UPDATE:

I read the other reviews and responses and my opinion is unchanged: The final response to my questions was not convincing overall in terms of the insights provided."	1
"I prefer to weak reject (marginally below the acceptance threshold) this paper due to:
- the work is incremental.
- the empirical evaluation is weak."	2
Overall, I do not think this paper addresses the uniform prior limitation of Lap-Rep. It is an ok paper if it positions itself in proposing a new representation learning framework. If so, experiments on more challenging environments are necessary. Therefore, I lean towards rejection.	3
"The contribution of the paper does not seem to be strongly connected to Laplacian RL but seems to propose an hierarchical temporal abstraction method for improving exploration. The result that the approximation scheme of Wu et. al does not work well if the sampling is biased around the starting state and thus, non-uniform is not too surprising.

Thus, the paper should discuss and compare its contribution to other exploration schemes based on intrinsic rewards. 
Furthermore, the authors should address the limitations of their approach w.r.t. the properties of the environment. "	3
Overall, I think this is a well-motivated work but the novelty and contributes are limited.	2
"The proposed approach is scientifically sound, the presentation is of good quality. 
But the experimental results may lack some SOTA comparison."	3
Overall, I think this work is well motivated. But I have some concerns for both technical and empirical novelties.	2
The jointly model data and model uncertainty seem some novel. However, the contribution is incremental when compared to SNGP. The experimental analysis is not sufficient and strong. Overall, I tend to reject this paper. 	2
By offering a novel perspective on transformers and models of biological memory systems, this work may lead to fruitful future work both in machine learning and neuroscience.	3
In general, I like the scope of the paper, and I am inclined to vote accept provided that the authors can address the comments above.  	3
"In my current evaluation (prior to discussion with the authors) it seems to me that the claim that Transformers have a special kind of relation to hippocampal grid cells is not supported by the evidence. The evidence does show that the condition is ""sufficient"" (Transformers do form grid-like representations), but it does not show that it is also ""necessary"" (meaning it does not have to be a Transformer to form a grid-like representation). If my understanding of the evidence is correct, then it does not support the claim of the paper and I would recommend rejecting this result.In my current evaluation (prior to discussion with the authors) it seems to me that the claim that Transformers have a special kind of relation to hippocampal grid cells is not supported by the evidence. The evidence does show that the condition is ""sufficient"" (Transformers do form grid-like representations), but it does not show that it is also ""necessary"" (meaning it does not have to be a Transformer to form a grid-like representation). If my understanding of the evidence is correct, then it does not support the claim of the paper and I would recommend rejecting this result."	3
The paper proves a parallel between the Transformers and the Tolman-Eichenbaum Machines, thus offering a computationally efficient and scalable implementation for this biologically-grounded model of the hippocampus. Even though no new biological insights were obtained in the scope of this paper using that knowledge, the parallel between the two frameworks deems useful for models of the hippocampus in follow-up works and highly relevant to the community. I, therefore, recommend accepting the paper. 	3
In general, this paper considers an important problem. However, some points must be clarified before the possible acceptance.	3
This draft proposes an online covariate shift detection algorithm based on two-sample tests. The theoretical guarantee is based on the size of sampled data, and thus, it is usually hard to make the error (false positive/negative rate) small in real-world online applications. Empirical studies show good performance on the modified benchmark datasets with a relatively large number of sampled data.	2
"My main concern is the lack of control on the False Positive Rate (FPR) observed empirically. This is an important issue since it claims to have exact FPR control. This deserves further investigation since the given explanation based on multiple hypothesis testing is not convincing. The proof of the theoretical result that guarantees the FPR contains some obscure parts. 
The paper also  lacks comparisons to existing sequential two-sample tests, which, even if  they do not provide guarantees on the False Negative Rate in non-iid scenarios,  are natural competitors since they do provide FPR guarantees (under the hypothesis of equal distributions).

Therefore, I recommend rejection in this current form.

**================== After rebuttal ==================**

The new comparison with Wald's test seems a bit limited by the alternative hypothesis with a fixed epsilon=0.2. Also, the power depends on the choice of the classifier. Cited nonparametric sequential two-sample tests are more flexible and should be considered instead.
My concerns on FPR control have been addressed.
 
For these reasons, I increase my score to 5.
"	3
"Though this paper is interesting and theoretically solid, the experimental parts do not convince me. Currently my rating is marginally below the acceptance threshold. I will raise my score if the concerns about the experiments are addressed.

### After rebuttal
I raise the score from 5 to 6 given that the experimental parts are improved and most concerns are solved. But I will not act as a champion for acceptance.
"	3
See Disclaimer above	3
"Please note that my initial review of the paper is based on what I understood so far. I look forward to the author's response and interacting with them to understand their approach better. I encourage them to initiate a conversation sooner rather than later. I would be very happy to change my scores as I better understand the contribution/significance of the work.

In the current stage, the paper is missing several important details, making it hard to understand if the paper is novel/significant or not. Several important references are missing and it is not clear how does the method compare against standard modular networks."	2
"Unfortunately, I recommend the rejection of this work. The submission lies at the intersection of theoretical and empirical research, yet falls short in both these axes in terms of results. In particular, the theoretical results seem sufficient for an approach that is empirically applicable to realistic problems, yet is evaluated on a set of simple toy problems only. On the other hand, the empirical results on toy data seem sufficient for an approach for which there are major theoretical results, but the theorems in the submission are, in my understanding, fairly minor. The manuscript's structure is quite odd, which makes it hard to follow and contextualize the contributions, so substantial editing might change my overall perception of the work. 
"	2
I think there are some major positive aspects to this paper including that the authors provide theoretical analysis of their modular approach, which is rare in the literature. Additionally, the authors discuss extensions of the approach to more ambitious settings. On the other hand, there is lack of clarity in the writing in some places and the paper lacks positioning with respect to existing modular approaches in the literature. The empirical evaluation of the approach is very limited in comparison to prior modular architectures, which makes me question if there are some undisclosed scalability concerns. As such, I lean towards rejection at this time. 	3
"The contribution is a straightforward application of Gordon's result weighing mainly on the empirical side. I believe nonetheless that the paper gives new insights for training in random subspaces. Overall, I like the paper and would be happy to raise my score based on the authors' response.
"	3
Overall, I would recommend accepting the paper. It contains interesting theoretical results explaining the existence of threshold training dimensions.	4
In summary, the authors provide a theoretical explanation for many recent results that support training on a lower-dimensional subspace of the NN weight space and the back the theoretical claim via experimental results that show the existence of such phase transitions. Although, a lot remains to be said, this paper raises more interesting questions and I think this will be of value to the deep learning community.	4
The article is clear and well written. It studies an interesting phenomenon with mix of empirical and theoretical results. The theoretical results are clear and interesting though they are direct consequence of Gordon's escape theorem.	3
I like the paper due to its simplicity, effectiveness, and interesting take-aways from theory. There is some room for improvement in the experiments.	3
"While I appreciate the author's clarity of presentingtheir ideas/theories/proofs/experiments and the paper's finding is quite interesting, it is perhaps not so surprising especially given the recent works and findings on mitigating the effect of weighted CE failure for imbalanced data. Also, the implications of the finding are not made clear. How does the proposed loss compare to state-of-the-art loss adjustments? Does it apply to multiclass settings? 
"	2
"This paper provides interesting theoretical results for using polynomially-tailed loss to resolve the incompatibility between importance sampling and training overparameterized neural nets. However, the theoretical setting is quite restrictive. I think this paper is on the borderline, but it is ok to accept this paper.

------------------
Post-rebuttal update:

I am glad to see that the authors have conducted more experiments to compare their method with baselines. I'm not an expert in the empirical methods for dealing with distribution shift, but I think the current empirical results they have are great additions to their theoretical analysis. I have increased my score for the empirical novelty and significance accordingly.
As for the theoretical analysis, which is the main selling point of this paper, I still believe the current setting is restrictive, although I know some previous works are studying it. Therefore, I would like to continue to vote for weak acceptance."	3
Overall, I think the paper provides answers to an important question. Both theoretical and empirical results are provided to corroborate the claim.	3
I think this work is not ready for publication, thus I lean towards rejection.	2
I think this paper introduces a straightforward technique for global explainers that has clear execution time advantages over existing explainers. 	3
Experiments are not sound nor convincing.	2
While I do find the paper interesting and find some of the contributions meaningful, I have some issues with the paper in its current form. First and foremost, I find the lack of any discussion about causal explanation and especially counterfactuals to be highly problematic. Second, the qualitative analysis of the output of MAGNEx compared with LIME is unconvincing. Third, the empirical results are not as strong as the authors claim, apart from the important and substantial gain in time efficiency.	3
I think there are some really interesting results in this work, however, there seems to be a conflict in the motivation (constrained-RL problems) and methodology (reward-free RL solutions). On a high level, the exploration phase in the reward-free RL seems orthogonal to the safe exploration problem in Constrained-RL. If exploration is not an issue, then the motivation behind the tabular and linear VMDPs is not clear (as one can use other CMDP solvers).  I wonder applying the methods from the reward-free literature to the constrained-RL setting gives us any meaningful results. I think in the current form, it is not clear what the benefits of such an approach are. Hence, I’m voting for a weak reject. I’m happy to revise the score if the authors can further motivate the benefits of applying RL-free methods to the constrained-RL setting.	2
I have concerns about novelty and contributions, and the fact the bounds may be improved.	2
The paper presents a simple approach to an interesting (and still open) problem, but in my opinion the mentioned flaws of the approach must be addressed in much detail before the paper is published.	3
The analysis looks interesting, but the training and eval data used for this analysis looks too simplistic. A direct disentangled representation could also be possible with an easy way to make smaller embeddings corresponding to each fj.	3
"Although the paper is well written, it's main findings seem likely to be of interest to only a small segment of the community.  Unless the proposed technique is more broadly applicable to more complicated domains, it seems like this work might be a better fit for a more specialized speech venue.
"	2
This paper presents a nice extension from the literature. However, the novelty and applicability is limited and the generality of the proposed method for controlling other attributes is unclear. Hence, this could be of limited interest to the audience of the conference	2
"This paper shows interesting findings on speech VAE.
The proposed method is straightforward and I think it can draw some attention from the speech signal processing field.
I recommend [6: marginally above the acceptance threshold].

Questions:
1. Why was IS-VAE chosen to model speech?
2. On Page 3, Section 3, second paragraph, ""Di denote a dataset of artificially- generated speech vectors (more precisely short-term power spectra) synthesized by varying only fi, all other factors {fj , j ̸= i} being arbitrarily fixed"", does this mean that when making D_0, f1, f2, and f3 is fixed? If that's true what values for f1, f2, and f3 are chosen?
3. Can we still find subspaces when we do not have the artificial generator? (e.g., by manually selecting speech samples we would like to take control of?)

References:
Some papers the authors can consider to add in the reference:
[1] shows that one can disentangle source and filter in a supervised manner.
Recently [2] showed that the source and filter parts can be disentangled even in a self-supervised setting. 

[1] https://arxiv.org/abs/1908.01919
[2] https://arxiv.org/abs/2110.14513
"	3
"The paper is well written, the approach makes sense, but it does not feel like they are actually accomplishing their goal of not explicitly encoding the constraints (and learning purely from the solved instances). Given how much they are actually encoding the constraints into the graph, it feels like they should at least compare it with other solutions which would require an equal amount of constraint encoding (such as using a SAT solver).
Further, the results of the algorithm are not particularly impressive, if we consider ""board accuracy"" (pointwise accuracy feels like the wrong metric for combinatorial problems). 
On the pro side, this paper does address a problem that I've not seen tackled before and that seems to be important. "	3
"The paper targets a new interesting problem and suggests two elegant ways to solve it. There are, however, several aspects in which the paper can improve, including explaining the underlying assumptions and the design choices better. 
  
**More comments**
- Intro: define CSP CNF  constraint graph
- figure 1 - add more details about the problems, constraints, and representation. Currently, it is difficult to understand.  
- Lifted CSP formulation is difficult to understand. What do you mean by reference? After reading it several times I think I understand what you mean but I think this part can benefit from a revision. Perhaps add an illustration? 
- Description of networks and message passing: too dense. Difficult to read. Visualization must be added.


**Post rebuttal:**
I would like to thank the authors for addressing my concerns. The paper has definitely improved, but I would like to maintain my score of 6 for now. 
"	3
This paper proposes two methods for the output-invariance issue based on the RRN framework. An elegant training method for one of the methods (the multi-valued method) is proposed. The authors tested their methods on three problems and achieve very good performance. Though the three test problems are similar, the problem itself is very novel and essential. So I would recommend for accpet.	3
"Based on the strength and weakness I mentioned above, I strongly recommend this paper to be accepted because its novelty on closing the gap between 3D GAN and 2D GAN problem and this is the first 3D generative view synthesis work that can render high resolution images with interactive speed while preserving very high rendering quality. Its proposed component can be very useful to push the boundaries of this area in the future.
"	3
This is a borderline paper.  It achieves very interesting results. However, it has quite a few issues in terms of novelty and evaluation, and many algorithm designs and technical details are not well explained.	2
Overall, I appreciate very much the engineering effort behind this work. Although the lack of significant conceptual novelty, it has truly brought up the 3D-aware generative model onto a new level with great quality. Please check the concerns expressed above for not giving a higher rating.	2
 I think the contributions of this paper far outweigh its shortcomings and this paper can be accepted.	3
The paper proposes an intuitive idea of leveraging on user comments to improve video-text retrieval, but the technical novelty is rather incremental.	2
Please refer to the discussion in the Main Review.	3
Overall, the paper proposes an interesting idea, which I believe to have potentially broader impact than what the current manuscript claims. I encourage the authors to conduct additional experiments to apply the idea to other modes, as well as comparing with other methods on more datasets in a fairer way.	3
The proposed method works well in case user comments are available, but even so, compared to a conventional method; CLIP4Clip, it performs similarly, so the technical merit is limited.	2
"In summary, the proposed method is intuitive and simple, as well as computationally attractive. 
However, the novelty is limited and the paper is primarily empirical as it does not provide novel theoretical insights.
As such, the presented empirical evaluation is insufficient to convey the benefit of the proposed method, in particular:
(i) apart from MNIST, from real-world datasets results are present solely on CIFAR-10, moreover (ii) these results are incomplete in terms of analysis with varying hyper-parameters (attack radius and the number of attack steps), and (iii)  does not show a clear advantage over existing methods, see above for further comments. 
"	2
The authors propose an algorithm for adversarial training of MIMO models. They claim better robustness against a wide variety of threat models, and demonstrate this with experiments on CIFAR-10 and MNIST. The claims are mostly well supported, however the improvements seem marginal.The technical contribution is therefore somewhat limited. The authors also need to clearly specify the threat model that they assume for their experiments. However, their results for the cross-gradient  across the sub-models is an interesting addition and does show that their method holds promise. Reviewing all of this, I currently this paper needs some work, and therefore recommend a marginal rejection.	3
"My main concern is the novelty of this paper;
Besides, this paper is not well written. 



##### Post rebuttal #### 

Many thanks for authors' feedback. 
I have read other reviewers' comments and corresponding feedback. 
I agree with other reviewers' evaluations such as “limited novelty“, ""weak experimental evidence"", etc. 
Thus, I keep my score unchanged. "	2
The proposed method is very interesting and the motivation behind is intuitive. However, I think the experiments are not enough to prove its effectiveness, and some settings are even problematic.	3
See above. 	3
Overall, I feel that the novel components of this paper compared to existing work in offline-online RL is not clear. Hence, I am recommending that the paper be rejected. However, if the authors provide a clear discussion and empirical comparison to prior work (AWAC, for example), I would be willing to raise my score. 	1
Overall the paper has limited novelty and new insights. 	1
Overall, though the setting this paper studied is interesting. The experiments do not provide any new insight into this setting and I would recommend a reject.	1
"This empirical paper has some clarity issues, but is exceptionally strong otherwise.
"	3
Overall a relevant investigation into the effect of representations on generalisation in RL. While some insights are trivial and the only investigated factor of variation for generalisation is color, there are aspects in the paper worth distributing and discussing (in particular the evaluation of proxy metrics for downstream performance).	2
While the empirical evaluation is extensive and rigorous, the two key claims made in the paper are debatable. I, therefore, recommend reject. I'd be happy to change my rating if the author can better support the two claims during rebuttal. 	1
I believe the paper is analyzing an important issue in reinforcement learning. However, more experiments on TriFinger with different visual backgrounds and other visually complex environments such as Procgen, and experiments with different pretraining techniques are needed to make the analysis more thorough and convincing.	2
I think this paper has comprehensive theoretical analysis and experimental results, so I recommend to accept it.	3
"The paper provides a useful method and conducts a worthwhile empirical study. I explicitly encourage the authors to keep working on this paper. In the current state, however, the paper lacks support for several of the central claims made in the paper. Most importantly: Is the decoder-centric hole-criterion really useful? Are the results in Table 1 not explained by the power of the decoder language model?
If these reservations are addressed, I am willing to significantly raise my score."	3
This paper discuss an interesting problem, the holes, in the representation space of text VAEs. The is indeed a well-observed problem yet not thoroughly discussed. I think the authors are generally in a good direction towards good solutions. My major concerns are the clarity of the algorithm, the scalability and sensitivity, and the experiment settings. 	2
The points highlighted under strengths and weaknesses form the basis of my rating. In particular, the weaknesses influence my current rating of the paper the most. While the paper is generally well-written, I think the issues highlighted under weaknesses in terms of motivation, clarity issues and lack of strong experimental evidence makes me less convinced of the utility of ModInv. I would encourage the authors to address the first three points under weaknesses. Addressing those will greatly help in reconsidering my rating of the paper. The minor points highlighted under 4 are addressable, I think.	3
"I think a lot of claims are not justified either by experiments or citations. Especially the main claim that the authors' algorithm learns a minimal representation.

I also found the vision experiments not convincing: on CIFAR-10, they used SimSiam in the linear prediction case instead of the usual 2-layer predictor in which SimSiam performs best.

From the elements provided in the paper, I could not tell whether the proposed algorithm was indeed helping to learn minimal representations, or that such a representation would generalize better compared than one learnt without the proposed method."	2
The main theme of my concerns is that it is not that the training has learned minimal representations but just that it is the diversity of predictor heads that provides the gains. Without sufficient empirical or theoretical evidence, I think the main claim of the paper of being robust to spurious correlations due to the implicit information bottleneck is weak. 	2
While the method is admirably simple and the writing clear, the main claims of the paper are either questionable or without any supporting evidence. Thus, in my opinion, this paper falls well short of acceptance for a top-tier conference like ICLR. If evidence can be provided in support of the main claims, I would be happy to revise my score.	1
Overall the scratchpad idea sounds simple and interesting, however, the current evaluation in the fine-tuning setup has it's practical limitations (see above). I advise the authors to either position this paper in one of the two directions discussed above. Overall the reproducibility and transparency in experiment reporting would benefit from major enhancements. 	2
I think the paper presents an elegant idea with extensive experiments and would provide useful insights to the community.	3
This paper presents a new, simple idea, is well written, and has strong empirical results. The research methodology here, of improving LM performance without actually changing the architecture, is super interesting, and I hope that this paper leads to more work that pushes the state of the art forward without modifying the model. The paper presents results on artificial tasks and python code but I believe that conclusions from it are relevant to the entire natural language modeling field. I am strongly in favor of accepting this work!	4
"Although training a model using a scratchpad provides significant empirical improvements in learning to execute, and I appreciate the extensive empirical evaluation conducted by the authors, I believe that the explanation of why scratchpad helps is incorrect and confusing the readers. Further, the motivation of this task and settings of learning the python interpreter given a python interpreter at training time, is unclear to me. 

Thus, I recommend rejection at this time, but I am open to changing my mind if convinced that the main weaknesses listed above are mistaken.  
"	2
"The empirical findings are promising. However, it is still not clear in what way SLR can improve IMP, and the reasons behind the improved accuracy / reduced training time.


=== Post rebuttal ===
Thanks for the authors' response and paper revision, which addresses some of my concerns. While the paper introduces inspiring findings on how SLR (or CLR) help IMP, most components are from existing techniques. I choose to keep my score, and encourage the authors to provide more in-depth analysis behind the improvement by SLR, via either new methodologies or perspectives."	2
I like the author's view and the motivation to make a realizable and unified baseline for weights pruning. However, I think it's better to have more evidence on different models and tasks to validate this conclusion. If the proposed claim can be further validated, I think it will be a significant contribution to the community.	2
The proposed work does an extremely good job of explaining the landscape of pruning and setting up the missing link of IMP not being explored sufficiently. However, the experimental takeaways follow similar patterns to Tanaka et al.(2020) and Renda et al.(2020), which weaken the novelty of the work. The work sounds more like an extension of IMP into existing experimental frameworks and results than a purely novel instance.	2
The argument about the vitality and importance of the reward and temporal prediction seems to be an over-claim, or the experiment results do not support this claim enough. Also, many findings of the paper are quite what can be expected and not much surprising. 	2
The paper contains a discussion on how to best learn representations from pixel data for control tasks. The authors discuss the insights of  various techniques in the literature and show empirically that simpler constructs that focus on task relevance, system dynamics etc. have the potential to outperform a lot of methods in the literature. While the paper's evaluation focuses on fairly simple tasks, I feel that the empirical results, the extensive analysis and the discussion of strengths and weaknesses of other classes of methods is useful for the community. 	2
Overall, there are good values in the large-scale studies presented in this paper. However, I'm not convinced by conclusions as the authors try to generalize behavior of specific implementation to a family of methods. If I were to implement a new agent, I don't feel like I can believe these conclusions so that makes me question what knowledge this paper can add to the community. Furthermore, many details are either missing or not made clear, and the main story isn't very strong. Therefore, I don't think this paper is ready for publication in the current status.	2
In summary, the paper executes a fairly extensive experimental exploration of different RL algorithms in different domains. As is often the case for these types of works, some prescriptive claims made are not fully supported by experimental evidence (e.g Section 4.2). This happens more often when the paper itself argues in favor of a specific algorithmic detail (`baseline` here). Some of the results show evidence for aspects of representation learning in RL that could be of general interest (though they are not all novel in themselves, only summarized together). I am also of the opinion that not including other baselines using reward prediction does not inspire confidence that this approach to its incorporation is any better than the others that exist. I am willing to reconsider my current evaluation.	2
"To summarize, the authors proposed a momentum method based on AEGD using gradient and function value information. However, the theoretical and experimental results are not convincing enough. Hence I will encourage the authors to improve this paper and resubmit to another venue. 

---
I thank the authors for your responses and the improvement of your manuscript. After the discussion period, I decided to keep my initial suggestion."	2
The paper contains substantial novelty in providing convergence analysis for SGDEM. However, the theoretical results rely on additional assumptions which are rather strong including bounded gradient, bounded function value (and bounded domain for convex setting) apart from standard assumptions. Numerical experiments only consider deep learning examples which is not extensive enough to illustrate the practical performance of SGDEM.	2
I have checked the response. I still think the idea is not novel enough and the experimental results are weak. Therefore, I would like to keep my score.  	2
"The main issue:

In Theorem 4.2, the reviewer finds the LHS of the bound is deterministic (after taking expectation), while the RHS is stochastic due to the presence of $\min_i r_{T,i}$, which is confusing to the reviewer. 

This seems to be the following mistake in the analysis. In the first equation under Eq. (33), the authors seems to argue that 
$$E\Big[ \min_i r_{T,i}\cdot \sum_{t=0}^{T-1}||\nabla f(\theta_t)||^2 \Big] \geq \min_i r_{T,i}\cdot E\Big[ \sum_{t=0}^{T-1}||\nabla f(\theta_t)||^2 \Big].$$
However, this is not right in general since $\min_i r_{T,i}$ is a random variable and is dependent on $E\Big[\sum_{t=0}^{T-1}||\nabla f(\theta_t)||^2 \Big]$. It is also not possible to write $E\Big[ \min_i r_{T,i}\cdot \sum_{t=0}^{T-1}||\nabla f(\theta_t)||^2 \Big] \geq E\Big[\min_i r_{T,i}\Big]\cdot E\Big[ \sum_{t=0}^{T-1}||\nabla f(\theta_t)||^2 \Big].$ 

The reviewer does not have any idea to fix this issue. And it should be formally pointed out by the authors. In the worst case, the authors should at least formally make the following assumption, and point out that this assumption is UNCHECKABLE. 

Assumption: there exists a constant W>0 s.t. $\min_i r_{T,i}\geq W$ for any $T$ and any iteration sequence generated by the algorithm.







Minor issues:

1. The authors use both $f(\theta;\xi)$ and $f(\theta,\xi)$ in this paper, please unify the notation. 

2. On page 2, related works, the ""differentialequations"" should be ""differential equations"".

3. In the first equation under (13), a coefficient of $\frac{1}{1-\beta^t}$ is missing. 

4. In the second equation under (13), the ""$\leq$"" should be ""$\geq$"" and ""\sqrt{a}"" should be ""$\sqrt{B}$"".

5. Throughout the paper, the authors use the notation of $w^Tuv$. The authors should formally write $w^T(u\odot v)$, in order to distinguish from the common understanding $(w^Tu)\cdot v$. 


--------------------------------------------------------------------------------------------------------
After revision, the author clear the mistake mentioned above. I have changed the score from 5 to 6,
"	2
Overall, the motivation is good and the method achieves a good performance on a public ALFRED benchmark. However, the authors seem to have made many design choices specific to this benchmark which are likely to not result in better results on realistic tasks. The technical contributions of this paper are unclear and the writing clarity needs to be improved significantly.	2
Overall, I have a mixed feeling. On one hand, the performance is really good and significant. On the other hand, the two claimed technical contributions are not stated clearly about their significance of differences than the previous methods or the validity of the design. Also, this paper is more like a system report other than a well-written research paper with clearly presented and analyzed novel techniques. Adding qualitative figures and analysis will definitely help on this front.	2
The paper proposes an approach which is heavily engineered for the ALFRED benchmark and I do not see a major novelty that generalizes to other embodied tasks. The method is mainly a mixture of heuristics and a set of pre-trained components that are glued together for the ALFRED tasks. However, it achieves good performance on the ALFRED benchmark, hence, my borderline rating. 	2
"This paper proposes an adaptive tree search algorithm for NMT models, and the proposed algorithm yields BLEU improvements when the model score correlates well with BLEU. 
"	3
The paper studies an important problem and proposes a new decoding algorithm based on adaptive tree search, which outperforms beam search in experiments. But the paper writing is poor, some technical details are missed, which hinders the reader from understanding the method. Some metrics are missed in experiments. Therefore I suggest a rejection.	3
This paper inspects the biases of commonly-used decoding heuristics and provides a variant of MCTS algorithm for better decoding in NMT with empirical verification. Overall, I feel like the submission is a good one and would give a concrete contribution to the community. However, the authors should address some of the issues I stated above. 	3
Nicely presented paper with some interesting proposals, but the novelty is not as significant as it seems due to some similar but missing related work. The small performance gain, together with doubts on evaluation soundness and/or real-world applicability, would probably also limit its impact.	3
Overall, I think the paper proposes an interesting idea to model the training NN as a dynamical system and is potentially a good paper, but the some part of the method needs more elaboration.	3
The paper introduced a novel formulation of neural networks based on prior dynamical systems theory for an important machine learning problem. However, there are some parts of the paper that require further explanation and in its current state, I think it is fairly difficult for a broad ML audience to understand the paper and it's significance. 	3
	3
Overall this paper is very thorough. The authors set out to investigate the role fine-tuning has on OOD robustness and they successfully identify several key variables to consider. There are many experiments in the main paper as well as in the appendix that validate their claim. This work will be very valuable to the community as it provides some insight into what variables lead to OOD robustness for pre-trained, fine-tuned models.	3
Conclusions seem 'obvious' to this reviewer, but willing to consider other opinions.	2
Overall, the paper has breadth in the number of experiments and the directions that it explores without enough depth and justifications to a majority of findings. Also, the paper lacks novelty or detailed analysis of the proposed concepts. I would give it a score of 4.	2
"I think this paper doesn't make enough contributions and the claims are not well supported by results. Also, they don't provide analysis for the observations and the observations may not be helpful in understanding the problem. Thus, I think this paper is not ready for publication. 

***[Post Rebuttal]***

After reading the rebuttal, I think my major concerns still remain: the contributions are not very significant and the findings may not be useful. I still think that the models studied in this paper are not enough to represent all models that exhibit ER. The authors need to explore other kinds of models that have ER (and also have high accuracy). Thus, I keep my original rating and think the paper is not ready for publication. "	1
Up to my understanding, there are some technical flaws and the paper does not investigate the proposed bound comprehensively (particularly, the motivation is not supported and the advantage is not clear).	2
"Overall the paper is well written and (mostly) easy to follow. The ideas and contributions are novel and solid: connecting f-divergances with TVO. However, it lacks some more motivation (and insights) about the connection with deformed geometry. The empirical results are not convincing, since the reported performances, while similar across methods, do not include standard error nor more discussion, which would better highlight the robustness and benefits of the proposed f-TVOs. 

----
Post-rebuttal update:
Many thanks to the authors for addressing some of my concerns. However, I still think that the experimental setup should be improved and some design choices (eg. wrt the custom divergences) better motivated and evaluated. With this in mind, as well as reading the other reviews (and responses), my initial recommendation will remain the same.
"	4
The extension of TVO to f-TVO is an interesting direction, but I think the experimental results are too weak to support the usefulness of the proposed method. Moreover, the description of the experimental settings is limited to reproduce the results. I recommend improving writing and experimentation. 	3
In all, the theoretical part is intriguing but not convincing enough. There are many typos and inconsistencies in the notations and computations, I expect the authors to provide some supplementary materials to support the theory, including all the mathematical derivations.	3
"Reason for the Score:

The write-up and experiments in this paper are of good quality. The method itself is novel and the empirical finding in this paper might be particularly interesting for the audience of text-based RL. I have minor concerns author's claim about why this method works better than existing exploration algorithms while I'm happy to increase the score if they are addressed.
"	3
This paper is well-motivated, written overall, and demonstrates state-of-the-art performance in the Jericho environment. However, there are relevant but missing baseline algorithms (Go-Explore, PC-PG) for the main table of experiments. I think the results of these algorithms should also be included in the main table, and I think this can further support the main arguments of the paper.	3
The paper is well written, presents a marked improvement over the baselines provided (I’m not sufficiently familiar with the text adventure game literature to be certain those represent state of the art, but I will assume they do unless corrected), and provides an interesting approach to the exploration problem through the two-policy architecture. I recommend acceptance, but I also feel the paper could be strengthened by addressing the questions raised in the main review section.	3
"While I like this paper in general, my main concern is its novelty and contribution. As mentioned in my questions and concerns (Q2) above, the entire Section 3.1.3 is describing prior work (Yao et al., 2021), the ""Learning from trajectories"" part of Section 3.1.2 is describing another prior work (Yao et al., 2020). Actually, neither (Yao et al., 2021) nor (Yao et al., 2020) is compared in result table. As a consequence, to my understanding, the contribution of this paper is the two-phase pipeline and the sampling strategies in Section 3.1.2. I am not sure if this paper contains enough contributions to publish at ICLR. 

Please correct me if I understood wrong. "	2
"It is my opinion that although the problem is quite interesting and relevant, a large body of literature is missed, and the proposed method is not sufficiently novel. Moreover, the theoretical contributions do not provide improvements over the related literature, and could use further specification in certain parts (e.g., although ""for sufficiently large K"" is a common qualifier, a reasonable lower bound is usually made specific in the the theorem statement). Additionally, I do not find the numerical experiments to provide convincing evidence that the proposed method is indeed better suited for large-batch distributed deep learning than existing approaches."	2
"I think that this article is poorly written. 

First of all, I want to mention that Authors decided to decrease the font size of all important equations and plots to fit the conference size requirements. This makes this article extremely hard to read after printing.

Second, for me as a colorblind person all the plots are extremely unclear, since the difference between the lines is minimal.
Third, I missed the main problem formulation. Moreover, the algorithm itself is not mentioned (only update without any details about hyperparameters selection).

Second thing to mention is a novelty and a practical interest of the algorithm. I understand what is a difference between this algorithm and the local SGD, but I am hesitating in the following. Since during the full update (not local) we communicate completely (M^2) exchanges what is a reason in such a delayed gradient computation. Usually the communication is much more expensive than the gradient computation so the reason on delaying the gradients is not clear for me.  In work (https://arxiv.org/pdf/1806.09429.pdf) authors use delayed gradients almost the same way as in this paper; however it allows to save the total amount of communications. 

About the experimental part I have no comments since I failed to recognize the lines difference.

All in all, I find this article in a very draft stage to be published.

"	2
In this paper, the authors present DaSGD, which overlaps the computation and communication of distributed training. The paper is well-written and easy to understand. However, I have some concerns in the novelty.	1
I believe the proposed method is not novel and [2] has proposed a very similar method.	1
On the theoretical side, the main theorems are either immediate results of or simple extension to the previous work. On the technical side, they draw on ICP and ReLIC in the linear and nonlinear settings, respectively. On the experimental side, the baselines are not convincing enough to judge the validity of the proposed approach. 	2
Overall, I found that this paper has a good, theoretically motivated derivation to improve low-data generalization in MBRL, which is a clear bottleneck. However, the proposed practical algorithm was hard to understand, and fundamentally disconnected from the theoretical analysis presented in this paper. Hence, I do not support acceptance as is. In a revised version of the paper, I recommend that the authors draw stronger connections between causal prediction to the practical implementation that they propose.	2
"In summary, the paper focus on an important topic of the reinforcement learning area. The paper is also well-written, but the analysis and the experimental support are not strong enough, so I give 5 in this procedure, but I am willing to increase my score if the authors fix them in the rebuttal process.
"	3
This paper is well-motivated, the empirical results are impressive. My main concerns are (1) the existence of small model-invariant abstraction, and (2) the intuition of Alg. 2. As a result, I can only recommend a weak accept at this point. But I’m open to raising my score if my concerns are addressed.	3
Overall, I think that the proposed method is not well-motivated. Also, the experiments do not include important baselines, and it is not clear how strong the evaluation used is (see above).	3
As explained in the main review, I admit the idea is good but it is at an early stage and needs more refinement.	2
Despite the better empirical results obtained by the proposed method, this work can be improved by identifying key difference to GAL and further investigate/justify the choice of the layers, which will help people understand why PARL is effective (currently it is not that clear). Also, I need some clarifications from the authors on the weird trend in the results. These concerns/issues currently prevent me from recommending acceptance.	3
The idea makes sense, but results are weak.	3
The theoretical results in the paper seem interesting. However, the presentation of the proposed model is not clear, and the model itself does not seem novel. Overall, I think the paper needs improvement to meet the acceptance threshold.	2
"Overall, this is an interesting paper that takes a less common approach to RNNs: provable stability and net-of-nets. The results are at places more difficult to read, but overall it is clear.

I want to add that I cannot evaluate whether the mathematical derivations are correct.
"	3
"Overall, I would accept this paper. Although it was difficult to follow and required a lot of consultation with the literature, I do ultimately think that this is a direction that DL algorithms are going in and that the theoretical and practical results from this work could be quite powerful. To make the paper better, I would like to see some results in a different domain and more effort towards improving the readability. Too often, valuable theoretical works go underutilized because they’re difficult to understand or don’t seem relevant to the empiricists and engineers who could build on them.
"	3
The paper constitutes an evolutionary step in understanding and designing stable RNNs. The theoretical results are novel and noteworthy. Unfortunately the experimental results lack a clear conclusion and at times do not follow best practices (i.e. reporting only the best run out of many).	3
Overall, the paper does a good job in addressing a relevant problem in a well motivated manner. Even though there are some open concerns regarding the evaluation, the fact that the current approach is one of the first to facilitate model auditing for generative models makes this a good contribution. Authors, please address my concerns in the weaknesses listed above.  	3
"I am on the borderline tending toward accepting the paper given the interesting discussion around authenticity and memorization, but I am not fully convinced this is a practically useful paper.  

"	3
See the main review.	2
Key requirements: I would urge the authors to empirically validate if overfitting has occurred to validate their claims (maybe through MI attacks).	2
My overall recommendation is weak reject. While the paper tackles learning for a difficult optimization problem, it is not clear from the results whether it succeeded in learning something meaningful.	2
This is a novel paper. The proposed covariant attention-based neural architecture (CAM) is a novel architecture as well as the modeling of MRTA problems as MPDs. Moreover, the use of the CAM architecture allows solving MRTA problems in a better way than using tradition methods. MRTA scales well with the number of robots/agents and with the number of tasks. 	3
"This paper proposes a new learning architecture which is called Covariant Attention-based Mechanism (CAM), to solve time-constrained multi-robot task allocation (MRTA) problems using a learning-based approach. The method works in an appropriate manner, and empirical results show the advantage of the method. Also, the approach itself is based on a modern approach and is worth studying. 
As a whole, the paper is a good paper involving meaningful new information. 
"	3
Although the idea of using learning to address MRTA problems is interesting, the paper is preliminary and will need significant improvements on novelty justification, terminology definition, and experimental result presentation. 	2
"An application paper with adequate evaluations.
"	2
The ideas are interesting but the evaluation is subpar.	2
"TL;DR: Great area of research, but I found little to no novelty in the paper. Moreover, as a scientist, I believe this approach is likely the opposite of what we should actually be doing for program reasoning. That is we should be lifting program semantics *upwards* to understand their meaning, not lowering programs *downwards* to understand their meaning.

Reject."	1
Overall, I think the paper does not pass the novelty test. 	1
Contributions are not well-justified. A very relevant work that addresses similar challenges and applies similar approach is not discussed. Demonstration of improvements are shown comparing to a weak baseline instead of more advance and relevant existing works.	2
The work puts forwards an interesting work to gradually align two domains. Though different experiments are shown to support their claim, there are few clarifications needed as mentioned in the main review.	3
This work proposes an iterative self-training approach for unsupervised domain adaptation. In particular, the authors aim at gradually adapting a model trained on a source domain to the target domain. The main contribution is a strategy to generate examples from the so-called intermediate distribution between the source and target domains. Despite the reported empirical improvement over the considered baselines, I found the motivation of the proposed approach unclear, and I found that the main claim is not well-supported (i.e. is GIFT indeed capable of generating examples from intermediate distributions? Also, what are such intermediate distributions?). Moreover, the manuscript lacks clarity in several aspects and I have concerns regarding the significance of the reported results since it seems only a single run was considered in each test case. All in all, my initial assessment is that this manuscript is not ready for publication yet.	3
The technical novelty of GIFT is limited. Hence, the authors should focus on showing the empirical benefits of GIFT. However, the experiments are highly insufficient, with missing ablations and baselines. Moreover, the evaluation datasets are not so popularly used. Therefore, it's hard to judge how much empirical value this work brings. 	2
The proposed method in this paper is not novel although creating virtual examples is a reasonable direction for addressing gradual domain adaptation. I suggest the authors can still work on this direction but propose a more novel way to create virtual examples that contain better intermediate information. The experimental results do not support what the paper claims and do not show significant improvement over previous methods. Therefore, I recommend rejecting this paper.	1
My major concern is the effectiveness of the proposed method on datasets with natural distribution shifts. Although this method works well on synthetic data, its applicability to real datasets should be further justified and evaluated.	2
This paper proposes a scalable and effective method for extracting mini batches (subgraphs), which can be applied to both training and inference of GNNs. However, I still have several major issues/questions about this work, and would ask the authors to address them in their rebuttal.	2
"** After modifications and discussion, I have increased the score to ""marginally above the acceptance threshold"".

The recommendation of the review is “marginally below the acceptance threshold”. 

The paper is considered to propose sufficiently novel techniques, although aspects of the approach are similar to prior work. The impact of the paper may be somewhat limited because of the heuristic nature of the proposed methodology. 

The paper introduces an increased pre-processing overhead in order to achieve a speed-up in training time. Although the pre-processing time is reported in tables, it is excluded in the figures that compare the overall train times. There appears to be a need for considerable hyperparameter tuning. This process, conducted via grid search, might take considerably longer than the actual reported training.

Although the experiments appear to have been conducted thoroughly, results are reported for only one set of hyperparameters. The selection process of these for the baselines is not particularly clear.
"	3
"This paper studies an important question. The proposed method, however, has multiple issues including timing, storage, convergence, and insufficient empirical evidence.
"	2
To summarize, this paper basically applies message passing neural networks to solving PDEs. I strongly recommend authors to present motivations and design of networks with more details and analysis, and the experiment requires improvement. 	2
While the paper seems generally well written and the method appears sound and thoroughly evaluated, I'm not sure what the novelty of the paper is given that the contributions overlap significantly with Pfaff et al. (2021). Moreover, the authors do not cite this paper or explain how their work differs from it. Thus, my rating is to reject the paper.	1
This paper proposes a new method, which is innovative, but only shows some simple experiments and lacks detailed discussions.	3
The method is well laid out and written with enough clarity that the setup is understandable and intuitive. The method makes sense given the problem and is a logical step forward in this space. The experiments, however, are insufficient to back up the claims laid forth in the first section of the paper. I do not think I could reimplement the experiments based on the writeup and I do not see a separate supplementary material, where I may have missed these details (please correct me if I have overlooked them). Furthermore there is no comparison with any baselines (I’ve mentioned a few sensible ones in the main review), neural or classical, so it is hard to say how significant the numbers in the submission are. There are also no ablations for the reader to understand the impact of each design choice the authors make. It is mainly because of the experimental section that I recommend a reject for this submission.	2
Overall the paper showed promise in scaling up the learning of asynchronous macro-action-based multi-agent policies. However, it still requires further work in improving the presentation and performing elaborate analysis of the framework on the three settings discussed -- such as limitations of the gradient updates derived. The work seems to be only incremental in comparison to prior work. Hence, I am inclined to reject it at the moment.	2
While the technical contribution and the experiment results are, in my opinion, not super significant, the paper indeed introduces a practical research direction and brings several intuitive first steps towards this direction. Thus, I recommend acceptance of the paper.	3
The writing is ok. This is paper is easy to follow. It would be great if the author can make videos to visualize the learned results presented in the appendix. I would like to increase the score to Week Accept if the authors can address the above concerns during the review session.	3
The main contribution is relatively interesting and contains some novelty. It is a challenging domain and there is a clear industrial application for those that can get this right. I think there are some weaknesses in explanations and definitions in the main paper which weaken it though. The interpretation of the  experiments as it relates to macro-action methods is clear and insightful and convinces me that there is value in these ideas. However, the comparison with individual action methods does not acknowledge the high level of manual code/knowledge injection that favours the macro action methods. As such, I am falling below the acceptance threshold for this paper. However, I would encourage the authors to continue with the work as it is interesting and promising.	3
"Overall, I think PACT and the LEANSTEP dataset are significant contributions. The claim that PACT significantly improves the theorem proving success rate has been well supported by well designed experiments and ablation studies. The system has already contributed theorems to Lean library and has a potential of significant impact on the Lean community in future. I recommend acceptance of the paper.
"	3
"The work is an important practical step for Lean, but there are major evaluation and presentation issues.
"	2
"Overall, this paper advances the techniques of training deep networks for theorem proving by co-training on proof terms. I recommend accepting this paper.

==============================
After reading the authors' responses and other reviewers' comments, I maintain my previous rating of this paper."	2
"This paper gives a useful demonstration of how an existing idea (extract additional training data from the Lean engine) can be scaled up to give a tangible benefit in practice (improved theorem proving results).
"	2
Potentially interesting paper with some problems in the presentation and in the experiments.	3
The paper is in a preliminary state. The explanation of the method is insufficient. The description of experiments lack important information.	2
Please refer to my main review. 	3
From my perspective, the motivation is unclear, the experiments are weird and incomplete, and the presentation quality is poor. In summary, I think that this submission is below the standard I expect for an ICLR paper.	3
I really like this paper but think the off-policy section can be re-written to be more coherent. 	4
"I support the paper for publication. I come from more from the theory community, and therefore ask the metareviewer to discount my ( unfortunately brief) review accordingly.

"	3
Based on the previous comment, it is marginally above the acceptance threshold due to the excellent experiment result.	2
Overall, the paper is well written, and the empirical study well supports the result. However, the paper lacks the theoretical analysis, and the novelty of deriving MDPO from MD is limited. Given that, I recommend rejection for the paper.	1
This is a well-written, creative paper. The models are interesting. The figures are excellent. The connections to related work are extensive and thoughtful.	3
The paper opens a more user controllability in music performance modeling and this is a nice contribution. Also, the idea of having rule-based method with trainable networks in a series of a chain seems useful in many other applications.	3
"Overall, this works is very well written. Everything should appear pretty clear to anyone familiar with the DDSP framework. The principle is simple yet extremely effective, as it improves reconstruction quality and parameters estimation compared to the state of the art. This new control layer represents a lot of added value compared to MIDI2Params, which was already a great addition to the original DDSP framework. In addition to the paper, the website provides very convincing audio examples which further assess the quality of this work.
"	3
The paper ran a large scale study to establish the benefits of self-training for data distribution shift. The technical contributions of the paper are only marginal and more of incremental nature, however the study itself is valuable and can be beneficial for the community. This study can significantly get boosted by diversifying the range of datasets and tasks under study. 	2
The paper tests self-learning as a complementary addition to improve robustness. However, the premise that self-learning improves robustness is already somewhat well-established - the main contribution here is a systematic application to different methods and datasets. The restriction to the pre-training + test-time adaptation setting has also been considered to some extent, but not as systematically. The value of the proposed dataset ImageNet-D is unclear, whether it gives insights beyond DomainNet itself, and whether it is a worthwhile goal to generalize to such disparate domains. Finally, the analysis of self-learning dynamics seems interesting and predicts some empirical behaviors nicely. I think the paper could have a good message solidifying self-learning methods for robustness, but could use some tightening up in the story/clarity of the paper, and some inconsistencies in the experimental reporting. I'd be happy to raise my score if the issues are addressed in the rebuttal.	2
Overall I like the paper. I think it would bring value to the research community and it serves as a reminder that the simplest methods often work very well in practice.	3
While I think addressing both key weaknesses would greatly strengthen the work, I think this is already an impactful and highly rigorous, technically sophisticated study which meets the bar for this venue, so I vote accept here.	2
Overall, the reviewer tends to vote for accept since this work makes huge efforts in terms of experiments and some of the conclusions are beneficial to be heard in the community.	3
"A brute-force, unashamedly empirical study of large-scale pre-training, albeit in a relatively specialised setting (low-shot, mostly with very recent, not yet widely established network types). The truly large size of the study means it will be interesting and potentially helpful for many people - we probably have to be grateful for such empirical studies, as very few would be able to replicate them, even if they are only ""aggregated"" and not ""trained for the purpose of the study"" (although it is perhaps a bad sign for the community that a small oligarchy of organizations now are the only ones who have that capacity). It is the sort of paper that does not strike me as particularly brilliant, but whose message will, and should, raise some eyebrows and spark discussions in the community."	2
This paper gives an interesting study on the prediction power of US accuracy. Extensive results are provided. The conclusions are solid.	3
"The presentation of the work is meandering and unfocused, some typos are present which negatively impact reading. 
Multivariate time series classification is a very well studied topic. The proposed method, LightWaveS, is a relatively minor extension of the ROCKET method. The overall contribution of this work may be incremental. Moreover many design choices seem rather arbitrary and lack either intuition or rigor (see i-iii). For these reasons I believe the manuscript does not meet ICLR's standards and should be rejected."	2
While the results are promising compared to existing ROCKET models, the proposed solution is not well explained and the experimental results are not fully justified. 	2
See the comments provided above.	2
"The paper is largely well-written and it is clear that the authors have put a lot of effort to implement and evaluate their work and benchmarking it again existing methods and have also tried it on new datasets. 

The presentation of the results in terms of providing detailed Precision/Recall/F1/AUC would have been very helpful. Comparing the speed/complexity and/or performance of the other methods at the same level of accuracy would have provided a more balanced comparison. 

Figure 6 is hard to read and comprehend. It seems the mean difference is shown in Figure 6(b); but it is not clear what for example a -0.3 difference means. Does that mean 30% lower accuracy?  

Overall, the idea of wavelet scattering as a feature extraction is interesting. However, as the authors have mentioned ""LightWaveS can act as an initial fast channel filtering method that precedes another deep learning solution""; the latter could have been a good starting point to start the work and build a processing pipeline.

In terms of code and reusability, a set of python code have been provided but it would have been very helpful if dependencies or a read me or a notebook code accompanied the code to make it more readable/re-usable. "	2
Overall, the paper does well at handling an important problem. However, there are some key details about the algorithm that need to be clarified and a key aspect of the experimental results need to be clarified.	4
The method shows good speedup, however, there is a significant loss in accuracy too. DNN literature for compressing models is missing but seems relevant for this problem. The solution is not clear how it generalizes to other problems as claimed. No proof/experimentation is provided at least.	2
The authors present an optimized pipeline with good results, whose generality is not possible to assess.	3
The paper seems to address a well-known important class of problems. The universal approximation theorem might make significant impacts in solving optimization problems with constraints. 	3
"Justification: The constraint examples are too unnatural. For example, in geometric deep learning, authors argue that when K is a disjoint set, then the closest point computation becomes an unmanageable dilemma, claiming that the linear relaxations are meaningless for integer program (IP) while LPs are routinely used to solve IPs in practice using a branch and bound scheme. While some pieces of notations are kept constant throughout the paper, there are many confusing notations at various places that can easily be avoided - ""Attention"" in eq (4) is different from ""attention"". Adding to this issue, another central issue with the presentation is that all the technical statements in the main paper are Informal theorems with no rigorous explanations provided. This makes it very hard to read the paper because while the statements themselves may be locally correct or correct in isolation, it sometimes does not even make sense when put together as a whole, and makes it possible to verify the technical correctness of the paper. Somewhat related to the previous point, some technically deep concepts (such as Wasserstein-1 distance) are introduced in passing and handled in a slightly cavalier manner. It could have been nice if the authors provided some explanations to the figures -- at the moment, the reader has to guess what the authors intend to say. 

Given that the paper shows quantitative results in the infinite sample setting, it is not clear whether the results hold in the finite sample setting where transformers are being used in practice. It would have been nice if the authors can formulate and show some toy experiments to even check whether such universality may be possible for dataset in the hands of a practitioner.

After response: Thanks for the detailed response. I agree with the modifications, revisions, additional sections, and references proposed in the response. These changes were really helpful in parsing the paper for me since I have adequate mathematical background, but not so much experience in theoretical computer science. With more clearer notations, and figure descriptions, I can now see that the paper indeed focuses on universal approximation rather than generalization, and now I feel that my criticism about finite sample settings has been answered by the toy experiments also.  I choose the option of ""minor issues"" in correctness only because some statements still read handwavy in Section 1, for example, there are many places where the authors use the word ""any"", as in, any loss function, any constraint set, any goood activation etc.. While it may be correct in some sense, I find it odd to use in an otherwise rigorous paper. As an aside, I would like to point out this certainly does not hold if one wants to train from scratch since we know that loss functions and activation functions have a significant effect on first order algorithms. Thanks for providing additional references for us to check, and while going through them I found that some of the theoretical papers have very relevant practical use cases and I believe that they may add significant value to the overall scope and presentation given here, for example, Meta-learning with implicit gradients. I have raised my score."	3
My review for this paper is rather short because overall I think that this paper is excellent.  It's clear that the problem is well-motivated and there are few results in the area.   To this end, the authors provide highly technical results to specifically address this novel and important question.  Moreover, the authors make a great effort to explain their results to a non-technical audience.  This is a clear accept in my opinion, and should be highlighted at the conference.	4
I am not convinced of the novelty of this work and the quality of the write-up is not up to ICLR standards.	1
This paper provides some empirical study of whether pretraining in machine translation could help with solving symbolic integration and differential equations, and showed promising results. However, the effect that pretraining on language tasks could help other modalities has already been show in previous works so that novelty is limited. Furthermore the current draft contains several flaws: the title and some claims are not well supported; some of the writing, for example, Section 3, is confusing. 	2
This paper is poorly written and makes confusing + unsupported claims, the evaluation isn't thorough at all, and their approach doesn't even seem to help consistently very much anyway.	1
"Overall, the paper is interesting and addresses an important subject. However, the results could be made more convincing by making mBart and LC more comparable. This includes : 
- comparing models with similar number of parameters and features
- testing the impact of the bias towards shorter expressions
- better measurement of the number of samples needed to train in both cases
- a better analysis of the distribution shift comparison

Hence my note of 6, which I would raise if these issues can be addressed."	2
"Overall, I find this paper presents an interesting case study of feature transfer focusing on few-shot learning, it lacks enough novel insights and technical contributions to warrant publication at ICLR. 
"	1
The idea of improving existing baselines to be almost comparable to SOTA with some techniques may not be a novel one but this paper stands out in experimenting and in-depth analysis of the various techniques and also combining them with heuristic tricks like feature selection and voting to handle the issue of small sample size. Hopefully, the authors can address my queries/concerns (mentioned above) in the rebuttal period.	2
Interesting case study of feature representations for few-shot learning. However, it lacks depth (only MAML analyzed as few-shot method) and the proposed solution lacks technical novelty and convincing results.	1
Although the paper studies an interesting topic, I have some concerns about correctness of various statements in the paper, the relationship with prior work that has explored similar questions, and the need for additional experiments and analyses to aid in better understanding where the observed performance gains come from. Finally, a weakness of this work is that the study is limited to simple datasets and a linear transfer learning adaptation technique, which makes me question whether the conclusions will generalize to other scenarios.	2
"From the contributions reported by the authors:
- contribution 1 and 2 are not really novel and do not really provide some new insight with respect to previously published works and current trends in training a backbone for Few Shot Learning
- contribution 3 (feature selection, voting) is quite novel and provide some interesting gains, but more experiments would be needed to assess the relative benefits of those proposals.

Considering that the focus in this paper is more put on contribution 1 and 2, a significant reorganization of the paper as well as more experimental results should be done in order to focus more on contribution 3. That's why my vote is to reject this paper."	3
Although the main idea seems promising, there needs to be a lot work done on presentation and evaluation.	3
"I find the idea of the paper intuitive and interesting. However, I have a number of concerns about the computation and memory cost of the method and the sensitivity of the proposed approach to the exactness of gradients. 
I am open to raising my score, given that my concerns are (at least partially) addressed. 
"	2
For a paper proposing new methods for Federated Learning, one generally expects either that theoretical convergence analysis is provided, or a very thorough experimental evaluation is provided. This paper doesn't provide any theoretical guarantees (and the motivation for the proposed approaches is not always clear). There are a lot of questions about the experiments, which reduce my confidence in the overall conclusions. Moreover, the proposed schemes appear to be fundamentally incompatible with privacy, which is a critical factor for Federated Learning methods. Consequently, I don't believe that this paper is acceptable.	2
The paper is overall not well-written and I recommend rejection.	2
The authors present investigations regarding the connections between curvature and many initialization methods and techniques. However, no clear conclusions are made and some investigations are not deep enough. 	2
Overall, this is a good empirical paper with lots of experimental results and good insights. The learning rate warmup part is the most insightful among all. 	1
Strong empirical paper which well presents many experiments and observations of the interplay of the maximum eigenvalue of the Hessian and learning dynamics.	2
Although the results presented are not particularly groundbreaking, the paper is rather well written and easy to follow, and gives some potentially useful insights on an important topic : learning rate scheduling. Therefore, I think this paper could be of interest to the community.	3
"This paper considers the well-defined feature selection problem. The authors pointed out that the redundancy issue of the weight-based feature selection methods. To tackle this, the authors explored the second-order feature covariance matrix and proposed a two-stage framework including feature correlation matrix via knowledge contrastive distillation and feature selection on the masked correlation matrix via graph segmentation. Extensive experiments demonstrated the effectiveness of the proposed methods.  The motivation is well introduced with illustrative examples. The graph segmentation-based framework is novel and interesting to me, which is different from the mainstream weight-based feature selection. The philosophy of knowledge contrastive distillation is logically sound.
 It is suggested that the authors would like to do another ablation study without knowledge contrastive distillation. Although the authors provided the illustrative examples in Figure 1, it would be better to demonstrate the proposed method does not suffer from this issue on the same datasets in the experimental part. "	3
Interesting idea, but some unclear presentations and motivations.	2
In general, I think this paper is relatively interesting, and I was initially interested by the title of this paper. I would like to increase my score if the author(s) could give convincing responses to the previous comments/questions in the Weaknesses part. Thanks.	3
This paper proposes a rather complex and expensive new method for unsupervised feature selection that performs marginally better than the baselines (many quite old) on the set of datasets they have chosen to present in the paper. The exposition could also be clearer, but the idea seems more complex than it is interesting. In all, the paper does not rise to the standards of significance and novelty of ICLR.	2
Overall, I think it is an interesting idea with promising results, but more experiments have to be done to clearly state the performance of the algorithm, as well as a clear reasoning behing all the decisions the authors made in it.	3
"In summary, while the paper presents an interesting turn on previously presented ideas, and the mathematical foundation is well worked out, the experimental evaluation is insufficient to support conclusions about the method.
"	3
In general, the paper addresses a general cross domain imitation learning problem where only expert demonstrations are available and proposes a novel GWIL. Though the experiment results are not visualized well, the work is highly likely to show new insights to researchers in imitation learning and domain adaptation domains. 	3
"The paper introduces a novel idea for imitation learning. 
It likely has many limitations, but the idea of find suitable imitation-based correspondences is one that is being
pursued on multiple fronts, and this is a new approach, with a mix of theory and some initial proof-of-concept examples.
The paper could do better at explaining core ideas, and still needs learning curves in order to understand the benefit of the
cross-domain transfer."	4
"I enjoyed reading this paper, and think it adds greatly to the conversation around cross-domain imitation learning. The proposed approach has a number of strengths and limitations which I would appreciate hearing more about, particularly when it comes to convergence speeds, repeatability and computational requirements, but also whether strengths/weaknesses of optimal recovery of a policy up to an isometry have just shifted the need for specification of a mapping between expert and agent into a different domain.

==== Post rebuttal comments ====
Thank you for engaging in the process, I still believe that this is a good paper."	4
In summary, although there are empirical contributions in the proposed method, the paper appears only to apply the well-developed BYOL scheme to graph representation learning tasks. The technical contribution is sort of incremental.	2
"It feels to me that no obvious weaknesses can be spotted. The BGRL method is easy to follow, is simple and also powerful, and the authors did an extensive set of experiments to verify the effectiveness and also efficiency of BGRL.

# Suggestions
* In Table 5, the memory consumption for both GRACE and BGRL should be given so that readers could understand how much memory is needed for sufficient number of negative samples to achieve similar performance as with BGRL.
* I believe some graph-level evaluations may further expand the applicability of BGRL. In this case, how to choose appropriate representations (e.g., graph- or node-level embeddings or some sort of context representations) for both branches is important.
* As the authors point out, theoretical analysis on the learning dynamics of the online and the offline branch [1] may be helpful for understanding the superior performance of BGRL.

[1] Y. Tian, X. Chen, and S. Ganguli, Understanding Self-Supervised Learning Dynamics without Contrastive Pairs, in ICML, 2021.
"	3
"Considering the empirical contribution of the paper, I am inclined to accept this paper.
"	2
The paper shows good results, but i) the novelty is relatively limited; and ii) there is no intuition on why the method works. Therefore, I believe the paper is just below borderline.	2
In summary, the methods presented can be of potential interest to the community, however the presentation is weak, scope and novelty is limited, and experiments are rather minimal.	3
The proposed approach concerns an interesting topic and provides excellent improvements over existing approaches. However, I'm not able to mathematically verify Theorem 1 and the derivation in Section 2.2. This is why I preliminarily vote for a reject but would be very happy to re-consider my score if additional details are provided by the authors and discussions with other reviewers.	3
Given the numerous advantages of the proposed method over existing works, I am inclined to recommend acceptance of this paper. However, given the questions I raised concerning the empirical evaluations, I am reluctant to recommend a clear acceptance.	3
This paper provides a new perspective of systematic generalization, and gives empirical results to prove that methods based on this perspective can improve model's generalization on toy and real tasks. However, the explanation of the new perspective is not clear enough, and the proposed methods look pretty simple and only solve simple generalization problems.	2
The technical contribution is relatively weak and the arguments and stories of this paper make it hard to understand the specific contribution of each part. 	2
The paper is decently written and the statements are well supported. However, I failed to see the significance of the paper nor its contribution compared with other existing papers.	2
The paper has some merits but the overall claims are not well justified. The work can be improved after a major revision and narrow down the claim a bit.	1
Following the aforementioned consideration, l recommend to accept the paper. The proposed research direction is promising and could open perspectives to several domains, mainly physical process tasks.	3
"The research field of this paper is very interesting, but unfortunately there remain many shortcomings in theoretical and experimental results, which need to be forcefully addressed. The reviewer therefore thinks that the paper is not good enough to be published on ICLR.

After reading the authors' rebuttal, I increase my rating a bit. But I am still not contented with the stuff in this submission, which should undergo a thorough revision to convince and be easily understood by the readers."	2
The bad thing of such mathematical ideas inspired paper is that the experimental performance are always far from sota. But I think the performance is not the only thing, it is not bad to accept this paper for me.	4
"I believe the paper is not there yet, mainly because of limited novelty when compared to [1, 2] (W1), lack of comparison against the state-of-the-art of the tasks proposed (W2), and not very convincing experimental results due to lack of details (W3) and ablations (W4).
"	2
"I think the main idea is novel and worth publishing.
However, some of the claims are not grounded properly and should be toned down."	3
To me, the paper seems interesting not necessarily because of the technical novelty, but because of the theoretical side: the information theory based explanation of the final loss. However, I want to mention that I do not have much experience on this particular field so it might be the case that I missed some similar prior work. 	2
"Please see the details in my main review. In summary, I think this paper clearly indicates their ideas of incorporating stochastic attention into neural processing. However, the novelty as well the comprehensiveness of the study and experiments need more elaborations and works. Thus, I recommended marginally below the acceptance threshold.


----------------------------------Post rebuttal-----------------------------------------------------------------------------------------------------------------
After carefully reading the authors' response, I am still holding the question regarding the novelty of this paper. The theoretical proof in this paper is very limited and basic. And most of them including the contextual prior, stochastic attention, elbo, etc have been fully covered by the bayesian attention modules [Fan et al 2020] paper. Thus I don't agree with the author about the theoretical proof. Thus, this paper can only be seen as a simple implementation of the bayesian attention module in the NP field.  
In addition, although I see new results in the rebuttal, my concern is that the accepted version of the paper might not be a substantially improved version of the paper. The rebuttal stage is not similar to a journal revision where you can submit an improved version of your paper. Rather, it should be used to clarify misunderstandings.
Overall, I would like to hold my original score for this paper and still think this paper is not ready for the ICLR conference."	3
"The paper develops on a promising idea towards incorporating stochastic attention weights to the Attentive Neural Processes framework. However, the current manuscript is far from being ready for its publication.


--- Post rebuttal

After interacting with the authors and considering the comments and revisions, I am happy to raise my score. The paper can make a good contribution to ICLR 2022. "	3
This paper addresses a timely problem and the proposed evaluation metric and representation probing methods are sound. However, this paper needs to explain some points more clearly to better understand the work.	3
Overall, this paper presents a significant advance in evaluation of object-centric generative models and provides a sound new metric for this task. Despite my concerns mentioned above, I believe this paper can be accepted (although it is borderline).	3
Interesting and novel ideas, but the often unclear writing, questionable claims/framing and weak experimental evaluation leave me unconvinced of their efficacy. I believe this paper is currently just below the acceptance threshold, but am eager to improve my score if the authors address the aforementioned concerns.	4
The paper is ok... but lacks of a contribution for community in my opinion and does not advance state of the art in any terms (computations speed, parameters, overall accuracy results, explaibaility).	1
My main concerns are that the experiments are solid enough and the method is trivial. The authors only apply wavelet to extract multiscale features and test them separately.	3
The paper lacks enough novelty in the technical approach. The authors tackle a very simple problem of source identification. The authors try to perform evaluation on unseen GANs but lack of baselines to compare it with makes the analysis redundant. The experimental evaluation is not thorough across different datasets for deepfake detection which seems to be the main motivation of the paper. 	2
While the paper provides an interesting method to detect GAN generated images using wavelets, thus combining pixel and frequency domain, more experiments are needed to provide a good assessment of the method. 	3
The paper is well written in some places, however, does has the opportunity to be improved, especially it was lacking justification in some places and could be helpful to include some more comparative study to prove the efficacy. Please refer to the points mentioned under weaknesses section in the main review to get details.	2
"This paper pursues two complementary aims. A first one is building a predictor for contact residues using the structures of both proteins interacting. To do so, the authors propose leveraging a larger data set along with a novel architecture. However the choice of the methods included in the benchmark is not motivated enough and should include more recent state of the art.

The second one introduces a novel invariant geometry-based attention for use in graph transformers. It also includes using edges as pseudo-nodes in this graph. These ideas seems very interesting and could be validated onto other classical tasks involving protein structure such as binding site prediction. However the chosen formulation sounds a bit complicated and could be motivated by comparing it to a simpler version. Such an analysis should be complemented by a more extensive comparison to existing equivariant methods. I think such a contribution could be a standalone contribution, but the current version of the paper is not motivating this approach enough.

I would advise to reject the paper in the current form, and encourage authors to pursue the validation of their paper along these two avenues.


########################

After an interesting discussion with the authors and inclusion of further results, I changed my evaluations twice and now recommend this paper for acceptance. I would advise the authors to work more on the writing of the paper using the supplementary for the equations and emphasizing more the novelties of their methods compared to previous work (backed up by ablation studies disproving the usefulness of these technical choices). "	3
"In summary, the proposed deep learning architecture for protein interface contact prediction is somewhat new,
though aspects of this contribution exist in related work. The performance metrics used and baselines may be improved.
"	3
Considering the noticeable improvement made by the authors, I reconsider my original view and agree with the other reviewers on the significance of the proposed approach.	2
Considering the above pros and cons, my recommendation of the paper is marginally above the acceptance threshold.	2
"Trees are a great function approximators and have been studied in MDPs before. 
Recent successful algorithms are based on deep neural nets but authors do not compare the two approaches in terms of their scalability, interpretability and/or theoretically. "	1
I do not think the paper is ready for publication in its present form although the method is potentially promising. An expanded discussion on related work is needed alongside a much more developed experimental section. 	2
I think that without some comment about the issue of how well the obtained decision tree based policy approximate the optimal LP based policy, the paper is weak. It present a nice but obvious explanation of why by coarsening the state space one can reduce the complexity of computing a good policy. 	2
"I vote for acceptance as the described method is an intuitive approach to exploit lower dimensional structure to solve computationally difficult high-dimensional problems.
"	3
The problem of FL with periodically shifting distributions is well-motivated by practical needs. The idea of tackling the non-IID problem in this case by assigning different clients to different branches also makes sense. However, this paper then reduces the problem to FL with client clustering, which is a crowded topic with several similar methods published recently. Given these works, the EM algorithm in this paper is not novel. Moreover, there are a great number of works in the FL community studying the statistical heterogeneity problem and they are all likely to be directly applied here since the day-night shifting is a special case of it. However, the experimental comparison ignores all of them and only compares to baselines developed(modified) in this paper, which fails to provide convincing evidence of the advantage of the proposed method over existing heterogeneous/personalized FL methods. No real day-night periodic shifting exists in any of the three datasets. Knowing the shifting parameter p is somehow cheating. The proposed heuristics are not sufficiently justified whether they are compatible with the EM algorithm and the Gaussian mixture model. Hence, I think there are currently many non-trivial problems that need to be fixed for this paper.	2
This paper handles the distributional shift observed in data in clients in a federated learning production setting. In particular, the distributional shift is modeled as a mixture of distributions. The innovations include learning a GMM in a federated fashion and the development of the algorithm FedTerm. Experiments show the performance gain with the proposed algorithm. However, there are some gaps in the paper which require additional clarification and experiments. Upon addition of the clarifications and experiments, the paper will be a strong paper to be accepted.	3
In this paper, the authors propose a novel setting for federated learning, and then design a solution to handle the periodic distribution shift problem in FL. However, the claim is not well supported by the contents.	2
Given the technical contributions as well as the well-conducted experiments and its overall quality, I am in favour of accepting this paper.	3
"Interesting paper on robust metrics for measuring transferability having some interesting results and good experimental validation.  Some revisions could improve the paper :
 + Improving the motivations of the different choices in terms of their impact and meaning of the transferability measure.
 + Discuss more the different transferability measures in terms of their meaning in relation with the difficulty and hard settings of transferability tasks."	3
"While I like that the paper makes a case that existing measures of task transferability have limitations that have been overlooked and that the authors propose simple solutions to fix these problems, I am still confused at the core hypotheses that are not always empirically verified or the actual call to action.

I am willing to raise my score if the authors address the main weaknesses."	2
The paper is a good paper in my opinion, it tries to address an important problem question in transfer learning: a good transferability metric. It addresses some issues of existent metrics and proposes a fast implementation of the proposed metric. The main concern is that does this metric only work for image classification tasks?	4
I appreciate the use of stale gradients to speed-up backward computations in GNNs. In principle, the IGLU is architecture-agnostic and can be applied to many GNNs. Therefore, I am more inclined to acceptance than rejection.	4
Overall, this paper has some insights on tackling the scalability challenge training GNNs on large graphs. And IGLU is an effective method for scaling up GNN training and shows better empirical performance compared to well-known techniques. On the downside, the technical novelty is not significant. Moreover, I think authors should provide a more detailed comparison between IGLU and GNNAutoScale. 	3
"Overall, I like the technical contributions made in this paper.
My concerns are:
1. The algorithms are not clearly described, making it hard to understand the implementation of IGLU
2. The evaluation setup is not rigorous, making the claims on the effectiveness of IGLU questionable"	3
"Two primary element should be resolved to provide convincing arguments about the utility of the proposed approach. (1) Consistent baseline model comparisons throughout section 4 experiments.  (2) Provide clear comparative analysis with MHVAE and show what is specifically novel (or identical to) this close competitor and substantiate improved performance in clear experiments.
"	2
The paper is well-written and the results are promising, but there are concerns over the motivation and novelty as compared to existing work, as well as possible confounding factors in the experimental setup.	2
"Overall, I think the idea to consider the hierarchy for the latent variables between shared and modal-specific variations to learn the representation is novel and interesting. But, I do have concerns about this method’s application in real situations of multimodal learning. 
"	4
"The authors proposed an HMVAE where unimodal latent variables depend on a shared latent variable. 
They demonstrated that the model improves generative modeling performance on multimodal data but is restricted to dense modalities such as images and text. 
The proposed method is well-motivated but the paper can be improved if the author clearly demonstrated how a hierarchy of latent variables improves identifing modality-specific latent variables. "	3
"This paper provides an interesting utilization of Schur complement in the in-painting task. However, the evaluation is very weak. 

////////////////////

I read the rebuttal from the authors, and I maintain my evaluation. "	3
The paper studies an interesting problem for flows and proposes a nice solution. However, in the current form, the paper suffers from many weaknesses surrounding the experimental setup and being valid for only a restrictive class of flow models.  	2
While I appreciate the novel idea  and derivations, it is not clear to me that there are any use cases where this method should be preferred over the alternatives. Given the lack of a convincing case for its usefulness, I am recommending reject.	3
This is a strong empirical paper that extends our understanding of scaling laws in deep learning. The research questions are clear and the results are convincing. I think it will be cited often in the future. 	3
The research questions explored in the study are important and interesting, e.g., the scaling properties of composition bias and the influence of source/target naturalness on the scaling behaviors. However, the relationship among these research questions and findings is not clear.	3
"All the ""strengths"" pointed above justifies my recommendation for the paper."	4
While the paper has some minor issues, it conducts extensive experiments in large-scale settings and the results can provide insights for future research, thus I’m leaning towards an acceptance of the paper	2
The authors provide a thorough evaluation of MuZero, a model-based agent, on procedural generalization in Procgen and task generalization in Meta-World. They identify three factors that improve procedural generation and demonstrate challenges in task generalization. 	3
"Overall this is a strong paper and I recommend acceptance. It investigates the application of a model-based RL (MBRL) to procedurally generated environments, which contrasts with most existing MBRL works which on run on singleton environments. In addition to strong results on ProcGen, the ablations are quite informative in understanding the effect of the different algorithmic components. The MetaWorld results are a bit disappointing, but these are nevertheless helpful to include.

My main issue with the paper is that the authors do not mention code release in their reproducibility statement. While the detailed appendix is appreciated, this is not a substitute for releasing code (I am also not aware of thoroughly tested open-source implementations of MuZero). I strongly urge the authors to make their code open source, otherwise it will be difficult for the research community to build on this work. "	2
This paper shows that self-supervised representation learning not only improves the training performance of MBRL but also the generalization performance and also points out that researchers may want to evaluate their algorithms on tasks with high data diversity. However, I feel as though the argument that task generalization differs from procedural generalization is not well supported by the experiments. I do not recommend accepting this paper in its current form.	2
The work is very clearly presented, easy to understand and presents a number of ablation cases on some important considerations for RL agents, e.g. model-free, planning and model learning. The learnings from the work are clear, though not necessarily very surprising. 	1
Overall, to some extent the technical contribution is significant but experimental section should be further strengthen to justify the model and its applicability.	3
Overall, I recommend accepting this paper but the authors are expected to address the concerns.	3
This paper has several reasonable contributions in the area of one-shot generative models for sets.  The extension of the notion of equivariance to learning algorithms is quite sensible and clearly presented, and the proposed Top-n set creation method is a solid contribution, providing significant improvements compared to competing approaches on several benchmark datasets and tasks.  There are a few issues with the experimental evaluation, as described above.	3
Although the topic of the paper is very relevant, there are too many significant weaknesses listed in the main review for me to be able to recommend acceptance at this point. I welcome clarifications by the authors.	2
Futher comparisons on existing benchmarks along with characterization (empirically is more than good enough) of environment where the proposal works would really add value.	2
"Pros:
1. Provide an interesting perspective that regards RL as MORL problems.
2. Propose a better MORL algorithm DiME.
3. Experimental results are interesting and convincing.

Cons:
1. Lack of theoretical results.
2. Did not compare with all baselines, such as MO-MPO."	3
Overall, this is a well-executed paper that presents a new algorithm DiME for multi-objective policy optimization. However, currently, I am afraid it is missing comparisons with the very related prior method MO-MPO, and hiding important implementation details with which the present algorithm becomes almost equivalent to that prior method. 	2
Overall I think it is interesting to introduce multiobjective to improve offline rl and policy finetuning tasks, although the computational costs might increase a lot compare with the native approach. 	2
On the positive side, the topic is interesting and timely and I expect this could be of interest to the research community. Furthermore, the empirical results are encouraging. However, as explained above, the paper could be greatly improved in terms of explaining the method. Furthermore, some sections (such as section 5) contain problematic derivations and inconsistent notation. The ablation study is not explained well. At this point I think the weaknesses outweigh the positives, so I recommend rejection.	1
"Both the strengths and weaknesses of this paper are obvious.

- From the technical point of view, the PseudSigma is promising.
- From the story telling point of view, it has some gaps, especially about how PseudSigma is connected to QM tasks.

I would give a borderline rate for now, and see what are authors response during reubttal."	4
The paper provides empirical evidence of improvement over competing methods, but the method is technically not very innovative.	2
Overall authors uses classic semi-supervised algorithm, pseudo-labeling, but they propose necessary modifications like specific learning schedule, similar idea from other domains on bootstrapping a single model, and, the most important, regularizing training with uncertainty estimation for pseudo-labels and proper accounting it during optimization. All these modifications allowed pseudo-labeling to succeed to improve results for quantum mechanics (QM) calculations across different models, labeled data size and out of distribution data. All these modifications make a novel version of pseudo-labeling which is applied to the QM domain at first time and specifically designed for QM. Having all this in mind I recommend to accept the paper.	3
"While I like the direction this paper is going, I have concerns regarding the expressiveness of the experiments compared to previous work on learning-based physics simulation, e.g., extreme extrapolation generalization on systems much larger than what the model was trained on.

There are also a lot of missing implementation details, making it hard to know (1) how the authors define and solve the constraints, e.g., for fluids, rigid objects, boundary conditions, etc., (2) how hard the constrained optimization problem really is, and (3) how does it affect the final performance.

As a result, I currently lean towards the rejection side.
"	2
Overall, given the limited novelty and insufficient experimental evaluations, I initially suggest weak rejection.	2
I appreciate the idea of using graph neural networks with a constraint-based learning paradigm. However, there are also a few technical questions remaining unsolved. I personally find this work is not satisfying me with solid novelty, but I am open to change if the authors address my concerns.	2
I recommend rejection based on the current status of the paper. My concerns are listed above.	2
"In sum, the authors tackle the problem of visual storytelling by proposing a method that captures sentence context and prior chosen image scene structure for sequence to sequence retrieval. The task is interesting, however, the authors' method lacks technical novelty and key components of the method are unclear. The paper is highly draft-like, with figures unreadable, portions unwritten, and writing errors throughout. The experiments show slight gains of the authors' method, but it isn't clear if these gains are significant and there is a lack of comparison to recent relevant research. 
At present, the paper is not ready for publication and needs substantial work."	1
The review is mainly based on the technical novelties/contributions, experimental designs, state-of-the-art comparisons, etc. Please refer to the Strength and Concerns parts in the Main Review section for more details. 	2
"This work is interesting but underdeveloped.

I think the proposed system passes the bar as a novel method. While most of its component are from existing works, the integration and design of the whole system for the story-to-images retrieval task is a significant technical contribution.

However, the experiment & ablation study are not extensive enough. Writings seem unfinished last few sections of the paper.

So even though this work is very promising, I'd recommend not accept the paper at the current state. The authors are encouraged to spend more time polishing it."	3
I like the writing quality and the problem. While it is common to start with simplified models for theoretical analysis, I expect a more profound theorem addressing broader PNN families given the existing work. I appreciate the authors for discussing the prior work and performing experimental comparisons. But I think both aspects are subject to further improvements, i.e., the experiments should emphasize practicality, and the literature review could help differentiate the paper's contributions. 	3
The paper offers an interesting approach to reduce the spectral bias of neural networks by introducing multiplicative connections, and supports this claim with NTK analysis of shallow networks and experimental evidence on low-dimensional synthetic and real data. Although both the theoretical and experimental contributions are rather restricted, I hope they will serve as useful building blocks for subsequent research on more realistic models and data.	3
The narrative is well structured and easy to read. The main claim is partially supported. Weak aspects of Π-nets are not discussed. I would assess that it is a borderline paper.	3
This paper is well organized and well written.  The proof is solid, and the empirical results support its claim. However, given the previous paper the, proof techniques are not novel. Therefore, I would only suggest a weak acceptance.	2
"While the topic and story of this paper are interesting, it has several drawbacks that need to be addressed. The setting is very restricted, and the theoretical results lack novelty.



========Post-rebuttal updates===========

I raise the score from 3 to 5, given that the presentation of the updated version is more rigorous and clear."	2
This paper has significant technical and empirical novelty. Therefore I recommend acceptance. However, I think we need to have a discussion about whether Out-of-Distribution Generalization is a sensible term.	4
"In summary, the paper has adequate theoretical contributions in terms of identifiability. The algorithm part seems less justified. Also, the method appears to be a DA method, not for OOD generalization, thus authors should compare their algorithm to recent DA methods. I look forward to authors' response.

== after reading response ==
Based on the current response, I decide to increase my evaluation."	3
"This paper introduces an interesting idea about a general causal model, and presents related assumptions and theoretical results clearly. There seems to be some technical flaws in the proposed algorithm and the experimental results could be improved by adding some popular benchmark  real-world data set.


detailed comments
- the general casual structure (Fig. 1(c)) lasks an edge from E to Y compared with Fig. 1(b). Will this difference affect its expressiveness such that some data-generating processes of Fig. 1(b) can not be characterized by Fig. 1(c)
- when determining Pa(Y) in phase 2, the algorithm compares the p-values of independent testing IndTest(X_i, X_j) and conditional independence testing IndTest(X_i, X_j | Y). Since these two methods derive test statistic from differernt rationale, I was wondering whether the p-values of these two test can be directly compared to conclude the evidence of independence.
- I was wondering whether phase 2 is able to distinguish Y's parents between the its ancestors. it seems an ancestor of a direct cause of Y will also be included in Pa(Y) after phase 2.
- this paper only provedes the results on VLCS data set while existing works usually evaluate the prerformance on at least two real-world data set. It would be much better to include one more real-world data set (e.g. PACS)"	3
"In all, I opt for arguing for acceptance. Since the contribution of this work is somewhat subtle, I am especially open to further discussions. The basis for my current position is this: 

- This works adds further weight behind a school of thought on combining deep learning and classical algorithms: instead of exactly running classical algorithms as subroutines in network architectures, we should instead insert network modules that behave similarly to classical algorithms, without exact execution.
- The key requirement in order to use this method, that there are problems where a cheap abstract simulator is available, is fairly reasonable. For example, any case in which we wish to insert a classical algorithm as a subroutine in a network architecture, we have available a simulator (i.e., the classical algorithm). 
- Given the previous two points, the takeaways from this work have to potential to be of use in various problem settings. 

I opt for a weak accept instead of higher since although I enjoyed reading this paper, I did feel that 1) this paper appears to be somewhat modest extension of existing ideas, and 2) I feel the paper is missing some ablations. 

"	2
The proposed approach is favorably evaluated in synthetic domains, but it is unclear from the present set of experiments whether and how this approach will be scalable to more real-world problems. There are also some additional experiments that need to be performed to evaluate some of the paper's primary claims.	2
The paper tries to tackle a meaningful and intriguing problem. However, I find the pre-assumption of the whole learning paradigm is questionable. And I do not recommend acceptance of the paper in its current form.	2
This paper contributes an implementation of an interesting approach to incorporating prior knowledge about a problem in neural networks that was outlined previously. While this constitutes a sufficient contribution, the experimental evaluation should be improved to validate the proposed implementation. I have suggested a number of concrete improvements that can be made, which would have me reconsider the current score.	2
The paper proposes a complex and interesting approach to transfer for RL. The authors do a throughout comparison and analysis of the method. The evaluated domains could be more complex, otherwise the results support the claims of the authors.	3
Small issues with the writing but the ideas seem new and to work quite well!	3
"I think this is an interesting paper, but it falls short in the empirical evaluations. I think the idea is interesting and the theoretical results may be of interest to others working in this space.
However, I don't feel the empirical performance justifies the complexity of the algorithm, which is why I'm somewhat borderline, but leaning towards an accept.
I look forward to reading what the other reviewers think, as well as the authors' response."	3
A strong submission on improving transfer learning which leaves a couple of open questions regarding its evaluation. However, these questions should be easy to address in the rebuttal.	3
Intriguing idea, but needs a lot of organizational work, clarity/presentation improvements, etc., and it's unclear what we learn from the results.	2
"I think the paper has too many unclear parts. In addition, the paper proposes using semantic role labeling to enhance interpretability, but I don't see how SRL enhances interpretability in the paper -- no experiments or studies about that. 
"	2
"
The lack of clarity and the weak empirical evidence are well below ICLR
standards, and thus I recommend a strong rejection.

"	2
The authors proposed the addition of linguistic annotation into a neural model for Question Answering. The paper clearly describes related work, and proposed model. The proposed model incorporates interpretability, however, it relies on manual or automatic semantic annotation.	3
The paper describes the application of tabular Q-learning to the DOME environment. I am worried that the novelty and significance of the algorithmic contributions in this paper are too focused on the DOME environment to apply to other areas of learning. Competitive baselines, and a comparison to state-of-the-art are lacking as well.	2
In summary, the authors apply Q-learning to solve a real world control problem for microorganisms. The approach is inter-disciplinary, though the method  (Q table) is well known. The authors demonstrated compiling results in both simulator and in the real world. 	2
The paper provides the application of Tabular Q based learning on a a novel application. The author's also develop a simulation of the environment for to test this policy. The paper is reasonably well written, clearly organized. However, the contributions, significance and empirical evaluation are limited. Tabular Q learning is a well studied concept. Only a single learning algorithm is evaluated, while the choice of baselines could be expanded to other agent types.	2
The paper makes some incorrect claims, does not cite relevant past work, and does experiments on very simple problems.	2
I find this paper very interesting but am worried about the complexity of the proposed approach and how difficult it would be to apply in most cases. I am also concerned that the baselines compared against are insufficient to get an actual sense of how well this approach compares to other approaches, and what's the sample complexity and computational cost (rather than just feature acquisition cost) associated with it. I am also concerned that essentially learning the VAE component offline makes the approach brittle in practice. 	3
"The paper proposes an interesting approach to tackle an important problem, but there are some critical comparisons missing related to prior work that preclude claims of significance, contribution, or novelty.

Post-response: 
A3-1: 
I'm still not convinced about the novelty of the VAE approach. I understand that the sequential VAE in DVRL does not have ground truth labels and thus requires inference of latent states, but that's because the assumptions don't allow for imputation of missing features/supervised learning. It could easily be adapted to such a setting, however, as representation learning with ground truth labels will in general be easier/more sample efficient than learning without. Similarly, I'm more impressed by DVRL's ability to continuously improve its state inference model online, which seems like a more difficult approach than the two-staged approach suggested by the authors here.

A3-2: 
The authors claim that ""when dealing with AFA-POMDP, if DVRL wants to learn a high quality representation that could effectively impute the missing features, it would need the access to the fully observed data to update itself during online training time based on some loss similar to Eq(5) [...] such an attempt would consume a very large amount of feature acquisition cost to get the fully observed data, which is way larger than adopting a multi-stage RL as we propose where we only acquire 2K trajectories to train the sequential VAE model."" --> It is not clear to me from the paper that, in general, online feature acquisition costs would be any worse than the cost of observing all features offline and collecting those in a batch dataset, as proposed by the paper. By contrast, consider a scenario in which the state includes thousands of noisy and useless features but a few useful and informative features. Then doing the batch representation learning as the authors suggest would lead to unnecessarily high observation costs whereas something like the online DVRL approach could potentially learn much more efficiently. So this claim feels unsubstantiated.

A3-4: 
I appreciate the authors' statement that ""the representation learning model could be updated in a continual learning fashion by the new data as well as the old data and then we could leverage policy transfer [6] techniques to effectively update the policy parameters without inquiring much fully observed data for training the pipeline."" But this is all still in the hypothetical. At the end of the day, the proposed method as written is fixed after some number of observations.

A3-5: 
I appreciate the authors' diligence on the Ethics Statement.

Separately, there is a recent paper at NeurIPS 2021 that seems closely aligned with the proposed work. It shouldn't impact the novelty of the present work, as the two papers are essentially contemporaneous, but may be of interest to the authors: https://neurips.cc/Conferences/2021/Schedule?showEvent=26563

In light of the comments and clarifications, I'm willing to increase my score to a 6."	2
This paper proposes an interesting and novel way of training VAEs with binary latents; although the method adds a lot of computational complexity for benefits which are not completely clear.	3
"This article is written clearly, but the experimental part is a bit lacking in persuasiveness.
"	3
The proposed method is novel and well-presented. I would recommend accept this paper.	3
"The presented idea is interesting and novel in the VAE setting, and could inspire new research in generative models with discrete latent variables. 
While the experiments show that the model performs well in inpainting and denoising tasks, I feel that to be more impactful this paper lacks a clearer analysis and discussion on what makes this model work. I'd be happy to increase my score if the authors improved the paper in this direction.
"	4
The paper proposes an evolutionary algorithm for learning encoders for binary latent variables. The paper allows a reader to understand the main idea, however, it lacks a proper presentation of some details (e.g., the idea of using truncated distributions in the discussed context, presenting explicitly the forms of distributions used). Moreover, the experimental section is interesting, however, it misses a deeper comparison with other existing methods and an analysis of various architectures of neural networks. Overall, I find the paper interesting, however, it is not ready to be accepted to the conference.	2
I have some concerns about the assumptions used in the methodology, as well as the experiments, which leave a number of open questions. In particular, the fixing of GP hypers seems to largely remove the need for scaling, which is the primary strength of HyperBO. Furthermore, though the experimental set up uses a large amount of data to achieve somewhat unconvincing results in my mind, and only one optimization problem is presented (though worth noting, is thoroughly analyzed). Thus, I can't recommend acceptance at the time. 	3
The proposed method is trivial. The theoretical part presented in this paper is very minimal and incremental. 	1
I think that this paper addresses an interesting problem and suggests a novel method as described above. Thus, I would like to recommend acceptance.	4
"This paper proposes a simple idea for wam-starting BO by fitting the parameters of a GP surrogate model on past data. Unfortunately, a lot of relevant prior work is ignored here and not compared against. Instead, the proposed approach is compared against simple baselines, as well as methods that mostly seem to have been made up (such as ""MIMO"").
"	2
"Overall, I rate this paper marginally below the acceptance threshold. It is interesting to see the efficacy of prompt tuning for lifelong language learning. However, with the current experiments, it is unclear how they compare with adapters? Also given this paper unifies existing approaches and adapts them for prompt tuning setup, it is marginally significant/ novel. There are several open questions (see weakness section + additional clarification questions) that need to be answered before understanding the overall benefits of the proposed framework. I am looking forward to the author's response in the rebuttal period and will consider updating my scores accordingly.
"	2
Overall I'm leaning positive about this paper. The proposed LFLL problem, in my opinion, is very important for the community to think about: how to adapt large-scale pre-trained LM to different tasks/domains with few samples? This paper sets up a strong baseline and benchmark for research along this direction. Although prompt tuning, KL regularization, and pseudo data generation are existing ideas, combining them and formulate this LFLL problem/benchmark is non-trivial contribution. I think this work has potential, and if the authors can try to add more tasks/datasets/baselines, it may create more impact to the community.	3
Good novelty in extending prompt-tuning to life-long language learning using the generative properties of language models. Good experiments demonstrating the effectiveness of the methods in preventing catastrophic forgetting. Some parts of the method are not sufficiently studied in the experiments.	3
This paper tackles an important task. I didn't understand why their method specifically applies to the few-shot scenario. I also struggled to understand why the proposed method solves lifelong learning over regular prompt tuning. One sentence in the paper seems like an overclaim to me.	3
The paper proposes a novel method to tackle the more practical problem of semi-supervised learning with imbalanced data. The experimental results are strong, and the ablation studies are informative. More discussions can be made about CDAR’s performance. 	3
I am in favor of accepting this paper as the problem and solution are well written and the experimental results seem reasonable. The main weaknesses are the lack of an empirical example to justify the proposed setting and demonstration of the proposed approach on only simulated cases of MNAR data.	3
Please find our comments above. 	3
The novelty of this work is incremental. It lacks existing equivariant graph neural network discussion and comparison. In addition, experiments should be improved. 	2
Overall, this is an interesting paper, that includes a nice formalization of the problem of defining equivariant operators in GNNs. However, the validation of the proposed method is not convincing enough, as it does not stress-test the method, nor does it provide insights into when the method should be applied with respect to other SOTA approaches.	3
"The paper is clearly written and easy to follow. The theoretically analysis is fairly clean and the model, as a whole, is very cleanly motivated. The results on link prediction tasks, particularly on Pubmed, is impressive. However, novelty is marginal in light of existing prior works and the performance for node classification and link prediction on Last FM is quite underwhelming.

In light of the above, I am inclined to rate this marginally below acceptance threshold.

----

Update post response phase:
Most reviewers do agree that this is an interesting and important problem and that the authors have done a good job laying out the various operations with equivariant property. Also, most reviewers also have identified that some of the experiment results are very positive and encouraging as well.

However, limited novelty is a point that most other reviewers have also raised. I also noticed that they requested for additional baseline comparisons which seems like a fair request. Also, the performance of the model doesn't seem to be stable, sometimes its good and in some cases, it much worse and there doesn't seem to be any reasonable explanation for the same. This is another point, that has quite come up in other reviews as well.

In light of this, I am inclined to retain my current assessment of marginally below acceptance threshold."	2
"This paper is studying an interesting problem of permutation-equivariant
heterogeneous graph neural network. Some of the experimental results are very
positive. However, the novelty of this paper is marginal, the results are
highly varying, and some important baselines are missing.
"	2
Overall, the metrics in this paper are sensible and clearly explained, but further experiments are necessary to demonstrate insights into what models are learning or what directions for performance improvements the metrics can suggest.	1
The suggested explanatory methodologies and approaches were pretty persuasive to explain the network behavior in microscopic context. Nevertheless, the scalability and practicality of the work should be examined carefully. 	3
Interesting paper and relevant subject. But I think there are some inconsistencies in the paper, and the overall impact, conclusions and applicability of the paper is somewhat dubious.   	3
I acknowledge the importance of the research direction in this paper. The contribution of this paper is not properly compared against existing methods, making it difficult to tell their significance. Meanwhile, the toy setting in experiments makes me doubt whether the techniques can be extended to real-world scenarios.	1
This paper proposes an audio synthesis approach that runs fast on CPU and yields high-fidelity audio. It combines VAE and GAN loss for non-autoregressive waveform synthesis. Although several use cases are discussed, no solid application (such as TTS) has been demonstrated.	4
My primary concern is that the technical novelty is limited or not well described. The two-stage training scheme, the combination of VAE and GAN, and the incorporation of multi-band decomposition have already been proposed in the previous studies. I expect that the authors clarify this point in the rebuttal. In addition, I hope that the authors explain the validity of the experiments regarding the issues raised above.	2
Thank you for a very nice read. While finding the results interesting, I have a hard time arguing whether the proposed innovations are in fact impactful. I'm lacking quantitative experiments, e.g., ablations studies, on the components and the optimization details introduced. 	2
The paper proposes a novel idea of learning skills correspondences between robots with different morphologies in an unsupervised way, borrowing ideas from machine translation. The contributions are solid, with a few minor evaluation weaknesses as highlighted in the main review above.	3
"- Clarify contributions and assumptions
- More discussion on the limits of the approach w.r.t. robot morphologies.
- Update related work discussion
"	2
While the idea of transferring skills between robots with different morphology is interesting, the paper is preliminary and will need improvements on explaining its novelty, premise, and terminology; more convincing experiments using robots with obvious morphological differences are also expected.	2
"The paper presents a method that is well formulated, sound and well motivated. The experimental results are convincing but require some further explanation. Details of implementation and experimental results are provided, so that it could be possible to reproduce the results.
"	3
Overall, this paper demonstrates some promise that there could be an advantage to explicitly specifying a label hierarchy during training. More experiments are necessary to demonstrate this method really works more generally on a wider variety of object types and label hierarchies.	2
Overall, contributions of the paper seem small. The proposal approach is not extremely novel and is limiting in practical settings (non-balanced class hierarchy). Experimental results lack comparison with literature hence not convincing.	2
"The contribution is incremental, and many of the claimed novelty overlaps with the existing work (e.g. the references listed in the main review).
"	2
The paper needs stronger empirical results. The proposed method is mainly built on top of Invariant Risk Minimization, thus lacks novelty. There are also ablation studies missing. I would recommend to reject this paper.	2
The major concerns of this work are the novelty of the proposed method and the sufficiency of evaluation. 	2
"This work is interesting and consistant but the fact that both the encoder and the generator are outdated seems critical regarding a pure NLP paper. Despite some clear qualities, I have to reject this contribution.
"	2
Overall, I thought the paper was well-written, had very interesting ideas and a good evaluation / comparison against prior work. Weakness #1/#2 will prevent me from going to the next higher score (of 8), but I'm in favour of acceptance.	4
I did not read the paper after Section 3.1.1. The article is erroneous and the results are misleading. Please refrain from misleading the readers. This alone makes me recommend a STRONG REJECTION of the paper. 	2
Although the method proposed by the authors seems to be empirically promising, it seems to be a direct combination of existing methods, and the novelty seems to be limited. The authors provide no theoretical guarantee/intuition for their method. In addition, some terminologies/notations are vague or confusing. My impression is that this submission requires some major revisions. 	2
The paper is well written and covers a topic that should be of broad interest to the deep learning/ML community. The authors do a good job of putting their proposal in context and explaining previous work. The experiment results are encouraging. Overall, a nice paper that I think is suitable for publication in ICLR.	3
This paper makes a solid contribution towards the challening problem of unsupervised domain adaptation under label and conditional shift. Based on a sound and (to the best of my knowledge) novel variant of the parametric OT formulation, the method proposed achieves remarkable results across a wide range of benchmark tasks. All things considered, I think this paper would be a good contribution to ICLR and would be relevant to the OT and DA communities. 	3
Overall, the paper is well-written and makes some contributions. For more detailed comments, see above. 	3
The idea of using optimal transport to address the problem of generalized target shift is interesting, but the main argument of this paper, optimal transport and its theoretical analysis, is not sufficiently evaluated and is confounded by the additional component information maximization. Based on the empirical studies, it appears information maximization has substantially more contribution than optimal transport, and the outcome of optimal-transport (CAL) is missing in the main results (Tab. 1).	3
This is a great paper that refines the information-theoretical analysis of the generalization behavior of SGD using the HWI inequality and the ISMI bound. 	3
This paper is good and well-structured. However, its novelty is not very strong and there exists some confusing arguments.	3
"Pro:
- Novel increments over a recent work on generalization bound
- New regularization based on the flatness term

Con:
- A few glitches in writing"	3
An excellent paper of high theoretical and practical relevance. Very well written, with many interesting insights.	4
"This is a well-written paper that investigates a narrow task setting which unfortunately does not justify its claims of practical applicability. This could change with more extensive experiments in realistic datasets/settings.
"	3
"The paper is clear and well justified.
However, I find the results unsurprising, since local alignment and congregation together mean that the features should be well clustered, which is intuitive and well known.
Personally I think the paper would need more technical contributions to be accepted.

=== Post rebuttal update ===

Based on the authors clarifications and additional experimental results, I have raised my score to 5. While I agree that the paper has improved after the revision and I acknowledge the contributions, it's still below the bar to me though only marginally.

Specifically, I am now convinced that the intuitive connection with knn is a plus, though I still think the theoretical novelty is limited.
Given the theoretical contribution, I'd expect stronger empirical results for the proposed method to be convincing. Potential improvements, as also mentioned by other reviewers, include 1) comparing with more baseline methods with a larger variety of features (e.g. expand Table 1 with results from other self-supervised methods, including non-contrastive ones), and 2) better justifying the criteria for model selection (i.e. the sum of the lower bound in Thm 1 and the upper bound in Thm 2) as it currently seems arbitrary."	2
The submission seems like a clear accept. It makes a clear contribution to an important topic and is very well written and clearly described. The statistical analysis of the empirical results could be improved. I made a number of minor comments that I think can be easily addressed to improve the quality of the paper. I have no major complaints. 	4
"While the intuition behind Definition 1 makes quite a lot of sense, there seem to be some errors in the derivation of Theorem 1. Also, importantly, it is not so clear that bounds on the loss from eq 2 will be well correlated with the 0/1 loss. (This could be investigated experimentally in sec 7.) The experiments could be carried out more thoroughly (as described above) to gain a better understanding of what is going on. Overall, my judgment is that this paper is not of ICLR standard.

==updated===
see post-rebuttal summary."	3
The paper is clear and seems moderately useful; broader experimental evaluation and more engagement with the literature would improve it substantially.	2
The paper proposes a method that uses warm-up for both solving the inner problem and the linear system arising from AID. It also proposes an improved convergence analysis of the warm-start strategy with inspiration from the singularly perturbed systems. The proposed method shows faster convergence than its counterpart without warm-up. But I would like to see comparing it with warm-up only for the inner problem or for solving the linear system in AID.	3
"* Pros: rigorous arguments, clearly stated assumptions, problem framework is applicable to practice, very good rates in all cases (strongly convex/nonconvex, stochastic/deterministic) except one case (in which the previous related work uses a slight stricter assumptions), comprehensive bibliography/related work section, compelling experiments on both synthetic and ""real world"" problems with comparisons to many other methods of interest in the literature, generally well written and clear given the subject and level of rigor employed.
* Cons: extremely dense and notation heavy for a conference paper (otherwise well written), I was unable to check several proofs in detail; in some sense this work is much better suited for a journal where it can be reviewed properly."	3
In general, I feel that a 'unified' analysis should be elegant and more instructive than existing ad hoc analysis, but this does not seem to be the case here. There is a lot of notation introduced without proper explanation of what they represent, so it is difficult to properly read and check the proofs of this paper in the limited reviewing time of ICLR.	2
In general this paper is theoretically interesting. This paper could benefit from more discussions on ${\cal A}_k$ and ${\cal B}_k$, as well as additional experiments.	3
Due to previous work  [Bilevel Optimization: Convergence Analysis and Enhanced Design], I deem that the novelty of this paper is limited. I vote to reject.	2
"In my opinion, the significant improvement in performance with a simple method is highly valuable.
The idea is simple, well-explained, and the results are promising.
This paper should be valuable for many applications.

"	3
A well-written paper that provides an interesting and relevant contribution to multiobjective optimization. One of the key benefits of the method is its generality, in that it can be incorporated into existing multiobjective solvers, and simple implementation. Numerical results suggest significant improvements with little implementation work. 	3
"This work tackles an important research problem that could be useful for different partitioners in the community (e.g., multi-objective optimization, Bayesian optimization, and NAS). The proposed method is a reasonable generalization from the LaMCTS method. However, due to the concerns listed above on both the method and the experimental results, I cannot give a clear acceptance to the current manuscript.
"	3
A good paper with clear motivation, interesting idea, solid theoretical and experimental demonstration, but some presentations are not very clear. 	3
"Overall I think this work investigates an interesting problem, but the main argument needs to be justified more carefully (as mentioned in (1), (2) above). Also, the takeaway and impact of the work are not very clear to me, other than showing the somewhat inscrutable power of GPT-3.
"	3
The work is good quality and well-explained, but could benefit from slightly more details on the rationale behind some jargon and claims about human cognition.	3
The paper provides new insights into whether large pre-trained LMs induce conceptual spaces that mirror those found in the real world.  This could be an important finding, that together with many related works, gives us a better understanding of what sorts of higher reasoning abilities such LMs are capable of.  I found the experiments straightforward, but well-done, and some foresight was given to what confounding factors may be creeping into these results.  And across all model sizes we learn that similarities between these spaces increases with model size, and that low model sizes, though still big in comparison to many models, here are truly insufficient to capture concept spaces anywhere comparable to the largest models.  The paper is well-written and references are good, although I may have missed an important reference or two since this is slightly outside of my research area, but barring the existence of similar uncited work, I think it's a paper worth publishing.	3
"I think this is overall a nice paper with some nice ideas and interesting results, but it falls short of adequately addressing the research question that it is designed to. The biggest flaws in my view are the inclusion of too much supervision in the unseen-concept experiments, which make the results less relevant to the ""isomorphism"" question inasmuch as it is interesting to the community, and the exclusive reliance on the few-shot learning paradigm, which adds a big confounding factor to the results that makes it hard to interpret any clear trends. I do think that this is really close to being a very nice paper as long as these issues are adequately addressed at least in the framing of the work if not the experiments."	3
The studied problem is important, but there are a number of major issues in the main claims, the empirical justification and technical novelty. So, I do not recommend it for publication at ICLR.	2
As the paper focussed on empirical evaluation of existing mechanism for providing practical recommendations, the paper's brings out the need for understanding unlabeled data distributions before blind application of OC or PU methods. However, there still exists key unanswered questions needed for practitioners to incorporate the recommendations in practical settings.	1
The paper attempts to address an issue of fundamental interest but falls short. The experimental results might still be useful to researchers.	2
The authors propose several modified one-class classification algorithms, where part of the original loss is replayed by the PU loss in Positive-Unlabeled learning. Although the experiments show the effectiveness, there lack theoretical analysis of the proposed approaches.	2
The paper is well-motivated and attempts to fulfill a critical need in the field, but feels a bit rushed and incomplete in its current form.	3
Good paper that describes the issues in results reporting in ML/AI papers. However, given the weaknesses listed above, this work seems to have a notable room for improvement. 	2
The submission contains an assessment of advantages and drawbacks of various model efficiency cost indicators. It comprehensively lays out issues and argues for a more rigorous and precise handling of reported metrics in academic work. Novelty and research insights remain limited, yet publication has good scientific value for means of awareness, reference and citation.	2
I agree with the authors that it is important to raise awareness and to point out the issues related to the adoption of only a few cost indicators. I believe that the key message of the paper is valuable to the community. However, I believe that the paper does not qualify for acceptance at ICLR because of its limited technical contribution. 	2
"the discussed problem is important and the proposed solution is reasonable.   
my concerns are a) the lack of direct comparisons to the relevant methods for minimal state representations, b) the lack of evaluation of the quality of the solution (See please 11 above), c) the lack of the definitions for the essential quantities used in the paper, e.g., What definition of sufficiency do you use? which leads to 'c)' above..., d) the lack of demonstration/validation of minimality and sufficiency on simple examples, where the essential quantities  (e.g., MI) can be evaluated. "	2
This paper addresses the important problem of learning simpler (lower dimensional) dynamics which are sufficient for RL and optimal control. While I believe this paper has a strong theoretical contribution, I believe in its current state, it is unclear if it paper achieves its goals of minimal state representation (how to tune the hyper parameters) and reducing the sample complexity. These concerns may stem from my misunderstanding, and I would be happy to revise my review if corrected.	3
I vote for a borderline rejection at present due to my concerns and questions listed above. I will adjust my rating according to the following discussion.	3
The paper proposes a novel method for learning generative models of environments with high-dimensional observations, identifying the sparsity structure of these models, and selecting only a subset of the state variables relevant to a particular decision problem for use by RL algorithms when computing the optimal policy. The empirical verification on two challenging high-dimensional problems suggests that the learned representations are indeed instrumental in improving the sample complexity of policy computation.	4
"The authors propose convex potential quantile (CPQ), which is an optimal transport approach through a Kantorovih dual objective, maximizing the correlation functional between a uniform distribution over the unit hypercube and target distribution. Their approach is heavily based on a recent work of Carlier et al. 2017. In my opinion, the experiment part of the paper highlights the beauty of the conditional vector quantile proposed in the work of Carlier et al. 2016, through neural networks implementation.
"	1
Although the empirical results are encouraging, it is unclear the method and theory in this paper is sufficiently novel. Therefore, I am as of now not in favor of recommending to accept this paper.	1
the paper apparently comes with some merit, although I must say that the exposition is so unclear that I couldn't really follow the narrative.	3
This paper addresses the class conditioned problem which need more elaboration. Moreover, the experimental results are not strong enough to offset the motivation weakness. Thus I am tending to give a reject but still open to see more updates to make the above points more clear.	3
In general, I think the method is an interesting and valuable contribution, I also enjoyed the precise notations the authors adopted. But either the impact/scalability of the method or the machine learning power needs to be more clearly demonstrated for acceptance. 	3
"I think the paper is good and recommend accepting.

The main weakness I see is that the contribution seems to be limited to using importance sampling for marginalizing out continuous variables. The actual presence of continuous variables can be seen in the Gaussian mixture models experiment of the memoised wake-sleep paper [1], where it was possible to treat them analytically. Of course, the general case doesn't allow for that and therefore the paper is still making a valuable contribution by investigating what to do in the general setting.

Further, the paper is specifically aimed at settings where discrete variables arise in a programmatic context and impact control flow. The experiments show that the proposed approach has an edge over other general methods in this setting.

[1] Hewitt et al., ""Learning to learn generative programs with Memoised Wake-Sleep"", https://arxiv.org/pdf/2007.03132.pdf"	3
Good paper, presenting an interesting new method.  A few concerns as regards evaluations + performance tradeoffs.	3
"To summarize, this paper includes a thorough exploration of better training of dual encoder model using harder negatives chosen in an interactive manner by training a separate, cross-attention model. There has been a number of prior work exploring similar approaches (which I pointed out that comparisons/discussions are not sufficiently provided), but this work is unique in using a cross-attention model for choosing negatives instead of choosing from its own dense retrieval.

There are a few critical concerns: (1) discussions of/comparisons to prior work, (2) justification of using a different base model (as it makes the whole comparison with baselines unfair), and (3) empirical comparison to prior work that uses iterative negatives --- this is an important ablation given that this is a core distinction from prior work as well as the main claim in the paper. They may be relatively easy to be added in the rebuttal so I'd love to increase the score based on the author responses."	2
The paper is an interesting proposition to train retriever-reranker couple for neural information retrieval within a single adversarial framework. The paper demonstrates state-of-the-art results and provide an interesting analysis of the model. There are some minor problems with the paper, but overall I recommend acception. 	3
"Summarizing from above:

The proposed approach has novel aspects that is also demonstrated by its state-of-the-art results on benchmark IR tasks. On the other hand, I expect the authors to conduct more comprehensive experiments to facilitate fair comparisons with baselines, which I have also mentioned under weakness above. The authors should uniformly select the model configurations and compare with the respective baselines from the literature. Other than this, I have several questions regarding the model initialization and training process.
Answering them will also help the readers develop a more coherent understanding of the paper.

Depending on the author's response, I am open to reconsidering my scores.

[Update]
I want to thank the authors for doing additional experiments and answering the questions raised in the reviews. I am happy with the inclusion of the new results and so I am increasing my scores to 6."	3
The paper proposes a new loss function to leverage ranker to help retriever for text retrieval tasks. Though the idea is interesting, the evaluations now lack important components so the significance of this work is not very convincing. The authors are suggested to provide more details in the rebuttal.	3
Overall, the approach is interesting and could benefit the training during self-supervised learning phase. However, the evaluation requires a significant amount of ablation studies to prove the claims about the effectiveness of this approach.	2
The proposed method is technically sound, though additional justification is needed to for experimental results reported in this paper.	3
The work is a very interesting idea in a relevant subdomain of SSL research. The idea of using Conditional Independence to rate SSL targets, and the technique for calculating CI, is novel. However, the empirical results do not support the usefulness of this method (experiment #2) or are not compared to strong baselines (experiment #1). Without stronger empirical results to support the usefulness or relevance of this method, I do not believe this paper is strong enough for acceptance.	3
The paper provides new insights on a unified neural network framework that can cover a range of popular tabular classification methods. The derivation is dense but solid. Although performance gain is marginal from experiments, the framework itself deserves more attention. It would be even better to pose new directions to improve models further.	4
The paper, in general, is well written, however the structure (in particular the RW and problem setup) can be improved.  The contributions, especially on studying and investigating the variants of F2NN (wrt architecture and activation functions) are interesting and seem solid. However, I have some concerns regarding the experimental set-up as well as the discussion of related work, which are reflected in my score. 	3
"Strength:
- The authors proposed a novel and well-grounded deep learning model for tabular data classification problem.
- The authors provided extensive experimental results to understand the performances of the proposed method.

Weakness:
- The experimental results are not that promising. The performances are similar (not noticeably better) than SOTA. If the authors can claim the unique advantages of the proposed method in comparison to SOTA, I think I will increase my scores.
- The intuitions of the proposed method are limited. 

-----------------------------------------

Thank you for the detailed answers from the authors.
I carefully read the authors' responses and other reviewers' reviews.
Overall, most reviewers are concerned on the weakness of the experimental results. This is also my main concern about this paper.
Although the authors claimed that the proposed method is ""faster"" than another baseline, it does not mean that this method is ""faster"" than other alternatives (we can easily find other traditional methods that are faster than the proposed method with similar performance).
For XGBoost and GBM, the authors said that those are not suitable for CTR prediction due to the high cardinality, However, the authors did not show how they are working in those kinds of datasets in the classification problem. If that claim is valid, it would be much better to show some datasets with that characteristic and show the failure of XGBoost and GBM.
Based on these, I am going to stay on my original score (5) because the performance concerns are not well resolved. "	3
There are some interesting aspects and the insights are supported by a good experimental set. In my opinion, with a careful revision of the formalism in the description of the method, the paper can be accepted for ICLR.	3
This work studies the loss landscape of adversarial domain adaptation in terms of the smoothness of the minima. Despite showing somewhat empirically promising results, I found this work lacks a solid motivation for the presented analysis (i.e. why should we expect that strategies that improve generalization in the iid case would help in a non-iid setting as well?) and a sound empirical validation of the proposed approach. Moreover, the relevance of the presented generalization bound is also unclear to me at this point. All in all, I think this contribution seems promising but due to the concerns I raised in my review, I believe it is not yet ready for publication. 	2
Overall, the author has well established a positive correlation between smoothness of classification loss and generalization capability on target domain, and a negative correlation between adversarial counterparts. The method of acquiring a flat minima also brings promising results on multiple DA classification and detection datasets. However, as I can see the experimental studies of the proposed smoothing method are well-designed and theories are rigorously proved, I am not completely convinced by the novelty of the work. Hope the author can make further analysis and explanation.	2
Paper with theoretical motivation and results on both classification and object detection. However, the limitations of the proposed method are not clear. When would smoothness not be useful and how do the improvements depend on the other tricks for training Domain Adaptation models. 	2
Overall, I would highly recommend to accept this submission to ICLR 2022. The theoretical and empirical results are convincing and the paper is of high quality.	4
Good originality (AFAICT) and good results; somewhat unclear formalism and presentation of main concepts.	4
"I was a reviewer for this paper for Neurips 2021. Even though I voted for acceptance that time, I now see that some of my concerns and concerns pointed out by other reviewers are not fully addressed. For instance I do not see a comparison with this paper: https://arxiv.org/pdf/1601.02828.pdf  . I also do not see a comparison in terms of # FLOPS and latency for which the improvement seems marginal from the results added to the Neurips rebuttal. 

I therefore vote for marginal rejection. "	3
"The paper provides a personalization method by conditioning non-linear activations based on the speaker embeddings. There are extensive experiments. One concern is that the paper may need some additional clarification on details for better reproducibility.

"	3
"While the proposed method is simple to understand and solves an important problem, I do not think the paper is of the standards required for ICLR primarily due to the weak experimental evaluation and lack of clarity in the writing. 

++++ Updated review ++++

The authors have provided many clarifications and additional experimental results which do strengthen the paper. As a result, I have increased my score from 3 to 5. Unfortunately, the experimental evaluation is still a little weak which prevents me from increasing my score further. Similarly, I am not sure that the updated presentation is quite clear enough."	3
"The idea behind the paper is interesting. Instead of using MC Dropout of Ensemble methods trained with different seeds, use a single model and do multiple forward passes adding a perturbation to the parameters for computing the output distribution needed to estimate uncertainty.

As this is basically a multi-model approach (using variations of a single model), would be interesting to see a comparison with ensembles. In the end, ensembles are the best approximation to Bayesian Uncertainty estimation.  
For MC Dropout, I wonder about the parameters. I do believe the results are taken from one of the references, but details there are needed. MC Dropout easily outperforms single models like Entropy when the number of forward passes is large enough. How many models are used here? Would be good to see, at least 50-100 forward passes. It is actually surprising that MC Dropout performs on par or worse than random on CIFAR. 

Same with the proposed method. How does the method vary as a function of the number of forward passes? How does that compare to the same ensembles? A similar comparison was done (for classification) in: Chitta et al. Training Data Subset Search with Ensemble Active Learning arXiv 2020. The number of forward passes needed is larger compared to models trained with different seeds. 

The paper talks about uncertainty calibration but there is no empirical evidence. That would be very interesting to see. 

I would also be interested in understanding how the noise is applied to the layers and if noise in all layers is needed. That would have an impact in the inference time needed for computing uncertainty. An ablation study here would be interesting. 

Same for cityscapes. I am missing Random as comparison. 


For all the experiments, I would suggest adding Entropy as a baseline for comparison. Entropy is straightforward and provides very solid results. 



"	2
"Overall, the paper proposes a simple method for active learning, provides some theoretical connection with existing literature, and shows its effectiveness where it improves SotA. While there is room for improving the paper, I tend towards acceptance. 

--Update--
I think the authors have addressed most of the concerns and comments, and I remain positive about the paper."	3
I find the paper contributes innovatively to the field nds is worth the attention of the community. Hence I recommnend it for acceptance for the conference.	3
The theorems mentioend in this work is insightful. Authors also discussed how to implement compression and restoration jointly by a deep neural network.	3
"The paper addresses a difficult and interesting problem, and I see no problems with the math. It's not clear to me how impactful these results are, however, which leads to my positive but low-confidence score.

"	3
original scenario for source coding that may be of interest to some specific applications. The connection with optimal transport is definitely interesting. There is much to do regarding validating the proposed approach in real large scale applications.	3
"The relevance of the paper to practical data compression is unclear, and it seems that mathematical quantities without practical relevance
are considered. The stated theorems look false."	1
In summary, I recommend a weak reject. I am quite familiar with the related work in this area and I believe that the authors should put in more effort to identify where their approach fails in the real-world and subsequently see how they can relax assumptions made in the paper by analyzing these failure cases. Domain generalization research has seen a rapid increase in methodological work that propose various lenses to look at the problem, however, it is hard to disambiguate and see the value of these approaches when run in the real-world.	2
"The paper proposes an interesting approach based on the information bottleneck. However, I think the discussion and comparison with related work on more realistic datasets is necessary.

---  
After authors' response  
I appreciate the authors' feedback. They provided additional results on more realistic datasets (PACS and OfficeHome) and results for different $\mu$. After reading the feedback as well as other reviews, I would like to keep my score. As also mentioned by other reviewers, the theoretical guarantee of the algorithm is based on quite strong assumptions, which do not likely hold under practical conditions, and I am a bit skeptical about its significance."	2
Authors should address the weaknesses of the paper. Improved literature survey and comparing/contrasting with suggested papers will help improve the score. Stronger experiments section can further improve the score. 	3
Overall, the paper proposes an interesting approach to combine information bottleneck and model based domain generalization ideas recently used in [Ahuja et al.] and [Robey et al.]. However, there are severe shortcomings in the paper, the theory seems vacuous under assumption of access to $X$ (authors should please correct this). The proxy for $X$ is not reliable and instead $G$ model from Robey et al. should be used to get good results in large scale settings.	2
"I vote for accepting this manuscript for the reasons listed above. However, several important issues remain to be addressed.
"	3
The proposed method is interesting, but it combines too many ideas together without quantifying their contribution. Moreover, performance evaluation and comparisons with the existing methods are lacking.	3
Overall we found the paper to be clearly written and offer a nice formulation with a matching modeling approach for the important problem of rigid body protein docking. In that sense we definitely liked the work. However, the results are just unimpressive and no other insights/utility are offered beyond improved running time compared to standard techniques available.	3
"The authors perform extensive experimentations on many languages and perform statistical tests by training many models with different random seeds. The paper is also well written and easy to understand. However, there are several limitations in the scope and relation to prior work. Few major limitation in my opinion are - (1) Not relating transliteration to grapheme to phonemes. In fact this realization would have encouraged the authors to expand from Indo-Aryan languages to a much larger set of languages. Additionally, this would have also have led to nice set of relation to prior work. (2) Although it's important to consider internationalization of language technologies, the core-idea and the contributions made by this paper is not really novel. A lot of prior work have already shown the efficacy of this direction of research.

**After the Author Response** -- 
Thank you for the detailed response and the changes in the paper. The paper has indeed improved quite a bit from its previous shape. As the other reviewers also pointed out, I am still concerned about the novelty of the work so I am inclined to keep my score the same. However, with the new changes I think now the paper brings some insights that could be useful to some readers in the community. I would highly encourage the authors to submit this paper in a core NLP conference. Finally, please refrain from making significant edits between revisions as it really increases the burden on the reviewers. "	2
The main contribution of the paper - that transliteration can help multilingual language models is an important observation for related languages. Though many in the community have believed this hypothesis, there were no solid empirical data to prove this for NLU.  Some work exists to show the benefits of single script mapping for NMT. Hence, this work fills that gap. Concurrent work by Dhamecha et al (2021) also confirms the major finding of the paper. 	2
In summary, this work studies an important problem and tries to provide an effective solution. The experiment is done on a small scale and limited setting, and many underlying hypotheses of this work are not widely applied based on the insufficient results. I would not recommend its acceptance.	1
The paper does not provide a significant contribution and many necessary details are missing. It lacks the substance and depth to merit a strong recommendation.	2
"Overall, I think the paper significantly lacks novelty as only suggest an adoption of an existing method in a straightforward way that anyone would have thought of. The performance contribution is limited to a subset of languages and there is limited prospect that it would work in other languages and application.

**AFTER READING AUTHOR RESPONSE**

I have read the author response and very appreciate the authors' effort in responding. Here are my afterthoughts:
1. The novelty concern remained the same. The similar argument that ""method X is applied in task A but never in B, so it's novel"" has appeared frequently in past ICLR conferences. The general consensus is that this does not have sufficient technical novelty, as we value technical novelty in terms of machine learning techniques. It would have made a difference if the authors propose a novel technique Z based on X that makes things better for task B.
2. The authors acknowledge that this may only work in Indic languages, so it has limited application.
3. The paper, and the responses, show that the paper has lots of linguistic contribution. So I guess it will have better chance in core NLP venues, such as EMNLP.
4. I change the score to 5 to appreciate the authors' effort in the response and revision. However, this still does not warrant an acceptance for me.
5. The authors responded very late, on the last date of the deadline, which is not appreciative.

"	1
This paper introduces the task of joint visual-linguistic grammar induction, and presents models, metrics and empirical results on it. While I appreciate the impressive results, I am concerned about the unrealistic model selection process (comparing model outputs to a large set of ground-truth parse trees) and the unfair comparison (the proposed model has access to much more unlabeled text data than baselines).	3
I think this is a reasonable piece, with good writing and a nice set of experiments. It would be helpful for future research in this domain.	3
"Overall, the proposed method is interesting and inspiring. 
The idea should be interesting to both unsupervised parsing and multimodel communities."	3
"Although a certain amount of experiments is provided in this paper, it lacks some comparison results to show how well the proposed model performs compared with other existing methods. 
Adding SPH-related physics information to the model architectures can be a good direction to improve the data-driven simulator, but the models used in this paper seem to be simply replacing some parts in the classical SPH solver, which makes me have some concern about the novelty. "	2
Overall, the paper is well written, providing a very good introduction and overview of the modeling tasks, which further motivate the importance of using prior knowledge in such scenarios. That said, while the study seems correctly executed, the overall technical contributions of the work seem marginally significant. In particular, in a more general sense, the benefits of knowing particular characteristics of the dynamics (or the whole structure) and using them for modeling dynamics are known. However, in many real-world scenarios this is not the case, which leads to different approaches for data-driven modeling. The authors set-out to evaluate this trade-off (in a synthetic setting), but do not provide deeper discussion between the model-variants across different aspects (such as efficiency, accuracy, memory, scalability, robustness (to noise) etc) which might be of more interest for the ICLR audience.	2
"While the paper reads well and presents results carefully, I found it hard to formulate a clear take away from the paper. This aspect is reflected in the main review section and motivates my rating of ""not good enough""."	2
Although the paper is technically sound and the results are correct, the use of neural network-based models is not well motivated, given the fully physics-informed model (which outperforms all of the NN-based models) of the system does not require NNs (only standard gradient descent) to find its parameters. Comparing the relative strengths and weaknesses of the different models, and arguing the use-case for the NN-based models would greatly improve the paper. The paper demonstrates the methodology on synthetic data but would be more convincing with an example of it applied to real data.	2
I don't think the paper in its current form contributes much new insight for the ML community. There isn't much technical innovation in the presented model, and the main finding is quite obvious-- under the conditions studied there really isn't much of a case for using ML methods at all.	2
"This paper proposes an empirical study of different mechanisms often set in place to prevent overfitting in Deep RL.
Although the empirical findings are interesting and well-conducted, they don't fully cover the span of regularization methods. More importantly, the authors point out the challenges and weaknesses but the discussion is very shallow and they do not address them, limiting the impact of the contribution.
The paper itself needs an important effort of editing (it is still a very preliminary version, assembled in a rush), and a significant contribution to improve the generalization capability of RL algorithms, to have a meaningful impact for the community."	2
"Overall, I think this paper is the beginning of some interesting investigations. However, I am not convinced the experiments described in this paper support the claims made by the authors. In addition, I think it would be better to focus on a one or two questions, clearly state them, and try to answer them as clearly as possible and provide as much insight and depth as possible. In its current form, it is not clear what questions the authors are trying to answer and what the conclusions are. 

Given all the above, I do not think the paper is ready for publication. However, I do think the subject of the paper is important and would be of interest for the community. I encourage the authors to take the feedback into account in order to improve the paper, and resubmit at a future conference. "	1
The whole paper is not groundbreaking but makes its point in a clear and understandable way and seems to be natural consequence of previous research. 	2
"First, I do not see any novel conclusion in the paper:

(1) It is already shown by several papers that inverse models are helpful.

(2) It is already known that data augmentation techniques are required for better transfer.

(3) It is expected that using the same parameters for all games results in inferior performance compared to using game-specific parameters.

Second, the conclusion that improving visual encoders is not that important contradicts other works."	1
"Overall I think this paper has its own contribution. It finds that using an additional pretext header can improve the performance. However, I don't think the story itself is convincing enough and believe that might mislead readers. How the model performs with longer training iteration on ImageNet-1K also remains unclear. These prevent me from giving a higher rating.

--post rebuttal

After reading the authors response I raise my rating to 6."	2
I quite like the idea and experiments, but I have some reservations related to my questions.  Depending on the response, I’d be happy to raise my score. 	3
The paper claims proposing a general E-SSL framework, but ends up adding an additional loss from prior work (four-fold rotation prediction) to popular SSL methods.	2
My main concern is with the writing and novelty. The paper proposes a general framework. However, it is written as a combination of invariant SSL + four-fold rotation; see additional suggestions in weaknesses section. The authors should better highlight their novelty and differences from existing work. Hence, I recommend a weak reject.	2
The paper proposes a data poison attack to fool the adversarial training algorithms. The novelty of the proposed method is weak and the attack seems to be weak when the poison ratio is lower. I would like to recommend rejection.	2
The work presents a new technique that effectively poisons adversarially trained models at train time. However, the authors do not properly motivate the technique, the baselines are inadequate, and the paper can be sometimes unclear. For these reasons I give a score of weak reject.	3
Overall, I thought the threat model under consideration is interesting and worth exploring, but the authors may need to explain more motivations for their choice of threat budget. I would increase the rating if an updated draft addresses the mentioned issues in the main review.	3
As I mentioned in the detailed review, I think this paper silently changed the threat model and allow modification of labels -- this is a significant change, and actually puts AT to a worse situation compared to ST (at least there are intuitions that would support such an argument). More revisions are needed -- in fact I think a significant revision is needed.	3
The paper makes a contribution to automating the privacy analysis of algorithms within the elegant GDP framework, but doesn’t manage to make a strong case for the usefulness and applicability of the new tool it develops.	4
The paper lacks technical novelty and makes no real contribution/improvement to support the new transformation for characterizing introduced. Thus, I think it is not good enough for publication.	1
"Overall, while I do believe that this paper tries to address a very important problem and is formally sound, I am not fully convinced that there is enough novelty in the existing manuscript to justify the acceptance and therefore I believe that this paper is just below the bar of acceptance.
"	2
"## Recommendation

Overall, although I think this paper advances the understanding of GDP, I am unconvinced that GDPT yields significant novel insight that cannot be achieved otherwise and more empirical evaluations have to be made in order to determine the practicality of the tools proposed here. Due to this, I recommend rejection."	2
The paper provides helpful insights to connect methods for OOD detection tasks. It has both comprehensive theoretical and empirical analysis. The paper can be further polished to make it easy to follow. 	3
"The paper was poorly written and difficult to follow. I recommend that the authors try to make their presentation more clear and concise.

"	2
"The paper provides a good survey and insights of existing OOD approaches discussing their similarities and differences. While I commend the mathematical derivation  and analysis of multiple existing OOD methods, the novelty of empirical contributions seem marginally significant to me. I am open to increasing my score  if authors can address some of my concerns above, provide some judgments on more recent methods and/or further insights on the statistical significance (like standard deviation) of the top performing models i.e.  OE, BGC and SHARED_COMBI and provide evidence to significance of the  superiority of SHARED_COMBI over others. Especially since the scores in tables for OE and BGC seem slightly different from the ones reported in the original paper (comparing Appendix F since a different dataset was used in main table). 

### References:
[1] Neal, Lawrence, et al. ""Open set learning with counterfactual images."" in  ECCV 2018.\
[2] Thulasidasan, Sushil, et. al. (2021) ""An Effective Baseline for Robustness to Distributional Shift""\
[3] Mohseni, et al. ""Self-Supervised Learning for Generalizable Out-of-Distribution Detection."" AAAI. 2020.\
[4] Zhang et. al. ""Universum prescription: Regularization using unlabeled data."" in AAAI 2017.\
[5] Hendrycks et. al. Using self-supervised learning can improve model robustness and uncertainty. In Neurips 2019.\
[6] Golan et. al. ""Deep anomaly detection using geometric transformations."" in Neurips 2018.\
[7] Winkens, Bunel, Roy et. al. (2020). Contrastive training for improved out-of-distribution detection.\
[8] Tack, Mo et. al ""CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances"" in Neurips 2020.\
[9] Liu et. al (2020) ""Hybrid discriminative-generative training via contrastive learning.""\
[10] Liu et al. ""Energy-based Out-of-distribution Detection, in Neurips 2020.""\
[11] Li et. al. ""Background Data Resampling for Outlier-Aware Classification"", in CVPR 2020.\
[12] Hendrycks et al. ""Deep anomaly detection with outlier exposure"". In ICLR, 2019."	2
"Theoretical insights that unify different methods under the same light and allow new analysis are so valuable to the community, especially when they are so clearly exposed. The authors also show these theoretical insights are actionable, by drawing new conclusions later supported by empirical evidence. I think people working in the field will benefit from such a clear and encompassing narrative.

My review could be positively biased: Since I am not an active researcher in the field it was difficult for me to provide absolute meaning to the numbers in the experimental section, although on a relative scale I was able to follow the authors' conclusions. Because of my lack of expertise in this specific field I cannot provide positive nor negative feedback on this and on the related work reporting.

For these reasons I will take into great consideration all other reviews and discussion for the final score."	4
The authors present well-supported contributions that seem interesting, but do little to explain what problem their contributions solve or why their contributions are needed.	3
Overall, I don't find the paper very exciting as a follow-up work of some recent publications. The results' proofs seem to be oversimplified. And the convergence rates are only in the asymptotic sense. The assumptions also make the paper a mismatch for several of the authors' optimality and universality claims.   	2
The paper analyzes average case convergence rate of first order methods on quadratic problems and establishes very interesting results for different e.s.d.'s and knowledge of the e.s.d. only around the edges of the support. The main concern is the limited application of the results as only quadratic problems are considered. 	3
The contributions of the paper are probably correct. However, the mathematical contribution is too closely tied to previous work. Moreover, the experimental contribution is not significant. Finally, there are the writing/correctness issues mentioned above. When put together, these observations justify my low score for this paper.	2
"The paper is well-written and everything is explained clearly. But I don't think the paper contains enough technical and empirical novelty.

I consider this paper to be marginally below the acceptance threshold."	2
"This paper  give a simple example of tractability under the natural assumption of covering number (somehow necessary for PAC learnability). Even if it is a simple example, it is not trivial that this lower bound is achieved by an efficient algorithm, and the fact that the semi-supervised method presented in this paper works is not obvious. However, the author falls short of convincing that this covering number could a feature of real dataset that ensure tractability. For these reasons, my recommendation is marginally below the acceptance. If the authors can convince me that this covering number assumption has strong theoretical or empirical backing, I would be willing to raise my recommendation!
"	3
"This is a well-written theory paper, but I am unclear on the extent to which it accomplishes its stated goal of providing insights into how CNNs can be efficiently learned in practice relative to the existing literature (on neural network training and on learning theory), given the very general target function model and the fact that the final bounds essentially depend on a covering number for the entire space of input patches one will observe. The CNNs that can be modeled by the authors' target function model are essentially ""shallow"" models with a linear probe evaluation: no hierarchical structure is captured."	2
Based on my review above, I am leaning towards rejecting the paper based on the utility and scalability of the algorithm, and the lack of empirical study to verify the theoretical contribution of the paper.	2
This paper tries to address the difficult question of understanding CNNs through a simplified model. While this is its main focus, I believe more works needs to be done to get to this result. Indeed, if the authors could certify that their assumptions are correct and are reflected by the data, then I’d be inclined to raise my score. In other words, that the method proposed here is actually working and employs fully the structure of some (complex) data, and is not vacuous.	3
"This paper proposed a novel neural network-based de novo assembly method. Since the proposed method can reduce extreme computational costs during the genome reconstruction process, it could be a promising direction for de novo assembly algorithms.

However, the utility of the proposed method is not sufficiently demonstrated from the perspective of biology researchers and practitioners. Also, the technical novelty of the proposed method seems to be limited in terms of machine learning techniques. 

Therefore, I am leaning towards rejecting this paper at this moment.
"	2
This paper provides a promising approach to use GCNs to assemble genomes. To the best of my knowledge, this is novel. The demonstration is focused on synthetic data with no noise. While this can be reasonable approximation with HiFi reads, it remains unclear how it extends to other kinds of data, eg. short- and long-read RNA seq for which there exists many data. To be impactful, a comparison on more realistic synthetic data is important, such as noise added and longer genomes. While there are many good things about this paper to be excited by, critically, a demonstration on real data is needed. 	4
"The proposed idea in the paper is interesting, however, the limited set of results are insufficient to support its claims and highlight its usefulness for de novo genome assembly. 
"	2
Processing assembly graphs with graph neural networks is an intriguing direction, but this work hasn't sufficiently demonstrated its value.	3
"The paper provides novel insight into medical image segmentation. The technical introduction is sufficient and makes sense. The experimental results support the theoretical part. 

It can be hard to read and reimplement. The improvement over the baseline is not obvious.
"	4
Although an important part of the paper presents well-known theory, I like the idea of estimating closed curves and I suggest accepting the paper.	3
Given the weakness stated above, I felt that the paper is not strong enough.	2
"Pros:
The paper is well written and the proposal is very interesting from the point of view of the medical image segmentation.

Cons:
The mathematical explanation must be improved to help the understandability of the paper. 
The validation of the proposal must be compared with other wavelet proposals.
Authors should be fairer when adjusting the hyperparameters (not limited to the proposed model). "	3
"1. novelty is limited compared to the recent work on self distillation and instance-specific label smoothing
2. missing relevant baselines in the evaluation section"	2
"The idea is interesting and novel enough. However, considering the motivation and the experimental results, I vote for ""5: marginally below the acceptance threshold"".
"	4
The paper proposes a new formulation of an objective function for instance-specific label smoothing regularization to unify label smoothing (LS) regularization and knowledge distillation (KD), which is quite interesting but is also largely based on the CVPR 2020 work by Yuan et al. The paper further formulates the proposed objective function as a bi-level optimization problem and finds a closed-form solution for the inner level of the problem. Experiments confirmed the effectiveness of the proposed objective function on both image classification and natural language understanding tasks, while the improvements over the baselines are somewhat limited. Overall this is a solid study with reasonable contributions. 	3
"While the paper is interesting and the idea of performing ""distillation"" (pseudo) is appealing the experimental section needs to be improved."	2
The authors propose an interesting approach to unsupervised skill discovery tackling multiple issues with some existing prior methods. They also provide meaningful empirical results and analyses.	3
"Overall, the method proposed in this work is novel but the experiments, while showing performance improvements for the proposed method, do have several flaws which I point out in my review. If these issues can be addressed, I would be open to updating my score from rejection. 

=====================================UPDATE=========================================================
After discussion with the authors, I have raised my score as my experimental concerns have been addressed. "	3
Good paper that has all required components. The proposed algorithm is novel, the paper provides both theoretical analysis as well as a practical version of the algorithm, and the experiments indicate substantial improvement over prior works. 	3
"The paper has many issues and in my opinion the proposed algorithm is not viable in more complex domains. However, it is possible that some of the limitations can be lifted, such as using discrete skill variables only. Further, the paper empirically presents good performance on the challenging ant domain and contains several important insights that might be useful for the future researchers. Therefore I am leaning towards accept.
"	2
Overall, this paper is well-written and well-motivated. I think this paper attracts lots of researchers and will inspire future works in parameter efficient training. 	4
This paper provides a unified framework for parameter-efficient NLP, an important task when serving models at scale. The experiments provide new insights into the performance of existing methods, that, when combined, result in further performance improvements. I think this is a solid paper and recommend its acceptance.	3
Compelling framework to unify parameter-efficient transfer learning approaches, with interesting new methods emerging from it. The set of experiments is well-chosen overall, but the lack of standard deviations and the focus on generative tasks / enc-decoder models makes the results less robust and general. The writing would benefit from more work as well. 	3
I have highlighted my major observations from this paper above. While the paper is written in a very elaborate manner, I do believe it is still not clear if the technical contributions are advancing the state-of-the-art to clear the ICLR bar of acceptance. Authors should tease out the technical contributions around exhaustiveness and others more clearly and remove big sections on assumptions, metrics (can be written briefly with citations). I would also recommend trimming down the graphs to only keep the most relevant DAGs with a clear messaging.	2
There is a soundness problem with the paper. 	1
"Even though the idea of improving imputation is excellent, the logic used in this study is not robust and does not take into account true realistic situations.  

Use of only synthetic data, when we have access to many real-world datasets is not acceptable. How the synthetic data was created has a direct effect on how the methods will work. It is possible to use real-world data and introduce additional hold-out values to evaluate the methods. see for example: https://www.nature.com/articles/s41746-021-00518-0

The importance of real-world data is key here because the whole idea behind this paper is that missingness patterns are not completely understood. "	3
The paper is well written in general. However, the overall contribution may not be sufficient enough to reach the acceptance threshold.	3
"+ The method is interesting and effective.

- The contribution and novelty are limited.

- The experiments (and ablations) are incremental.

Overall speaking, the reviewer thinks that this paper stands at the borderline, but marginally above the acceptance threshold."	2
"This paper applies the instance-adaptive image compression method to video compression, and proves the effectiveness of the instance-adaptive data compression method in video through the implementation on two existing models, but I want to weak accept it because of the following problems:

a The method of this paper is lack of innovation. In fact, most of the work is done by combining other people's methods. There is no doubt that this paper applies the method of I-frame adaptive compression to video and combines it with other better models to get a certain effect. Authors need to highlight their innovative contributions.

b In addition, there are many errors in the references of the manuscript, such as inconsistent format, author abbreviations, etc."	3
The main concern is the novelty as a similar approach has been used in image compression. Basically, it is a borderline paper and I am open to other reviews. 	2
"A simple, clean preprocessing method to help map word embeddings across languages. The article is clear and well organized, and extensive experiments produce consistent gains in a number of cross-lingual tasks. The experimental setup raises a few questions regarding the significance of these improvements.
"	3
"The core SpecNorm method seems reasonable, but the iterative centering + SpecNorm + length normalization seems more ad hoc. Although the experiments are thorough and show improvements, I would like to see more principled motivation for method and/or deeper insight into why it works.
"	2
"The recommendation is based on the lack of introspection into the method itself rather than the empirical improvement. The work could be improved if it was better understood what the method is doing and how this ties into performance improvements. 

I believe the direction of this work is novel, but overall there is a lack of appropriate setting which ultimately makes it unclear what problem SN solves, and how this fits into existing pre-processing approaches. The case for SN is not fluently well-motivated and there is little analysis to confidently support that SN is superior to prior art or generalises existing approaches. 

I support the merit of this method with empirical utility and wide applicability, however, I recommend that significant further revisions are required prior to publication in a later venue. 
"	2
The paper proposes a simple method for improving CLWE with good empirical results. However, the impact may be limited as multilingual transformers are replacing CLWE in many applications. Therefore, my recommendation is weak accept.	3
"I recommend acceptance given how clear the paper describes related work, motivations, and proposed model. The authors perform an exhaustive evaluation of the proposed model with different language downstream tasks. Moreover, the model outperforms related work on the downstream tasks.
"	3
Overall, the paper has some interesting ideas but unfortunately has limitations, and fairly poor performance in practice. My actual score would be 4, but the rating is unavailable.	4
The paper is technically sound and well written. However, due to the two major concerns above (unrealistic MCAR assumption and missing important baselines), I can hardly recommend the paper as ready for publication.	2
This paper works on a more practial fairness scenario and gives a solid solution in principle. However, there still exists some issues about writting and minor technical details. If these issues can be solved well in rebuttal phase, I am willing to support this paper.	3
Due to the weakness written in the main review, I choose to give a weak reject.	2
I find the idea proposed in this paper to be interesting, and the fact that the proposed method is unsupervised by carefully parametrizing a part of the PDE solution with neural networks is also a plus point. However, I am inclined to weigh heavily towards penalising this paper for a lack of an extensive empirical analysis, since I am not convinced on the efficacy of the proposed solution in comparison to existing approaches (learning or otherwise). 	2
In summary, I do not think the paper in its current form meets the standard of ICLR hence recommend a strong rejection. 	1
To summarize, I regret that I am inclined to reject the submission since  the contribution of this paper is incremental and is not be sufficiently novel for ICLR.	2
"This is a very narrow paper, which focuses on a particular method for solving linear PDEs.
However, there is very little motivation.  There are already decades worth of methods for solving PDEs. Using neural networks probably does worse than most off the shelf methods.  It would take a lot more evidence to be convincing that this method is better than the best standard PDE method for each problem. "	1
My current assessment of this paper is slightly below the acceptance threshold because I think the intuitions are not well explained and there are questions that I think the authors should explain to readers. 	3
This paper is largely based on prior work (Zhang et al. (2021))  which follows the same setting and experimental design. The authors propose to prove the effectiveness of $l_\infty$-distance net where they come up with a regularized hinge loss to learn robust classifiers guided by their proposed Theorem 3.2.	3
The paper is well-written and the proposed method has an impressive empirical performance. My concern is mainly on the experiments and reproducibility.	3
Overall, I like the paper (in particular, the design of l_infty distance net, which is not the focus of this paper but of Zhang et al. (2021)). The design goes towards solving the robustness issue by new network design, rather than regarding the neural network as a black-box. The experiment result on 8/255 is promising and exciting.	3
This paper proposed a novel idea for using the transformer in the routing network. However, the paper missed many important evaluations including comparing it with SOTA MTL approaches, model effectiveness, etc. Besides, the paper was not well presented which makes it hard to follow. Overall, the paper needs a major improvement.	2
The paper presents an improvement to a previous multi-task learning model called routing networks. The proposed improvements are not well motivated. The experimental validation is also limited given the lack of comparisons to prior work. Thus due to the limited novelty and poor experimental validation, I leaning towards rejection.	1
The proposed method looks novel and reasonable but the experiments cannot well support the effectiveness of the proposed method.	2
I have some concerns on this paper; the novelty of the proposed method and a limited set of experiments. The authors should address the concerns clearly and compare the proposal with other similar works.	2
Please refer to the weaknesses of the main review.	2
Overall, this paper address an important problem of performing attention across different layers in a neural network and proposes a novel approach that carefully handles the computational complexity.  The method is demonstrated to work effectively in several image restoration methods.  Numerically the results are only slightly better than competitors, but it does reflect an advance.  This reviewer is somewhere between marginally above and accept for this paper.	3
"- This paper is technically sound and quite new. However, the performance difference according to the proposed method is not significant in most experiments.

- In other words, it is difficult to say that the value of the proposed method is that high. Even technically, I think that difference with the existing methods is not significant. (Of course, the proposed method have different points compared to existing methods)
"	2
Please see the paper weakness. 	3
Although the paper tackles an important problem, it has several weaknesses in terms of motivation and missing baselines. Lack of differentiability and poor scalability severely limits the applications of the proposed approach.	2
I personally like this approach, but I feel that there are key missing questions that the paper does not answer in order to determine whether the claims are successfully substantiate or not. I am willing to increase my score depending on whether the author's response address my questions.	4
The paper presents a solid approach to pathplanning that solves the problem in an interesting way. There is a good amount of novelty and  experimental evidence and the approach is technically sound.	3
The method is good as a pipeline but it needs few careful experiments to position the claims with respect to state-of-the-arts. While part of the method's philosophy is already existed like neural guided path planner, the other part where the resolution invariant is being claimed is good. Some of the comparisons are important to position the entire pipeline against SoA  and few metrics needs to be shown (like the time) to understand the computational complexity.	2
The proposed work tackles an interesting problem and provides a useful technique to perform automated sparsification of graphs based on user required edge kept ratio and property specific objective. However, the overall  approach is not novel from previous works in reinforcement learning on graphs and the differing design choices are not discussed or analyzed adequately. Further some important comparisons are missing and it is not clear if this method can scale in performance critical applications as it is shown by the authors that time vs performance tradeoff is significant depending on size of sampled subgraph. Hence, this work is not ready for publication in its current form.	2
"The paper is not well written in terms of the lack of clarity in stating technical details and the unconvincing presentation of simulation results. Please refer to Main Review session for point-by-point questions.

The paper is not ready to be published."	3
The paper provides a novel RL framework for learning based graph sparsification. The work is technically sound and well-organized in writing, However, empirical validation on large-scale graphs is missing, which prevents me from an acceptance decision.	3
"In summary, while I think that this paper is well-rounded and introduces a novel technique that outperforms a wide range of baselines, I am unsure about the significance of the considered problem. The paper has lots of headroom for improvement in terms of clarity and in terms of the experimental evaluation, which would help clarify its significance, the reliability of the results, and the necessity of core architecture choices (by adding additional ablation studies). Overall, I think that this paper is not ready for publication in its current form, but I am positive that many of the described issues can be addressed in a (potentially major) revision.
"	3
While I think the paper addresses an important problem, I have major concerns about the claims the authors make given the experiments they have. Specifically, I've raised questions on the choice of baseline agents and comparison with them. Furthermore, the writing can be improved significantly.	2
"While the premise of generating new environments to enable more diverse skills to be learned is intriguing, the experiments do not convincingly demonstrate this. The paper lacks evaluation on more complex environments where DADs is known to work well, and also doesn’t include quantitate performance metrics for the simpler bipedal walker considered. 
"	2
Although the method is interesting and tackles a relevant problem, the experimental evaluations are not mature enough to provide a clear indication of the method's merit.	3
"ROEL is a logical and synergistic combination primarily of rewardless learning (DADS) and open-ended learning POET.
The algorithm has a clear applicability and a good presentation.
The experiments are reasonable and support the primarily claims of the paper, although the experiment measure indirect aspects of the claims as direct experimentation is intrinsically (to the claims) difficult.
"	3
Since the methodological novelty is limited, the paper could benefit from more comprehensive empirical experiments. Since ODST is quite similar to Noisy-Student, it could be helpful to see experiments on Imagenet. If that is not possible, experiments on more datasets such as STL-10 or SVHN could help, as currently the only experimental evidence for ODST is on CIFAR-10/100. 	2
The authors propose an out-of-distribution aware self-training method with significant improvements over several baselines in the open-world SSL setting with extensive experiments and ablation studies.	3
Given the above considerations, I cannot recommend acceptance at this time, unless the above issues are addressed during rebuttal. I would increase my scores if the above concerns are clarified. 	3
This paper focuses on the open-set semi-supervised learning algorithm and proposes a new pseudo-label selection strategy to improve the robustness of SSL algorithms with open-set unlabeled examples. However, the proposed algorithms are mainly a small modification of current pseudo-label selection-based algorithms, the contribution and novelty of the proposal are limited.	2
As listed in **Main Review**, the novelty seems limited and the proposed method is complex yet not well explained. Therefore, I recommend rejection.	2
The paper is a nice MARL approach to the IM Capacity Problem. I like the experiments and the modification of the public data used. The baselines are not ideal and the reward function doesn't resemble a real capacity issue - but it is a good step towards a more general IM solution	2
"Same as above 
I'll update my recommendation based on the answers of the authors. "	3
This paper presents some novelty in addressing the issue of large number of SKUs (agents) in applying MARL to solve the multi-SKU inventory replenishment problem, by exploiting the structure of the problem and devising a model-based approach. The empirical results show that the proposed method achieves better sample efficiency in the global environment than the model-free baselines. However, there are several important algorithmic issues not investigated in this paper, and the important related method in the literature is not compared with.	2
"The introduces a potentially useful approach for an important real-world problem. But the benefit of tackling a real-world problem vanishes because the proposed approach has been evaluated in very simple environments and with baselines that are not very relevant for this problem. The paper would have higher impact with some of these improvements:
- The algorithm is evaluated in a realistic environment.
- The algorithm is compared with relevant IM baselines (operation research approaches actually used in the real life) even in this toy scenarios.
Also, the paper misses two important lines of related work, namely potential and mean-field games, which could offer new insights to the  proposed approach.


Meta-review
=========
I think the paper tackles an important real world problem with an interesting approach and, although the experiments were a bit lacking, the authors have included a sensible baseline and a more realistic scenario.

I still see some scalability issues and I would appreciate a deeper and insightful discussion with related approaches, like potential and mean-field games. But I think the paper is in better shape than before and it might be useful for other researchers in the field.

So I am happy to increase my score to 6 if the other reviews don't see major flaws."	3
The problem studied in this paper is important and well-motivated, and at a high level, the proposed approach seems reasonable and to work well. However, descriptions of several critical components of the theory and evaluation are either vague or not provided, making it difficult to truly evaluate the soundness and performance of the method.	1
The paper does not provide technical justifications for the claims. The approach is compared to a classical technique that is not expected to be very efficient when applied to a large and complex system, in general. 	1
In summary, this work is a good attempt on extending the traditional method with ML techniques. However, due to multiple concerns I have (see weaknesses), I recommend reject. On the other hand, if the authors can improve the work that can satisfy the standard of ICLR, the community should welcome such works dealing with the realistic problems.	1
"This work focuses on a meaningful task and constructs datasets for better evaluation, but the limited contribution at the methodological level makes me inclined to think that it has not yet reached the bar for a top conference.
"	2
"########################################################################## 

Reasons for score: 
 
Overall, I vote for rejection. I like the application side of the paper. But, technically, I am unable to see significant contribution. Moreover, the paper lacks in comparison with the state-of-the-art systems in the relevant domain.


########################################################################## 

Questions for rebuttal:
Feel free to answer to all the concerns."	1
Overall I think the paper addresses an important problem, but it needs a bit more justification on why RL is necessary for this type of tasks, and some details of the paper can be clarified further.	3
Overall, this is an OK work with limited novelty. I have some concerns about experimental results (see Main Review) which need to be clarified in the author's response.	1
The proposed algorithm is incremental over the existing ones and the corresponding impact on results is marginal.	1
This paper is an application paper, on Lithium-ion battery parameter estimations. While I generally appreciate the application, I think the paper is thin. Method-wise, it is incrementally novel compared to the prior work GTS. Experiment-wise, given this is an application paper, authors are expected to show more experiments and give more analyses, including more datasets and compared methods. Even the given experiment shows the proposed method is only marginally better than the only compared method. The paper is neither well-organized nor self-contained. The authors should keep improving the paper.	2
The idea of incorporating causal structure learning with graph-based forecasting is interesting and novel, but the motivation is unclear. Moreover, the adjacency matrix used is not guaranteed to be acyclic, and, in that case, the authors should explain how it is related to causality.	2
"
Other comments:

- The functions of g_1 and g_2 are not explained.

- In 3.1.2, the equation of W_(Q*A)Y is confusing. This function is not mentioned in the rest of the paper. 

- In Fig3,4,5, the results of the proposed method compared with the baseline seems not significant. "	2
"However, the reviewer does have some concerns on the paper.
(1). A major concern of the paper is about the model's novelty. The reviewer has doubts on the argument that a new combination of existing techniques (BN, LSTM, S&E, skip connection) for the task of video prediction is significant enough to publish in ICLR. It will be important to show what is the design motivation of the combination and what it can outperform existing models on the task of video prediction.

(2). A comparison of components of different models is needed to help clarify the novelty of the proposed method. It will be also easier for readers to understand why previous models suffer from underfitting and how the model is more data efficient.

(3). Although the authors provide experimental results on 4 datasets. However, most of them contain only two simple baselines and the results on these datasets are not consistent. It will be more convincing if the authors can provide results on future and counterfactual predictions of the video dynamics on the CLEVRER[A] dataset and compare the proposed model' performance with recent stronger backbones like transformer [B], GNN[C] and Differentiable Physics Engine [D].

[A]. Yi K, Gan C, Li Y, et al. Clevrer: Collision events for video representation and reasoning[J]. arXiv preprint arXiv:1910.01442, 2019. 
[B]. D. Ding, F. Hill, A. Santoro, and M. Botvinick. Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures. arXiv 2020.
[C]. Chen Z, Mao J, Wu J, et al. Grounding physical concepts of objects and events through dynamic visual reasoning[J]. arXiv 2021.
[D]. Ding M, Chen Z, Du T, et al. Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language[J]. arXiv  2021. "	2
The paper focuses on an important problem on scaling video prediction and manage to overfit to large-scale data as the first step followed by improving generalization by data augmentation. However, it is not clear to me why and how their proposed method is crucial to achieving this. 	2
The paper proposes a simple and scalable variational video prediction model FitVid, which could overcome the underfitting problem better.	2
"I think the paper could provide more insights on why the FitVid network is able to overfit without bells and whistles using even fewer params than previous methods. It would make the paper even stronger. 


-------------------------------------------------------------------- Post rebuttal
I have carefully read the reviews and the rebuttal. The main concerns revolve around a lack of novelty. The authors rebut by providing a detailed study of the proposed components. I think the rebuttal addresses well why carefully designed training schedules are no longer necessary, which might be useful to the community. Novelty-wise, it is still a bit limited, and the overall impression to me it is a bit empirical. But given the strong performance, I am keeping my original rating."	3
The contribution is novel and significative, although there are a couple of things that should be addressed in the revised version of the paper. 	3
"The paper is not self-contained. Also, even though their motivation for the improvement is to improve the computational efficiency, the experiments did not provide any results on that. There are some issues in the mathematical statements and proof thereof. 
"	2
While I like the idea, I think more work needs to be done on the structure of the paper and the experiments. I think section 4.2 could be cut, allowing for more space to do experiments and a more fleshed-out discussion section.	2
Overall, this paper proposes interesting ideas by building a mapping function from the prior to the posterior. However, the authors did not explain their ideas well and there might be some mistakes in their formulation. The author may also need to provide more detailed empirical results and corresponding analyses.	3
"Though this paper proposed a different way to explain GNNs, using message flows instead of nodes/ edges/ subgraphs, it lacks clear intuition of the novelty of using message flow. Relevant work is missing. Contributions are somewhat incremental compared with the existing Sharpley-value-based approaches for GNN explanation. 
"	2
Basically, the idea is interesting and the solution is also sound. I would champion the acceptance. However, I have still some concerns as listed. Thus, I gave a weak acceptance.	3
In a word, although this constitutes important work with a rather nice original intuition, the technical contribution is limited, the concept of Shapley Values was already leveraged by two GNN explainers (SubgraphX and GraphSVX -- which are not both used in the evaluation), and the evaluation is a bit light.	2
"Check the main review.
"	3
Overall, the paper is relatively hard to follow, and it would be greatly appreciated if the authors can kindly clarify the questions/concerns as detailed in `Main Review`.	2
It is meaningful to establish a guarantee for generalization in fairness-aware learning, but theorem and proof processes are too standard, so novelty seems to be insufficient. Considering practice, additional theoretical results or more diverse experiments expected to be added.	2
While the topic of the paper is interesting and some of the results look promising, I do not recommend acceptance due to the lack of clarity about the objectives of this work and the insufficient comparison to prior work.	2
"Although the paper explores an important gap in looking at fairness in the PAC setting (with generalization), I recommend a reject given my listed concerns about novelty. In particular, the proof techniques are not novel and the results of Theorem 2 does not seem to extend the prior work of Agrawal et al. (2020). Furthermore, the experiments do not seem well connected to the theoretical results explored [1]. The  notation choices, typos, and missing definitions further cements that the paper needs major revisions.

[1] Personally, I would even recommend excluding the experiments in the main text for further analysis, i.e., how Rademacher complexity terms might change when considering different fairness criteria; or connections of $ Z_{S} $ to known fairness quantities like data representation rate."	2
"The paper provides an empirical study to understand the role of pre-training in lifelong learning.  Given that the paper draws conclusion based on empirical results, it's important to have a systematic study over the existing continual learning settings and baselines (e.g.,  various training/evaluation settings,  evaluation metrics such as Backward/Forward transfer,  datasets, etc.). The current manuscript pushes one step further towards understanding the role of pretraining, but still more studies can be added.  
"	3
"In summary, authors provide a study with a lot of potential, but unfortunately the paper in its current state does not provide substantially novel conclusions or innovations. While the proposed experiment are substantial and do provide additional confirmations, main conclusions are mostly well known (pre-training achieves better performance, larger datasets and models are better), or already discussed in the lifelong learning community (flat loss landscape).  The proposed SAM optimisation constraint is a direct application of another work, and its interaction with different continual learning methods, which often rely on optimisation constraints, is not discussed. 
Finally, the paper is presenting a lot of content which is severely hurting presentation and readability, with essential elements relegated to supplementary material or missing. I would recommend refocusing the work or submitting in settings where page limit is not a concern. 
"	2
Although there is no novel algorithm or model introduced in this paper, but the direction it explores to minimize catastrophic forgetting in lifelong learning methods via pre-trained weight initialization is interesting and practical as large pre-trained models have become a norm because of the wide range of diverse knowledge they can capture. Even though the Finetune method is not the best performing method in all of the experiments (however it outperforms in most of the experiments), the key conclusion I can take form this paper is that instead of completely focusing on improving upon fine-grained details of forgetting aspect of lifelong learning, the focus should also be on learning more general representations as they appear to result in robust lifelong learning.	2
"As explained in the review, the paper has several technical merits and interesting experiments but is clouded with ill-justified and what seems like arbitrary choices and solutions. Currently, I feel these hurt the potential value of the paper to the field. 

My view is that the paper in its current form should be rejected, but I suggest the following improvements:
- Discuss regime adaptation (increasing number of steps) similar to Hoffer et al. by performing additional experiments with a varying number of epochs. I understand that a complete 117K steps training in full-batch is too cumbersome, but the current single datapoint of 3000 epochs seems arbitrary and not justified in any way.
- Regularization on baseline SGD -- does it help? will be very interested to see a discussion either way.
- LR adjustment and gradient clipping justification -- perhaps additional measurements of gradient noise may show why these are needed over ""traditional"" scaling techniques.

If authors work to lift these concerns in their rebuttal, I will consider raising my score.



----- Post rebuttal update -----
The authors answered most of my concerns, so I'm updating my score as promised. I still feel some of the choices are arbitrary (or at least not substantiated enough) and may apply to the specific case presented only (dataset, model), so I will raise my score 5->6."	3
This paper provides convincing contrary evidence that shows stochastic mini-batching by itself is not unique, and can be substituted with explicit regularization, and techniques such as gradient clipping.	4
The paper provides a strong contribution to the community. The main conclusions of the paper are supported with strong empirical evidence. There are a number of improvements / clarifications that can be added to the paper (which I have listed above).	3
The achievement of the paper is an empirical result: good validation accuracy on CIFAR 10 using full batch training and a number of regularization techniques.  But the paper makes a claim of implications for deep learning theory which is not substantiated. 	2
"Update: I am satisfied with the author's reply. I think the message is sufficiently novel and important to be published at ICLR. Though I actually would like to give a 7, due to ICLR's scoring constraints, I can only give a 6. I do not feel that giving 8 is appropriate because this paper only suggests the possibility of SGD noise being non-essential, but does not give a definitive answer. 

To be specific, the main speculation/conjecture is only validated for a single example, it is completely unclear whether these results are generalizable or not. While I do find meaning and insight in the single example the authors give, I feel that more convincing empirical evidence is required to obtain a score higher than 7 and, more importantly, to reduce the speculative nature of the present conclusion.

--------------

I lean towards acceptance because I find the discovery surprising and important. However, I give a 5 because I feel that the authors missed one crucial conclusion, which might mislead the readers, and missed one crucial control experiment. I will change the score if these two points are addressed satisfactorily"	3
"In conclusion, I found this paper interesting, and tackling a rather novel problem setting. However, it currently is quite theory centric albeit without contrasting the novelty of their analysis compared to previous works as much as I’d like it to.
In addition, the empirical instantiation is rather divorced from the analysis, and is not novel compared to the large body of literature assessing model-based regularization and representation learning.
"	3
The novel problem setting proposed by the authors is of practical interests and the authors's proposed solution is well justified. Though the proposed method does not provide significant benefit when it's hard to learn the transition dynamics, the novel problem formulation could inspire future work along this line to develop new methods that can achieve better transfer for more challenging environments. 	4
"I generally like the idea of the paper but in my opinion the paper is not ready yet for publication in ICLR. The main reason for that is the lack of experimental evidence and lack of comparison to previous work exposed to ablated variants of the proposed problem setting. Moreover, the application of the methods seems quite narrow given the assumptions of drastic representation changes during evolvement of environments.

*** update of the review ***

I would like to thank the authors for their thorough submission update. The other reviews also share some of my criticism and the authors imho did a very good job updating their work. Exposition and motivation are much clearer know the the experiments have improved considerably.

Bottom line: an increase of my score is definitely justified and I will increase my score to 5. The reason while I am still not leaning towards acceptance is because I still have concerns:
The fine-tuning experiment is not what I actually meant. In the experiments only the policy head is kept but I suggested to keep on training with the new representation (on the cases where the observation just gets bigger; just add the missing neurons at the beginning and initialise them). This should be faster/better. 
And against such a baseline, I am still missing an ablation study that answers the following question: How much different must a representation be such that the transfer approach is better?

P.S. there are two typos:
- feature is corresponding [to] which old observation feature.
- Page 8, last line: 'Fine-tine' instead of 'fine-tune'"	2
"While the paper often is well-written and considers an interesting problem, the motivation for the problem is lacking. Additionally, lack of technical clarity on both the empirical and theoretical sides makes it hard to assess what the paper really proves. At last, the empirical evaluation is problematic -- some results are not statistically meaningful, many parameter choices are not motivated and it is not clear that the baseline DQN implementation works well in the first place.
"	2
Overall interesting paper. It explained the proposed idea very well. However, the experiment section is incomplete. I think the authors need to show how the proposed method perform using multiple datasets (i.e different than the Wireline trucks) such as (ImageNet, CORe50). Moreover, they need to compare against the REMIND network (Hayes et al. 2020), since the main contribution of this paper is to overcome the REMIND drawbacks.	2
The paper is quite below the standard of ICLR in all aspects of novelty, experiment, and writing. 	1
I think the contribution and novelty of the proposed method is somewhat limited and fails match the standard of ICLR.	1
Please refer to the Weaknesses	2
While the paper has a couple of good ideas, it is hard to follow, does not introduce and validate novel concepts and methods, and models the errors in an unrealistic manner. 	1
The paper takes a very simplistic approach to classification of these sequences not taking into account any biological domain knowledge. There are concerns about justification for the proposed model, the experimental methodology and evaluation.	1
"While the paper aims to investigate a relevant problem, the ICLR venue may not be the best place to present the benchmarking work. It lacks new contributions towards the method or application domains. The paper also requires major revisions and additional results to improve its clarity and to support its claims. 
"	2
An interesting paper with a self-supervised novel architecture for node clustering using an objective based on missing edges in a digraph. There are some concerns regarding the empirical gains provided by the method.	3
I have concerns regarding both the problem setting and the empirical evaluation, so I do not view this paper as ready for publication at this time.	2
The paper proposes an interesting approach to end-to-end flow-based clustering in directed graphs. Nevertheless, particular aspects of the paper, especially those related to the properties of the loss function and, partly, the experiments, need further development.  	3
"In this work, the authors try to address a major obstacle in simulation-based inference. Very few SBI approaches can operate in high-dimensional parameter spaces or with implicit priors, but GATSBI constitutes a convincing proof-of-concept that ought to be investigated further. Experiments are carried out properly, although the experimental validation could have been more thorough. The technical novelty is limited.

Despite some acknowledged weaknesses, I am overall positive about GATSBI. I am willing to recommend it for acceptance."	2
Overall, while the paper is written excellently, my doubts about novelty and the extent of evaluation are significant enough to put the paper slightly below the bar for acceptance. More extensive evaluation and/or discussion in the context of the prior work would help.	2
"Technically, the novelties are limited, as both the original GAN and the reverse-KL GAN are well-known in GAN research fields.
However, if the simulation-based inference (SBI) for black-box simulators is quite important in some research fields, then there are certainly contributions from this work.

Empirically, the performance of the proposed GATSBI is only comparable to the baselines. "	2
The paper has big potential impact. However, some of the extensions in the paper don't make much sense as presented.	3
Overall, this is an interesting, but not groundbreaking paper with a few issues around clarity and the empirical evaluation, so I am currently **leaning towards a reject**.	2
In summary, I raised several concerns about the theoretical and empirical significance of the proposed method, which currently prevents me from rating this paper higher. I will consider raising my evaluation if the authors address my concerns or point out my misunderstandings.	2
"
The proposed approach is suspicious on theoretical grounds, in that it relies essentially on the ratio of some estimation of training uncertainty and density in representation-space in order to estimate epistemic uncertainty.

The experimental results are also worrisome because it does not look like appropriate efforts were made to tune the hyper-parameters of baseline methods.

Finally, there is no attempt to actually compare the estimated epistemic uncertainty with the ground truth epistemic uncertainty."	2
I think this paper contributes to the SF research areas, but the motivation is not well discussed. Also, there are some details, analysis of results should be well discussed. Based on this, I give a borderline reject currently.	3
"In summary, the paper is an interesting effort in extending the linear SF framework. 

But why the linear assumption in SF framework is bad is not well supported, especially when the features can be an expressive and arbitrarily non-linear function of observations. 

The experiments failed to show very convincing results due to simple environments and problematic experimentation setup.  

The authors are encouraged to address these concerns. It would be a nice contribution if the concerns are resolved. "	3
Despite enjoying the reading of the paper, the main reason for my recommendation is that due to the lack of discussion / comparison with another method that adresses the non-linearity of the reward function to learn accurate SF, I am not convinced by the necessity of the xi-function, which is the novelty of the paper.	2
"I am in the borderline mode for this paper. Due to the difficulty of the generative modeling problem associated with $\xi$-learning, I am not quite convinced that the generalization presented in this paper would actually lead to something more expressive than SF in the types of setting where the linearity of SF proves limiting. That said, this paper does have merits and adds something new to the SF literature, as mentioned in the main review above. 
"	3
Interesting approach based on the notion of Zone of Proximal Development for modeling goal-oriented reinforcement learning with social partner assistant 	3
"The authors consider a solution of a task (5-block manipulation) using ""self-directed""/goal-conditioned learning and social learning. According to the reviewer, the contribution of this work is limited, since it is difficult to understand if the solution generalizes to other tasks. In fact, the evaluation is based only on that specific task.
"	2
Generally, this paper propose an innovative research question in teachable autotelic agents. However, some aspects of the structures, especially the semantic knowledge graph constructions.	3
Given the above comments, I believe that the paper lacks of technical novelty, numerical results and relevant comparisons with several existent methods in the literature. To conclude, I believe that the work contains some valuable ideas which deserve to be further investigated (both theoretically and practically)  but in its current state, the contributions and the presentation of the results are too short in terms of exceptions to be accepted to be published in ICLR conference. 	2
"
I agree with the general sentiment of the paper, that having a framework for auditing privacy could be helpful. But such efforts need a much more detailed and justified approach, arguing for “complete” attacks that at least “capture known attack techniques so far”. Otherwise, just focusing on 5 attacks might be dangerous as that might suggest that schemes are private while they are not. I also think that the paper needs to be much more clear in its criteria that it proposes (in addition to the success rate of the attacks) to be part of the “report” on privacy.
"	3
"This paper is well-motivated and the proposed framework could potentially be impactful. However, there are many empirical limitations on how to apply the attacks for auditing state of the art models. Furthermore, the evaluation and comparison of baselines methods are not thorough. I therefore recommend a weak reject.
"	3
The theoretical results are novel and solid to me. 	3
I find the paper's findings quite interesting and to be the best of my verification correct.	3
"Please provide a short summary justifying your recommendation of the paper.

**Strengths:**

1) The math is solid and seems well developed.
2) The results are interesting as they tie generalization to Lipschitz continuity.

**Weaknesses:**

1) The abstract and introduction are poorly written
2) The implications of the mathematical results have serious potential logical problems. I asked the authors to clarify in case there is some confusion. However, as it stands, it seems to me that their limiting-Lipschitzness idea does not actually reflect good generalization in practice, which is what they claim. This is because in their framework, one can use the 0 function for generator and loss and “generalize perfectly” although this is not intuitively what we want.
3) The consistency results are also questionable because they have a hypothesis in the theorems that is almost as strong as the result, which causes an almost circular reasoning. To be blunt, the actual logic is correct, but their result is very weak given this hypothesis.

If the authors had addressed these two weaknesses, I would have given them an 8 since this paper is solid otherwise.
"	3
While the paper proves several generalization bounds, its main result (Theorem 1) models a neural network by only its Lipschitz constant and tries to bound the generalization error uniformly over all 1-Lipschitz functions. Therefore, the bounds grow exponentially with the data dimension and lose their power for even moderately large data vectors including all practical deep learning settings. Therefore, I think the paper's analysis does not help with understanding generalization in GANs and needs to be significantly improved by considering the design of neural net players and the optimization algorithm.   	2
"My vote is -  marginally above the acceptance threshold due to the aforementioned reasons.
"	2
The authors presented a link between game theory and domain-adversarial training. In similar topic, such as GAN, such link opened new research perspectives and impacted positively the area. I believe this could be also the case here.	3
This paper is well-written and easy to follow. The theoretical contribution is solid, and the experimental studies show the superiority of the proposed method on the domain transfer task. As a consequence, I think the quality of this paper is marginally above the acceptance threshold. 	4
The proposed method is theoretical founded and the experimental part supports the claims and the analysis is novel. I vote for accept, good paper.	3
A simple combination of existing techniques, with poor experiments to support any conclusion. No story.	1
"While the motivations of the paper are well-founded, there remain multiple significant concerns, in particular over the methodological details and the comprehensiveness of the evaluation.
"	2
While the work shows promise, particularly in the medical imaging domain, more experiments and comparisons with existing methods (as mentioned above) are required to improve the paper further and justify the use of EV and multi-objective function to handle the imbalance between sensitivity and specificity. 	2
The motivation for this work appears to be flawed, the technical description is unclear and lacks novelty, there are flaws in the experimental design, and the paper contains several errors and lacks clarity.	1
Overall, I like the idea of the value function lower bound, but don't think the paper is ready for publication yet. The paper could benefit from some tighter analysis and better examples showing the benefit of the algorithm. Moreover, the current experimental results are not quite convincing as explained above. 	3
"This paper is studying an important problem, but is missing a discussion and comparison to a number of prior works. I therefore am voting to reject the paper.

-------------------------
Updates during/after review period:
* 11/23: Increasing score 3 -> 5 after new experiments showed that the proposed method outperforms other value estimation techniques.
* 11/24: Increasing score 5 -> 6 after clarification of difference with (He 2016)."	3
This approach is not novel and does not provide any meaningful insight, theoretical or empirical, over existing papers. 	1
Interesting work which can be of interest to the general RL community and lead to follow up research.	3
Overall, I believe the theoretical result of the paper has merit but the presentation need to be improved 	3
I thinks this paper could be improved substantially, but currently would like to recommend reject. 	3
The reviewer thinks this paper contributes a new and effective gradient based optimization solver. Overall, the reviewer would like to recommend acceptance.	2
The NAG-based MCMC sampler extends the underdamped Langevin with a drift term and Brownian motion and is proved faster than the alternative. I would recommend this paper to be accepted.	3
"Please the authors refer to 

Bin Shi, Weijie Su and Michael I. Jordan,  On Learning Rates and Schrödinger Operators, https://arxiv.org/abs/2004.06977

Bin Shi, On the Hyperparameters in Stochastic Gradient Descent with Momentum, https://arxiv.org/abs/2108.03947

"	1
"The method is novel in the sense that it is derived based on the idea ""inject noise into Nesterov’s Accelerated Gradient"".
However the derived theory is not significant as it rely on extra assumptions and has worse iteration complexity."	3
Please refer above.	3
The paper proposes an interesting idea, and the experimental results are promising. There are some minor concerns to be clarified.	3
This paper presents a novel idea about set representation learning. Experiments cover multiple tasks and support the claims well. Though more analysis on time complexity is needed, I think this paper is above the acceptance threshold.	3
"The paper seems sound and provides new insights for set representation, with convincing experiments. I tend to recommend acceptance but it would be great if the authors could answer my questions.

"	3
"While a benchmark for prediction from DNA/RNA is a useful contribution, I find the following main limitations:
1 - Limited contribution (given the scope of ICLR).
2 - Not sufficiently justified use of a common benchmark for very diverse tasks.
3 - Sometimes not detailed/justified enough elements given the field."	1
"On surface, the presentation and motivation seems great (to a researcher outside of biology). But once you realize data/model performance are limited compared to existing supervised models, it may mislead the NLP community to focus on ill-formed problems and only provide marginal ""relative"" gains with no impact and no engagement with the bio community. There is a lack of clarity of the datasets, i.e. where they come from and basic intuition for the important features. The baseline models are poor choices for (regulatory) genomics, they are not common. Moreover, prediction performance is only one aspect of evaluation. In biology, evaluation needs to incorporate other aspects, such as model interpretability and variant effect predictions (of GWAS or eQTLs). While bridging both communities (NLP and comp bio) is important, this work seems too premature. I encourage the authors to take this feedback to guide them to generate a much needed set of benchmark tasks, baseline models, and better evaluations that would also be of interest to the comp bio community. "	1
While the authors have curated a number of non-protein biological datasets, I do not think the objectives of the tasks are refined enough, nor are the methods compelling enough, to justify publication.	2
"Although the paper proposes a useful resource for the genomics ML community, there are serious issues with its presentation of the biology, the data splits, and the baselines evaluated that prevent me from recommending acceptance as is. I do believe that these are addressable (although redoing splits and running baselines may be a tall order during the rebuttal period), and am open to revising my score. 
"	1
As the paper currently stands, I see major issues with this being a useful benchmark for the community. It appears to be mixing too many domains (e.g. RNA vs DNA, human vs bacteria). I am open to changing my mind after discussing with other reviewers who may know more about this field and after hearing feedback from the authors. I thank the authors for tackling this challenging and often overlooked problem of benchmarking, but I would like to see more care taken in the construction of the benchmark before this paper is ready to be accepted. 	2
"An interesting work proposing a semantic segmentation approach that is robust to variations in the lighting conditions (day/night) and the presence of snow in the scene.The main novelty of the proposed work is the combined use of adversarial networks and model-based training (Robey et al 2020) to achieve a robust semantic segmentation in the presence of variation in brightness and snow in the scene. However, a few important details about the approach is missing in the work. I would strongly recommend the authors address the limitations mentioned in Pt no 1,2 and 3. A clear definition/description of how the variations are modeled, what is the target images, and how work is different from the existing two works:  are needed before this work can be accepted for publication. 
"	3
"I currently tend to reject. The problem tackled is relevant, and the intuitions behind the proposed approach interesting, however the technical novelty is limited and the experimental validation not sufficiently convincing on its current form as discussed in the weaknesses. Maybe authors can better discuss and explain the main concerns that hinder clarity on the novelty claims and the validation in the rebuttal.
"	2
"As stated, the direction is valuable and the ideas somewhat interesting however I feel this submission is currently in a premature state. Lack of clear exposition and thorough real-world experimental evidence weigh rather heavily in the rating. I encourage authors to work on some of the methodological, presentation suggestions.
"	2
This work overstates its novelty. Using sinusoidal activations on RL tasks is of limited novelty, and no comparisons are made to related methods / few changes are made that are specific to RL.	2
"The pedagogical aspect of this paper is certainly an appreciable contribution. This paper also contributes to understanding how certain biases of DNNs can be dealt with in deep RL, but only within a limited scope. While it certainly refines our intuition and could help the field focus its research in the right areas, the novelty of the concepts introduced in this paper is fairly limited.

I think that, as it is, this paper meets the bar for acceptance, but its potential seems much higher."	3
"The paper discussed an important problem in RL that was previously under-investigated. Overall, I think the paper successfully addressed it by suggesting the usage of a random Fourier feature network. However, the novelty is limited since the main contribution borrows the methodology in existing papers, together with the aforementioned concerns, I am not convinced that the paper meets the acceptance criteria.

### Post-rebuttal
I really appreciate the authors' work for providing new results on DMC tasks. The empirical results are much more convincing and comprehensive now. I would like to raised up my score because I think the current results have successfully supported the motivations. However, now the only and the largest concern is that necessity of a new round of review for such a major revision (unfortunately this is not possible now). After all, I set my recommendations to weak accept since I cannot (and should not) access the newest revised version"	2
The proposed approach seems novel, but the motivation and presentation of the idea is confusing. It is not clerar whether the reported performance improvement is due to the proposed aggregation method contributes, and further discussion or experiments with related works may be needed.	2
Overall interesting method, but more experimental detail needed.	3
"The paper is addressing an important problem of sequential decision learning research.
The proposing is interesting while novelty to existing approach could be improved.
The proposition of a new dataset for zero-shot generalization of sequential decision-making in an offline context is valuable.
The comparison on another popular benchmark like D4RL and against the cURL approach would have been appreciated.
The introduction of generalized value function seems novel and pertinent in the context of self-supervised reinforcement learning and is clearly illustrated in the proposal of this work and could be valuable to the state of the art.
"	3
The reviewer thinks the topic of this paper is significant. The approach seems interesting. However, the reviewer has some concerns about the experimental results and the clarity of the presentation. 	3
I think the paper is well written and the technical contributions are important and solid.	3
The theoretical analysis of this work is interesting and new, but the framework and the two major components of the algorithms have been studied in prior work. This is a theoretical work, so the empirical novelty and significance does not apply.	3
As mentioned in my main review, my major concern is about the correctness of the result. Therefore I vote for rejection.	3
Overall, I think this paper has the potential to deliver a solid theoretical contribution to the literature of zero-sum Markov games. It needs to be further polished before being seriously considered for publication.	2
"Overall the proposed BAT algorithm achieves promising results that improve clean accuracy at similar robustness levels, but the design of the algorithm seems to involve elements that are orthogonal from the initial motivation and study of atypical examples. The importance of the atypical-ness of an example should be clarified during the discussion period to better motivate the method. 

**Post response update:** 
The authors have comprehensively addressed my concerns about relation to margin and importance of the atypicality to the components of BAT through ablation studies. I have increased my score accordingly."	3
"Overall, despite its shortcomings, the paper presents some findings that would be of interest to the community.

--- 

Post-rebuttal update: Given the authors' clarification of their theoretical contributions and the provided small-scale user study, I increase my score."	3
"Overall, this paper explores the memorization effect in adversarial training and proposes benign adversarial training which further enhances adversarial training. However, although the direction is promising, the major concern is about the novelty of the main contribution since the similar findings in this paper have already been pointed out and analyzed in previous studies [1,2,3], and simply dropping some called ""atypical"" data can also improve adversarial training [3]. Considering the similar results from the perspective of data quality for adversarial robustness,  the current submission may need some further exploration to provide some different and in-depth findings.  "	2
Interesting observations on the memorization effects of atypical samples in adversarial training, but the empirical evaluations should involve stronger baselines (e.g., using early-stop) and adaptive attacks (e.g., considering the BAT modules), while it would be much more informative to have, e.g., AA results on CIFAR-10.	2
"This work presents some interesting ideas but it is not ready for publication in its current form. As aforementioned, the empirical analysis is not complete and the authors should revise the manuscript carefully. For all these reasons, I recommend rejecting the paper. 
"	2
"This paper is easy to follow and has practical merits. However, it lacks the formality of an academic paper, with jumbled symbols, important elements that are not explicitly defined, sections and paragraphs that are too short, etc. I would recommend the authors to make careful revision and correction.
						

"	3
"The paper technique is very novel and has clear practical applications. As a practitioner in this domain I would happily use this technique. The key weakness that reduces my score is the lack of rigor in evaluating the results. This makes it difficult to tell how strong the results are.

I would move my score up if the authors provide statistical evidence of improvements, particularly if using prediction accuracy, rather than correlation coefficients."	4
"While the authors' proposed method seems like a promising solution to a challenging and important problem, this paper is let down by a lack of clarity in presentation, and a narrow range of experimental evaluation. 

++++ Post-revision update ++++

The authors have largely rewritten the paper which has resulted in a much clearer presentation. They have also expanded on their experimental results and clarified that the scope of their experimental work is larger than I initially understood. With these two points in mind, I have increased my score from 5 to 6. Unfortunately, I do still feel that the experimental evaluation could be more comprehensive, which is why I have not increased my score further. Additionally, I have not had the time I would like to go through the heavily revised paper, thus I have decreased my confidence. "	3
"
While I believe that the ideas of this work are novel and very interesting and provide a potential pathway away from data augmentation method for out-of-distribution generalization I have two issues with the paper: 1) the evaluation domain may be too narrow to provide confirmation of the approach with confidence over other methods, 2) more clarity and details of the approach would be helpful in understanding this work, how it contrasts with others, and where it exceeds those. 

Post Rebuttal:

My initial two main concerns with this work: 1) how well the approach generalizes and 2) the clarity of the paper. The updated draft addresses the second concern very well and so I consider that resolved. The sheer volume of updates here is impressive. For the first point, the authors have carried out a large number of new experiments which strengthens the case for control domains but to me the overall claims of the paper seems to be more general than that. That said, I believe this is a worthy contribution given the efforts put forth by the authors and will also increase my score to a 6.  Great work!
"	3
"This paper tackles an important problem, but contains technical inconsistencies, lacks rigor and provides very little detail about the proposal as a whole. The use of a GAN, a major part of their architecture, is completely unjustified, and we are told that it's necessary because the system doesn't work if we remove it... but there's no explanation as to why. No code is provided, no supplementary details are provided.

Empirical results seem good, but there's little support as to why this is the case. "	3
I don't think it would require a huge amount of work, but a bit more meticulous explanation of the loss functions and the training process is needed.  At the moment it seems that only those intimately familiar with the method and code (that is not provided) would be able to implement it.  The direction seems good, results are very encouraging, just need to flesh out the actual method a bit more, otherwise it feels there isn't enough information to reproduce.   	3
The paper proposes a retrieval augmentation to the language models and use GNNs on top of a graph structure of the original input and its neighbors and shows that it consistently improves over the vanilla lm model. The empirical results are good and the authors provide examples showing that the retrieved examples indeed help prediction in language models. The only concern is that through modeling additional neighboring contexts, the method introduces significant overhead in running time. 	3
Overall, base on the novel idea of creating global context graph, GNN-LM and show the significant improvements on all LM datasets, I would like to recommend accept this paper.	3
"1. Relatively incremental technical contribution to the community. 
2. Relatively weak comparison with baselines.
3. Many vague points that impede us from understanding (reproducing) the work. 

——————————————————————————————————————————————————————————————

The authors' response answered most of my questions. though I still feel that the technical contribution is big enough. I slightly raise my score accordingly. 
"	2
"The paper has several fundamental issues, including a misunderstanding of the existing work.
Notably, the algorithm proposed in this paper is very similar to the algorithm in IRM.
"	1
Overall, the theoretical part (analysis, foundation for the algorithm) is weak. Given that the empirical results are not very strong, I cannot recommend an acceptance. 	2
"The toy example described above shows that the proposed algorithm should not be able to learn invariant representations. There is however the possibility that I have misunderstood parts of the paper. I’m willing to increase my score should the authors address the above-mentioned issue.
"	2
See the main review.	4
It is a good technical paper, but it requires better writing. Although the proposed ProsPr, which is implemented by meta-gradients, is effective, I think the novelty is limited.	3
"This paper is well-written and provides detailed derivations. As a result, the proposed method has the potential to apply to a wide range of real-world applications. On the other hand, I think the proposed method may not be well studied and the experiment can be improved by adding the results of using $w_{init}$ learned from unlabeled data. Therefore, my initial recommendation is ""marginally below the acceptance threshold"". I will go over the review comments by other reviewers and the responses by the authors and adjust my recommendation accordingly."	3
I generally think the novelty of this work is a bit trivial. Leveraging information from early training steps is a reasonable strategy, but is still straightforward.	2
This paper proposes a new model CMFNet to handle multiple image restoration tasks. The results seem promising on several existing datasets. However, the proposed framework and modules seems similar to widely used techniques. And there is no clear and solid explanations for the design choice.	2
The paper provides a nice intuition on how to simulate the human visual system in terms of designing a generic architecture that can work for multiple tasks. However, the mentioned points of weakness substantially impact the quality of the work. These include the proposed design (3), claimed contributions (4), empirical approach to ablations (5), and the fact that results are far worse than the state of the art (6,7,8). I have provided the details above with feedback on how to address each point). To address the mentioned concerns and achieve at least somewhat comparable performance to the SOTA, the paper needs both much further experiments and major revisions. Therefore, I cannot recommend the current version for the conference. However, this should not stop the authors from improving on their current work and hopefully submit to a future conference when it is ready.	2
The paper proposes an intriguing model inspired by the human visual system and shows impressive qualitative results. At the same time, quantitative evaluation definitely requires more work. The paper would benefit from the addition of practical details about the model and more explanation about motivation behind some design decisions.	2
"Considering the strengths and weaknesses, the contributions are minor and the authors’ interpretation of the proposed method seems void. 
"	2
My biggest concerns are the first and last comments as listed above in the weaknesses. This paper does not clearly explain why the proposed architecture can help improve the generalization ability for various tasks. In addition, the experimental results are not good enough to demonstrate the effectiveness. Therefore, I recommend this paper with rejection. 	2
"To me, its contribution to AI/ML is not significant. Many designs are just
heuristics. It is more like a security engineering paper whose study object is
DNN. Also, it falls short in evaluation."	2
Please see the main review.	3
This work is promising and contributes to generating successful backdoor attacks in practical settings with good results. However, there are some concerns that affect the clarity of the method and of the threat model.	3
Overall an interesting paper, but I think some additional justification for why such attacks are not easily defended against is perhaps required.  Clarity could be improved in places. Given this I believe the paper is currently below the acceptance threshold.	3
While the ideas in this paper are sound and are justified with ample experiments, I have some concerns about how this approach would scale with a significantly larger number of tasks, which was one of the main motivations of the work. Hence my initial rating is borderline	3
Motivation is reasonable and the proposed technique is simple yet seems to be effective compared to a task-equivalent selection schedule. However, there are several concerns.	2
Overall, I like the topic and proposed method. However, the exepriments need improvement. I may consider raising my score if the authors can: (1) clarify the practicality of the trade-off mentioned above, (2) combine MCTS with recent replay techniques and show clear improvement, and (3)  prove that MCTS is better than simple search/optimization methods.	3
I feel that the paper is relevant and clearly written, however I am insecure about its impact due to the lack of a valid baseline method and its efficiency implications.	3
This paper has some interesting and innovative ideas about recounting visual stimuli from brain responses. However the paper is not very well written, hard to understand and the results are not very convincing. 	2
Although the paper tests all of their contributions, the benefits do not seem to be dramatic. I do not see proper evaluation metrics that are used in previous methods for the model evaluation and also not tested on other existing fMRI datasets. The experimental section is weak, and it is hard to reproduce the results, so the paper does not introduce a significantly better technique.	2
The idea using F-attention to guide the network training is fine from the view of network training, but the way of using it to do image reconstruction from the FMRI data seems incorrect.  Fitting the F-attention model to the artifical attention map does not make much sense.	2
In summary, the paper appears to have solid basis in the literature and tackles an important problem in meta-RL, and it appears to propose an interesting combination of known approaches to solve the stated problem of sequential-task meta-RL without task repetition. The experimental results are encouraging. However, it fails to make clear claims or clearly explain the key novel aspects of the algorithm, how and why they are sufficiently novel. 	2
Overall, this paper is an interesting step towards solving the problem of continual Meta-RL. Its presentation is clear, and the proposed general framework is reasonable. One major concern is its novelty, which resembles the previous approach GMPS. Although several key limitations have been identified in the discussions of this paper, addressing some of them is important and can significantly strengthen this paper, particularly requiring the data storage of previous tasks and no mechanism for handling forgetting.	1
I like the paper overall, it is clearly written and I think it tries to address an interesting problem. I would however, have liked to see some more ablations and analysis of the approaches and maybe some better motivations too.	4
The issues considered and the solutions proposed in this paper sound reasonable, and the proposed method does achieve the better performance. But some details of the whole pipeline need more justification, and the optimization between sequential tasks seems to be independent. I'm also not sure that this is able to address the issue about catastrophic forgetting.	2
Interesting meta RL setup and nice experiment construction. At the same time, the algorithmic novelty is questionable and the ablation study is not convincing enough. 	2
"Overall, even if the idea behind the paper is promising, given the unconventional approach to technical description (only words, not a single equation), and the associated lack of clarity and rigor, I recommend rejecting the paper in the current form. 
"	2
Using neural architecture search to increase the efficiency of neural network architectures for private computing is a very interesting direction. Providing more insights on the cost of the search process and the optimization of the key parameters such as the regularization factor would help the reader better assess the benefits of neural architecture search in this context. 	3
--	2
Major issues with clarity, algorithm design, and general structure of the writeup make it difficult to endorse this work.	2
The work proposes a novel idea, but the presentation, evaluation, and analysis is not yet sufficient to warrant publication.	2
Even though the idea of orthogonalization of the gradients is interesting, this work lacks the comparison to prior art and proper baselines. I recommend authors to work with proper baselines and compare their method against other orthogonalization techniques like orthogonal initialization and regularization. Furthermore the cost of doing SVD at every training iteration needs to be justified and discussed in detail. 	2
Experiments seem not to align with questions asked by the authors. Technical quality is not sufficient. Reject. 	2
This paper describes an approach to adaptive control incorporating a learned component. The novelty relies in the development of a proof technique which doesn't require the learned component to have Lipschitz like properties or exhibit universally bounded error on a space of inputs. The paper is interesting to the community and combines theory with experiment in a coherent manner. 	3
"Of the three sections in the ""Main Results"" of the paper, only the adaptive control design section is substantive; the proposed neural network training is just naive regression on state-input tuples, and the inclusion of SITL needlessly complicates the paper's exposition before being reduced to a trajectory tracking task. Adaptive control theory alone may be an unsuitable fit for ICLR. Regardless, the novelty of the adaptive control design is questionable, as it appears similar to well-known previous work and the authors' have not contextualized their design amongst the adaptive control literature.

%%
Note: I have raised my score after the authors' rebuttal."	1
It seems to me that the open-loop trajectory and the online adaptive tracking controller are doing the heavy-lifting here and the learning is not that important. The authors should demonstrate the benefit of learning.	2
Although I like the general idea and do believe that reparametrization can balance out different convergence speeds of different modes to some extend, I found the presentation to be a little confusing. The appoach seems to reduce to a linear reparametrization, which seems to relate it to other (more classical) approaches. Along with the list of minor aspects that make the paper a little difficult to follow, I need some clarification on this aspect.	3
This paper is clearly unready for publication. The main idea -- using a structured linear reparameterization -- is under-developed, and the experimental results are on problems that the ICLR audience don't really care about.	2
Overall, I lean slightly towards acceptance. This is due to the clarity and novelty of the paper, as well as encouraging experimental results. However, I think some more experimental verification is needed for ablating the different components of the proposed approach and for demonstrating its applicability to a broader range of problems. 	3
Overall, the reviewer finds the paper is a bit hard to follow, and the presentation of the paper can be significantly improved. The experiments are interesting but the comparison is not quite comprehensive. 	2
Overall I think the paper is interesting and could be further improved if more results are added.	2
Good intuitions about DP algorithm but the results and arguments are not convincing. Overall, the review would like to give a weak reject to this paper.	2
None	3
The paper is clear and easy-to-follow, combining three existing losses to form a new one that empirically improves DP accuracy on some vision datasets. However, the experiments should include non-vision tasks and provide (at least discuss) the insight from a theoretical viewpoint. Also the improvement is not significant enough for ICLR venue.	2
This paper is overall interesting and significant for the problem of two team zero sum games. My main concern is how it relates to previous works and I hope the authors could clarify this point.	3
I appreciated the theoretical results of the paper, though I believe more needs to be done to justify the model. I also think the writing could be significantly improved. Overall, I am somewhat on the fence. I welcome a thorough discussion with the authors. 	2
I think this paper is blow the bar of acceptance. Currently I am negative on it due the weaknesses I mentioned, but may change to be positive if those issues can be addressed.	2
"This paper cannot make us understand two-team zero-sum games better.

***After the discussion:***
It seems that the authors have misunderstood the relation between different solution concepts.

The statement “the notion of""per player Nash equilibria"" can be seen as a smoother figure of merit for the performance of a defensive team against an adversary” is not correct because a per-player Nash equilibrium may not be a team maxmin equilibrium (TME). Similar to TME, a two-team maximin equilibrium is still a per-player Nash equilibrium, but a per-player Nash equilibrium may not be a two-team maximin equilibrium. This paper focuses on min-max optimization in team zero-sum games, but it only achieves a per-player Nash equilibrium. I think this is inappropriate."	1
Although the motivation of the proposed framework seems to be pretty specify and properly positioned, the experiments does not seem to validate the superiority over existing methods. The results are quite limited. It also missed the citation and comparisons with many recent works. 	2
A solid first step, but more qualitative insights are needed given to make this paper shine as much as it should. Weak Reject with a possibility of increasing my score during rebuttal to accept.	3
The paper presents the results on texture synthesis using a new image representation model based on wavelet phase harmonics. The results are interesting and show that these representations are powerful enough to encode complex image features. What this paper presents starts clarifying how CNNs are encoding image comprehension	3
For these reasons above I am inclined to reject this paper for ICLR, and suggest the authors find a more suitable conference for this work, as well as reanalyzing some of the results and rethinking the overall presentation of the work. 	3
Though the idea of using learning-based approach for IM problem is interesting, the proposed method did not demonstrate advantage over existing methods (the best performing one is more like existing heuristics). Also, there are a few missing pieces in the evaluation.	2
The proposed technique is not very novel, and its practical utility needs better justification. 	2
Influence Maximization is a quite surprising application of GNNs. Authors employ a systematic approach to build multiple methods with more departure to the traditional approach and increasing levels of computational advantage. Some of modeling decisions look heuristic, however, and at least requires a better explanation.	3
Overall, the paper proposes novel methods for Influence Estimation and Influence Maximization. In my opinion, the main weakness of the paper is the missing details in the experimental section. 	3
The proposed method is simple but intuitively make sense. However, the method can be limited and only works in certain scenarios. Also, there are efficiency problems and exposure bias problems that are not mentioned in the paper. Therefore I think the paper is below the bar in its current form. I believe it can be improved by addressing the aforementioned weaknesses.  	2
The iterative decoding proposed in this paper has limited to toy datasets. The conclusion about relative attention and copy decoders is not well-supported. 	2
I believe this paper is slightly below acceptance threshold because of the listed weaknesses. I may not be convinced in the strength / significance of all the results, but perhaps this would be made more clear after addressing problems in the writing.	3
While the approach does improve compositional generalization in certain synthetic tasks, it is almost completely inapplicable to real, interesting problems such as translation or semantic parsing. 	2
The paper investigates an interesting hypothesis concerning the parameterization of neural networks, in that increasing depth leads to solutions that have lower effective rank. This is backed by various experiments and ablations, and is in itself convincing. However, a significant part of the paper focuses on consequently linking this to generalization --- but which remains questionable. This is because their empirical observations and theoretical statement (in the deep linear network case) indicate that increasing depth results in a monotonic decrease in the effective rank, but in practice, they find this holds up to some moderate depth and beyond which the performance deteriorates. This weakens their connection of low-rank gram matrix solutions to generalization. But not only that, there are some critical yet unexplained choices/aspects in their framework, such as: whether effective rank is the right measure of rank, what happens with the actual (discrete) rank, choice of the kernel, etc. which raises additional questions and further serves to dilute the claims. 	2
The paper presents somewhat interesting observations on the algebraic properties of feed-forward neural networks, however their relevance to the generalization power of such models is not entirely clear.	2
"As mentioned above, in summary,

(+) The paper is very well-written and easy to follow.\
(+) The finding are interesting and well supported by the experiments.\
(-) Most of the findings have been discovered before in related work. However, I might have missed something and am looking forward to hearing the authors opinion.
(-) Figure 5 is not clear at all. However, I believe it can be fixed easily and I would like to encourage the authors to modify it.
(+) Depth as a regularizer has been using by practitioners however. Nevertheless, increasing the depth by adding linear layers is still interesting.

Finally, I would like to let the authors know that I am willing to increase my score if the authors could kindly answer the questions above."	2
Missing the understanding of these issues built up in the infinite neural network literature, and in the literature relating infinite to finite networks.	2
In summary, this paper propose a simple yet effective GSMA attack to evaluate adversarial robustness of models in the transductive learning setting. Some results are promising but there are a few places require further explanation/clarification. 	2
"Overall, while I found the paper to elucidate an important shortcoming in previous work on evaluating the robustness of transductive robust learning, the attack methods used lacked novelty. In fact, in a number of cases the simple FPA algorithm had the best performance. I urge the authors to consider improved methods to circumvent the bilevel optimization problem.

+++++++++++++++++++

The clarifications have improved the paper. While I still believe more innovation is possible for the attack, I have decided to raise my score to a 6 since the paper is likely to lead to interesting follow-up work."	2
Good formalism for adversarial machine learning in transductive setting, plus other smaller contributions.	3
I give a weak reject score for now. I may change my score after I exchange my opinions with other reviewers.	3
This paper propose a simple yet interesting generalization to FID by leveraging recent advances on approximating wasserstein distance between GMMs. However, as a purely empirical paper, I think there are some major flaws/issues in the evaluation procedure and thus the empirical study is not convincing enough to demonstrate the pratical advantage brought by the proposed approach.	2
See the main review.	1
I believe that this is a very clearly motivated paper, with a solid structure and experiments which support the main claims of the authors. Some comparisons with previous work on the subject are still needed in my opinion, but overall this is a solid work, and I believe it should be accepted.	3
Authors propose to use GMMs to evaluate GANs. I think it benefits from MW$_2$. I think the contribution is not enough. Furthermore, the experiment should be improved. 	3
The paper presents a method with two key insights/contributions that are not present in prior work. The ideas are validated quantitatively and qualitatively. The technical details, figures and tables could be improved with better explanations and descriptions. 	2
Overall, the proposed architecture is designed simple and shows state-of-the-art performance. There are minor issues as commented in [Main Review], but I am leaning towards positive at this moment. Since the topic is out of my scope, I would like to see other reviewers' opinions.	3
Though the paper is built upon many existing techniques, including the IODINE, NPE and R-NEM, and the novel part of this paper is using a transformer structure to align the IODINE latents and find discrepancies, I still consider this paper very interesting and strong. It may provide some insight on how to model individual object dynamics from videos. Moreover, the proposed method is very effective as experimental results show superior performance. 	3
"Although the proposed method seems to show the better results in the experiments, the explanation is not sufficient to convince if the experiment is appropriate and fair. Additionally, the explanation of the method is not clear to understand the detail. Additionally, it is not clear that what kind of information is extracted by the part, Dynamic Distillation, introduced in this paper. It seems no experiment is shown that object dynamics is distilled by the proposed method. Consequently, the reviewer thinks that this paper is below the acceptance.
"	3
"The modules proposed in this work are well motivated and designed and produce strong results on video reasoning. Yet, I am concerned about the insufficient evaluation. Without the ablations and the baseline I discussed in weaknesses it is difficult to quantify the actual impact of the proposed method and its underlying modules. Moreover, experimental details are missing which makes it difficult to reproduce its reported numbers. That is why my initial rating is below the acceptance threshold. But I am open to adjusting my rating if the authors do sufficiently address the points raised in weaknesses. 
"	3
Good paper combining known methods to solve a well-motivated problem with a number of compelling applications.	3
"Originality and significance: The combination of flow networks with separable functions to form tractable integrals is a novel idea (to the best of my knowledge), with potential use in other ML domains. The experiments on OOD detection tasks show parity with SOTA baseline in some but not all the studied cases.

Quality and clarity: The paper is hard to follow in parts, mostly due to the fact that it is not self-contained. The reader is expected to know the background on bijective networks. The experimental section is lacking details in the main body of the paper.
"	3
Although I like the idea that we should pay more attention to integration error, I don't see how we can get around it in ML/DL. The proposed approach of assuming (\***) is certainly not the way. Furthermore, the function $\Omega$ in (\*) that would be encountered in ML/DL cannot be accounted for in the proposed framework unless some type of bound is in D8 is employed. Replacing MC error with an upper bound on a nice loss function as in D8 simply does not seem like a good trade to me.	1
To summarize, I think the current paper put forward an interesting idea (perform computationally tractable integration using separable functions parametrized by neural nets). However, it is unclear (at least based on the current presentation) whether the idea finds practical application in machine learning. The experiment section of this paper focuses on OOD detection, in which case the proposed method performs OK-ish but fails to compete against contemporary baseline methods. Thanks to its generality, the proposed integral method may be useful for other ML applications, which I encourage the authors to demonstrate.	3
Although this work tackles an interesting problem, the novelty is weak. Also, more experiments with different GAN datasets would be helpful. It is limited to StyleGANs. Also, the writing should be improved. Reproducibility is not certain as they did not provide the code. 	2
The effectiveness of the proposed method is validated by experiments, while the superiority (over existing methods) is not. The experimental results need more detailed descriptions.	2
Overall the idea of of the paper is nice but the evaluation is barely convincing.	2
The approach is reasonable and has the potential to lead to new findings in face memorability studies. However, given that the technical novelty is relatively incremental and the experimental results obtained at this point are still in the early stages, the research contribution is not sufficient.	1
With the reasons above, I am leaning towards rejection.	3
The experimental results of the paper may have potential. However, the existing theoretical results are relatively weak and overclaimed. The paper also fails to demonstrate its contributions compared with prior works. Therefore, I do not think that the paper can be accepted in its current form.	1
The strong assumption makes theoretical contributions of this paper incremental. The authors also didn't do enough literature survey to support the novelty of their algorithm. Therefore, I tend to recommend rejection.    	2
I think the theoretical part is not very consistent with the motivation and the experiments. It is not very clear why to consider such two options and why the experiments only use option II but in non-linear case.	2
Good piece of work, can be enhance some more with more extensive analysis of the results. E.g. more explanations of the observations.	4
The paper studies an interesting idea and shows that the proposed idea improves some baselines in several applications. But I feel that it needs additional study on why the idea works and more comparison with more advanced methods.	2
The presented method is interesting and should be accepted for publication. However, the paper could be improved and there are some open questions which should be investigated.	2
From the weakness I described above, I recommend this paper below the acceptance threshold. The work may have high importance in the application to wireless communication, but lacks discussions related to existing domain adaptation work and comparison to them. 	2
Overall, the paper needs further improvements to reach ICLR standard. See detailed comments above.	2
"Overall, the proposed idea looks OK. But it is unclear why training with such a small dataset will not lead to overfitting. Also, the overhead of recollecting the labeled dataset and retraining under a rapidly changing channel is not discussed.
"	3
The paper has significant weaknesses in the proposed applications side and has limited novelty on the algorithmic side. I therefore do not recommend acceptance. 	2
"The addressed problem is of interest and the authors test the proposed technique both on simulate and real data. However, the main technical contribution of the paper relies on the well-known property of Gaussian distributions (hence the paper provides only marginal novelty) and the two working hypothesis (same number of components between the two Gaussian distributions and a one-to-one
correspondence between the components) are not properly discussed from a practical viewpoint. Finally, considering the fact that the authors promote their approach because it is fast and light-weight, a proper complexity analysis is missing in the paper (the one in the appendix is partial and does not seem to include the cost of the MAP rule)."	2
"-The paper is not novel. The idea is previously exploited for other tasks. 
-Experiments is not completed."	1
To summarize, this paper is clearly written, and demonstrates an engineering trick for training cnn classifiers. It boosts the classification accuracy at costs of larger computation and memory consumptions. The experiments are also not convincing enough.	1
"My opinion is that the idea in the paper is at best a nice engineering idea, which aims to improve on the class imbalance problem, which typically lead to lower accuracy for those classes. 

The novelty of the paper is limited, due to reasons listed above.  ICLR is not the appropriate venue to present this idea.
"	2
Overall I am on the acceptance side of the paper. While non-parametric classification has ben studied in the literature, the paper presents an interesting angle of checking its effectiveness for down-stream transfer. The proposed approach is not only quite effective according to the paper's experiments, but also giving some insight about why the popular SSL methods based on instance discrimination has good transfer abilities. Some revision of the paper is surely required (on top of proof reading), but to me it is above the bar of acceptance.	2
"See in the Main Review

--- Post-rebuttal
Overall, I am still on the fence about the paper. However I am not against accepting it.


"	2
This paper is generally well motivated and well written. The experiments have sufficiently validated the effectiveness of the proposed method on downstream classification tasks. My minor concern is the performance on other downstream tasks.	3
"I am concerned about what actually sub-clusters are emerged from the LOOK model. Based on the current algorithm description, I believe that any positive examples even if they do not belong to the current sub-cluster would be sampled and drawn closer to the query sample, and hence no sub-cluster could be found. I guess lots of nearest neighbor visualizations would help answer this question.

---- Post rebuttal ----
I am satisfied by the additional analysis provided by the authors. I raised my rating to accept this paper."	2
Approach for modeling contextualization in language models using tools from category theory and manifold learning. Interesting approach but there is a gap between the methodology and the experimentation. Additionally, experimentation is similar to studies conducted previously with a focus on substitutions in context rather than center token. 	2
"It is important to mention that I do not have a background in category theory or manifold learning so I have judged this paper purely on the basis of convincingness of the arguments, quality of experiments, and interpretation of the results.
Overall, I think that paper identifies a very niche problem and provides interesting ideas about how we can think about representation of the contexts in language modelling. But in the same light, I do not think that the paper is well-written and rigorous enough to be published at this conference. The paper needs serious revamping in terms of story, writing style and presentation. I provide detailed explanation of my concerns in the main review. 

"	2
Although the goal of this paper (to analyze the effects of context) is good, it suffers from severe issues as mentioned above. I would recommend a re-writing, especially the basics (like what kind of variable a context is), with more motivations, explanations, and sufficient experiments.	2
"While the paper attempts to study an important and interesting angle in understanding how language models represent semantics, the ideas and results are presented in a very vague manner which prevents clear understanding and judgment of the contributions in the paper. It is still unclear to me how exactly are ""context"" represented in language models and what insights can be obtained from the paper, even after reading the paper several times."	2
"I vote for rejection because 
1. The findings are not significant. 
2. The analysis method is not novel.
3. The paper presents a few very simple experiment results using a very complicated way, which makes the paper very hard to read. "	1
Though this paper donot introduce quite new algorithm, it is still valuable since it concludes the recent RL based sampling methods in accelerating MRI and analyze the benefit. However, I'm not sure if this paper is suitable for submission in ICLR. Maybe it is more suitable in a medical imaging journal.	1
"Overall, I am in complete agreement with the motivation and sentiment of the paper, but as presented, it does not make a sufficient contribution to warrant publication. 
"	2
"Overall, the paper contains certain empirical value, arguing that RL-based approach is not effective. But it lacks the typical methological novelty necessary for an ICLR paper.

This paper is more suited for MICCAI or medical imaging related conferences."	2
My main issue with the paper is that it is hard to follow the analysis. It would benefit the paper to combine the analysis of both RL methods and rather ask questions about which design choices have the highest impact. Similarly, the analyses should incorporate the different choices of evaluation metrics. This would enable the authors to propose a recipe of how to approach the problem of MRI acceleration. 	1
"Overall, I think the proposed problem is novel and new. The proposed method is reasonable to solve the problem.   
However, there are issues that need to be addressed. "	3
This paper makes some significant contribution towards a meta learning approach for time series forecasting, including releasing a dataset. However, due the issues in the experimental evaluation/results, and the dataset collection process, I am leaning towards a reject. I am willing to increase my score if my concerns are satisfactorily addressed.	3
"strengths:
s1. interesting set of t.s. metafeatures.
s2. interesting meta dataset for t.s. forecasting.

weaknesses:
w1. problem is basically a cost-sensitive classification problem,
    but the loss does not reflect this.
w2. baselines seem to be very weak.
w3. complicated notation.
"	2
While I think that the problem that the authors address in this paper is of great practical importance and has plenty of scientific challenges which attention, I cannot recommend the present paper for acceptance. I base this primarily on the exposition of the work which leaves too many too important details open. This inhibits my understand of the paper (and the appendices do not shed light on those details when I checked for it).	2
This seems like a particularly strong result. More could be said about the implications of their results, but otherwise seems like a good paper.	3
This is a strong paper tackling an important problem. The approach is interesting and the technique is sound. The claims are well backed by impressive empirical result and theory. Therefore, overall, the contributions of the paper will have a certain impact to the community.	4
This paper is clear and technically solid. The theoretical justification is interesting.	3
The paper is an empirical study on a topic that is not very popular (at least based on how the paper discusses about the existing literature). 	1
The Zest method presented here is sensible, clearly presented and shown to be successful for a couple of applications (model stealing and unlearning evaluation). It would have been stronger if it had been tested additionally on another problem domain and another ML model class rather than focusing solely on DNNs and image recognition. The topics are also only of moderate significance. For these reasons, I recommend a weak accept. 	4
Paper is interesting, adding a significant novel contribution to the existing literature and providing an effective solution in at least two important and somehow still open ML issues. A wider analysis (both in terms of models and data) would be recommended to strengthen the authors' claims.	3
Overall I think this is a good paper and can be accepted to ICLR. Although it does not have much theoretical novelty, I believe applying LIME in the area of calculating distance between two models is pretty new. If my listed concerns can be addressed, I willing to increase my ratings one level further. 	2
This paper has only limited contributions and suffers from several significant issues, both technical and in the writing quality. It does not meet the standard for publication and therefore I recommend reject. 	2
This paper has several concerns for publishing as a conference paper, such as low technical novelty because of the missing previous similar method, and weak empirical evaluation that cannot sufficiently support the authors' claims about contributions.	2
"I propose rejecting this paper because it
1) does not compare with a single baseline compression method (most important reason for rejection),
2) lacks novelty (very similar to product quantization),
3) often attains significant drops in accuracy, and
4) lacks clarity (especially Algorithm 2).
"	1
Overall, the paper proposes an interesting idea to share parameters across words and reduce the size of the embedding which hasn't been explored in the past with promising results on XNLI task. My main concern is the lack of clarity and issues with the framing, claims, and evaluation. There is no discussion or comparison to alternatives, some of the claims made are weakly or not supported, and evaluation lacks emphasis on efficiency aspects and relevant efficient word embedding baselines. The results are interesting but the claims about comparable performance hold only in one of the two models explored.  	2
Overall, the paper presents some interesting and useful analyses, but the analytical techniques are not very novel and they do not (at least to me) provide significant clarity on why SAM outperforms SGD.	3
I believe that the collection of theoretical results on SAM is both novel and useful to the community. Moreover, the paper is well-written. If the issues mentioned in my main review can be addressed, I am inclined to increase my score and recommend acceptance of the paper. 	3
Is the main contribution of this paper to understand the original SAM algorithm or to propose a new SAM-like algorithm with a theoretical guarantee? I think that the paper has to make more clear what are the main contributions. The proof technique used in this paper is quite standard, and the results are based on the square loss. So the impact of the results may be limited. I also have some concerns about some details, as I have mentioned in the main review. So currently, I would like to suggest a rejection, but I am open to discussion and willing to increase my score.	1
The aims of this paper are laudable, but the explanations for short of being convincing. I would suggest the authors, in a new version of the paper, to focus solely on one aspect of SAM, and explain it fully (using theoretical results and comprehensive experiments), and comparing it to vanilla methods. 	2
Interesting direction, but lack of novelty. Still lot of work is needed to get a good score.	2
I think this work largely relies on an existing work, which limits its technical contribution. The paper clarity can be further improved by giving more intuitions or explanations to some tricks (or definitions maybe). As such, I think this paper could be much further improved.	2
"The idea of combining structure learning and path-specific effect regularization is very interesting. But the gaps/incorrectness concerns me most.

Based on my understanding, the three challenges are not well addressed:

1. The requirement on causal graphs: The proposed framework does not require the causal graph but it cannot correctly define the path-specific effect or readily recognize the root/non-root cases. The proposed approach faces the challenges of unidentidication.

2. The complex calculation of PSE: PSE is calculated for parameter tuning.

3. The root assumption is not necessary for PSE. In addition, the proposed method requires domain knowledge to recognize the root/non-root case."	2
Based on paper methodology contributions and above merits (explained in Main Review section).	3
I find the topic very interesting and the video-based approach is certainly among the most relevant in robotics. However, I find the clarity of the presentation could be improved to fully appreciate the essence of the approach 	3
"This paper studies a very challenging problem and presents a novel method, but the experiments are not convincing enough.

----

UPDATED the overall recommendation after authors' response. see details in thread."	3
The paper tackles a challenging and compelling problem: meta-imitation learning of human demos with a small amount of paired robot demos. However, the experiments lack relevant comparisons to contextualize the significance of the proposed method. The heuristics used in the adaptive loss could also be better analyzed, e.g., by looking at their correlation with the MSE between the translated videos and true robot video.	2
The novelty of the paper in terms of the methods is not much. The analysis and synthesis approach as well as the method for each stage are not particularly novel. Several prior works have used such methods. The experimental results and analyses are also not strong and does not provide interesting insights. Overall, the paper lacks strength on both fronts in the current form. 	2
"The reviewed paper proposed two new approaches for speech restoration. The first approach is a single-model-based generative speech restoration technique that can cope with a variety of speech restoration tasks simultaneously. The other is a generative speech restoration frame called VoiceFixer which provides enhanced performance over the well-known existing techniques as well as the proposed GSR approach. 
In spite of some unclear and weak points mentioned above, the reviewer thinks the two proposed approaches highly and believes that they could impact positively on the related society.
"	2
Overall, this is a well written paper which discusses an interesting idea for addressing the problem of removing speech distortions in a more general way with strong results. The paper discusses many experimental design choices that researchers would need to replicate the results, which is a strong plus. It is also important that the authors have conducted subjective evaluation experiments to show how their algorithm performs. I think that the paper could have a great potential in the field, however, in its current form, the paper has several limitations which downplay its true potential. Specifically, showing how the proposed method is different from ones presented in the literature as well as why it is more important than simply employing more computational power. Moreoever, there are a few misconceptions in the experimental section that authors need to address before the paper is ready for a publication.	2
I think that the paper is not at the level of ICLR standards. Although it could be considered that it achieves interesting results in an existing but rather unexplored task, it contains almost no learnings for the ICLR audience as there seem to be no novel machine learning parts. In this sense, a more specialized venue could be perhaps more appropriate (e.g., InterSpeech, ICASSP, WASPAA). However, in my opinion, both results and writing/presentation also have serious problems that should be solved before resubmitting to such venues. 	1
Overall, I think the paper is not good enough due to aforementioned nontrivial weaknesses.	2
The paper does not demonstrate that the method is a significant contribution to the state of the art in quantizing neural networks. The experiments are only applied on image classification task with small architectures, and in the setting that I found comparable in the literature, the results do not look great, which questions the significance of this work. 	1
This work utilizes a learnable linear combination of high, medium, and low-bit quantization at the beginning while converging to a single low-bit quantization at the end of the training. In the quantization procedure, multiple quantizers and the corresponding attention matrices are adopted to fuse the quantized weights or activations, which will increase the computation and storage cost. Some experiments are conducted to evaluate the proposed method. However, it lacks some comprehensive and fair comparison.	2
Overall, this work presents a new approach to help improve the training convergence of low-bit quantization for neural network. Due to the weak results on large-scale datasets and unclear technical details, I do not think it meets our bar at ICLR.	2
The paper proposed two simple methods for a novel issue in PrLMs. The empirical results are promising in two benchmarks with ELECTRA model. However, more technical description, empirical analysis, and experimental result are still needed to fully support the claim of this paper.	2
This paper studies a critical problem of false negatives in pretraining language models and demonstrates good results. But the proposed solution lacks enough novelty and might have some technical flaws.	2
"Although the paper shows good results compared to the baseline, current writing is not convincing.
It needs more details and extensive analysis to support the claim."	2
The methods proposed in this paper have clear theoretical and empirical limitations. The experiments do not fully support the claims by the authors. My current recommendation to this paper is reject.	2
Technically sound methods but unconvincing motivation and ill-considered methods; More empirical studies needed. 	2
Overall, I found the paper is clearly written, well-motivated, and addresses an important problem. Although I feel one may concern on the technical contribution and experimental results, I still feel the work is worth to be shared to the community, given that incorporating label uncertainty for intrinsic robustness is yet a novel aspect, suggesting some useful insights not only for the context of adversarial robustness, but for, e.g., confidence calibration or label noises.	3
"The paper, contribution, and discussion are clear and well written. It is an interesting paper.
"	4
The paper is mathematically rigorous and well-motivated. However, some unclear descriptions hinder understanding of the paper fully and it lacks direct empirical comparisons to prior works. I would be happy to raise my score if the authors can address my questions. 	3
This paper focuses on an important problem in adversarial learning. The motivation, writing, and theoretical analysis are appreciated, though there are some mentioned issues needed to be addressed. Therefore, in my view, this paper reaches the acceptance line of ICLR. I recommend accepting it. 	3
Although there are several points need to be addressed by the authors, I feel the theoretical contribution is strong enough to recommend at least a borderline acceptance at this point. I am open to raise my score if my questions can be addressed.	3
While the paper provides an interesting connection that yields a (to my knowledge) novel algorithm, I don't think it is ready for publication at this time. There are major issues with clarity that make it unclear how the algorithm is implemented at all and I am wary of a few parts of the experimental setup too. But, the core idea is interesting enough that I would encourage the authors to resolve these issues and re-submit the paper to another conference. 	3
"PROS

The paper considers an important problem and proposes a sound solution to it. I did personally enjoy learning the application of the importance sampling technique to reduce the estimation variance. I think that the theory shed new light on off-policy policy gradient algorithms.

The theoretical and empirical analyses are satisfying. 

CONS

The clarity of exposition could be improved by making the theory more linked to the actual RL problem. 

The authors considered only low-dimensional problems in the empirical evaluation, raising the question of whether the algorithm could be applied to more complex domains. However, I think that the theoretical analysis compensates for this minor weakness.


I would like the authors to clarify the doubt I wrote in the main review.
"	3
"Overall, I think the paper is solid in its theoretical contributions, yet is a bit lacking in a few important practical aspects.

1. Trajectory-based formulations of policy optimization is not practical when horizons are long. The application of such techniques to RL feels very black-box, as they do not leverage the Markov structure of the problem at all. I think extending such techniques to step-wise based policy optimization algorithm would make the approach more practically appealing.

2. Empirical results are a little bit suspicious in that TRPO underperforms in a significant manner. I would even argue that the combination of this approach with ideas from TRPO can make a more solid empirical case."	3
"
This paper proposes to tackle the heterogeneous budget in FL scenario. A weak accept is given based on two reasons. 
(1) The Split-Mix not only considers how to customise models on different clients but also includes how to maintain high robustness of client models where they are trained over non iid data. I think the paper is well motivated and the solution proposed is intuitive and practical. Though the technique used to improve robustness (ie.e ST and AT, parameter sharing and layer-wise mixing) is not new, it well suits the problems and the authors have successfully integrated them with Split-Mix. Moreover, the evaluation shows promising results that Split-Mix could train client models with much fewer parameters but have similar accuracy performance as FedAvg under the budget constraints. 

(2) As mentioned in the Main Review, more clarification could be provided to improve the claims in the paper. Why can't conventional model customisation apply to FL scenario? How to handle customisation on other aspect? And the evaluation could include more details regarding training setups and efficiency. 
"	3
"### Recommendation

The main weaknesses of this work is that is that the contributions are not properly explained, Split-Mix requires training the base models in all clients (which is unrealistic for large FL settings); for ResNet18 the FLOPs and number of parameters reported are not computed correctly for the x1 baseline (the others might be inaccurate too). The Authors, without argumentation why, directly state that FedAvg baselines can only afford training 0.125x models, why is this? Which device cannot afford training a 0.25x ResNet18 ? Even a RaspberryPi 4 (i.e. CPU-only) can train a x1 ResNet18 using batchsize=32, which takes ~15seconds per batch. As the Authors state themselves, the advantages of Split-Mix for CIFAR10 and DomainNet (the two most challenging task tested in this work) at somewhat limited. What the results evidence is that only models that are somewhat over-parameterized for a given task (as seems to be the case for Digits -- only a 4% acc drop when going from x1 to x0.125) work well with the proposed Split-Mix strategy. In addition, the Authors should evaluate other types of client heterogeneity distributions. 

Summary: The Authors have motivated well the paper but need to improve the presentation of the technical contribution. A good way of achieve things would be to present Algorithm 3 sooner and then explain its main stages. To me it does not sound natural to give such brief explanations about `split` and `mix` only at the end of Page 6, just before the experiments section.


### Supporting my Recommendation

Please see points above.

### Minor points


- Is it fair to add as cite to ""Federated Learning"" a paper from 2019 (first sentence of the Introduction) ?
- When describing Fig3 (in Page 4) the sentence starts with ""To validation, we investigate [...]"". Maybe it should be ""To validate this""?
- 6th line in Sec5: ""The dataset first is [...]"", should be ""The first dataset is [...]""
- When Authors mention ""layer output cache"", I have assumed this is equal to layer activations or ""activations buffer"". Please correct me if this means something else.
- It is not clear which architecture was used for Digits. Adding a 1-line description of what the task is for Digits and DomainNet would be useful for readers not used to work with these datasets. "	2
Overall, I think that this is a nice contribution with compelling empirical results. Although the manuscript is largely well written, it was not clear to me how model split and aggregation are done in the proposed approach, and HeteroFL, SHeteroFL, and Split-Mix are different and how these differences bring performance improvements. Also, the assumption about the computational resources of clients would be better clarified.	3
I recommend this paper to be rejected mainly given the issues regarding significance and lack of proper explanations for various claims throughout the paper (see Main Review section).	3
The main claim of the paper is not supported by the evidence. It's not hard to imagine scenarios for which the main claim can be demonstratively false. In my opinion, the paper must clearly mention when the proposed method would help and hurt, and tone down the main claim.	2
The paper is well written and clear. The main contributions are novel, interesting, and well motivated. The experimental results seem informative and in support of the claims made throughout the paper, but the low choice of seeds leaves room for doubt in some cases.	4
"+) novel and simple approach and nice results on meta-world.

-) Unclear and useless SAC + TempRL description in section 4.2 and appendix A. Needs more environments for experiments.

I am on the fence on this paper. Since the results are nice and the method is corrects (they finally used the original SAC), I would be more favorable for a weak accept, but I strongly suggest to remove the paragraph about the entropic regularisation in section 4.2 as well as the appendix A and re-write Algo1 to explicite that the used RL algorithm to learn the exploitation policy is SAC.
"	4
While this paper introduces some simple and interesting ideas, without a more comprehensive experimental evaluation and a clearer explanation of the properties of the algorithm, it is not yet ready for publication.	3
The paper proposes an interesting idea, and current experimental results are impressive. However, I think the proposed method is not well-tested or discussed deeply. Accordingly, I am more inclined to the rejection unless my concerns are resolved.	2
The paper presents a set of temporal priors for exploration, empirically showing good results for certain transfer tasks. I'm concerned with the generality of the approach beyond the selected environments and the lack of competitive baselines that the paper considered. 	2
Given the concerns raised in the core part of the paper that relates to the policy objective, the paper cannot be directly accepted, but not completely rejected either. There might be a good motivation behind the objective that the authors may be able to clarify, something that would lead to a minor modification of the paper.	2
I feel the paper has solid contributions, they address both deterministic and stochastic variant of Policy gradient. The stochastic variant is more challenging due to the limitation of the non-Lipschitz gradient. 	3
"The paper has some interesting results for very generic Stochastic Game settings and Markov Potential Games (very similar to [Leonardos et al. 2021]). But I think these settings are too generic to study and extract meaningful results. Given the above comments on the strengths and weaknesses, it seems that there is more investigation and characterization that might be required. 

After the response:
The authors have clarified most of the concerns and the characterization seems adequate for this work to be published."	3
  This paper is a rather long paper and certainly over-exceeding the content that a conference paper can hold. Given the number of pages, it is challenging to evaluate for the correctness of proof for a 50-page paper within such a short time period. However, I believe the result is generally correct, for example, the sample complexity on Markov Potential Games has been discovered, probably in parallel, by many other papers. My major concern is how this paper poses itself among the peer work and if the result is still significant. 	4
Please see the detailed comments above.	2
Overall a good paper, but I am still giving the “marginally above” score due to some questions/concerns as given above; however, upon getting reasonably satisfactory answers, especially to the convergence analysis question, I will be happy to move higher to the positive side.    	3
This work has strong motivation and adds novel contributions to federated learning, i.e. studying federated learning robustness under heterogeneity, The proposed model is simple yet easy to understand, and is theoretically grounded. Experiments are convincing, although a few questions remain to be clarified by authors. 	3
"Robustness is an important issue also in the federated learning context. Overall I identify the noise detector as the main bottleneck of this work since a “smart” adversary can incorporate it into its attack strategy and can therefore probably significantly deteriorate the entire FedRBN pipeline. Also, the main results appear confusing to me and require some further discussion. 

======= Edit November 23 =========  
Raised my score to ""marginally above the acceptance threshold"" after reading the rebuttal. "	3
The rationality of technical details used in this paper is not well explained, thus I recommend the rejection of this work for ICLR.	3
This paper presents a new volumetric representation to learn 3D representations. However, some technical details should be further discussed and analyzed. Strengths and weaknesses are both discussed on the proposed CSGNN in terms of data process, method design, and experimental settings and results. Therefore, the reviewer gives rating of 6: marginally above the acceptance threshold.	3
Simple idea, clear writing, but lacks thorough discussions and evaluations.	2
Overall the paper is sound and interesting with good results. The application in atomic system is interesting and new. However the paper doesn't do a good comparison with leading papers in this field thus it is hard to judge the effectiveness of the proposed method.	2
In summary, the paper is easy to read and follow. Description of the method is clear and complete. Experiments include one for 3D point cloud classification and one for DOS estimation. For the latter one, I'm not familiar with the experiment difficulty and details and thus could not evaluate the methods from the results. Overall, I appreciate the efforts of the work to propose multi-resolution version of spherical CNNs. The downside is that evaluations are limited, and some baselines are not compared. 	2
This paper presents a novel exploration bonus, called ACB, that can be used to design viable exploration strategies for linear bandits and deep reinforcement learning. ACB admits a solid and novel design, and shows competitive performance with state-of-the-art. Finally, the paper is well-written and well-executed. 	3
Overall, I tend to vote for acceptance. Although the proposed method shares some similarities with existing approaches, the theoretical part is well-motivated and clearly explained, and the empirical part demonstrates the effectiveness of the proposed algorithm.	3
"Overall, this paper proposes an interesting way to scale exploration bonuses from linear stochastic bandits for deep RL. However, the paper does not provide convincing evidence that supports the advantages of using ACB both in bandit and deep RL setups. In particular, computational and performance improvements pertaining ACB are not explained well. 
"	3
The manuscript establishes new improved non-asymptotic O(d^0.5/\epilson) convergence rates for the unadjusted Langevin Monte Carlo (LMC) algorithm. This is a significant contribution and will be of interest to the ICLR community.	3
I found the paper interesting and I recommend accepting it. I think that a deeper  discussion on assumptions could help readers to better understand main results.	3
This is a good paper that can benefit from a clarification and improvements.	3
The two main theoretical contributions of the paper are novel, well presented and improve upon previous results. The general mean-square analysis framework is likely to be applicable to a wide variety of SDEs / numerical schemes, and thus would aid future research in SDE-based sampling algorithms. The new $2$-Wasserstein bound on ULA scales optimally with dimension and step size, whilst requiring a smoothness assumption on $f$ that is weaker than the standard Lipschitz Hessian condition. Overall, I believe this is a strong theory-based paper and would recommend it for acceptance into ICLR.	3
The paper is written and organized adequately. The paper is based on extensive experimental results. The idea is relatively simple and performs well. 	3
The paper idea is creative and has made a significant improvement in saving memory, but more experiments are still needed to support the conclusion.	3
This is an interesting paper by introducing the idea of subspace modeling of tasks into continual learning and further proposing two model ensemble strategies to improve the performance. The proposed method also achieve good performance on benchmark datasets.	3
"In my opinion, the paper introduces a very simple model for continual learning and showcases extremely encouraging results in class-incremental learning. The submission could further improve by including class-incremental experiments on a second dataset (miniImagenet) and clarifying the points raised above about model ensembling and risk bound.

---- POST REBUTTAL COMMENT ----
As detailed in my answer below, I will increase my score to 8 after reading the author's feedback."	3
"The research topic is of value and the proposed method is simple and very effective on toy examples.
Leaving Triangular Dropout for convolutions or transformers to future work makes this paper less informative than expected."	3
The paper is well written and presents an interest idea, but the weaknesses outweigh the strengths, to my assessment of the paper. The main concerns are related to the novelty of the proposal and the discussion with existing works, the depth of the experimental setup. I believe that a more in-depth analysis and stronger connections and comparisons with related techniques could make a stronger paper.	2
"This a good paper that proposes a simple yet effective idea and conveys it very clearly. However, I believe that it is currently below the threshold for ICLR due to the scale and relevance of its experiments. I will update my score if authors can provide experiments on more realistic settings (please see my review above for suggestions). I also have some minor concerns regarding the related work section and some technical details that can be solved throughout the discussion period, please see my list of concerns above for more details.


===== POST-REBUTTAL EDIT ====
Updated score from 5 to 6"	3
"Sound idea, with good potential, but experimental validation lacking.
EDIT POST REBUTTAL: Downgraded to strong reject, high confidence. Merely acknowledging the gaps in validation in the paper doesn't address concerns about experimental validation."	3
The paper proposes a method to learn surrogate losses, compared to related works, the proposed method gains significant improvements on various datasets. The originality and significance are clearly above the bar, though there still remain some minor concerns.	4
I think the proposed method is well-principled and provides meaningful improvements on various tasks. I would like to lean on the positive side for this paper.	4
In summary, the reviewer thinks the paper gives an effective way to obtain better losses, the experiments are well-conducted, and even improve the state-of-the-art methods, which would be helpful to the community. I would recommend it for acceptance.	4
Even though some idea is interesting, the theoretical work in this paper is insufficinet. 	2
Overall, the paper presents an interesting technique and opens interesting questions, but without further empirical results or theoretical underpinnings, the results presented are insufficient to assess how generalizable the findings might be.	2
Accept. While I am not entirely satisfied with the motivation behind the choices of reported experiments and the missing statement of a falsifiable hypothesis to test, the paper proposes an interesting experiment and provides a technically correct evaluation of the claims in the paper.	3
The paper, although is interesting, lacks the technical/empirical novelty to merit publication	1
This work proposes to use traversal-based positional embedding and graph-based relational embedding to encode morphological information for inhomogeneous multi-task reinforcement learning tasks. The proposed approach seems reasonable. However, the technical novelty is limited. Experiments with more seeds are needed to strengthen the paper.	2
"The reviewer thinks this paper is well written and the proposed positional encoding is simple yet seems effective. However, the reviewer has some concerns about the novelty of the paper, since positional encoding is widely used along with transformers. 
"	2
Overall, this is a well-motivated paper with good performance. The proposed traversal-based PE seems quite useful for MTRL task. However, the proposed RE seems less useful and the ablation studies are not quite sufficient. I also have doubts in terms of the PE's generalization ability, which could be addressed through zero-shot policy transfer experiments. I vote for a weak accept for now, but may change my score based on the authors' response.	3
"I enjoyed the proposed simple additions to existing state-of-the-art, showing how certain body morphology encodings can significantly improve the performance. While this work again shows that structural information can be helpful in MTRL, it brings up the question of the appropriate ways to encode the structure for inhomogeneous MTRL for successful policy learning. 

While there seems no proven ""optimal"" way for encoding structural information, the proposed PE and RE features in transformer-based policy seem an excellent first step."	3
The paper presents 2 very important ideas that I believe are of high utility to low precision efforts. The idea of weight hysteresis could actually spawn many new methods that lead to better hardware going forward.	3
"Overall, even though this reviewer finds some interesting ideas including Hysteresis and experimental results are good for ResNet-18, the followings need to be addressed.

(a) FP134 cannot be found by measuring the magnitude of the error?

(b) Do the authors suggest FP134 as a format to be applied to a wide range of models (especially even with large model size?). If so, please provide supporting data and theories.

(c) Most experimental data are only for ResNet-18."	2
Experiments show promising performance. However, there are still some concerns regarding the proposed method. Some notations in the experiments are not clear. More explanations are required.	3
"Overall, the idea of hysteresis quantization seems neat and new. However, the effectiveness of the performance indicator needs to be further clarified. The empirical success of FP130 with hysteresis seems to further weaken the motivation of finding the optimal data format. 

The authors are encouraged to address the weakness. I will consider changing the final rating according to the authors' feedback."	3
Overall I find this paper quite interesting with great potential contribution to the community, and recommend an accept.	3
Overall it is a solid paper. Please refer to the above session for the detailed discussion. I had a few concerns regarding evaluation setup and novelty. If the authors could reply accordingly, I'm willing to reevaluate the paper.	2
The paper makes a strong contribution to long sequence temporal forecasting, although there are some aspects that need to be verified before it is ready for publication.	4
I like the overall idea and the execution is decent (please see the pros and cons above). In my mind the pros outweigh the cons and I am currently rating it as above the acceptance threshold. I can strengthen my reviews if the authors answer the questions asked in the cons section.	3
I think the paper's result is great and I agree with the overall motivation behind the paper. However, many parts of the paper are unclear that I will need further clarification to correctly assess this paper.	3
This paper presents a novel plug and play text decoding method that allows improved text generation for knowledge intensive tasks via reinforcement learning based technique. The method allows several new future investigations in this direction and is very interesting (addressing exposure bias problems with LM tuning). There's work on knowledge retrieval, data structures for knowledge encoding and text decoding with state of the art integrated methods. The results are also superior compared to many existing decoding and knowledge aware decoding methods.    	3
The proposed method is fairly sound and shows good empirical results. However, some details are missing (e.g., knowledge graph construction and traversal), and hyperparameter selection is not well explained (e.g., learning rate and the number of optimization steps). It is also unclear how much KID will harm the original LM in terms of naturalness and grammaticality. Additional human evaluation or generation examples can remedy these issues.	3
"Updated to weak accept after author response.

Recommending a weak reject (5) pending the discussion on concerns raised in the main review (questions 3,4,5,6) regarding baselines and results. These seem important enough to clarify before updating the recommendation. "	3
I think the paper provides a very interesting new approach of `time-parallelization`, which can benefit the machine learning community a lot. Thus I think this paper can be accepted.	3
	3
A paper describing a novel parallelization strategy for GRU networks that adds dramatic scalability with some minimal loss in accuracy. 	3
"I find the idea to apply MGRIT to GRUs interesting, however I am not sure that there is sufficient differentiation (and major advancement) between the methodology discussed in this paper and previous work cited by the authors (I am aware of some of this paper by my own reading). This is further pronounced by the fact that previous work also considers larger hardware installations of MGRIT; the amount of 
parallel resources considered by the authors does not allow for a trustworthy scaling analysis as inter-node communication is not currently present."	2
I think the paper makes an interesting theoretical contribution in a limited setting. It may need a bit more work to bring the message home and make it significantly meaningful and illuminating in designing new algorithms to practitioners in batch RL.	3
The paper attempts to connect an iterative fitted policy evaluation method with stability based generalization bound analysis of stochastic minimization algorithms. However, the reduction to a fixed risk measure for the partially fitted procedure is not discussed and is likely not applicable in this setting. Hence, the analysis, claims and connections of the proposed algorithm and stability of ERM are not clear in this paper. Further, various key quantities are not defined in the paper and therefore are not adequately discussed in the formal analysis. As such, I feel that this paper is not ready for publication and needs a significant and detailed revision.	2
"One big concern is the writing of the paper: I do not think it is nearly ready to publishable form, as it looks that a large amount of work needs to be done to get there. These deficiencies affect the paper's readability and quality. 

I think the main idea of the paper might be interesting, and the results may have some value. I did not find any obvious flaws. The theoretical contribution is not groundbreaking, as the presented results largely re-use proofs of existing results, but nevertheless there is some merit in extending to a new setting. I am not so sure that the section on experiments adds much value.  

My evaluation is based mainly on considering the amount of fixes that need to be done for this work to be acceptable for publication."	3
Although the proposed definition is natural and useful as a criterion for evaluating the reliability of explanations, it cannot be said that the analysis using the criterion leads to useful results for improving explanatory methods, and it seems that more detailed analysis and deeper insight into the relationship between explanatory methods and the proposed criterion are needed for publication.	2
Problem studied are good, but theory and experiments have room for improvement. 	2
The paper presents theoretical guarantees for different classes of explanation methods. However, the notion of stability/robustness/astuteness has been extensively studied in the existing literature and the novelty of the work is thus a bit unclear.	2
"The core idea and the benefits of this technique are exciting, however experimental details are lacking and the performance is a little lack-luster. 

- More details are needed about how the Random Forests were used.

- The paper should focus more on the transfer learning aspect of this technique. I suggest you'll evaluate this technique on more datasets to build a strong case.

- The paper should list the cost of evaluating each method. 

"	3
This paper is novel and have demonstrated its potential. In order to make it more compelling, they must add more empirical evaluations requested above. If they do that, I am leaning towards an accept.	3
The problem of embedding various architectures into the same fixed-length vector is an interesting one. The present paper proposes an interesting approached based on contrastive-learning. However, the method is not described clearly, and some of the algorithmic choices are not well motivated. Given these considerations, I don't recommend acceptance in its current form.	3
"Overall this is a solid paper that makes a reasonable contribution to a problem of some interest to the community.

+++++++++++++++++++++++++++++++++
Having read the rebuttal, I retain my score. "	3
Overall, I think this paper is a good one.	3
This paper proposes a novel black-box decision-based space adversarial attack method based on the evolution algorithm. The basic idea is to use the L2/L1 distance with the original image as the fitness function to adjust the current images towards the target images. The experimental results are good. I am only concerned a little bit about the comparison in the targeted attack since it is somehow unfair (see in the Main Review).	3
The paper shows good experimental results, but there are some concerns about the experimental part (whether it's fair and valid). Also, the novelty of the method and the relationship with the literature are not discussed in detail.	1
The paper has several major issues from the large assumption of interventions in the expert dataset, to insufficient comparisons to baselines, and insufficient experimental evidence.	2
An interesting direction let down by poor paper structuring, a lack of detail, and a mismatch between motivation and algorithmic implementation. I highly suggest adding an appendix before addressing the other issues, as it is highly likely I've made some incorrect inferences about these missing details.	3
The paper does not clearly backup all the claims and some elements are unclear.	2
Overall, although the paper is not free of weaknesses, I think it addresses a very important problem of IRL making a first step towards the understanding of the generalization of reward functions recovered by IRL.	4
"The paper proposes a  stochastic gradient descent algorithms applied on finite sums of nonconvex functions. The main novelty of the paper lies in the application of  the SGD ideas in the context of the continual learning.  In principle, this seems to be an interesting direction. However the  writing is rather poor in general and the authors fail to give an intuition of theoretical results presented in the paper. There also seems to be some wordings that make it difficult to understand some technical details of the paper (see above). I believe that the paper in its current form does not meet the high standards of ICLR.

--------------------------------------------
Comment after authors' responses:
I appreciate the efforts of the reviewers to address my comments and the changes they have made. However, I will keep my score to 5 since I believe the paper still lacks clarity of presentation and the significance of the contributions is not sufficiently illustrated. Moreover, the writing needs improvement and I encourage the authors to further work on it and resubmit an updated version of it to a future conference."	2
"In this paper, the authors conduct a theoretical analysis of the memory based lifelong learning algorithms which is an important problem. However, the writing of the paper needs improvement and  the experimental results are not convincing enough to support the theory. 

After response:

The authors' responses partly address my concerns. But in the current form of the paper, it is not ready to be published. The authors should conduct more experiments to make the theory more convincing. "	2
Overall, I feel this paper provides some initial steps to understand the convergence of continual learning, but given some weakness I list above, I am slightly negative about it. However, I am open to increase my score based on the authors' feedback and other reviewers' comments.	3
 The paper is interesting, provides new insights and theoretical guarantees, and in my opinion should be accepted for publication.	3
Though one might see the contributions of this paper as incremental over other works in smoothing, I think these are valuable and important developments in the field of smoothing which can enable further applications to gain guarantees via smoothing. 	3
I vote for accepting. First, the paper proved a nontrivial result about the certified robustness of top-$k$ predictions. Also, the paper provided enough justifications for its unique contribution to the defenses using randomized smoothing & randomized ablation type, so I believe this paper can be distinguished from other existing works. While I believe that the work itself is publishable, I’m a little bit reluctant to give a score of 8 for this paper yet due to the reasons I provided in Comments.	3
I appreciate the theoretical analysis and the impressive empirical results in the paper but the written is not clear. I also have some questions on the experiments.	3
The author derives an almost tight l0-norm certified robustness guarantee of top-k predictions against adversarial perturbations for randomized ablation. The corresponding theoretical analysis show both the l0-norm certified radius and the tightness. The empirical results demonstrate that the l0-norm certified robustness is substantially better than those transformed from l2-norm certified robustness. 	3
"The paper proposed a VAE model for molecular conformation generation, where the decoder borrows the idea of Alphafold to iteratively refine the generated structure. Personally, the paper seems an extension of the recent progress of protein folding to the small molecule setting, but I appreciate the authors’ efforts to make it work. So currently I vote for a weak acceptance.
"	3
"
I think that this paper is an advance on previous work and probably represents
the state of the art in generating molecular conformations conditional
on the graph.  The paper could use a review of the language to clarify
certain aspects of it, but I don't think that it is strong enough for ICLR.  
Importantly, it should attempt to clarify if the improvement is mainly 
due to model size changes or due to algorithm changes.
"	2
The idea of the paper is not novel. The experiment part misses several important baselines. The text should be revised to improve readability and clarity. The ablation study section should provide experiments to justify loss function choice.	2
The work presents strong results on molecular confirmation prediction. A novel method is developed that mitigates concerns of many previous methods by ensuring adherence to geometric rules. The results shown are very competitive, though I would like to see slightly broader comparisons and more exposition on various points that are raised in discussion. 	3
"My current recommendation for the paper is marginally below the acceptance threshold. I like the novel ideas in the paper and I admire the thoroughness to include lots of baselines in the random TSP instances comparison. However, there are some unclear points in the algorithmic details. And for the more realistic TSPLIB instances, the number of baselines is much smaller. These two points left me some doubts about the correctness and the significance of the paper.
"	3
"I would vote for reject. My main concern is about the applicability of the proposed contributions beyond the TSP. Clearly the proposed approach is great for the TSP, but it is also somewhat hand-crafted for it (see weaknesses). To me the significance of a TSP-specific learning technique is limited — unless it clearly outperforms specialised TSP solvers such as Concorde or LKH, which is not claimed here. The ultimate goal of learning-based heuristics for combinatorial problems is to replace the handcrafted problem-specific techniques by end-to-end ones. Proving the relevance of the contributions for solving other CO problems would make the paper much stronger.


--------
### Update after rebuttal 

I have appreciated the high quality of the authors rebuttal. My concerns and questions were properly addressed, in particular regarding
* the explanation of the proposed smoothed gradient, 
* the novelty with respect to previous works,
* and the applicability of the proposed contributions beyond the TSP.

Overall, I think the paper provides a set of well-motivated contributions that lead to an RL-based TSP heuristic with a significant improvement in generalisation performance, compared to current “similar” methods. Some of the proposed techniques could be adapted to other CO problems/ML-based CO heuristics. Therefore I believe this paper can be interesting for the NCO community to participate in addressing the well-recognized problem of generalization of ML-based CO solvers.

I am happy to revise my score from 5 (marginally below acceptance threshold) to 8 (accept good paper).

"	3
"Technically, the proposed model uses a standard RL framework with GNN and attention as encoder and decoder. The authors introduce an equivariant model to capture the combinatorial structures of the TSP, which has some novelty and improves the overall performance, but the ablation study does not show that this model provides very significant added value. 

Empirically, the proposed model can outperform most learning-based algorithms on large-size TSP but still can not beat the state-of-the-art learning-based algorithm [1] both in computational time and objective values. Meanwhile, compared with the state-of-art OR solver LKH3, the proposed algorithm does not have enough advantage in terms of computational time.

Therefore, my decision is weak reject. 

[1] Zhang-Hua Fu, Kai-Bin Qiu, and Hongyuan Zha. Generalize a small pre-trained model to arbitrarily large TSP instances. In AAAI, 2021."	2
The paper proposes an effective deep learning model to tackle generalisation in the TSP. While the ideas of the paper are not all novel and somewhat restricted to the TSP, the authors do a great job at integrating previous work on deep learning for combinatorial optimisation into an effective model capable to generalise across problems of different sizes, which is an important challenge in the field. In particular, the use of equivariance properties and the ablation study in the paper should be quite insightful for future work on deep learning for combinatorial optimisation problems.	2
This paper is an attempt to combine the DRL framework with scheduling problems. It deserves credit for modeling the scheduling problems as MDPs to fit the RL framework and for providing new implementation insights for solving mSPs by utilizing reinforcement learning. However,  this paper has no major innovations or new theoretical insights at the methodological level but minor changes to existing techniques to make a REINFORCE variant applicable to the scheduling problem. Therefore, this paper's contribution to the community may be limited.	2
The paper solves a combinatorial multi-agent scheduling problem using graph embedding and RL methods. Both type-aware message embedding and practical suggestions to improve REINFORCE algorithm are interesting and performs well. I also appreciate extensive empirical results on two general problem classes against both state-of-the-art OR and RL based techniques. Having said that, I am not confident on how the multi-agent aspect of the problem is tackled. No theoretical result is provided to demonstrate that the ScheduleNet can handle all the hard domain constraints of a problem and can tackle the multi-agent interactions in a principled way.	3
"The paper is an interesting application of RL coupled with some tricks to the
well-known optimization problems.
In its current form, it is below the acceptance threshold, but I am willing
to increase my score if the authors address the issues raised above."	3
This paper proposes a multiagent reinforcement learning approach to solve scheduling problems (including mTSP and job-shop scheduling (JSP)). However, the techniques are not clearly described and the experimental results are not fascinating. 	2
Overall, this work fills the gap between the empirical success of SLTH in CNNs and the lack of theoretical interpretation. However, the review is not sure if the technical contribution is significant enough compared to previous theoretical work on neural networks with fully connected layers.	3
The paper fills an important gap in existing research by proving that the Strong Lottery Ticket Hypothesis holds for Convolutional Neural Networks with only logarithmic overparameterization. However, the proof and construction is very similar to that in Malach et al. (2020) and Pensia et al. (2020), and hence the technical novelty in terms of proof techniques could be considered incremental.	3
Because of the lack of novel theoretical ideas and the fact that the proofs are limited to positive inputs and an unrealistic parameter initialisation approach, I recommend that the paper is not accepted for publication at ICLR at this stage.	2
My score only indicates the initial opinion and I am willing to change my score if the authors can address my concern. Currently, I cannot clearly see the connection between the theorems and the LTH, so I would give a score of 5 at this time. 	3
An interesting paper which could open further avenues of research at the intersection of spectral methods and neural-based approaches.	4
This is a thoughtful investigation spectral clustering in classification problems for graph-based data. However, there are several concerns regarding missing forms of analysis such as deeper investigations into why certain methods work well on certain datasets.	3
I appreciate the main point of the paper, that simple models can perform surprisingly well on standard graph benchmarks, and I find such methods interesting. However, in its current form, the paper insufficiently distinguishes itself either from existing theoretical works such as the cited Lee et al., or the essential empirical point of Huang et al., and so I recommend rejection. 	2
While I like the general idea and I appreciated the effort in explaining the underlying theory, I think there is no significant methodological novelty and also a significant lack in the experimental and analysis settings. I do not consider it ready for publication in the present state.	2
This paper is very well written. But it lacks originality and also did not derive new insights.	1
"Overall, I do not find the paper interesting. The approach is not particularly novel and it is not clear how it benefits the end goal. The experiments are very shallow and do not provide very interesting insights. The paper lacks strength on both methodology as well as experiments. 
"	1
Overall, I think this paper needs to further improve its experimental section, with more explanation for the results. 	3
Overall, I recommend rejection. I didn't find the paper very clear, and the conclusions are vague and not well-supported by the experiments. The novelty seems relatively minor, since this is using the p-vectors defined before, and just adding a clustering step to their training.	2
The paper presents an interesting and well-justified task, but in the present form it is most suitable to a domain specific (applied) venue, rather than a more general conference such as ICLR. Because of missing comparisons and ablations, there is little information in the paper that I am confident can be generalized to other tasks and/ or problems, so I am not sure the paper will be valuable to someone not working in that field. 	2
The problem definition seems interesting and the application the authors tackling (voice search system for voice casting) sounds interesting (introduction is well-written), however, the proposed method seems not enough novel and some details of the experimental setup is missing.	1
"Overally, the proposed algorithm is discussed at a good abstract level, and experiments on a few small datasets are provided. 
See my comments above for more details."	3
Although the paper proposes these modules and tries to solve an interesting problem, it is unclear how these modules work together. Also, they use a sparse regression method (STRidge) which seems like a round-about way of solving the problem directly, which may potentially be faster. The paper is well-written in parts with some notational issues and incomplete description. That being said, I hope the comments are useful for updating the manuscript in the response period.	2
"The paper presents a clearly described and sensible approach to PDE discovery. The main problem they seek to solve is robustness to data quality and scarcity, which they address with a combination of sparse regularization, fine-tuning, and prior knowledge encoding architecture. I am not extremely familiar with the field but I believe, especially the latter technique, is quite novel. My only real complaint is that only simulation data was used, and it's not ""stress tested"" in the experiments or against competitors in terms of practical/empirical significance (and hence fully supporting the claims of robustness). Note: I scored myself as a 3 because I'm not that familiar with the related literature.


Edit: My confidence on this topic is low because I do not know the literature a priori. However, I am not convinced by Reviewers x24u, zdB8 that the Pi-block is redundant with PDE-Net (to me it seems more efficient and sparse, at least), and I call into question Reviewer zdB8's claim that this is a ""more complex"" system than DLGA-PDE. All in all the other reviewers have shaken my confidence in the empirical results and rigor is methodology descriptions, but not as much in the novelty. I will lower my score to 6 to reflect this."	3
The paper is technically correct but lacks novelty and clarity. The experimental demonstration is not sufficient to support that it can also work for state-of-the-art PDE discovery models and for more complex PDEs. The reported robustness is also not exceeding the state-of-the-art models. Besides, the presentation and notation definitions can be improved. Based on these observations of the submitted paper, I lean towards rejection.	3
"This paper addressed an important problem, PDE-discovery and was clearly written. Some points are new but there are some critical issues on novelty, experiments, knowledge and data which suggests a reject.
"	2
"+ The idea to built a structured query following a grammar is interesting. This follows the work of MuZero.
+ The use of pseudo relevance feedback to generate candidate terms to add in this context is also interesting.
- Unfortunately, the performance is not at the level expected.
- The complexity of the method is very high, making it difficult to use in practice.
- The presented grammar is used only partially in the best performing case. It is unclear if the grammar defined in this way is reasonable.
"	2
This paper contributes to an interesting direction - training RL agents to find information via iterative query refinement. The proposed method achieves descent performance in a more interpretable way. But one concern is the model reproductivity. It would be very helpful to have the model full training recipe for reproductivity and benchmarking.	3
While this is, as the authors explain, an endeavor that required customization of several components to reach comparable performance with DPR, the method proposed appears to be computationally expensive without significant improvements over neural retrieval. Alongside the lack of comparison over prior work on RL for query reformulation and the lack of discussion over other dimensions apart from performance (and maybe some qualitative analysis), the paper will benefit from further extensions and revisions.	2
"I find this paper an interesting read. The comprehensive study into different variations sheds light on effectiveness of language model based translation models in different settings. The approach, experiment setup, and evaluation methods are reasonable and provide confident insight into the questions that the authors ask. 
This is valuable in the progress of research in developing new translation models. 
One downside of this work is that there is no novel approach proposed in this paper, as well as a more comprehensive study into existing models and combining various setups is more useful to the research community. "	2
This is an OK paper but the current version is not exciting, which could hardly attract attention from the community. The authors should answer the question: how could the paper findings benefit the community?	2
This paper is a good empirical work investigating the unified LM for machine translation and shows some good findings. However, the motivation yet remains unclear, plus few insights are delivered. I feel like the paper is not bad in terms of empirical study, but feel a bit not interesting regarding the bar of ICLR. I tend to consider it as a borderline paper but are willing to change my mind if I did miss something upon the author response.	2
This paper has conducted extensive experiments to examine the scaling and transferring laws of LMs for machine translation and has concluded several interesting findings which could be inspiring to the future work.	3
My main concerns are about the clarity and correctness of the paper (the empirical results are already great!). If the paper were revised to address these concerns, I'd be inclined to change my vote.	3
A somewhat obvious extension to GCSL (online goal-conditioned BC), but that is well-explained and shows convincing experimental results.  I vote for acceptance, and believe this provides a potentially new baseline for offline goal-based methods.	3
"While the empirical results of the proposed approach looks promising, as pointed in the weaknesses, I have concerns about clarity of the paper, presentation of the theory and empirical rigor. If the authors can address these issues, I'd be willing to re-assess the paper.

--- After rebuttal --- Updated my score to 6."	2
In its present form, I believe the paper should be rejected due to the lack of background information necessary for a large part of ICLR's audience to understand the paper and its contributions. 	3
As the paper stands, I suggest rejection. The method is interesting and I find the empirical results promising, but the methodology sections need large amounts of reworking to improve clarity and to fully justify and explain the choices made by the authors. The paper would benefit greatly from these changes.	2
Authors proposed to joint optimize low-dimensional embedding and topological loss, but the proposed method might be thought as a straightforward extension of existing work. The experiments lack the relevant methods for comparisons and the proposed method seems not significantly better than one of simple baselines from visual perspectives. Some claims need to be clarified in detail.	2
The paper is based on a series of several 2020-2021 papers, with which I am unfamiliar, so it was difficult for me to judge on the novelty. I found the paper interesting, and the method seems to work well. My biggest criticism is that the goal of the method appears somewhat artificial: low-dimensional embeddings are typically done for data exploration, but one does not want to enforce any particular structure for the purposes of data exploration, and the topological structure is typically not known a priori. That's why I am giving a borderline score.	3
The paper proposes an original contribution for the graph representation learning community. The proposed model is novel, while the paper provides some theoretical analysis of its benefits. The empirical results are strong since the PF-GNN model outperforms the baselines on almost all datasets.	3
Overall I think the paper deserves a weak accept: the proposed idea of generalizing IR is novel and important, however the method is still not practical enough to use, and without good theoretical theorem for the expressiveness of the designed sampling version IR.  	4
"The paper proposes PF-GNN, an interesting addition to the literature on individualizing GNN node embeddings which demonstrates strong empirical performance. However, the value of the approach cannot be fully established, as the gap with existing methods is very large, implying that multiple factors (e.g., randomization techniques, initialization choices) could also be responsible for the performance improvement, rather than the proposed approach. Moreover, further evidence and experiments is required to validate the strength of PF-GNN over real-world data. Hence, the authors should conduct further experiments with intermediate randomization and individualization choices, and evaluate PF-GNN on standard benchmarks, to better quantify the impact of their proposed approach and confirm its significance. 

DISCUSSION UPDATE: Rating increased from 5 to 8 following additional experiments (details in review replies). "	2
The paper is theoretically sounded and the empirical performance of PF-GNN is impressive. Still, I think the explanation can be improved and I also have some regarding the experiment. Together I lean to vote for acceptance of this paper.	3
As is, I found the results interesting and convincing enough to be just over the bar for acceptance. However, I am fairly uncertain in my assessment in part due to a lack of familiarity with the relevant distributional RL literature. In any case, I would feel more strongly about acceptance if the writing clarity were improved and additional diagnostic explanatory experiments were added, as outlined above.	3
The paper is well-written and the algorithm is well-justified. However, the main technical contributions seem to come from previous approaches (agent-wise risk level from Weighted Q-MIX and environment-wise risk level from DFAC). Although the experiments show an advantage of the proposed method in hard multi-agent tasks, it is critical that the authors better clarify the algorithmic contributions of the proposed method. Especially, it is important to distinguish the risk level controls of DRIMA from the algorithms it was built upon. Hence, my score for the paper in its current form is a “weak reject”. I am willing to raise it if the authors address the concerns pointed in my review.	3
The reviewer finds the problem under research in this paper interesting. However, the reviewer is not convinced that current methods can solve the problems and disentangle the environment- and agent-wise risk. Current experiments do not resolve this concern.	2
The heuristics are poorly motivated theoretically, and the experiments do not clearly highlight how disentangling risk sources can be helpful to drive risk-sensitive behaviour. 	2
"My starting recommendation is: weak accept.

I think the empirical results speak for themselves, and the concept of disentangled risk is well motivated. In addition, I think the ability to separately tailor risk-preference based on the source of risk is an interesting contribution. Unfortunately, much of the contribution is hard to appreciate because various technical descriptions are vague and overly reliant on jargon.

I am confident that this paper could become a clearer ""accept"" if the communication were improved. In addition, some extra analyses to more deeply explore the impact of disentangling risk sources would validate the technique. I am happy to increase my score, to the extent that these suggestions are followed."	3
"I appreciate the two novel identifiability theorems. However, the ablation study  in the experiments section lets the conditions appear to restrictive. Furthermore, it is not clear what distinguishes this work from Sorrenson et al.'s article apart form the identifiability theorems. Therefore, I tend towards rejecting this article but I am open to enter a discussion with the authors and the other reviewers.

Edit:
Given the helpful clarification for the authors to distinguish their work from Sorrenson et al.'s article I will raise my score by one."	2
The paper provides an interesting theoretical analysis of an existing method. I weakly recommend acceptance, as the assumptions are not sufficiently motivated, the assumptions are shown to not be necessary and the experimental evaluation is not very strong.	3
Overall, my impression is that this work provides some solid contributions to the interesting nonlinear ICA problem. One issue I hope the authors can address is including the source codes in a code appendix.	3
The paper proposes a simple and elegant solution to the disentanglement problem encountered in ICA. So I recommend in accepting.	2
This is a good paper even though I do not have familiarity with the recent developments on that specific topic.	3
"I don't feel qualified to comment on the method's limitations in the broader context of previous work. Outside of the method's quality, the experiments seem well executed and the work is relatively clear. It seems like the method presented does better than the baselines considered, although these might be limited (as mentioned in ""originality""). Overall, I'll mostly remit to other reviewer's judgement on the method, and wouldn't be opposed to this paper being accepted."	3
Overall it's a good combination of exisiting ideas in latent-variable modeling and CTDE MARL to demonstrate effective ad-hoc teaming behavior.	2
Overall this is a reasonably solid paper which presents a novel approach for ad-hoc teamwork policy learning via learning latent embeddings of partial teammate observations. 	3
"I believe the paper is interesting and provides a novel contribution in terms of ad hoc teamwork (addressing partial observability). It would be good if additional discussion could be made regarding the assumption of ""centralized training""."	3
"While I enjoyed reading the paper, my biggest concern is lack of real world experiments. I appreciate the authors on conducting experiments on multiple runs, repeating the experiments with different model sizes, visualizing the results effectively and conducting carefully crafted simple experiments that illustrates the specific points being made. The carefully experiments were all designed well. However, for practitioners, a pressing question would be if these architectures can be used as a replacement of the traditional transformer models. Can this be used for training language models or benchmark NLP tasks?

To this end, it would have been really nice if authors had shown one experiment on real world datasets. I understand that some institutions might not have enough resources to train large-scale models, but a proof of concept at a decent scale on a realistic dataset would have been nice. Such an experiment would tell whether these findings would translate to large scale datasets and tasks. I would also like to point out that the authors have one experiment on protein-protein interaction graph prediction, but I am not aware of this benchmark and I am unable to gain much insights from this experiment aside from the fact that the model works.

While this issue is certainly concerning to me, I believe the findings in this paper are a reasonably good contribution that could help others in the community. It's just that the paper would have been really strong had the authors gone beyond synthetic dataset and done some benchmark NLP experiments.
"	3
The authors theoretically analyze the limitation of using softmax function in attention module and propose an unconstrained surrogate in the paper. However, the empirical evidences shown in the paper are insufficient to validate the method. 	3
The paper is well-motivated by the problem of sensitivity to hyperparameters and propose a sensible algorithm to solve the problem. The experiments on the synthetic task are sound. However, it seems lacking some convincing results on real-world dataset. And the relation to some prior works needs to be clarified.	3
While the practical viability of the proposed method is still uncertain, the insights on using softmax attention from this paper are valuable. The proposed synthetic evaluation framework is also a good foundation for further discussions on softmax-attention alternatives. 	4
"Overall, I am leaning towards accept as I like the idea and the results are very strong. I have provided potential ways to improve the paper in my main review, specifically points 2 and 5.

*****************
After rebuttal: I have increased my score to 8, thanks to the authors rephrasing certain parts / adding certain comments/references to the paper. I believe the paper could still be increased by tuning hyperparameters of baselines, however, this does not detract from the quality of the proposed method."	3
In summary, the paper presents a crisp solution to a challenging and important problem with mostly clear writing and experiments. Some of the concepts introduced here (e.g., gradient direction change and FEL) can be useful not only in continual learning research but applications where forgetting can be an issue or decorrelating features can be advantageous.	4
I have two issues with (i) the assumptions made in parts of the derivations, and (ii) the missing important baselines that should be derived from your approach.	3
This paper provides an empirically very strong method for task-based continual learning, as measured by average task accuracy, alongside a solid theoretical derivation and justification for it, which will be of interest to the continual learning community and for these reasons I am recommending it for acceptance. The main weakness of the method is that it has a runtime that is quadratic in the number of parameters at each layer, as a result of the gradient projection at each update; the paper could greatly benefit from an empirical analysis of the runtime compared to competing methods in order to quantify this limitation. 	4
I believe the paper is good and intuitive, however needs some improvements that can improve its contributions.	3
"The research questions asked in this paper are very interesting and go in an orthogonal direction to most of the CL research, which could be beneficial for the CL community. The proposed solution was evaluated empirically with satisfactory results and it's very easy to implement, making it a useful tool for CL researchers and practitioners. On the other hand, the paper would benefit from a more thorough investigation of the problem and fixing certain issues in terms of empirical evaluation. As such, I would say that at the moment it is marginally above the acceptance threshold.

**EDIT after the discussion period:** During the rebuttal process the authors provided important clarifications and introduced significant improvements to the paper. As such, I have decided to increase my score to 8 (accept, good paper). "	3
The proposed Continual Normalization is novel, however, the reasoning and intuition behind the idea should be elaborated better. Moreover, additional experiments without replay-based method can better demonstrate the effectiveness of the method.	3
"Overall, I vote for marginally accepting. I like the idea of cross-task normalization and handling it by the proposed adaptive normalization layer. My major concern is about the limited novelty of the paper and the performance of CN on catastrophic forgetting (see weakness above). Hopefully the authors can address my concern in the rebuttal period.

[After rebuttal]
After reading the other reviewers' comments and the authors' rebuttal, I confirm my rating."	3
"Balancing the weaknesses and strengths from the list above, the paper is, in my view, at the threshold between acceptance and rejection. Since there is no neutral score I can select, I am voting for weak reject for now, but I am open to changing my score during the discussion period. 

EDIT: Updated my score to 8. "	2
The ideas presented are physically well motivated but only incrementally advance on the state of the field by combining many existing ideas, and the application scope is very narrow so it is not the case that the paper derives a significant contribution by showing that these techniques taken together are greater than the sum of their parts etc. The presentation is very professional and their is some novelty in extending the equivariant message passing scheme of Satorras et al., but these are not strong enough contributions to justify acceptance alone.	1
"The paper presents and incremental improvement over previous works and was done cleanly, so a marginal accept. If it had been exploring a fully learned generalized coordinate transformation that uses an inductive bias but does not need to hand design the coordinates and kinematics I'd give a higher score.

://EDIT: updated my score"	3
"The paper has a reasonable balance of novel contributions along different axes. I am not sure any one of them is sufficiently novel/interesting to meet the ICLR bar on its own, but put together and following corresponding revisions, I think the could make make a bigger ICLR-worthy contribution. My only concern is that it may be hard to reach the right audience, since I suspect many readers would be interested on each of the contributions individually, but not sure many will be interested on all 3 of them simultaneously. I would perhaps recommend including ""E(n)"" or ""equivariant"" in the title of the paper to reach a larger audience."	3
This paper is a pilot study of an improtant problem (continual knowledge learning of LMs), but the problem formulation is overly simple and there are still many important yet missing points in both data construction and experiments. Also, there are no much insightful and non-trivial findings with deep analysis of existing methods. Please find more details above. 	2
The paper studies an interesting problem. The proposed benchmark and metric are technically sound. However, there are some concerns about experimental settings.	3
This work is quite insightful for us to understand more about how LM continuous learning works, although I think more experiments could be beneficial as I mentioned in the weakness section. If we can make sure the numbers from this paper are reproducible and comparable, then I think it could be a good testbed for future research in this direction.	2
The paper formulated the problem of Continual Knowledge Learning and benchmarked the performance of large language models on this task with different CL methods. The tradeoff between forgetting existing knowledge and updating old knowledge/acquiring new knowledge is quantified through a new metric, which would serve as an important optimization goal for future research. This work is a big contribution to the community and would invite more research into this topic. 	3
"I believe this paper should be accepted on the merit of having:
* A pertinent problem: action embeddings for large action spaces.
* A well thought-out approach to tackle this problem: Using a set of reasonable priors on action similarity to provide a general embedding for actions given an existing dataset of behavior on the environment.
* Convincing experimental results: Both in the on-line and off-line case the approach works well, and is shown to work better than other action embedding schemes.

However it has issues in its form:
* Clearly missing description on Figures 5 and 6.
* Hard-to-read figures for results.
* Not always easy to follow explanation of the method.

If the authors can manage to clear up at least most of the form issues I will support strong acceptance."	3
This paper is well-written and the experiments are well-done to illustrate the effectiveness of the proposed metric. However, the novelty of this paper is pretty low, as a bisimulation type metric is well known. Though adding a new distributional component in the offline setting helps, it does not well-justified at all. Given this, I recommend a weak reject for the paper.	2
In summary, I think the proposed method BMA is interesting and the evaluation is promising. It would be great to add a few experiments as mentioned above to make more fair comparisons and obtain a more convincing conclusion. 	3
This paper has interesting experimental results, but the proposed method and the theory are not novel enough to accept. 	1
"Overall, I think a lot of careful thought went into this paper and the design of the NODE-GAM and NODE-GA$^2$M architectures is clever enough to be inspiring in its own right. However, the missing comparisons, particularly to the ""univariate networks"" for modeling main effects from the ""Neural Interaction Detection"" paper (which is the first thing that tends to come to mind when I think of a ""neural network based GAM""), leave me hesitant to recommend the paper outright. I hence currently rate it as only marginally below the acceptance threshold, but expect that can easily change on revision."	3
"The paper proposed an interesting architecture for interpretable deep learning models. However, based on the empirical results, the method does not seem to be superior or only marginally better than traditional methods such as the spline model.
"	3
I would tend to accept this paper as it is novel enough and supported by empirical experiments.	3
While the main motivation of this work is clear, there are several parts of this work that need to be revised. In particular, it should be clearly stated which parts of the theoretical results in this work are new. Also, the experimental part of this work can be significantly improved by including stronger baselines and more datasets. 	2
In summary, the paper explores a very relevant problem in certification against adversarial examples. The main ideas of the paper are pretty novel and would help future research. The paper does a great job of presenting the ideas and discussing some of the potential limitations. However, it would be better if the authors briefly discussed the mismatch between the training objectives and the test time prediction model. Taking all of this into account, I would recommend the paper be accepted.	3
Collective robustness certificate is an important research problem. The proposed method is novel to some extent, but its key difference with the existing work is not unclear for me, and so as the performance comparison. My another major concern is on evaluation. For instance, the image segmentations results are not reported using reasonable metrics and the models and datasets are rather outdated on my end. 	2
See above.	2
I will weakly recommend rejection, as it is unclear the extent to which the effect of this method is from the parallelism v.s. the intrinsic  motivation, and it is unclear how this approach would compare to the standard diversity promoting intrinsic motivation methods.	1
This is an interesting setting, but the method lacks novelty (see first weakness). Additionally, the results do not paint an accurate picture of D&E compared with other baselines (see second weakness). Perhaps this project could look more at reconciling options/skill learning work with intrinsic motivation and do more evaluation and comparison of baselines in these areas.	1
This paper clearly presents a simple multi-agent intrinsic reward method for encouraging agents to not only explore regions that they have rarely encountered, but also those that the other agents have rarely encountered. Their results are promising, but lack proper baselines to get the full story about the effectiveness of their method. If the authors address the weaknesses pointed out above, I will enthusiastically raise my score to an 8.	3
This paper presents interesting ideas; however, they are extremely similar to prior work that is not discussed or compared to in the paper. It is difficult to judge the significance of this work without comparisons to (or at the very least discussion of) this prior work.	2
Overall, the paper is convincing and has reasonable suggestions in creating a correlated continuous latent space for pathological data. However, there could be more comparisons important in proving their case. Also, there could be more considerations on new ways the model could be developed, given how GAN has been evolving over the years.	4
This paper proposes a Surreal-GAN method for learning representations of underlying disease-related imaging patterns. However, the comparison experiments are not sufficient, and the novelty section needs to be improved. 	1
Overall, I would recommend this paper to be accepted. Personally I find the problem the authors are tackling legitimate and usually neglected, and I am persuaded that the proposed solution has some generalizability.	3
"The presented methods present some interesting ideas for modelling disease-related representation. However, the description of the presented method needs some improvements. 
"	3
Overall, due to the points mentioned above I found the work to be very deductive and not systematic. Thus I vote for rejection.	2
Reviewing this paper gives me the impression that the paper is a little outdated (for example the most recent paper cited was in 2019). I found a very similar submission for ICLR 2020 and I agree with the reviewers/CPC's discussions. Although the paper is very well-presented, the previous concerns have not been fully addressed in the current manuscript. Thus I lean toward rejecting the paper.	3
I really like the simple and elegant nature of the solution proposed in this paper but, overall, I’m not convinced about the novelty wrt prior work, so I’m going recommend a borderline reject (however, I’m willing to be persuaded)	2
This paper conducts interesting experiments to demonstrate that robust networks' features are more discernable via representation inversion and feature visualization. However, it is unclear to me how this work is different from previous paper (Tsipras et al. (2019)).	2
This work introduces a semi-supervised reward learning approach to reduce the efforts of reward engineering. However, the technical novelty is limited. Some assumptions made by this work are too strong and may not hold. Some important descriptions (e.g., the reward function w2 and w3 in Main Review) are missing.	2
"My recommendation score is marginally above the borderline. I like this paper overall but the discussions of ablation study need to be enriched.

The methodology introduced by this paper is inspired by related studies in supervised learning. I think it is quite important to understand why and how these techniques benefit RL algorithms.
"	3
The proposed method is sound and interesting, but of limited novelty. The support for the full method is good, but without an extended ablation study, the support for TDA is not sufficient. The method is likely significant within the area of PBRL, but but not for a broad audience. The paper is clearly written and can be easily understood. 	3
The paper is theoretically interesting but the results are limited to small graphs. 	3
The motivation and the theory of the paper is clear and correct. But there are some space for the authors to further improve the quality of the paper, also the value iteration part of the paper is to restrictive and it need to be extended.	3
The paper uses fixed-point iteration to find the equilibrium point of the graph network and constructs the transition map to be contracting and input-dependent to ensure a unique fixed point. The paper provides extensive comparisons with other equilibrium models and on different types of graphs. It is a clear accept for me.	4
The paper is well written, clearly explains the underlying assumptions and limitations, and shows promising results on a range of experiments. 	3
The paper is generally well-written and given the technical content. The reviewer considers the proposed method quite interesting, but some issues are to be addressed empirically. 	3
I cannot recommand the acceptance of the paper until the above issues and concerns are addressed. But I would be happy to increase my score if any potential misunderstandings are clarified. Please see detailed comments above regarding technical concerns. 	3
"The central proposal of the paper is the noise-contrastive loss for VIBN, but its effectiveness is obfuscated by other two proposed tricks that are not well motivated and studied. Though the proposed combined method leads to better accuracy and calibration than output regularization methods, the claim that the noise-contrastive loss increases the separability of correct and incorrect samples is not clearly backuped with sufficient empirical evidence. Thus, I recommend a ‘reject’ for the paper but will consider raising my score if my concerns are well addressed. 
"	2
Overall, I am giving a score of 6. The novelty is not ground-breaking but the authors achieve good results for an important topic. Importantly, the design choices made are well supported by ablation studies.	3
"## Strength:

1. Very comprehensive experiments.
2. Relatively simple and reproducible method.

## Weakness:

1. Lack of very strong theoretical justification. It is hard to have a straightforward interpretation of why this works, especially based on pseudo labels.
2. Some experiments seem not relevant to the problem this paper aims to solve."	2
Overall, this paper conducts thorough empirical analysis of the SSA approach, which can be incorporated with robust training methods. I recommend a weak accept because there are portions of the scheme that I think needs better justification. 	2
Overall, this paper has strong empirical results. The paper could be improved with more precise / clear descriptions of the methods and design decisions, and additional ablation experiments or metrics that help explain how the methods actually perform so well even in the extremely low-annotation setting (in lieu of theoretical analysis, which the paper does not have). Thus, my current score is a weak acceptance.	3
Overall, this paper proposes a novel and general yet effective framework. The paper is well-motivated and well-written. Moreover, comprehensive experiments have been conducted to demonstrate the effectiveness of the proposed approach.	3
I think this paper is below the acceptance threshold because it is just a simple addition to the method of Khalifa et al.	2
This paper addresses an interesting direction to connect RM and DM paradigms and shows promising results for better controlled language generation with their proposed variance reduction technique. Though I'm not sure if the technical novelty is sufficient and the empirical results are strong or practical enough given limited technical novelty.	3
Baselines for DPG methods are introduced, which improve optimization performance. However constraint satisfaction, the ultimate goal of the process, does not appear to improve, and neither qualitative nor qualitative evidence of improved test performance has been included. This, and the fact that DPG baselines are quite straightforward to work out, make the contribution of the paper seem quite low.	2
"The idea to use a baseline to stabilize training in DM is rather simple but quite efficient as demonstrated by the experiments, and I think this is an overall useful, if somewhat incremental contribution. That being said is not quite clear to me that there isn't a simpler, more straightforward way to reduce variance in DM (see my full review for details).

Overall I think that the paper would be interesting to the community and I recommend acceptance, however I set my score as ""borderline"" until the concerns I raised in my review are addressed by the authors."	3
"I found the motivation of this paper is weak as its link to the LTH is not clear.
Further, results don't seem to suggest the idea of winning tree is very useful.
Therefore I suggest not accept this paper.
It would be interesting see if the author(s) can push this idea further (or implement in another manner) and show its usefulness - I'm looking forward to see more research on symbolic methods in the ML community."	2
"The work is insufficient in many aspects in order to be considered for acceptance.
- The main motivation behind the work seems unfounded due to a misunderstood concept from the deep learning area.
- Related work discussion is lacking in regard to some relevant works to the method presented.
- Experimental evidence is incomplete.

My recommendation is a clear reject-"	1
Although the idea seems interesting, this paper is not technically sound. Experimental results cannot fully support the main conclusion of this paper. Without a strict mathematical definition together with some theoretical analysis, it is difficult to fully understand the new concept of “winning trees” in GP.	2
Let me state first that I fully support further research in evolutionary computation and especially genetic programming, which I believe could develop in a feasible alternative (even extension) to neural networks in real-world scenarios where access to labeled data is insufficient. This paper proposes an optimization to GP that could improve its performance, which is its main limiting factor to a broader application. Nonetheless I feel the present contributions to be (i) relatively minor (just simplification and restart), (ii) not particularly novel (again, just simplification and restart), and (iii) insufficiently supported by the experimental section (no baselines, no state of the art comparison, no clear exposition of the relevance of the datasets, extremely hard to read presentation). Overall the sensation is that of a work in progress, which may very possibly lead to a crucial contribution down the line, but is not yet ready for publication especially at a major conference.	2
The experimental results are not strong enough to demonstrate the benefits of the concept of magnitude vector of images.	2
"The reason that I have recommended this paper are several fold. 

Chiefly it is unclear what the novel contribution of the authors are in this paper. Taking the contributions one by one: 
 - It is unclear what the benefit or novelty there is from the study of outliers and edge detection
- The improved algorithm is not explained, presented, or studied empirically with reference to any baseline.
- The new layer the authors present involves a computationally prohibitive matrix inversion that will limit its applicability. "	2
The paper requires major improvements to motivation, empirical results, and rigour of theoretical contributions.	3
This paper presents an application of magnitude vectors to image, which is not convincing both from a theoretical perspective (why should the given definition be useful? why should the given speed-up algorithm work well?) and from an empirical perspective (the results on edge detection and adversarial robustness are weak).	1
The analysis just considers the SGD with fixed step size however in practice we use decreasing step size to train DNNs. What are the challenges of analyzing this scenario? 	3
To conclude, the results of the presented paper may be of great interest, but under the current form, without any proper mathematical formulation, more rigorous comparison, or further perspectives with SGD behavior, I consider that this paper should be revised before resubmission.	3
There is a deep inconsistency in the theory proposed in this paper.	1
The paper demonstrates the use of a conditional VAE in the context of solving stochastic integer programs. The approach seems technically sound and the empirical results show improved performance over both clustering the raw scenarios and over recent baselines. On the other hand (conditional) VAEs have already been used for similar combinatorial optimization tasks before, and their use here towards scenario clustering has small additional significance and novelty.	2
"The improvement over baselines are nice and the paper is well written, but its positioning within the broader literature and the meaningfulness of the numerical results leave fundamental questions that I would like to see answered before improving the score.
"	2
This paper considers an interesting problem, but I am not convinced that the proposed models are warranted in theory. In addition, the practical utility is not significant or clear with the presented experiments, and there are many settings needing justifications. However, I look forward to the authors’ response. 	2
The mathematics is clear and convenient to read but the overall story is somewhat confusing due to the connections to neural networks and some other branches that do not contribute too much to the main message. The closed form solutions of the leave-one-out computations are not very novel and the most can be straightforwardly derived from the existing literature. I find the idea of considering the double descent behavior emerges from the closed form leave-one-out formulations to be interesting and it may deserve more attention.	3
This paper studies generalization of deep learning with LOO error using neural kernels. This is a relatively novel tool that has not been thoroughly explored in deep learning theory and opens up a new window of studying generalization properties of wide neural networks. There are still some questions regarding the effectiveness or usefulness of the proposed metric detailed above.  With current limitations, the paper is borderline and leaning rejection. However if authors could clarify some of the doubts, happy to move up the score for acceptance. 	3
"Based on the concerns with the experimental setup and what exactly it shows I recommend to review the perspective at which the results are presented. In the current form the paper cannot be accepted.

---
Based on the authors' explanations and editions of the paper I change my score."	3
Although this paper focuses on a very interesting issue, there are still some places in this paper that make me confuse. At present, I think the author needs to further clarify my confuse.	2
"I think this paper introduces a new distributed value-based multi-agent reinforcement learning framework to ease the demanding data transfer and accelerate the training process, which is a benefit of large-scale training. However, this paper has some fatal flaws:
- The approximate equation in subsection 2.3 is wrong.
- The diversity design lacks innovation with a fatal error.
- The design of a multi-queue manager needs to weigh the pros and cons.
- The basis for computing priority is unclear, and another ablation study is necessary.
- The experiment needs a more convincing comparison.

Some other comments: Figure 1 need more clarification and refinement. Too many colors are used and the pipeline is unclear. Its title also has no explanations. "	2
"The paper does lead to some improvement in the field of multi-agent reinforcement learning, although the techniques adopted seem relatively incremental. Furthermore, explanations on experimental parameter choice and novelty should be included.  I’d be happy to change my scores, if the authors address such concerns accordingly. 
"	2
The paper presents clearly some interesting ideas that improve the training time of MARL algorithms. However, the presentation of their results is lacking in both baselines and in metrics (convergence or sample efficiency). At this stage, I cannot recommend this paper for acceptance. 	3
The ideas in the manuscript are novel, but clarification is needed as stated in the Main Review. The figures need work. Moreover, additional empirical novelty, either further evaluation on the datasets already examined as a part of this manuscript, or on other datasets, is recommended.	3
"I found the proposed approach interesting; however, I think the paper can be further improved in terms of format and empirical evidence. 
################### POST-REBUTTAL COMMENTS ########################

I found the authors' response adequate; hence, I'm adjusting my score. "	3
The authors clearly state the contributions of their paper compared to the previous literature. The proposed inference method is novel, and the math derivations look solid. And the quantitative comparisons in the experiment results look competitive. Overall, I recommend this paper to be accepted.	3
Overall, the manuscript discusses an interesting approach, and has wide application potential, given that it promises to facilitate the tuning of Kalman filters. However, I have concerns about novelty. I also think the paper could be better organized.	3
The paper proposed an interesting few-shot link prediction problem. However, important related work is not discussed in the paper. More work should stress the difference between the proposed method and prior work. 	2
A novel application of few shot learning and domain adaptation to machine learning from graph representations.	4
The paper should clearly claim its novelty and run more experiments that are closer to real-world scenarios. I am going with a rating to marginally reject the paper, but willing to change the rating if authors can address the above concerns.	2
This paper is not well motivated and needs further justification.	2
Due to the above issues regarding the results and the presentation, I recommend rejection for the paper.	2
The paper presents a novel framework for bounding the generalization error of neural networks. The techniques introduced in the paper are likely to have a broad impact. But the algorithm-independent nature of the developed bounds is likely to make the bound compare inferiorly to bounds derived specifically for SGD.	4
"I recommend rejection, for the reason that the theory in this paper (i.e. the main contribution) is either incorrect or missing many crucial steps. The statements and proofs brush many details under the rug and hence are very difficult to verify. Even if there is a correct underlying argument, I believe the paper would benefit from substantial revision and submission to a later venue.
"	2
It seems this paper is not in its shape to be published due to the above concerns, including the concerns on the proof and the unclear statements. More explanations on the theorem and the proof procedure may help reshape the paper. 	2
Although this paper is technically sound, I think the polynomial assumption on activation functions of neural networks is not very convincing, making the contribution of this paper limited. 	2
This is a nice paper on an important topic and with a successful proposal evaluated extensively. The novelty may be less than ideal due to the integration of existing ideas.	3
The authors propose an interesting combination of known methods to learn the geometry and topology of the  graph for molecular-based applications. The results reveal that some improvements over existing method is obtained. Some of the limitation could be further discussed, and more experiments would strengthen the contribution of the paper.	2
"Overall, the problem studied is interesting. The authors aim at designing new NN architecture to generate geometric graphs. The experimental results look promising. Moreover, G3 is much faster than current strong baseline methods. 

However, the generalization ability of the proposed architecture to other datasets is unknown. The encoding part seems heavily based on other works (similarly decoder parts). Compared with other methods, the reviewer does not see the real novelty part of the technical section. (The problem of graph generation is out of my research area. I may miss something important.)"	2
"In general, I like the idea but given the fact a lot of concerns need to be addressed, I tend to give a borderline. 

However, I am willing to adjust my scores based on the reviewer's reply."	3
"The authors have done a commendable job in a very relevant research area which will become more prominent in the coming years. The key contributions are: 1) CloudBandit a best-arm identification problem for multi-cloud configuration optimization. and 2) the public release of a multi-cloud configuration dataset. 
However the novelty does not seem to be adequate keeping in mind the standards of the conference. Also there are some weaknesses in the evaluation results and the comparision with SOTA is not clear."	2
"Overall, the paper is well written. However, several aspects of the problem were ignored (see main review). The dataset presented could be meaningful but currently it lacks the rigor in generating the dataset or providing meaningful motivation for selecting ""dask"" tasks. This paper has the seed for significant contribution but the current quality of work is not sufficient for publication in a conference."	2
The authors study the problem of multi-cloud configuration in this paper, where the problem can be defined as how to choose the best cloud provider based on runtime or cost for a given workload. They propose an algorithm (CB) to help solve this and compare the algorithm for its effectiveness against SMAC, RB and show that it performs well at solving the optimization problem. More work needs to be done to show how complex workloads can be translated into configs that CB can solve and also account for more parameters when optimizing for runtime or cost as discussed above. 	2
In this paper, the authors try to solve the multi-cloud configuration problem with an algorithm from the AutoML domain. It looks good, but it requires more explanation and clarification why it is a good/best way by the proposed method, why it is better than other methods, why it works. And it also requires more justification of the correctness of the evaluation data.	2
Even though the paper takes heavy inspiration from the previous works, I believe the results are important and would be beneficial for the community. It provides new insights for effective transfer learning in the few-shot setting.	3
This paper claims to provide theoretical insights on the neural collapse in the transfer learning setting, but the theoretical results are not sufficient. 	3
 Even though there are some concerns about the experiments in the paper, I believe the paper has more merits than flaws, in proposing a possible explanation for the superior performance of foundation models in few-shot settings, and validating it theoretically and empirically.	3
Neural collapse is only investigated existing samples and classes before. It is significant that this paper shows that neural collapse generalizes to new samples from the training classes and new classes both theoretically and empirically. However, there are some minor concerns and I think this paper is marginally above the acceptance threshold.	1
"The paper did solid experiments on Neural collapse on unseen classes for transfer learning. The angle is new and the experiments are done neatly with a good amount of hard works of large scale computing. And their experimental results supports their main claim very well. The theory part is more standard and I have not found surprising theoretical insights from it. Overall it is a good paper worth recommending to ICLR as a solid incremental work in the field of Neural collapse.
"	2
"Overall:
Based on my limited understandings and mathematical background, the authors propose modifications to NAF, prior work, and prove it can solve certain classes of control theory problems. "	2
The manuscript cannot be accepted for publication in the current format. First, the setting looks artificial and non-realistic and the problem is not well explained. Further, in (6), it is not clear how it falls under the umbrella of non-anticipative RL policies in Sec 2. Also importantly, it is unclear why the proposed method is better than the classical/existing solutions for (6). The proposed framework, by (11) and Thm 2, is applicable only to small T setting, which is not the common setting in control or RL. The main claim of the paper is that Q-func is continuous with time, which seems like an immediate consequence of smoothness of the value function in Thm 1. Thm 3 is a counter example, but is artificial since simply extending the U interval leads to letting \mu be included in the family of policies. Some concepts and quantities are not defined, and here are several issues with the presentation.	1
The paper analyzes under what conditions the value function of an optimal control problem in continuous control spaces can be approximated by a quadratic function of the control effort, and prove that this approximation is reasonable when the cost functional itself includes a term that is quadratic to the control effort. Several modifications to the previously proposed NAF algorithm are described and tested, but it is not clear how much better or faster they learn in comparison with standard RL algorithms that do not make use of such quadratic approximations. 	2
"Pros
* Good idea to apply posterior sampling to model-based deep RL.
* Novel Gaussian dropout layers capturing objects for important events.
* Good experimental performance in 12 Atari games.

Cons
* Not enough explanations and discussions for the three proposed layers.
* No discussion on why EVaDE-SimPLe performs worse than SimPLe in some games."	3
A simple modification to an existing architecture is nice, but it’s unclear from the experiments that this particular modification is really helping, due to the small amount of seeds.	2
This paper provides a novel neural network architecture design for model-based RL exploration. Overall, I feel it lacks principle explanation and is quite specific to a small set of domains. 	2
I liked the paper. My grade is explained by the fact that I am not very familiar with the domain of vision and games, hence I might be unaware of important related works. I'll upgrade my score from the discussion with the other reviewers and the authors.	2
The idea presented in the paper is simple but quite interesting. Unfortunately, I find the experimental section insufficient to show the method's superiority in comparison to related algorithms. 	2
"The proposed method does accomplish its intent, which is to reduce a rule system or incrementally integrate rules so that the final system is rather concise and compact. It does not, however, learn the rule system itself, mainly selecting rules that are learned by an external method (in this sense, the title may be misleading). One thing that is not clear is whether the interpretability goal is achieved (but then most papers on interpretability do not go till checking actual interpretability), and how the method would compare to approaches that directly aim at building a compact, ""interpretable"" rule base? 

The paper could also benefit from some small corrections, but is overall quite understandable. "	2
The paper does not show convincingly that the proposed methods improve on existing rule learning algorithms.	2
This paper provides an interesting variant on the idea of slimmable networks that allows the technique to be applied to distributed inference. The idea is well-motivated and described in detail, yet the training procedure seems slightly ad-hoc and the evaluation falls short in multiple ways. The paper needs more baseline techniques, more model architectures, and larger parallelization factors for me to fully support acceptance.	3
The paper improves upon the slimmable network framework by introducing parallel sub-networks to reduce latency. However, the improvement has not been well empirically evaluated and justified. The main discussed scenario is distributed embedded computing, but important topics in this scenario like mapping between different traits of sub-network and the capabilities of devices, has not been covered. Not to mention runtime behaviors like load balancing and allocation of shared resources like CPU and memory. The paper may need be expanded significantly to include sufficient contents on the topic.	2
A careful study of distillation algorithm for the mobile/edge device deployment. The whole paper is well written with experimental results supporting the main contribution claimed by the authors. However, the significance of this approach is a bit limited due to the scalability issue (only works with convolutional models and  a very small set of switches.)	3
Marginally below the acceptance threshold.	2
The paper is interesting, proposing a clean framework to perform locomotion tasks on mass-spring systems. The benefit of using differentiable physics is clearly highlighted and the use of SIREN to learn periodic motions is on point. However the experimental section is weak. For this reason I only grade it as weak-reject but would be inclined to bump it if this part is strengthened.	2
"This paper gives us a good example of how to train a network using differentiable physics. Their video of interactively controlling 2D and 3D robots looks interesting. Although some of the techniques like staging, batching, since activation, and periodic signals are intuitive and existing tricks, it is good to see that a combination of them can successfully train a controller.
"	2
The paper demonstrates good results for learning soft body locomotion. Although the technical novelty is lean, I find that the analysis and ablation of various design decisions are helpful, and has practical values to guide further development in this direction. For this reason, I am slightly leaning towards accepting the paper.	2
The paper shows interesting results of locomotion tasks, but the overall technical contribution is limited as presented. The writing of the paper could also be further improved. I am willing to raise my score if the listed concerns can be addressed. 	2
It is an interesting work, however, lacks discussion about some baselines (not mentioned in the paper) and other performance metrics (about forgetting, BWT). There are some questions (mentioned in the detailed review) that need to be answered in order to make it a strong submission. 	3
Generally, the paper has some novel contributions but also some minor weaknesses. If the authors can address the weaknesses/queries I raised in the main review, I'm happy to increase my score. 	3
"The key contribution is proposing the criteria to select and replace the existing points in the memory buffer. However, the technical novelty is limited. The paper includes several techniques which have been widely used elsewhere. The surprise (uncertainty estimation) has been widely used in literature. The Bayesian linear regression and the rank-one update trick are also well studied.

Empirically, the performance may critically depend on balancing how much we want to “surprise” vs “learn”. This should be well studied in the experiments. Perhaps, the paper can propose a technique to adaptively learn $\eta$ over time.

The reviewer thinks the paper is currently at the borderline."	2
Overall, I think the paper makes a useful contribution to the vast literature on MARL. In particular, I like the idea of using switching cost to derive better learning algorithms. The experimental evaluation also demonstrates the benefits of the proposes approach. That being said, I thought that the paper should do a better job in presenting the main ideas. Additionally, I believe bounds of some theorems (e.g. theorem 1) can be improved.	3
I initially vote for a score of 5. While the topic studied in this paper is an important problem in MARL and this paper makes a logical motivation to MAMT, I have main concerns regarding the experimental results (i.e., fair comparisons against baselines and identifying the main source of the improvements) and writing of the paper (i.e., needs to be rewritten for a cooperative multiagent setting). After reading the authors' responses to my questions, I am open to raising my score.	2
The problem studied in this paper is interesting and I think it would interest the ICLR community. While the theoretical derivations rely on previous works, e.g., Bai et al., 2019; Gao et al., 2021, they are interesting and elegant. I have not checked all the proofs, but they seem correct.	3
"Although the research topic of this paper is important, some parts of the paper should be further discussed and re-organized before it's ready to get accepted. 

Update after reading the authors' response: I appreciate the authors' response and revision (especially the algorithm section) that have addressed my concerns. I'm happy to increase my score accordingly. "	2
An interesting, although small, contribution to a recent controversy in understanding deep neural networks.	2
I recommend publication, as the paper is well-witten and clear, and brings about a new scheme and neural net model to compute exaclty one of the most fundamental quantity in their training: the mutual information.	2
The paper nicely summarizes some recent findings regarding information-plane analysis of neural network training, the dispute that followed the original results, and further follow-up work. To resolve some of the main issues (stemming from potential estimation errors of mutual information) the paper proposes to use quantized-activation networks instead, where the mutual information terms can be computed exactly. This idea is simple, yet compelling - unfortunately the problem of how to choose the correct quantization bit-width remains. Crucially, different bit-widths can lead to qualitatively quite different conclusions and there is currently no objective way of choosing the bit-width. The paper shows this via ablations which are discussed in the main text. Additionally, it is currently unclear how the training dynamics of (heavily) quantized networks relate to the training dynamics of “non-quantized” networks; after all; the activation-quantization could potentially have a regularizing effect that might interact with the compression induced by SGD alone. I think the paper is executed well, which is why I am currently in favor of sharing the results of this fairly straightforward idea with the wider community. I am not sure though whether this approach in general is a fruitful direction to target further research at in order to understand SGD training dynamics - which limits the potential impact of this paper.	2
The paper evaluates the implementation of hyper-networks on text-to-image generation, for the technical novelty, it is not quite significant. Also, The discussion about the continuous image generation mentioned in the paper is not sufficient.	2
The paper proposes a new way of text-to-image generation by directly modulating GAN weights conditioned on the text input. The results seem promising and the evaluation is extensive. It would be interesting if this approach can scale to higher resolutions and can generate more complex images where we can also condition on invidiual object locations etc.	3
Although this paper first explore text-to-image generation by using hypernetwork, I am wondering the intuition and the contribution. If only using hypernetworks, I think it is limited to be accepted.	2
"Please see the strengths and weaknesses.

I have given this paper 5 (instead of 3) solely because of the empirical results.

I have given this paper 5 (instead of 6 or 7) because the importance of the proposed pointwise modulation is unclear emprically.
The authors should report the improvements of their proposed hyper-network based condiioning over a simple StyleGAN-style channel-wise modulation. (ideally controlling for same number of parameters / FLOPS)

Additionally, some architectural and implementation details aren't clear enough, which I have highlighted in the weaknesses section.

**Rebuttal Update**

I read the authors response. Unfortunately, I have to stick to my lean reject decision.

For a purely empirical paper, the authors have not convinced me their hyper-network based modulation is better than a simple channel-wise StyleGAN2 style modulation.

In these experiments in the rebuttal, https://openreview.net/forum?id=z-5BjnU3-OQ&noteId=-oi3O8JQB-H, the authors have shown that thier model beats a StyleGAN2 baseline with sentence conditioning on CLIP-R. But they have not run their StyleGAN2 baseline with word conditioning / DAMSM loss, which makes me a bit sceptic about the gains with the hyper-network conditioning framework as opposed to just using the StyleGAN2 baseline."	3
This paper provides an interesting theoretical analysis on the ability of FNNs and (a type of) CNNs to fit functions on infinite-dimensional inputs that are smooth in certain senses, though how the results may relate to the actual usage of neural networks is not completely obvious in my opinion.	3
Overall, the paper is well-written and technically sound. The results should be of great interest to the theoretical deep learning community.	4
A good paper tackling an interesting question, namely studying adaptivity of convolutional architectures to certain function spaces.	4
The setting with an infinite dimensional input space is very interesting. The rates of approximation and estimation by deep ReLU neural networks given in the paper are nice. 	4
The paper combines multiple interpretation approaches and evaluation metrics for those interpretation schemes. Thus, it demonstrates the need for multiple interpretation evaluation schemes to truly understand which ones are optimum. It also shows how these can vary across different datasets and models. So, although the paper is not novel, the experiments performed and the results obtained is very useful for researches looking for a way to perform post-hoc visualization. As a result, I consider it as a borderline paper.	2
The paper is excellent as a survey paper focusing on performance evaluation, but somewhat inadequate as a research paper pursuing technical originality.	2
The paper is well motivated, the experiments are well designed, and the insights are helpful to the community.	2
"The paper does not present either a novel method or any novel insight. For this reason, I would recommend rejection.

The post-rebuttal discussions did not change my view of the paper, but I am improving my score in light of author updates."	1
The method is simple and effective, but I doubt the current evaluation setup as it uses additional self-supervision to train the model against baselines.	2
The authors propose a labeling trick to enhance plain GNNs for subgraph tasks, and the theoretical and empirical results demonstrate its effectiveness and efficiency. However, the authors may want to detail the model structure and experimental settings in the main text. 	2
My main reason is incremental novelty (W1), and insufficient support for certain claims (W2/W3).	2
This is a well-written paper, that could be improved by providing more motivation and exploring the performance in more depth by using other baselines and providing ablation studies. 	3
 In short, the paper is trying to solve an important challenge of heterogeneous multi-omics data integration by proposing an advanced model compared with BayReL. However, what is the key difference and improvements compared with BayReL should be discussed in detail.	2
While there is a limited novelty and demonstrated improvement in performance over a single related approach, from the application and result interpretation point of view, the advantages of using MoReL are not clear.	2
The proposed method is convincing with respect to an important problem. The paper has nontrivial improvement over existing methods. Though it lacks comprehensive experimental validation, it is overall a good complement to existing literature.	3
The approach deals with problems that could not be previously easily tackled, however  a more thorough empirical investigation of how well the approach can deal with those cases is lacking. 	3
I am rating accept as the writing is clear, motivations and contributions are clear, theoretical results are convincing, and experimental results are mostly good -- the only issue is that the state of the art somehow performs very poorly in the given domains. I hope the authors can either explain this in greater detail or include a domain in the final version where the state of the art is at least able to learn something.	3
"The paper is suffering from a large number of grammatical as well as spelling mistakes that make it very difficult to follow. Based on this, the paper is not fit for publication in its current state.
"	3
The method is simple yet effective. However, it heavily relies on the ability of reseting the environment to a previous state, which is unfair to the baselines. The experiments also need improvement.	3
While this paper seems to get strong performance in the selected experiments, I find the experiments to be a bit suspicious and under-explored and I am unconvinced that the performance gained with this method are worth the strong assumptions (env is resettable to a selected state). I cannot recommend accepting this paper.	2
"This is a well-written paper that presents a plausible approach to a meaningful problem, but falls short in empirical validation.
"	3
"I think there is not enough interest or applications for learning-based compressed sensing. The fact that all experiments in this paper are based on synthetic datasets and no real datasets are used in this paper confirms my concern. 


Moreover, the algorithm is recursive and each recursion of the algorithm involves running an instance of the path-following algorithm. So even after fully training the parameters, just evaluating the algorithm on an input instance is several times slower than the original path-following algorithm. The complexity is even much worse during the training phase because one needs to evaluate the whole algorithm multiple times and compute its derivatives with respect to the learnable parameters. The paper does not analyze the runtime complexity, unfortunately.


Theorem 3.1 is somehow telling that the optimal parameters give a small error but it does not tell you how easy it is to find the optimal parameters. In particular, the parameters need to be found by solving some optimization problem but it is not even clear if this optimization problem is convex or not. If the problem is non-convex, you would only be able to find a local minimum and Theorem 3.1 would become useless. 

"	2
The paper aims at an important problem and provides a novel solution. The theory and experiment are generally solid. I believe the work could have future impact.	3
"The algorithm is somewhat interesting but the theory is of limited relevance and novelty. The implementation details and practical usage of algorithm are largely unclear in the current stage. 

=== Post discussion update ===

After reading other reviews and the author response, I would upgrade my rating to 5: marginally below the acceptance threshold."	2
"I think the proposed problem, regret formulation and methods are interesting and novel, so I vote for acceptance. I have a minor question about the proof, and I hope the authors could help me understand it. 

"	4
"This paper provides a non-trivial regret bound for the multi-objective online learning problem in the full-information, yet possibly adversarial, setting, using the online convex optimization framework. As far as I know, this is a novel result. However, I am not entirely convinced that this is significant enough for the ICLR conference since this Pareto regret bound only holds in the dynamic setting. Furthermore, it would be interesting to provide a lower bound in order to justify the tightness of this bound.

*** After Rebuttal Phase ***

The authors have addressed several issues, notably related to the tightness of the bound for the dynamic regret, and an $O(\sqrt T)$ bound for the static regret. So, I have slightly changed my score. I would recommend revising the paper by incorporating the interesting results obtained in Section D."	3
The main weakness of the paper is that the static regret is not bounded by the proposed algorithms.	3
The paper addresses an important question to bound the regret of online multi-objective optimization. The derived bound of O(mT^{1/3}V_T^{2/3}) is also an interesting result that scales with the number of objectives m, time horizon T, and the regularity metric V_T. There are some concerns about the connection to the literature, assumptions, technical and analytical steps, which are all detailed above.	3
This paper presents an implicit way to learn graph positional embedding. However, the manuscript is not yet ready to be published. It does not provide a clear empirical investigation or a solid theoretical examination for how the implicit positional encoding is better than the explicit way. The content, organization and experiments also need to be further polished and enhanced.	2
I would recommend this paper be accepted if other issues do not arise.	2
Overall, I agree that the method could be useful and could improve the GNN methods in situations when the positional encoding is important. However, the current form of the paper lack clarity and I consider that many aspects of the method/experiments should be better present in order to be clearly delivered to the community.	3
interesting idea, but the presentation is hard to follow and the empirical results do not seem very significant	3
This paper contributes to the body of work that tries to tackle both types of robustness problems (common corruption and adversarial corruption) under a unifying view, which is an important step. While their method seems to be computationally demanding, it demonstrates good empirical performance along with theoretical justification.	3
This paper provides a general framework to defend against unseen image corruptions and adversarial perturbation. The general nature of the framework makes it feasible to use it with a wide range of corruptions. 	3
"The proposed method builds on recent related work, and its design is justified. Provides insightful empirical and theopretical results, and unifying some related work.
However:
- Empirical results does not support the claims (considering single methods applied)
- Is not compared to adversarial training methods on adversarial robustness AND perturbation robustness.
- Some relevant approaches have been missed in comparisons, as discussed above.


## after rebuttal:
During the rebuttal, the authors responded to all my concerns, and strengthened their empirical evaluations with additional results. Also the claims of the paper are now aligned with the reported results, and the limitations of the proposed approach is clarified and sufficiently discussed.
I find this paper a good reference for research on adversarial and perturbation robustness method that benchmarks a wide range of methods from both areas, with a method that although has its limitation, a strong baseline for the future work in this direction. I therefore increase my score."	3
"The main strengths of the paper are its strong empirical results and that the proposed AdA is a generic approach that could be extended to other types of data. However, the strong empirical results come at the cost of highly increased computation at train time and it remains unclear if appropriately tuned SAM couldn't provide similar benefits. Moreover, the general purpose nature of the approach is not demonstrated because experiments are restricted to image classification. Finally, the theoretical considerations are based on an impractical assumption.
In summary, I consider the paper in its current form below the acceptance threshold (albeit marginally so). Showing the potential of AdA on other types of data _and_ comparing to SAM/AWP could bring it above the threshold.

### Update after rebuttal ###
The authors have convincingly addressed my concerns regarding a comparison to SAM/AWP, discussing the computational and memory overhead more transparently, and several further minor points. I am raising my score to 6: marginally accept.
However, I would still like to emphasize that the ""Theoretical Considerations"" in their current form are grounded on unrealistic assumptions and disconnected from the rest of the paper. The authors describe in the discussion how the theoretical considerations could be revised to be grounded on more realistic assumptions. It is difficult to judge the correctness of this line of thought based on a discussion in the review forum, without a proper revision of the paper. I highly encourage the authors to update the ""Theoretical Considerations"" of the main paper.
In summary, I lean towards acceptance because the paper is strong on the empirical side and provides a compelling approach - but this is in spite of (not because of) the theoretical considerations.
"	3
The paper presents an approach based on local learning rules to deal with two tasks: sequence memorization and prediction. The results are promising but crucial comparisons to related approaches are missing, making it difficult to judge the particular contributions of this paper.	2
"While the premise of the paper is interesting, I can not recommend it for publication. The paper is very hard to read and suffers from missing clarity. The experimental evaluation is insufficient.
I would suggest to the authors that they conduct a major rework of the paper (notation and clarity of writing), add additional experiments, and submit to a later conference."	3
Approching sequence learning problems with fast-weight architectures is interesting, but the experimental results are not yet convincing enough and the clarity of presentation should be improved.	3
Although there are several good things about the paper because of its simplicity and effectiveness, the authors should provide more experimental evidence to show the effectiveness of the proposed approach. There are also some concerns based on the setting of the experiment (pseudo-black-box setting). 	2
"The authors suggest an interesting effect but provide a little investigation of it.
The cause of this effect can be some specifics of used pairs surrogate-victim or more general effect related to the correlation of deep learning models trained on large datasets and on general transferability.

As I don't get from the paper, why this phenomenon occurs, I suggest starting a discussion to find out, if it is possible to improve the paper in this aspect. Now, this is the obstacle, that makes the paper below the acceptance threshold."	2
Overall, I find the paper to offer a solid contribution that may be interesting enough for the community working in the area of black-box adversarial attacks, if accepted to ICLR.	2
While the proposed black-box attacks show strong empirical performance, the technical contribution is marginal. Moreover, the empirical comparison is not enough to show the advantage of the proposed method.	2
This paper presents some rigorous theoretical findings for the distributed zeroth-order SGD. However, my main concern is the novelty of this paper. There are some modifications needed on the assumtion and numerical experiments as well. 	3
The comparison of results is not appropriate and accurate. Also, their results look strange (e.g., do not depend on the parameter of network topology). So I recommend a reject.	3
Overall, this paper has disadvantages in novelty, depth of analysis, and numerical experiments.	2
"The authors propose a simple new 'black box optimization' method for learning RL policies in stochastic environments. The method is simple and clearly explained. The paper is clearly written and reasonably presented.

This method contributes to a growing literature noting the effectiveness of simple optimization method and linear policies on many current RL benchmarks. However, unlikely notable previous working in this space (in particular, ""Horia Mania, et al. Simple random search of static linear policies is competitive for reinforcement learning"" who's work this paper references a few times), this paper presents only a limited experimental and sensitivity analysis.

I think the paper would benefit strongly from a more robust experimental treatment of the idea. Specifically, the authors might explicitly restrict themselves to linear policies and do a like-for-like experimental comparison against previous work."	2
The proposed algorithm makes intuitive sense and performs well on the tasks presented by the authors, but it is not particularly novel, and I have doubts about whether it can solve more difficult tasks. Further experiments could change my evaluation.	2
There are aspects of the proposed approach that seem desirable, but I am reluctant about its novelty due to two closely related pieces of work: (1) A Block Coordinate Ascent Algorithm for Mean-Variance Optimization, and (2) High-Confidence Policy Improvement. Additionally, I believe the paper's overall clarity could be improved.	2
This paper introduced some interesting ideas for gradient-free policy training for reinforcement learning. However, the novelty of the new algorithm design should be clearly highlighted and strongly justified. The theoretical strength of adopting a gradient-free approach for reinforcement learning also requires better justifications. Meanwhile, the empirical evaluation has some limitations.	2
"Overall, I think this paper is very marginally below the acceptance threshold. I like the self-supervised approach to logical reasoning in pre-training. My major concern is the clarity of the paper, which hinders some explanation of methodological choices and proper understanding of the technical approach. I am willing to move this paper above the acceptance threshold if the authors can address my questions appropriately.
"	4
"Overall, the paper requires extensive re-writing and better result presentation to be considered for acceptance in the conference. While the paper introduces a nice pre-training technique, it raises way more questions than it answers. Ultimately, the empirical results (and especially the presentation of those results) are not convincing enough to support the claim that the proposed pre-training techniques at all induces logical reasoning capabilities to BERT model. At this stage, unfortunately I cannot recommend acceptance.
"	2
Injecting knowledge into pre-trained models without resorting to external knowledge sources is interesting, and the effect is confirmed by empirical results. However, my concern is the limited novelty of the proposed model.	2
In summary, I think the motivation for adding additional dependency parsing has not been adequately articulated. And the experiments have not fully verified what the dependency parsing information actually solves. The simpler SpanBERT is in fact better than this paper. Therefore, I think this paper does not reach the threshold of ICLR.	1
"This paper studies an important problem. In short, there is little work on intersectional subgroups, even less on deep learning. In this context, I see the empirical study as an opportunity to make a contribution as it can highlight previously unknown issues. 

My recommendation is based on the following issues: (1) key findings are not well-supported by the empirical results (see comment above); (2) the proposed approach DIR is underdeveloped and under-evaluated in the text. I think that both issues could be addressed, but should be subject to another round of peer review."	2
The empirical analysis is interesting but the incremental contribution concerns me the most. In addition, the proposed DIR method is trivial and contains many uncertainties. I would not root for this paper.	2
Interesting experimental results but the fairness metrics adopted in this paper need to be clarified 	3
This paper provides an extensive empirical analysis of an important question, how fairness methods perform with regards to intersectionality. The paper would benefit from additional discussion on a few points, like hyperparameter tuning.	3
This paper combines two popular techniques in a nice way. I think the ideas in this paper can be pretty impactful. I do have three weaknesses about reliance on NAS-Bench-201, a fairer experimental setup, and code release, so I will give a weak accept.	3
Generally speaking, I think it is incremental work. Although there is a lack of theoretical contributions, a large number of experiments provide empirical contributions. It is difficult for me to put forward some new opinions about these large numbers of experiments. I give the above acceptance threshold, but reject is OK.	3
Although the paper has shown us the zero-cost proxy has great power in operation search in the differentiable neural architecture search, the paper stills lack insight on analyzing how it works. Therefore, I rate it as a borderline paper.	2
"In addition to the novelty issue mentioned above, some results and description of this work also makes me a bit confusing. For example, when demonstrating Figure 2, the author did not mention which concrete training-free proxy they used for $S$ in Eq. 5. Also, the purpose of ""retrain for 5 epochs"" is unclear to me. Why does the author want to conduct this experiment given the ""zero-cost"" motivation in this work?"	2
In light of the above concerns surrounding writing clarity, missing notable baselines, and questionable empirical methodology, I'm recommending strong rejection at this time. I believe a substantial rewrite and significantly more evaluation is needed.	2
The paper needs more baselines, ablations, benchmarking tasks to justify the effectiveness of the proposed sampling scheme.	2
While investigating an important problem, the paper is poorly written, the main contribution is not adequately described or evaluated, and the empirical findings that are presented to not convincingly support the conclusions. I recommend that this paper be rejected.	1
"None of the claims in the abstract, introduction and conclusion are actually supported by the results. Also the paper is not well written, with lots of typos and fragmented sentences.
"	1
I lean to positive on this work. The work generalizes two frameworks: OA-ARM and discrete diffusion model. The provided method is simple to implement and efficient is generating mode. Also, the authors conduct experiments across various domains, and the result are very promising. Therefore, I recommend this paper to be accepted in ICLR 2022.	3
This paper is well-written and introduces significant contributions. The contributions are theoretically significant (generalization of ARMs) and are of practical use (parallelization of the OA-ARDMs, depth upscaling). The method is well related to the literature. The theoretical findings are supported by a complete set of experiments.	4
"It is not an easy task (for me) to come up with a recommendation for this paper. I didn't find any technical flaws, I think the topic is very interesting and important, and the empirical validation of the proposed method(s) is extensive.

Nevertheless, I find this paper has problems with the positioning, and with the choice of what the authors consider their main contribution. Also, a lot of the proposed methods are based on other (recent) work, which are assumed to be perfectly known by the reader: this makes this paper not self-contained, and to some extent reduces the surface of the contributions, which seem additional engineering prowesses on top of existing methods.
"	2
"Overall, this is a well reported work that presents useful new modeling ideas that bring together previously separate methodology.  The work could, however, benefit from a more comprehensive set of benchmarking with different data sets / case studies to establish how robust the claimed performance gains are.

"	3
This paper is generally well motivated and well written. The experiments are sufficient and the proposed CACR shows impressive performance. My minor concern is better explanations of the conditional distributions.	3
Overall, I feel this paper makes the presentation of the proposed method over-complicated and the resulted approach has limited novelty. The experiment is not very solid and convincing. 	2
"Although the proposed method seems to be an incremental improvement to the conventional contrastive learning, the ""doubly contrastive strategy"" is shown to be effective by performing extensive experimental evaluations. This might be of interest to a group of audiences in ICLR 2022 and contribute to the community. On the other hand, there are some unclear statements I hope the authors could address in rebuttal. As a result, I recommend a marginal acceptance at the current stage and may change my rating after rebuttal."	3
Though the proposed framework is closely related to previous CL methods on the ideas of multi-crop and hard negatives, it still shows novelty on modeling the intra-relation among positives/negatives. The paper provides detailed theoretical analysis for understanding the method. Experiments and ablation studies are also sufficient to demonstrate its effectiveness. 	3
"
- It is unclear if the proposed strategy works to search a ViT architecture suitable for small-scale datasets?

- The advantages of automatically searched ViT architectures are not immediately clear. Their major limitation for ViTs is the time they require for training, and the number of FLOPs introduced by quadratic complexity in self-attention. Compared with existing “hand-designed” ViT architectures, the proposed automatically searched ViT architecture does not significantly reduce compute training time. 

- I believe an automatically searched ViT architecture, that is efficient to train, will help democratize ViTs research, and make it accessible to broader resource-constrained academic labs."	3
The paper AS-ViT is focusing on solving the scaling issue of ViT with searched topology and auto-scaling rules. However, it does not show significant improvement compared with other multi-scale vision transformers and my concern is that a multi-scale ViT baseline for fair comparison and evaluation is needed here. I will increase my rating if the concerns are well addressed here.	3
The biggest merit of this paper is to unify efficient search, scaling and training as one -- that is unique and strong. The results prove their concepts and can be considered as promising since the adopted search space is vanilla. However, the authors are expected to clarify a number of experiment and comparison issues as aforementioned. 	3
"There are too many strong arguments or definitions that do not have sufficient justification and are not convincing or reasonable. And this paper is not quite clear and not well-structured. Author response does not address my concerns and the authors did not show up to answer my follow-up questions. 
"	2
Based on the mistakes in the text of the paper, sloppiness of the formal definitions, and unclear contribution, I recommend rejection.	1
The summarization and formalization of robustness notions is a good contribution of this paper, which helps the community to sort out different approaches and their applicability. However, from the empirical front and theoretical front, the paper may not provide enough novel findings nor useful approaches. Thus, the paper may be below the acceptance threshold at ICLR.	1
This paper presents a more fine-grained model compared to other differentiable physics papers. Claims are well supported by the experiments. It would be better if the author can better demonstrate their application to more complex scenarios (like a T-shirt or dress instead of a single square sheet).	3
This manuscript proposes a combination of yarn-level cloth simulation and differentiable simulation and shows the application of system identification and control. The thing I am worried about is that the motivation of using yarn-level, which is originally designed for fine details but applied on over-simplified models. I would like to know more about why the authors chose the yarn-based method, how they would like to use this simulator, and where they see the simulator fits in. Generally, I appreciate the work but was not convinced by the motivation. I am open to changing my score based on the clarifications I get.	3
"Again, I am no expert, but I think this is a paper that fits its time. It appears to be well executed and evaluated. Whether or not the works falls under ""machine learning"" is probably a matter of personal taste. The contribution is a simulation that serves as forward model component in a regression framework, so I think that should qualify it.

As the authors acknowledge, neither the specific problem nor the general idea of differentiable simulation is new. Whether the specific solutions introduced in the paper generalize to other problems is unclear. Still, I think this is reasonably nice work to warrant acceptance."	3
Overall, this is an interesting paper with solid technical contributions. The main weaknesses include the presentation and experimental evaluations.	4
"This paper proposes a new differentiable force model for yarn fabrics which is novel and able to fit clean data for which the positions of the yarns are known. Yet the experiments fail to prove it is useful to fit general fabrics in a less controlled setting.
"	2
"Despite  that the experiment section of the paper can be improved in many aspects, I think the core contribution of the work is new and valuable, and the paper is acceptable condition on some minor revision.  

"	3
The paper is fairly well executed; however, I can't say that I learned much from the work as its contribution is mostly in the implementation details.  And it is unclear to what extent these details generalize beyond standard image benchmarks.	1
I admire the simplicity of the approach and I tent to recommend to accept this work at this stage. 	3
This paper provides interesting insights about normalizing flows and OOD detection. 	3
"The paper in my view does not present too many surprising results, but I also find it valuable to see non-surprising results that are clearly presented and easy to understand. And the result on generation quality and OOD detection performance being negatively correlated is also valuable to know.

**Update Post Rebuttal**
The revisions have further improved the manuscript, I increase my score to 8."	2
"While the paper studies an interesting problem, it suffers from presentation issues which makes it hard to grasp the main technical contribution of the paper. The paper seems to be written in a rushed way and consists of many sentences which have missing words or are grammatically incorrect. This gets bolder as we move on to the later parts of the paper. I tried to mention some of the typos and the incomplete sentences as I went through the paper but they are too many to mention in this review. Many parts of the paper need to be re-written in order for it to get ready for publication. 

The novelty is in re-writing the objective function, which tries to minimize prediction interval length while maintaining a certain coverage probability, in a differentiable manner and then optimizing it.  Some technical details are also missing (such as discussion on the complexity of the approach). Overall I think the paper is not ready for publication and needs improvements both in technical content and presentation."	2
The paper success in the construction of disjoint PIs. However, there is a lack of thorough insightful motivation, remain of some algorithmic issues. Presentation is not better.     	2
"This paper proposes a higher fidelity method for the evaluation of predictive intervals under multimodal predictions. The authors also propose a method to make multimodal predictions. Unfortunately, the author’s evaluation setup assumes that their proposed method is the only one to produce multimodal predictions (which is not the case). This creates a drastically unfair comparison.
"	3
This paper proposes an interesting method to solve an important biochemical problem, data analysis of tandem mass spectrometry (MS/MS). The paper may has a larger scientific impact if it submit to a corresponding high impact journals, which are likely have more appropriate audience. However, if the paper is interested in machine learning conference, some more contribution of machine learning novelty and more quantification of the methods can strengthen the paper. 	3
"I think this might be very good work, but the explanation of the model is problematic, and the empirical results fail to explain how the model's performance compares to the state of the art.
"	3
"This paper proposed a customized transformer to solve the molecular structure identification problem via MS/MS spectrum. The application itself is interesting but the authors need to justify their performance with a clear experimental setting and a better illustration of the evaluation process. 
With the current paper, I this it is marginally below the acceptance threshold. If you address my questions properly, I may adjust my score."	2
"To sum up, I am quite positive about the paper and I think it mostly satisfies high publication standards for ICLR. However, there are still major things which may be added during revision:

- Comparison with more recent methods (Sangalli et al. and Li et al.). Sangalli et al. is more recent and compares with the methods in Li et al. So, I think it would be sufficient to compare with the former one.
- Hyperparameter selection: I think this is the most crucial part. I wonder how this is performed without validation set. This should be clarified and all details about hyper-parameter selection should be given.
- Using additional metrics where possible would be beneficial."	3
The idea of utilizing MAML’s meta learning framework in class-imbalanced problems itself is interesting and good direction. However, the technical novelty of the paper is only marginal because it is rather straightforward application of MAML. The empirical novelty is not enough, too. It is good to try to evaluate the method on various tasks other than image classification tasks, but the proposed method should be compared with recently proposed methods in a fair protocol to show the advantage of the proposed method over these prior works.	2
"Overall, the problem is important and interesting, however the current draft is not well-justified on the actual algorithm design, the novelty, and (perhaps the most important) empirical comparisons to existing imbalanced learning methods. Further comments/questions are listed in the weaknesses / questions part.

The paper has its potential to the field, but issues need to be addressed first. I'm happy to change my score if the feedback addresses my concerns. Please refer to the points in the weaknesses / questions part. I would like to see feedbacks on these comments/questions."	2
While this paper studies a practical and important problem, the technical parts are not well-motivated and experiments are not very sound.	2
Overall the work provides meaningful analysis and insight into the use of debugging methods such as LIME with CNN and VIT architectures, and shows the superiority of token dropping in VIT to blackout, blurring or other feature removal methods in CNNs. Additional analysis by the authors into feature robustness and retraining with CNNs vs VIT to determine whether the superior explainability is due to the regions not being considered in the token drop method as argued by the paper, or whether the propensity for more robust features in VITs is the reason.	3
The paper makes an observation on the absence of features for debugging purposes and argues that ViT should be preferred over standard CNNs. The paper exhibits the use case on ImageNet, but it does not demonstrate how this can be useful for diverse datasets, or even the utility of the method in various cases (e.g. non-rectangular, small patches) and tasks. 	2
Overall, this work is good showing crisp thinking and analysis of the experiment design. However, few more experimentation would have been better to have. Some such experiments are listed in detail in the weaknesses section above. The approximation of “missingness of pixels” by supposed to be “meaningless pixels” is a longstanding problem in vision and researchers are aware of this. This work definitely will help progress the research in this. However, I would like to see the authors to discuss some possible solutions to this problem so that the work does not look like an illustration of an already known problem in Computer Vision only.	2
The paper proposed three novel methods to address some limitations of existing works. It appears to be technically sound, but I have not carefully checked every detail. The experimental evaluations convincingly support the main claims. This paper contributes some new ideas in this specific research area. 	3
The scientific contribution of this paper is solid in my opinion: a problem is introduced, solved with a generalization of a previous framework. The explanations of the improvements are clear, and because the contribution is fully technical and the results are good, I think, except if I missed an element (such as a working with a similar result) that this paper would be a nice contribution to ICLR.	3
The evaluation setup is flawed and therefore unable to judge the value of the proposed techniques. Update: ratings adjusted based on new results from authors.	3
Overall I like this paper and the proposed ideas were well justified. My comments on the weakness mainly involve the explanation of the numerical results, and whether it is possible to include more results on larger datasets. It would be good if the authors can provide a short summary on when their method would work the best. 	3
The paper studies an interesting problem.  However, it is unclear whether their proposed attack algorithm is still valid given that researchers have already proposed various robust fair learning algorithms.	1
In summary, I think the authors considered an interesting problem. However, the both the proposed algorithm and empirical evaluation have limitations. The main message of the paper is that robustness increases the cost of fairness. This fact has been observed by other papers before. Additionally, in comparison to several recent papers on fair and robust classification, I don't think the paper makes a major contribution to the problem.	2
I think the paper has lots of interesting points and its merits outweighs the points I noted under weaknesses which are not really weaknesses but some suggestions that can further improve the work. With that being said, the paper has already enough content for publication which are interesting and informative and the suggestions can maybe considered as a followup work. 	3
The claims are based mainly on experiments even under some particular algorithms and models. So it is hard to make a scientific judge as to whether the claims are really the case.	2
Overall I recommend for a rejection because the current version of the paper is not well-motivated and the evaluations are not sufficient to support the claims and contributions made by this paper. I am open to any discussions and will increase my score if my concerns are resolved.	3
Overall I enjoyed reading the paper. I only have a few questions, which basically asked the authors to clarify their design choices, especially those that are different from existing works. 	3
The idea is interesting but not a lot of novelty, as some existing papers (e.g. Li's Deep Learning for Case-Based Reasoning through Prototypes paper) already provide notions of distances between inputs and prototypes, prototypes and test data points, etc.	2
I see values in the paper as using a VAE instead of AE is a natural improvement over previous work. Nevertheless, I have major concerns as described above, in particular, the paper is missing a comparison with previous works especially (Oscar Li et al. AAAI 2018). 	2
All in all this is a nice paper, with a bit more depth could be a nice contribution.	2
The paper does not fully justify why the introduced hierarchical probabilistic model can lead to their claims: spatial information and physical plausibility, which makes those claims subjective. Besides, the baselines are limited and thus experiments are not convincing. Overall, I tend to reject this paper.	2
The proposed method is a straightforward but original combination of existing techniques. It improves over MulMON on NVS and generation, but the evaluation needs to be much more comprehensive, in particular adding comparisons vs. appropriate baselines for each task. Moreover, certain important technical points regarding the benefit of the model structure should be clarified.	2
The proposed approach, while competitive with a previous baseline, has limited theoretical novelty (hirerachical VAEs have been proposed before), and it is unclear what would be the advantages of using the proposed approach compared to alternative approaches. Therefore, I believe the paper is below the acceptance threshold in its current form.	2
The paper looks solid and well written, although the contribution and usefulness of the findings is not made totally clear. I am not an expert on this field, thus some more relevant related work might be missing. Most of the mathematical formulation seems correct, although I might have missed some details. My main concern is the way in which the findings are presented seem to point as to the proposed perspective (assembly model) being one of the explanations under Assumption 1, but not necessarily the only one, or valid when said assumption is not true.	3
The paper is well written and presents a novel theoretical contribution to the understanding of the recently highlighted phenomenon on how wide neural network architectures with linear output layer inherently evolve as linear models. I find this work to be of good quality in terms of theoretical contribution, whereas on the other side empirically weak. My rating would be to accept the paper for presentation at ICLR, as long as the authors can present some numerical verifications or discussions on the how to make an understanding of these results for a broader audience.	4
This paper provides a new perspective towards an interesting observation in deep neural networks, while the connection with previous work could be stated more explicitly. 	2
In summary, I think the idea of combine GNN and Transformer is very interesting. However, the novelty of this work is very straightforward, a mixture of many existing work.	2
I think this is a borderline paper, as graph coarsening to use the transformer on large graphs is not entirely novel, however, it also has clear merits: a novel scheme of information propagation across original and coarse graphs; an almost complete piece of work except for some minor issues in the weaknesses above.	3
Overall, I like the motivation of the paper. The proposed methodology should be improved though. And the experimental results are not very strong.	2
"I am slightly leaning towards acceptance. While I think that related work should be better discussed I think that many people can be interested in low memory overhead of the model and the whole community can benefit from that.

--------------- Post Rebuttal -------------------
After the latest round of improvements I believe the paper is above the acceptance bar."	3
In brief, the paper reveals the connections between feature spectrum and non-robust features under adversarial attack and proposes an interesting regularization term for robust training. However, the new method does offer significant improvement over SOTA and was not comprehensively evaluated on high-dimensional data or using different backbone networks.	2
"The overall presentation of this paper is clear and the authors have conducted experiments on different datasets, adversarial attacks, and different adversarial defenses. However, I still have several concerns as I list above, so I will give it a “6: marginally above the acceptance threshold”.
"	3
The authors make an interesting observation about adversarial robustness of DNN classifiers based on a PCA of the embedded data points. However, the observations are not backed up by theoretical insights and the empirical evidence is not very conclusive. 	1
A good paper with theoretical analysis and extensive experiments	3
I feel that the paper gives an interesting direction to consider virtual nodes for link prediction, but the motivation is unclear and experimental results are insufficient.	3
The authors present much interesting and insightful analysis for virtual nodes on both theoretical and empirical sides. But there are also many concerns on both two sides which should be well addressed.	2
Contribution is limited, as explained in the main review, and I think it is not enough to be accepted.	2
"Weak reject due to limited technical contribution and limited empirical comparison   

-----
Update after author response: I appreciate the additional experiments for l1 cost included in the revision. Moreover, although I appreciate the author's discussion of the motivation for the Gelbrich distance, I still think that the technical results are somewhat limited. Thus, I keep my assessment the same. "	2
"The paper tackles a very practical and relatively new problem regarding recourse actions. The overall idea seems reasonable to me and the experiments have demonstrated the effectiveness of the proposed method. The paper is also well written. 

However, the paper has a few weaknesses as described above. The missing comparison with ROAR is a main concern. The novelty of the proposed method (regarding the adoption of distributionally robust optimization) should be clarified too."	3
While the authors present a very clear and thorough exposition of the approach proposed and the approach is technically sound, the experiments presented are very limited and the overall value of the contribution is unclear. In particular, the authors do not compare their proposed method with previous approaches addressing the problem of generating robust recourse actions.	3
The paper in general lacks supporting formulation that would allow clear understanding and clear conclusions about the validity of the proposed statements. 	3
"Although it provides a new perspective in fine-tuning process, the core idea seems not novel and is not supported by robust experiments. 
Also, the writing seems not thoroughly reviewed. "	2
A nice investigation with limited significance.	2
"Very interesting approach, incomplete comparison with the current state of the art (e.g. fitness ranking), and complete lack of any experimental results or any graphics. While I would not recommend the current state as ready for publication, I look forward to future more complete implementation of this work as it seems to be very promising.
"	3
"Overall this paper attacks a very important problem of meta learning and few-shot learning. And considering the quantitative results, the proposed method does achieve better uncertainty calibration. 
I expect further discussion on justifying the necessity of adopting an additional set encoder to achieve this capability. "	3
"In my opinion, the paper is a good engineering attempt to solve a specific problem. It lacks, unfortunately, concrete scientific contributions.


======== Post-rebuttal ========
The authors have provided further empirical results to justify the use of the ST to estimate the covariance matrix. I have raised my evaluation from 3 to 5. "	2
The paper is well-written with clear motivations and easy to follow presentations of the methodology. The proposal is relatively novel, and could be a meaningful contribution to the meta-learning community.	3
"The reviewer has some concerns on the novelty of the paper.

1. The idea of using optical flow to help weakly-supervised or unsuperised object learning seems to be straight forward. Previous work like PSGNet[A] also adopts motions and depth as supervision signal and compare their performance with MoNet. It will be necessary to compare and disucss the PSGNet in the related work.

2. The paper only compares with  Slot attention or MoNet in unsupervised object segmentation. Object-centric representation for video learning and reasoning has been widely studied in previous frameworks like [A, B, C, D]. Note that previous work like ALOE [B] and VRDP [C] has also been using Slot attention or MoNet for unsupervised video proposal segmentation. It will be interesting to replace the Slot attention or MoNet with the proposed segmentation model and see their performance on the CLEVRER[E] dataset. 

[A]. Bear D M, Fan C, Mrowca D, et al. Learning physical graph representations from visual scenes[J]. arXiv preprint arXiv:2006.12373, 2020.

[B]. Attention over learned object embeddings enables complex visual reasoning. David Ding, Felix Hill, Adam Santoro, Malcolm Reynolds, Matt Botvinick, Arxiv, 2021.

[C]. Chen Z, Mao J, Wu J, et al. Grounding physical concepts of objects and events through dynamic visual reasoning[J]. arXiv 2021.

[D]. Ding M, Chen Z, Du T, et al. Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language[J]. arXiv 2021.

[E]. A]. Yi K, Gan C, Li Y, et al. Clevrer: Collision events for video representation and reasoning[J], ICLR 2020.
"	3
Overall, I enjoy the idea presented in the paper and I consider the empirical part interesting enough to recommend the acceptance.	3
"My main concern is that I cannot see how the proposed approach can generalize to new domains and tasks. Overall, I like this paper and its contribution, but I think the authors should clarify their contributions and explain how the video slot attention could be leveraged in other domains and tasks. Otherwise, it feels like a perfect model for the CATER/ CLEVRER/ MOVi/ MOVi++ datasets, but the full potential is not entirely clear. 

I am open to the authors' feedback and other reviewers' opinions.


After Rebuttal
------------------


After reading the authors' feedback and other reviewers' opinions, I would like to thank the authors for their rebuttal.

The rebuttal addresses most of my concerns. I am leaning towards acceptance of the paper since it maintains the high bar of the conference quality. I vote for 6.

"	3
"This paper introduces two improvements upon existing object-centric learning methods: offering additional object-level ""hints"" as inputs and using motion cues (optical flows) as pre-text supervision. The idea is good and I believe will be beneficial to the object-centric learning community. The writing is good, easy to follow. Experimental results are sufficient to support their claims. "	4
The paper is a good study of existing deep-learning instance segmentation models, and frameworks. I enjoyed reading the paper. The authors put a lot of effort on performing large number of experiments to benchmark various models, however they put little effort on deeply studying and analyzing the results and they left it to the reader to interpret the results.	2
I prefer to reject this paper. My main concern of this paper is the lack of contribution to the community. To me, this paper mainly conducted a set of experiments of existing methods on a new setting. And the new data setting doesn't make a lot of sense since the groundtruth is encoded in the image and there's no user cases for such setting.	1
There are gaps in the experimental design based on which we are concluding what features object detectors learn.	1
"My concerns are listed in the weakness section. If the authors can well address them. I would consider raising my score.

After reading the updated paper, my concern about the mask head influence is addressed. Also, I have read a new intro and abstract with better motivation connecting to the real-world instance segmentation challenges. Although image corruption using style transferring has no real-world cases, it does create novel object/background context. The experiments conducted are extensive and it provides some suggestions of instance segmentation model design for environment robustness. Thus, I decide to increase my rating to 6. "	2
The idea is clear and the results seem promising. However, the authors are supposed to provide more analysis to understand why the proposed method works.	3
The presented method is a general procedure that helps to improve performance of many methods in lifelong learning scenario. The presented experimental results are convicing. However, the paper would benefit from more research and intuition on why the approach works. I recommend to accept the paper.	2
Although the paper proposes a simple method for controlling catastrophic forgetting in the continual learning setting, there are major drawbacks flaws in the experimental results and the evaluation of the proposed approach. Given the nature of TAG, I think is is fundamental to measure and report memory/storage requirements in comparison to other methods that also require additional memory/storage such as replay-based methods and network expansion methods. Furthermore, claims around avoidance of catastrophic forgetting in the main results reported in Table 1 and explained in page 8 are not well-supported since it is clear that the method is strong at learning new tasks, while not necessarily much better than counter parts regarding catrastrophic forgetting. 	3
The method novelty is limited, and experiment needs to be improved.  	2
In sum, there seems to be a large gap in the proof of the main theorem. Even this gap has been fixed; it seems that a large portion of the paper's results overlaps with some existing work. 	2
The paper provides an interesting lower bound for the Jacobian of a neural network. But the main issue I have is that the neural network considered with normalization seems not to correspond to networks that are commonly used.	2
The paper gives a rigorous theoretical analysis that should be of interest to a wide swath of deep learning researchers and practitioners, and is presented in a very accessible manner. The experiments are well-chosen to complement this analysis.	3
"I recommend rejecting this paper as it is current written. In particular, I think there needs to be more clarity around the authors new idea of ""representation shrinkage"" for the technical result to be appreciated. If possible, it would be nice if the claims about the importance of preventing this were further supported -- either theoretically or empirically -- by an analysis of its effect on training. I also think it would be better to focus directly on gradient updates to make claims about gradient explosion rather than using proxy quantities in terms of preactivation correlations and the input-output Jacobian. Finally, I would like to see some evidence of the paper's claim that critically initialized networks with polynomial decay of correlations are problematic in practice.

### After Author Responses
While I appreciate the response and comments of the authors, I stand by my original score."	2
"I do think proving the tradeoff between gradient explosion and representation shrinkage is an interesting result. But this paper is not well written and many parts are very confusing to me (as pointed out in the main review). Currently, I think this paper is marginally below the acceptance threshold. I will consider raising my score if my questions are well addressed. 

------------------------------------------------------------

Thanks for the response. After reading the response and other reviews, I decided to keep my original score. "	2
I am not at all familiar with probabilistic programming; this does look like a serious piece of work, though I do not know how novel it is or whether the claims in the paper are correct.	3
The paper presents a learned inference algorithm, but it's not clear how it generalises either across program types (which is necessary to amortize the training cost wrt HMC), or across neural network architectures (e.g., changing the internal state space in response to increased program complexity).  Additionally it's noted that occasionally the test error diverges, but there's no discussion of how to detect this in practice if the system was used for inference.	3
I recommend this paper for rejection. While I think the approach has the potential to work, right now it's very hard to get a sense of what was learned by the meta learning algorithm, how well any of this generalises when model structure or even observed data changes significantly, or even if the language is too restricted to make this is a significant enough contribution.	3
"The claim that the paper provides generalization of compiled inference across models is not supported by the description or the simple examples. These appear to be covered by existing work on inference compilation. 

The focus on a very restricted class of ppls makes this work very limited. I don’t believe I learned anything from this paper. "	1
The submission demonstrates that a fairly straightforward calibration-based approach for an ensemble model can lead to good performance in both in- as well as out-of-distribution settings. While the empirical illustrations could certainly be made more comprehensive, and the mechanism of improvements analyzed a bit more closely, I believe this is an interesting enough illustration to justify drawing attention to it.	3
While I think the results in the paper are competitive against the baselines, the authors do not give any explanations for why the proposed method should work. Therefore, I'm leaning towards rejecting the paper.	2
"While I do think that this paper discusses an important problem and presents methods that perform reasonably well on real data, I think that its current form the paper is a bit too thin and requires some more work before it is published. My main concerns are the lack of theoretical analysis, as well as additional experiments and analysis with more advanced methods. I also point to a previous publication that might help the authors relate to in their analysis. 
"	2
"This paper explores an interesting idea of ensembling a robust and standard model to attain high accuracy ID and OOD. However, the paper would substantially benefit from a more complete exploration of baselines and thorough explanation of experimental details. For instance, even some of the baselines mentioned are partially included or absent, and accuracy of ImageNet-R is substantially lower than usually reported. I recommend that the paper could substantially benefit from additional exploration, and may have been rushed in current form.

Edit: many of my concerns were addressed and I have changed my score to 6."	3
I enjoy reading the paper and I find it theoretically novel and sound. However, I do not find the experiments very straightforward, I believe it's necessary to compare the empirical variance of the gradient estimates with the baselines. 	4
The paper is well written and the mathematical derivation is sound. My main concerns are about the experimental results: I think the analysis is too weak and not enough baselines are compared with. 	2
This is a marginal improvement of domain randomization with marginal improvement in experiments.	2
Because my concerns outweigh the strengths now, I am leaning towards rejecting the paper. 	2
Please see above.	1
Considering the novelty of this paper, I vote for 'weak rejection'. If the authors can addressed my concerns, I would like to increase my score.	2
"The paper study the empirical findings of scaling insights for Transformers on both pertaining and fine-tuning and provide a list of conclusions. Even though the conclusions are not surprising to me, I think this paper provides some evidence to prior works and future works. One main concern of mine is the randomness of the paper since this paper performs comprehensive experiments and concludes insights based on the results.
"	2
This paper conducts solid experiments to support its claims, e.g. discrepancy between upstream perplexity and downstream performance and scaling strategies difference at different compute regions and DeepNarrow strategy to improve model speed and reduce parameters. Some of the findings provided in this paper is kind of intuitive and the DeepNarrow strategy has been shown in previous literature, which hurt its novelty. 	3
"Several issues have been discussed: Experimental results should be compared to additional baselines, changes in the experimental setup are not justified and seem unnecessary, drawbacks are not adequately discussed.  In the current form, the paper is marginally below the acceptance threshold but the reviewer is inclined to change the rating, in light of new supportive evidence.

**update**: The authors successfully tackled the issues I raised with the manuscript, therefore I am happy to raise my score."	2
Based on the pros and cons written above, I think the paper is marginally above the acceptance threshold.	2
The paper proposes a novel relative molecule self-attention layer for the Transformer model. The model can be first pretrained and then finetuned on downstream molecule property prediction tasks. Experimental results demonstrate the effectiveness of the proposed method. However, there are some concerns about the evaluation as described in the Weaknesses section.	2
The proposed methods contributes a new generation approach and explicit generation of both primitives and constraints over prior methods; and reconstruction of CAD sketches from hand-drawn images over both prior and concurrent work. The authors also show good qualitative results. However, as a big minus, the result are not compared to any related work. On the balance though, I am still leaning slightly towards acceptance, since the contributions are good, and the qualitative results are convincing enough.	3
This paper is well-written and tackles an interesting problem in a sensible way. However, due to limited novel technical insight and comparison to past work, I think it requires some improvement before it is ready to be published.	2
Assuming the authors are willing to add (a lot) more details, I would suggest that this paper should be accepted.	3
"If the similarity to the concurrent work of Ganin et al. 2021 and Para et al. 2021 should not be accounted for, then I am in favor of acceptance because the described model is sound and effective. But I remain uncomfortable accepting a paper that shares so many similarities with papers that have already been reviewed and accepted, even if shortly before the deadline. I suspect that the authors have been inspired by these concurrent works, maybe in the development of their method, its description, its evaluation, which is why I have a hard time ignoring them in my evaluation.

To be clear, if [Ganin et al. 2021] and [Para et al. 2021] are to be ignored, then the technical novelty is of level 4, but if they should be considered, then the technical novelty drops to 1 or 2 and I would argue for rejection.

--- Additional questions ---

Since the constraint model is trained separately from the primitive model, is it trained with ground-truth primitive sequences from Seff et al., or from the (approximate) sequences predicted by the primitive model? Section 3.2 mentions that the constraint model must account for potentially imperfect generation from the primitive model, but I didn’t fully understand how this is achieved. Is it only achieved by perturbing the primitive parameters with Gaussian noise? What if the predicted primitive type is wrong, wouldn’t such error degrade all subsequent predictions?

I appreciate the practical details provided in Section 3.1 and 3.2, and in Appendix C. Yet, I would have liked a diagram of the two Transformers to better understand the flow of information, and to illustrate how the sequences are fed to the networks. This would be particularly useful for the constraint prediction network, as the mechanism to cross-attend the primitive sequence and to point to its elements remains a bit vague to me.

I didn’t fully understand the tokenization in Section 3.1. Since the ID token specifes the parameter type, why not also use it to specify the primitive type rather than allocate part of the value token for this purpose?
"	4
I recommend rejecting this paper because of its unclear method description, poor results, and insufficient evaluations.	3
I think the novelty is not enough and some details should be described more clearly.	3
The paper presents a reasonable multi-vector extension of Emb2Emb (Mai et al., 2020), but is a bit lacking in comparison with other existing works. 	3
Overall this is a good paper which, with some adjustments to the writing, would be of good value to the community.	3
contribution not clear, technical depth weak, writing and notation to be improved.	2
Overall, it's a good empirical paper, but it's better to address some of the above concerns before applying the proposed framework in practice.	3
This paper proposes a novel approach to tackle several problems in federated learning using selective knowledge fusion. However, the algorithm proposed is largely empirical and needs more thorough ablation studies to strengthen its core claims. 	2
"The problem considered in this paper is interesting and important. The need to accommodate on-device systems constraints (limited memory and computational power) significantly limits the performance of models trained using FL. This paper suggests an interesting approach towards overcoming this issue, and the experimental results suggest that the proposed approach is promising.

However the paper has a number of limitations. Some are related to the practicality of the proposed approach, but the majority of these are limitations of the experimental setup used for comparisons. Normally I would be willing to overlook some of the concerns, but there are so many in this case that my overall recommendation is to reject the paper."	3
This paper proposes a novel architecture based on graph-shift, time-shift and space-time shift operators (also proposed by the authors in the paper) to handle time-varying graphs. The theoretical analysis of this paper is very thorough and clearly elucidates the stability of their NN architecture. 	3
This is a well-written paper with a neat space-time convolution model and important theoretical contributions in analyzing its stability. However, as significant as its strengths are, its weaknesses are also concerning, which mainly involves two aspects: 1. the architecture of ST-GNN does not seem novel and may suffer from varying graph size; 2. it certain could have empirical studies done on more datasets and have more baselines compared.  Overall, I would be very happy to improve the score if we could see more extensive experiment with more baselines and positive results.	3
Although this paper theoretically proves the stability of the proposed convolution operator, it does not provide experimental evidence about the superiority of the proposed method against other baseline methods. Given this paper is not the first to study time-varying graph signals, I think baseline comparison is necessary. Based on the above consideration, I think this paper is not ready for acceptance.	3
This is a nicely written paper that contains an interesting idea. The lack of technical novelty and practical results lead me to rejection.	2
"As far as I can tell, the authors do a pre-computation step which results in a constant signal over the sphere, which I find an odd choice.  The experimental section is severely lacking, and the method has limited novelty. Hence, I cannot recommend acceptance.

Updated my score to a 5."	2
Overall, I am leaning to accept the paper. I like the simplicity of the idea and its effectiveness. Nevertheless, I find that some of the points in the cons section, need to be addressed, in order to make the paper clearer in some of its parts. Hopefully, the authors can address my concern in the rebuttal period.	3
"This is a good, well written paper that proposes an elegant and principled way of scaling spherical CNNs to high resolution inputs. My suggestions are mostly in the direction of making the experimental section more convincing and reproducible, and I will be happy to increase my score in case they are addressed. 
"	3
Interesting idea, but there are some concerns over the motivation, explanation, and experiments.	3
The authors presented a relatively simple improvement to solving POMDPs by replacing isotropic Gaussians with normalising flows. While this does indeed work, the technical and empirical contributions are reasonably significant, and so I would recommend this for an accept.	3
It is well known that VAEs with diagonal Gaussian distributions for the prior and posterior do not fit many datasets well. There are a large number of deep generative models proposed in the literature that are much more flexible and suited to different domains. The authors propose using one of these, normalizing flows, and apply it to learning a model of the environment dynamics in POMDPs settings, and in turn use it to train agents in a sample efficient way. While I think this is a good idea, and experiments indeed show the benefits of using such a model, the contribution in itself is not very novel, as prior work has suggested using richer models (see referenced works on particile filters, or the work of Gregor et al. which also refers to the idea of using flows). I think this paper would be more interesting if it compared a wide range of flexible models, and included a more extensive set of datasets. Alternatively, the contributions would also be strengthened if it provided novel ways to use the learned models when training agents in POMDP tasks. As it stands, I do not recommend accepting the paper due to its limited novelty.	2
This paper proposes to use normalizing flows as belief representations in a POMDP, addressing drawbacks of VAEs or diagonal Gaussian approximations in representing multi-modal beliefs. While not highly original, the work is technically sound, and provides the necessary empirical evidence to merit acceptance. Nevertheless, I encourage the authors to address the concerns raised by other reviewers as well, and to provide comparisons to related works the authors also agree are highly relevant.	3
Very nice contribution making a new connection between programming language and GNN to study their expressive power.	4
As far as I checked, I could not find any incorrect points in the proofs. I think this paper is technically novel and significant as it gave a model-agnostic approach for analyzing the expressive power of GNNs and solved several conjectures.	3
The paper's main proposal, TL, offers a novel and simple means to characterize the power of GNNs irrespective of specific design choices, and helps derive some new results for GNNs. All in all, the paper makes good contributions, but some of its claims, namely the simplicity of TL translation, as well as its novelty, should be better explained.	3
"I think theoretical sounds of the paper is quite strong and the main motivation is valid.
I am slightly to lean to recommend acceptance for this paper. But  above 5 points should be addressed.

After rebuttal,the authors pointed out all my concerns very well. Therefore I would recommend clear acceptance for this work.
"	3
"This paper provides a very interesting and novel idea for continual learning. However, the empirical evaluation and ablation study seem to be weak. 

=== Post-rebuttal Comments===
The authors addressed most of my concerns in the feedback.  I kept my score and leaned towards acceptance. "	3
"Overall, I think this paper proposes an interesting method and provides extensive experimental results. So I tend to accept this paper. However, similar ideas have appeared in the existing work, such as [A]. Many details of this paper are not polished. So the overall score is “borderline accept.”

[A] Choi, Yoojin, Mostafa El-Khamy, and Jungwon Lee. ""Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay."" CVPR 2021 Workshops.

&nbsp;

### === Post-rebuttal Comments===
The authors addressed most of my concerns in the feedback. They also provided the additional experimental results I asked for. I recommend acceptance. 

The authors should include the additional results and corresponding analyses in the next revision.
"	2
"The method is simple yet effective, which is good. However, ""zero-shot"" sounds misleading, and ablation study is not enough.
"	2
I believe that the work is interesting, solves an important problem and does well. However, further experiments are needed, for instance, to check why the model is not performing as well in the clustering problem. 	3
"It is a good paper with relevant contributions.

I am not sure if the results are well evaluated so I expect to conclude at the discussion period."	3
Although authors did a fairly good job in conceptualizing and developing the latter stages of their proposed deep learning framework, as I elaborated in my main review, the prior stages of the framework lack experimentation. Model-centric approach is important but data-centric approach is more crucial especially in healthcare where predictions play a vital role in assessing disease progression and therapeutic intervention.	1
This work addresses an interesting and important problem in medical machine learning. However, it would benefit from additional methodological descriptions and results' interpretations.	2
The paper provides a strong theoretical insight but the experiments and baselines are weak.  The contribution seems limited with a marginal improvement compared with the current work.  	3
Overall, the paper studies a pertinent and difficult problem. However, in the current form, the present manusript provides only initial proofs-of-concept for potentially interesting ideas. These ideas need to be demonstrated and explored in more detail both in theory and empirically to make the manuscript stronger.	2
"Although the paper extends an existing approach to learn generative models of distributions exhibiting mixed-tail behavior, the paper has a number of weaknesses: (1) it’s not clear when such mixed-tail behavior arises in the real-world; (2) the class of flows considered are quite restrictive (affine, coupled with a permutation that requires light-tailed and heavy-tailed marginals to be split into two consecutive blocks); and (3) the empirical results are lacking: they only provide experiments on datasets of dim=16. 
"	2
I highly appreciate the clarity and technical soundness  of the paper. However, I cannot recommend acceptance given the very limited novelty.	2
The paper contains some interesting ideas, as evidenced by the experimental results. However, the theoretical depth and novelty may not be enough to meet ICLR's standards.	2
"To summarize: I am currently under the impression that the apparent complexity-expressivity tradeoff is not as significant as the authors claim.
Although it is apparent that that filtering operations are implemented as low-order polynomials over a partition of the spectrum, their piecewise nature completely destroys the locality inherent to low-degree graph filters, which is why such filters are useful in the first place.
This is mere speculation on my part, but I suspect that this method would, for instance, fail to generalize well to graphs other than the one it is trained on, even if the structure is somewhat similar.
Based on these concerns, I do not recommend the acceptance of this paper."	4
I suggest the weak reject to this paper in its current form. I can consider raising my score if all my concerns (raised in the weakness section) are addressed properly.	2
"I find the idea of partitioning the spectrum to be very interesting. However, the idea is not fully explored and a lot of space is devoted to uninformative comparisons.

I am currently assigning a score of 4, but I really see promise in this paper. So I'm willing to raise my score to 7 if my concerns are either addressed, or an explanation on why I have misinterpreted the paper is given.

The summary of the changes I expect is given in what follows:

A) The numerical experiments are extensive, but unfortunately, the choice of baselines is misguided. Comparisons with general FIR filters as well as ARMA filters are absolutely necessary. Comparisons with non-convolutional GNNs are optional, but I understand it from a SOTA perspective.

B) A discussion on the relationship between the proposed method and scattering transforms is necessary, due to the nature of the spectrum partitioning and wavelets. While including numerical simulations comparing with scattering transforms would be very much welcome, it is indeed not necessary and maybe only left for future work.

C) The interplay between the bank of FIR graph filters (i.e. the d x e bank of filters) and the partitioning of the spectrum should be explored: how do we know that these partitions cannot be learned in the bank of filters? choosing the intervals acts as a regularizer?

D) The increase in computational complexity due to the computation of eigenvalues should be thoroughly explained and total computational costs should be reported for all architectures.

E) I would appreciate it if the authors clarify my concern on Theorem 1.

F) I believe the title could be improved to better reflect the paper's contributions.

--- 0 ---

Update after the first round of reviews:

I consider D, B, and F to be appropriately addressed.

I have minor concerns on E.

I still have major concerns about A and C, mainly with respect to the omission of graph convolutional filters upon the objection of overfitting.

Thank you very much to the authors for their time and effort in addressing my concerns. I will increase my score to 5."	3
"Weak accept.

Despite the numerous and contradictory experiments (heavy tailed priors are better for FCNNs, but worse for CNNs), and the difficulty the authors have to interpret their results, the empirical results are significant enough for helping other researchers to guide their research. 

For instance:
 * a heavy-tailed distribution of the weights is not necessarily a problem;
 * the distribution of weights after training a FCNN and a CNN may be structurally different;
 * the ""cold posterior effect"" can be partly explained by data augmentation;
 * the ""cold posterior effect"" can be removed when using a prior with heavier tail.

A major issue remains: the authors should have checked that their results about ""FCNNs"" and ""CNNs"" are consistent for *several* FCNNs/CNNs and several datasets."	2
The work studies an interesting problem of prior selections for BNNs. At the same time, the results of the two main experiments are mixed and do not clearly demonstrate the importance of the heavy-tailed priors. The study also considers a single inference technique. Overall that makes me conclude that results are not generalizable and reliable. 	2
I vote for acceptance of this paper. I believe it provides a thorough empirical insights on the prior choice for BNNs and a recipe for future prior exploration.	1
This paper studied the important question of what would be a proper prior for Bayesian neural networks, and proposals were made based on empirical investigations of weight distributions of different neural networks. Experimental results on some image datasets validated the choice of the priors. In summary this is a nice paper with interesting findings. Still there are some technical details that need to be addressed (see my comments above).	3
"After initial review, I am inclined to accept this paper; I am happy to update my score after discussing it with authors and other fellow reviewers.
------------------------------Post-rebuttal---------------------------------

I read the authors' responses and went through the authors' new experiments. I also read the reviews from other fellow reviewers and the authors' responses to them. At this moment, I am inclined to keep my score unchanged.

I think each task would encourage representations to respect specific transformations and thus may implicitly encourage disentanglement. Therefore, I believe the number of transformations and their relatedness matter; this is the motivation of my questions.

The authors try to address one of my questions on the number of tasks, but the conclusion is inconclusive. Regarding the experiments on exploring the effect of the number of tasks, it's unclear why 40 tasks is always a bad choice in all three datasets."	2
The idea is interesting and I would love to see more papers try to tackle fundamental analysis of multitask models like this, but there are enough confusing and counterintuitive results that I would like to clarify things before endorsing this work. I very much look forward to the author's response.	3
Overall the main claim of the paper is interesting and moderately well supported, so while the paper clearly has room for improvement, I would still lean slightly towards acceptance.	3
The paper aims to show that MTL implicitly improves the disentanglement quality of the learned representations, however, the crucial flaws in the experimental setup make all of the major results presented in this paper inconclusive.	3
See Main Review.	2
The authors propose an interesting idea, however I think that the presented comparisons miss a few key comparisons. Specifically, I would have liked to see a measure of FLOPs required to reach given accuracies. Additionally, while the results shown span multiple domains I would have liked to see more ambitious experiments.	2
The paper lacks novelty and is an extension of (Zhang et al., 2021) for the convolution case. This is a major concern for me. 	2
Simple and impactful idea. Results are convincing. It will be an interesting paper to the hypercomplex networks community. Nevertheless, writing must be improved to get accepted (not the outline of the paper but the content). 	3
"I recommend accepting the paper.

I read the overview of the main proof, but not the fully detailed proof."	3
"1. This paper provides a new complexity bound for adversarial training. 
2. The empirical validation of this paper is insufficient. "	3
Good theoretical result supported by experimental evaluation on relevant topic.	3
"This paper provides some important results about the adversarial Rademacher complexity so I vote for weak acceptance. But there are many issues towards the experiments, proofs, and the writing of this paper.
"	3
In my opinion, this paper is ultimately making strides at an interesting and important topic in GP active learning --- what role does the full posterior have in managing the bias-variance tradeoff, and how can these insights be used to design better active learning acquisition functions? I think the proposition of QB-MGP and B-QBC is an important stride in this direction. However, I believe the paper is not ready for publication. In terms of presentation, I found the arguments made about bias-variance in its relation to the full posterior and the motivation for QB-MGP and B-QBC unconvincing. I also found some statements made in the paper to be confusing or unsupported. In terms of experiments, I found myself unconvinced by the presented analysis (specifically due to the high variance in the results and the lack of more general summary statistics such as AUC rather than a single iteration slice) and am concerned with some aspects of reproducibility. Therefore, I would recommend **rejection** for this paper and encourage the authors to revise the writing to strengthen the presented arguments, better motivate their active learning strategies, improve reproducibility, and conduct a stronger analysis and visualization of the experimental results.	3
This is an interesting work that considered Bayesian Gaussian processes for active learning. But the merits of the paper can be better enhanced by addressing the aforementioned comments.	3
"The paper contains interesting material, but specially abstract and introduction are very confused.
The state-of-the-art discussion must be also improved."	3
I think there is potential in this paper: the problem is important and the method is sound. Improving the clarity of the paper, comparing against McInerney (2017) and Zhao et al. (2020) and applying the method on real-world data would make this paper significantly stronger.	3
"
Overall this paper is well presented and delivers a simple yet clear idea with clear and convincing results over AlphaZero by applying HER to AlphaZero.  I'm somewhat on the fence as I do believe this a worthy contribution showing clear results, however I think that this paper would benefit from a bit more clarity around the algorithmic details and would strongly recommend that something be included to this effect at least in the appendix, but more preferably in the main paper.  Also useful would be some analysis to help the reader understand why this method works better, lending insight into the execution model and helping us to understand why we may expect this to extend to more complex environments.

Post rebuttal:

Given the rebuttal additional clarity on algorithmic details and computational costs, tree re-weighting and it's relational to HER have clarified things on my end. I'm happy to increase my score to an eight.

"	3
The paper makes a relatively small technical and empirical contribution given existing results on HER (Andrychowicz et al., NeurIPS 2017). 	2
"This paper proposes to incorporate HER into AlphaZero to solve the problem of goal-directed planning under a deterministic transition model. Their method is particularly eﬀective for those sparse rewards problems as demonstrated in the experiments. While the method is interesting, the above comments need to be addressed.
"	4
Though the paper demonstrates an interesting idea—combining AlphaZero with recent insights from Hindsight Experience Replay, on how to train a learning-driven agent in the presence of sparse rewards—the paper is lacking in clarity and omits comparison against clearly-important baselines that are known to perform somewhat well on some of the tasks of interest. Until these limitations are addressed, the paper is not likely yet suitable for publication.	2
The design of the evaluation makes it difficult to understand exactly where the performance improvement comes from (W1) as the proposed method seems to be trained ~50% longer than the baselines. Additionally, I am not fully convinced that a joint metric for OOD and misclassification is desired (W2). Finally, some ablation experiments are needed to understand the effect of certain design choices (W3). 	2
In summary, this paper presents an interesting idea however the experimental evaluations are not sufficiently convincing in the current state. I would like to hear the authors address my concerns raised above. 	3
Overall, this paper proposes an interesting and to my knowledge novel approach to uncertainty quantification. However, I believe it has two main weaknesses: (1) there's a lack of discussion and experimental comparisons to recent work in uncertainty estimation and (2) there isn't enough introspection of the proposed approach to confirm it's working for the purported reasons.	2
Overall, the paper lacks a clear hypothesis statement (Item i)), it does not propose a novel solution (Item ii)), and it does not conduct a set of experiments that give conclusive results in favor of a solid scientific hypothesis (Item iii)). Hence my initial grade is a clear reject.	2
A potentially useful method with a nice idea and some promising results. But the paper structure and writing need to be improved, the scope and contributions clarified, and the experiments tightened up. 	3
"The paper makes strong contributions with potential high impact, however its presentation is over-complicated and lacks clarity. I suggest to improve on this point and I am willing to increase my score accordingly.

## UPDATE AFTER REBUTTAL
The presentation of the paper has improved and, while some notation are still hard to read, I believe that the paper's contributions are now more accessible. I am hence increasing my score to 8. I am looking forward to seeing how the paper will be presented in video lectures."	4
In sum, this paper provides a novel random features algorithm with theoretical guarantees and has good applications. Nevertheless, some parts on algorithm and experiments are unclear, which would devalue this paper. I would like to see the response from the authors	3
While the paper can be helpful for readers who want to learn about continual learning in text classification using pretrained language models, the paper does not seem to bring sufficient a novel viewpoint or conclusion to be published at ICLR.	2
A thorough empirical analysis with unsurprising conclusions	2
Overall the authors perform a quite deep study of using pretrained language models for continual learning. Despite some weak points in the analysis of the quantitative results and inconsistent organization/language around the CL approaches, the thoroughness of the study, in particular the analysis at a layer-by-layer level, is likely of interest to the broader community. 	2
"The problem, proposed CL algorithms, benchmarks and evaluation metric are suitable to answer the research questions. The performance and layer-wise analysis is conductive to deepen an understanding for the shortcomings and potential opportunities of PLMs for continual learning. Unsurprisingly, variants of ER are the most effective technique in incremental CL with PLMs, which is somewhat disappointing, but also a standard outcome in CL.

The paper is mostly well written, and the experiments (sub research questions) are logically structured. Some minor details can be improved and have been pointed out in the review. Insights are valuable and provide a solid foundation for followup studies. The details in the appendix were interesting and added to the paper and its reading flow, e.g. by moving cumbersome details like hyperparameters to a dedicated appendix section.

I thus feel confident to recommend this paper for acceptance."	3
I recommend an accept at this time because most of the weaknesses I identified with this paper have to do with the strength of the claims made, which can be addressed in a revision. I particularly like that the paper demonstrates significant weaknesses, reliance on spurious features, of current models on the ImageNet dataset. On the strength of that contribution alone, I think this work is an important one. 	3
"Based on the pros and cons discussed above, I believe this paper bears the good potential to bring useful insights to the community, though a more thorough discussion from the theoretical side should be improved. 

After reading the response from the authors, I am pleased with the paper revision and decide to raise my score. "	3
"Preliminary recommendation: accept

Key reasons
- Importance of the topic/addresses a real need of the research community.
- Simple method, no technical flaw, underlying assumption are verified empirically.
- Experiments with the new dataset already provides new insights about existing models."	3
"As far as I know, this is the first work, proposing a differentiable DAG sampling. However, it is not clear whether it has some advantages as some post-processing is still needed if one wants to get better performance. Regarding the time complexity, a thorough analysis of other related work is needed. Moreover, it is suggested to compare to some recent works like the following in their experiments:
""DAGs with No Curl: An Efficient DAG Structure Learning Approach"", https://arxiv.org/abs/2106.07197"	3
I recommend this paper for acceptance. It proposes a simple method that obtains accuracy competitive to baselines (often exceeding them) at much faster training speeds (between 5x and 10x).	3
Overall, this paper is well-written. However, the proposed method is only a simple combination of well-developed techniques, and the experiments need to include more metrics. Thus, I think the contributions are only marginally significant or novel.	2
Overall, I believe this paper is a worthy addition to this year's conference. It tackles an important problem and provides a better performing, simpler and more intuitive solution compare to previous work.	3
The insights and novel paradigm for entity linking proposed by this paper are attractive. Although the experiments can be more sufficient, the key claims are supported by the experimental results. I recommend accepting this paper.	3
Solid paper about an original approach to entity linking that performs pretty well.	3
"While the research question and initial evidence are promising, the main issue with the paper is Weakness 2 above, particularly W2.1. Evidence is required to support the claimed computational burden of planning with ground-truth dynamics model and rewards. Furthermore, the multi-task experimental results are only conducted in two environments (W2.2) and only for goal-directed tasks (W2.3), which doesn't support the claim that the quality of the proposal distribution is critical to planning in *general* **multi-task** settings; it instead only constitutes evidence that the quality of the proposal distribution is critical to planning in *some* **goal-directed tasks**. 

In my opinion, this missing evidence weakens the paper slightly below the bar of acceptability."	2
"Whether or not to accept this paper depends on what we think conference publications are for.  

This paper contributes some clear and well justified knowledge.  But the increment is small and it seems likely to me that substantially new and different techniques will arise soon and render this irrelevant.    It seems unlikely that this paper will inspire or excite new research.

Ultimately, this decision is above my pay grade, and depends on the program chairs' views about the role of paper acceptance.

My guess is that a good but not hugely exciting paper is thought to be on the positive side, so I'll lean positive."	1
This is a reasonably thorough evaluation of common assumptions related to model-based RL, particularly in the context of MPC methods. The ideas are tested on a range of problems, ranging from the simple to the challenging, i.e., the go-to-target-pose (GTTP) task. The documented benefits of learned proposals on difficult tasks such as GTTP, as evaluated using both CEM and SMC, are also a worthwhile contribution.  The empirical work is solid.  A better understanding of model-based methods in continuous control settings is important for the RL community.	3
"
I think in general the algorithm is very good. The environments are pretty interesting.
The release of code will be quite important to this project if it aims to encourage future research.
"	2
In my view, this paper is well below the acceptance threshold and more literature survey seems required.	1
"While the idea of MetaLoss seems interesting, the evaluation and comparison of it is not convincing and needs significant work. For instance, instead of comparing a single simple baseline to MetaLoss, the approach needs to be compared to the state-of-the-art forecasting methods (e.g., NBEATS, Deep Factors, Graph Deep Factors, DeepAR, and so on…). The authors should also include experiments showing the runtime performance across all these baseline methods and datasets. Such results are omitted in this work, though are crucial. 

The exact hyperparameters used are also seemingly missing from the paper. But most importantly, the results shown in the paper are for a single set of hyperparameters, and therefore, it is unclear if the model works better for the specific hyperparameters chosen, or if it routinely outperforms the other across a wide variety of hyperparameter selections. In other words, the results shown are not significant due to this issue. It is likely easy to find a selection of hyperparameters where the proposed approach performs poorly compared to even the simple baseline shown. 

Furthermore, the writing can also be significantly improved including the introduction, problem formulation, and motivation and importance of the problem. Overall, this paper needs significant work before it can be accepted. See above for other important issues that need to be addressed as well.
"	2
"The model proposed by the authors seems to have powerful use cases. Their proposal has been well explained and supported by experiments. I would love to see some more experiments/discussions around some of their claims. I am happy to reconsider my scores thereafter. 
"	3
Even though the motivation of this work is clear, I have multiple concerns about the scientific rigour of the experimental setup (see the review section). Based on the current status of the manuscript, I am recommending to reject this paper.	2
This paper compares methods by which to assess the value of data points for classification. However, insufficient analysis of surprising findings and a lack of comparison to a well-established method that assesses the value of data points (SVMs) weaken the contribution of this paper significantly.	2
"The paper studies a very timely and important topic. Yet, the main concern is that the paper lacks formal justification and the experiments are done in a synthetic setting. It is questionable how generalizable the findings are beyond the specific synthetic data and the specific model considered in the paper.

--
[post-rebuttal] Thanks to the authors for the response. This is an interesting paper but my concerns about the lack of theoretical justification and experiments of modern DNNs as well as missing baselines still remain. Hence I will maintain my score."	3
The approach of the paper is interesting, but the paper needs a lot of work to be a full conference paper. It makes more sense to present it at a workshop and get more feedback. As mentioned in the main review the authors present some preliminary evidence for their findings, but more experiments are needed in order to have a strong claim. 	2
"Paper is well written, readable by a wide audience and tackling a quite relevant problem.
Experiments are convincing and supporting the authors' claims, although limited to a single model type and simple 2d synthetic dataset.
Overall, the authors contribute novel insights on a interesting ML problem,."	3
The paper was an interesting and nice read. Skill learning and fast adaptation are key features that can enable widespread adoption of robots. I have however three main concerns: (a) the algorithm uses very big offline datasets + 10 full demonstrations (states and actions) and success rate of completing the tasks is relatively low (<80% on average); we can certainly do better than this with minor prior information, (b) the related work/motivation part needs more work (it is not convincing me at the moment) and (c) the supplementary video does not show the performance of the algorithm.	3
Overall, this paper proposes an intuitive and novel approach for skill-based few-shot imitation learning. The experimental results show the advantage of learning the inverse skill dynamics and the proposed semi-parametric imitation learning approach.	3
Overall, it is a good paper to read and the authors have done a lot of works. However, the major contribution is not sufficiently novel to be clearly accepted in ICLR. As a result, I recommend a weak acceptance. 	2
Overall the paper presents some technical extensions over SPiRL, and seems to have significant performance improvements. I have some questions about the experiments, but if addressed I think the papers results are solid.	2
The theoretical results needs to be more strengthened. The contribution is ok but not good enough.	2
While this paper makes some interesting contribution of a new stochastic algorithm for deep learning, it appears to be far from a complete work. I think the paper needs to be substantially improved in both theory and empirical study.	2
The paper is an incremental work and is mainly a stochastic variant of Öztoprak et al 2018. This paper doesn't offer anything technical since all the theoretical results are just slight modifications of already existing results. On the other hand, the empirical results also don't show a big advancement. 	2
Although the paper proposes an interesting, easy-to-implement algorithm for non-convex stochastic optimization, I am hesitant to recommend for acceptance at this point for its lack of significantly novel contribution and some confusing aspects of the experiments. I am willing to increase my score if my concerns are well addressed.	2
"The main claim of the paper (the current approach of jointly training experts and the sparse gates introduce a negative impact on model accuracy) is not well supported by the experiments. In addition to that, many of the experiments report training measurements rather than test or validation measurements. Comparing training perplexity is not enough to show the superiority of the proposed approach. Some of the details of the implementation are not clear to this reviewer and affect the FLOP vs. Accuracy comparison. Actual runtime comparison (on a particual hardware architecture) would be much better.

Given all these issues, I recommend to not accept the paper. 


**Update after rebuttal**
The authors have addressed many of my concerns during the rebuttal period satisfactorily. My concern about the claimed ""speed-up"" is still present, but the authors have stated why they decided to use FLOPs rather than actual runtime on a given hardware and implementation. Based on all this, I'm slightly increasing my score for this submission."	2
Overall, I think the training scheme from dense to sparse for sparse MoE gating makes sense, and this is supported by experiments in the paper. However, I also think the technical novelty of this paper is limited given existing work with similar ideas on improving the gating network training.	2
"I’m positively disposed towards this work, however, my main concern is on the lack of a clear demonstration of the magnitude and the significance of the gains.

Please clearly show me the asymptotic quality is worse using vanilla approaches (e.g. Lepikhin et al., 2020, Fedus et al., 2021). As far as I can tell, the only substantiating plot for this is 3(b), but in my opinion, this doesn’t clearly showcase the improvement of the method. At the very least, these curves should be extended until MoE-DTS is conclusively better than MoE-Switch on a FLOPs basis. Further, though the idea is interesting, showing stronger empirical results than a 5% FLOPs-efficiency gain is likely needed to convince practitioners and researchers. If this is the case, I will improve from a 6->8.

"	4
"While the algorithm is reasonable, the experimental results (only one plot, Figure 3b) do not suggest it provides any additional benefit compare to the standard baseline.

------------

Note: After the rebuttal, I increased the score from 3 to 5."	2
"While there are some clear advantages highlighted by this paper in obtaining more efficient transformer models, overall, I believe that the paper is not ready yet for publication.
"	2
The paper proposes simple techniques that clearly addresses the limitations of recent work on solving tasks in the easy-to-hard benchmark dataset.	3
"The proposed approach is compelling, and the results are promising, but there are a few issues that needed to be sorted out, including some additional baselines, and clarification of the selection criteria for the reported results.

Update after discussion period: the authors included a number of supplementary results and informative additional control experiments. I still think that the results would be more convincing if compared to a broader range of competitive baselines, but I think these new results/experiments are a significant enough improvement to merit a score increase."	3
Overall, I like the idea of this paper, However, compared to the original work, the proposed modifications are marginally significant.	2
Decent paper, but should be evaluated on more common benchmarks / less toy settings and should discuss and compare to [1].	2
"The idea and the model in the paper sound reasonable; however, the experimental results are very unconvincing: (1) Three of the baselines are operating on the embedding of the data potentially built by an ill-configured model (2) the claimed scalability is only empirically supported on *one* dataset of size 100000 (3) there is a ton of missed baselines from the graph clustering literature.

After seeing the rebuttal I raise my score to the accept category."	3
This paper proposes a novel graph clustering method with solid theoretical supports and strong practical performance. The paper itself provides complete reasonings of the motivations and derivations. Overall, it is a impressive paper that brings huge improvement to an important problem of graphs.	4
The paper has some strengths that could be useful in applications of graph or network clustering and inference. I find the continuous tree-based hierarchy and associated Markov chain interesting and results promising. I do have some reservations about some decisions made about what to report as I highlighted above. Overall, it may be an interesting addition to ICLR if the authors can improve on the weaknesses I pointed out.	3
"The idea is interesting, simple, and intuitive. Experiments show that it is effective for the classification task.
However, the two major concerns make me uncertain about the effectiveness of this method.
I believe this idea has good potential. More experiments and discussions can make the argument much stronger. If these two concerns can be properly addressed, this will be quite impactful work."	3
This work presents an interesting architecture, but does not support the central claim of adversarial robustness because they do not present white box results, and do not specify on which architecture the black box attacks were derived. 	2
While the idea of tessellation for heterogenous subnetworks is interesting, the paper is lacking in thorough empirical evidence to back up the claims of robustness.	3
I really enjoy the idea of the paper. It provides evidence that the hypothesis I gave in the Main Review above may be true, which can inspire more research in adversarial defense. However, though the current experimental results on MNIST and CIFAR-10 are encouraging, I have two major concerns regarding the data and the network capacity, respectively. On the bright side, if the authors can show that the story holds for larger images and stronger networks, then the paper is much stronger and more generally impactful. 	3
"Overall, the authors provide a decent dataset and benchmark for genetic knockdown experimentation in the setting of active learning. However, I believe the paper would be better suited for datasets/benchmarks venue. 
"	2
All in all, this paper proposes a benchmarking suite that can help developers of active learning algorithms improve their programs. While the paper only considers a limited set of models, active learning algorithms, and datasets, it is an important step towards establishing best practices for in vitro experimental design. With source code publicly available online, it provides necessary tools (and datasets) to advance the state of the art for experimental design in drug discovery studies that use CRISPR technologies.	3
"A straightforward paper that presents a useful-looking benchmark dataset for drug discovery. Machine learning novelty and significance is limited.

POST-REBUTTAL UPDATE:
After clarifications, it seems apparent that benchmark papers like this could fall in the scope of ICLR. I still find the benchmark potentially useful and something I might use in my own research. I do not have any particular shortcomings in mind, but I think a stronger demonstration of the ability to generate new machine learning insights using the benchmark would have been useful. I will update my score to borderline positive (6) to reflect these thoughts."	1
This work studies an interesting problem, but the presentation is not clear expressing the motivation.	2
I am still voting for acceptance since the problem motivation and formulation is done very nicely and experimental section is also strong. The proposed algorithm is simple and intuitive, but not that rigorous.	3
The paper is well written and technical sound, with strong results. 	3
"1. The case study is not convincing enough to lay the foundation for the model.
2. More details should be given for some confusing points."	2
This paper proposes some novel idea to defend adversarial attacks, but the paper should be better organized to present the technical contributions.	2
"This paper suffers from poor quality of writing, an unclear contribution to the already vast literature on the detection of adversarial examples and limited engagement with well-known recommendations from the research community on the evaluation of defenses/detection methods for adversarial examples.
++++++++++++++++++++++
No response so I retain my score."	1
I believe that the paper quality is far below the acceptance threshold. First, this paper does not contain any significant progress in the manifold-based defense method. Second, the average quality of paper writing is extremely poor to understand the paper properly.	1
Overall, I do not think the paper provides a significant contribution to the community. The proposed method is trivial and similar kinds of pre-processing methods have already been considered.	2
In this work, the authors introduce the concept of FLID, SLID, and RLID, and investigate these metrics in graph learning. The investigation is interesting, and these LIDs are helpful in determining the graph complexity, understanding the issues associated with GNN models (e.g., over-smoothing problem). However, it seems that these LIDs are not intuitive to be explained, and this paper also does not discuss how to leverage these LIDs in graph learning. Therefore, it would be great if the authors could provide a more detailed discussion about the reviewer's two concerns mentioned above.	3
Interesting research problem, but some definitions are unreasonable and most of the claims are incorrect.	2
I recommend the paper as marginally below the acceptance threshold due to the weaknesses mentioned above.	2
Overall, the paper seems to contribute to the field, but small changes are required before publication.	2
Overall the paper is of high quality. It is well structured and describes concisely and clearly the work done. The experimental evaluation is appropriate and demonstrates the benefits of the proposed method in a large variety of domains. I believe the paper could be improved by also considering recent work from the QD literature which could be applied in similar domains while scaling to larger collections.	3
"Although the paper reports some interesting results, some of statements regarding the proposed algorithm do not seem well-supported.
In addition, the experimental section requires some improvement to justify the claims. 

======== Comments after the author response ===========

I appreciate the authors' effort to address my concerns. During the conversation, the authors modified the several parts, which are crucial to support the problem formulation.  However, even after the in-depth discussion, I am not fully convinced that the claims are correctly proved. 
Therefore, I keep the initial score."	2
This paper presents a simple and effective approach to generating policies with diverse behaviors. I believe this will be an important contribution to the field.	3
In summary, I think the idea is reasonable and developing a cheap MCMC-based autoencoder will be of interest to the community. However, I have some concerns with respect to the methodology and the experiments mentioned above.	3
The paper proposes a new idea on sampling from unnormalized densities. It can be useful for learning deep generative models. 	3
The paper study an important problem, but the current presentation makes it hard to follow. Also the extra experiments and analysis need to be added to motivate and back-up their claims. 	2
"I like the paper, I feel that the writing could be simplified somewhat, the method treats latent posteriors over data points as unique linear projections of a global distribution over network final layer weights.

- to me, the proposed method is an instance Bayesian neural networks with a different distribution (the ELBO) over weights (eg https://arxiv.org/abs/1902.02476, http://proceedings.mlr.press/v48/gal16.pdf)

- the experimental results have very high variance amongst baselines, this is concerning, particularly, the ""truth"" that the proposed method aims to approximate performs *worse* that the approximation.

- treating all dataitem posteriors as a linear projections of a single communal distribution seems risky, further comment would be appreciated."	3
"This paper explores an interesting problem of learning counterfactual physics from pixels. It presents a solution by extending a previous work, CoPhyNet, with new forms of unsupervised video representations. 

One of my concerns is that it does not provide sufficient empirical comparisons with existing video prediction models other than PhyDNet and the modified V-CDN. Furthermore, I would increase my score if the effectiveness of the method can be validated in a real-world dataset."	3
"Overall I feel that the paper is a strong contribution towards visual counterfactual reasoning (making the framework unsupervised as opposed to CoPhy) and would strongly urge the author(s) to add the experiments mentioned in the Weaknesses section. Based on the novelty of the paper, I am voting for a weak acceptance of the paper.

=======
Post-rebuttal decision:
After reading the comments of the authors as well as reviews of the other reviewers, I'm very much satisfied with the additional experiments added in the paper and I believe this paper would be a good contribution towards unsupervised visual counterfactual prediction. Hence, I've increased my score to 8 (Accept)."	3
This paper introduces an ambitious new baseline for counterfactual 3d physics predictions in pixel space.  It introduces a sophisticated method based on separate modules for image parsing, parameter inference, and prediction, that outperforms two previous approaches V-CDN and PhyDNet.	4
In summary I think the methodology is simple and practical. It could have potential usage on many applications. But the empirical demonstration is not convincing, especially missing the comparison with MCMC methods.  	2
The reviewer strongly recommends rejection since the paper lacks both significance and novelty.	1
The proposed approach, QRS, is a novel extension of rejection-sampling which can be applied in settings where standard rejection-sampling cannot. The approach has a number of benefits over alternative sampling approaches such as the ability to estimate useful metrics for tuning. The approach can bootstrap on top of further development for learning proposal distributions to target unnormalized distributions. While the paper is well written and the method appears powerful and novel, the paper has issues with its evaluation. QRS is not compared with sequential sampling approaches. This leaves out much context and ignores much recent work on discrete sampling. The paper would be greatly improved with a comparison to some of these approaches. 	3
Overall, this is an interesting paper. This paper reveals some interesting results, but there are still some questions (see above). If the author can discuss these issues and improve writing, the paper can be accepted.	3
The paper has several positives, the concerns include missing references and comparisons. Hence, it is just below acceptance threshold.	3
"This is empirical work studying an important problem. The results are also interesting which suggest a high degree of sparsity for certain learable inverse imaging problems, that closes the gap with the traditional sparsity based inverse imaging algorithms. The presentation of the paper however needs significant improvement to connect the conclusions toward a coherent story. 
"	2
Even though this paper is interesting, I still argue the usefulness of the proposed LIP. This paper only attempts to do some exploration based current existing LTH related works, which weakens  the novelties and the contributions of this work.	3
"To reconsider my score, I would like to see the language around implications of the bound clarified throughout the text (in response to my comments under “the purpose of the bound”). Currently there is a huge disconnect between what has been shown and what the claims are.

Also, since the main contribution is the algorithm, I would like to see more evidence towards the algorithm potentially being adopted in practice. What are the computational costs? How do they compare? How does the algorithm compare to other recently proposed algorithms that were derived to improve generalization (like sharpness aware minimization)? How does the algorithm compare to other regularization techniques?"	2
The proposed method is appealingly simple, but yields mixed results. The paper's presentation and analysis does not present a strong link between the theory and the empirical section, which leaves me to recommend not accepting the paper in its current form. 	2
Very Interesting idea but the presentation can be improved. 	3
Overall, this is well-structured work with both theoretical analyses and empirical results. However, this work did not provide a justification of why diversifying activations is better than diversifying weights, and it did not compare to commonly used regularization techniques such as dropout and other diversity-promoting works. Therefore, I am not recommending the acceptance of this paper.	2
"Overall I feel this is a borderline paper with a mixed feedback from me. It has some interesting thoughts and empirical results with come concerns on the theoretical justifications.


#######Review after reading the authors' feedback########


I would like to thank the authors for their explanation and the revised draft. Still I feel the assumption in the theoretical justification is kind of strong even though similar assumptions have been used in some previous work. I would keep the ratings unchanged. "	2
This is a good paper - it proposes a novel idea to solve the traffic prediction problem and conducts sufficient experiments to prove the effectiveness of the proposed idea.	3
I recommend acceptance.	4
The paper is not well presented, and critical information regarding the problem and model setup is missing, which makes the paper difficult to understand. 	2
"1. The writing of the paper should be carefully improved;
2. Current experimental results can not demonstrate the effectiveness of the method;"	3
"This paper addresses important non-IID and privacy problems in federated learning, and proposes a frameworks with some components are evaluated. However, components of interplaying labeling and ServerUpdate are not well-evaluated in this paper. Moreover, the details of experiments are inadequate. 
"	2
The proposed idea of using synthetic data, generated by local GANs of each client, and pseudo labelling at PS seems to be effective to resolve the non-IID issue of federated learning. However, the overhead of training local GANs in each client can be huge, which makes this approach not very practical. The overhead should be clearly addressed to compare this work with other baselines. The current experimental results only measure the overall test accuracy, but since the trade-offs between quality of synthetic data, pseudo-labelling, and overall performance are main factors in the proposed scheme, presenting such trade-offs in more detailed manner would be helpful to support the idea. 	2
"This paper focused on a challenging problem -- non-iid in federated learning and tried to combine methods from different areas to solve it, which is interesting itself, however, this paper mainly suffers from the following aspects:
1. Technical correctness. The privacy is not well maintained;
1. Lack of novelty. The proposed algorithm is more like a rough combination of several existing methods, with little innovative thoughts or theoretical explanations;
2. Design of experiments. More experiments should be carefully designed and conducted;
3. Missing proper citations."	2
The paper proposes a new federated learning algorithm on non-IID data. Although the proposed algorithm is interesting, many details in the algorithm and experiments are missing. I believe the paper needs to be significantly improved from both the theoretical and empirical perspectives.	2
Overall, I think this paper is marginally below the acceptance threshold. I appreciate the novel approach of the von Mises-Fisher loss for post-processing, and the discussion surrounding how to think about it. My concern is in regard to a more intuitive explanation of the behaviour of the hyperparameters governing the von Mises-Fisher loss, and missing analysis with regard to some experiments or comparisons. Without them, the paper in the current state is difficult to accept even though the idea is technically interesting.	4
"The adaptation of fairness metrics to the face recognition setting is
interesting. The evaluation of the method proposed has some gaps that
lowers the quality of the paper."	3
Although the motivation and the target problem are promising, this paper has a large room for improvement (as explained in the main review). Thus, I believe that this paper is currently not good enough to accept.	2
The proposed ethical module and the way to train it is novel. Besides the paper is well written. The experimental evaluation part could be strengthened, to better validate the effectiveness of the proposed method.	4
A solid and inspiring theoretical work. Novel use cases are introduced based on theoretical grounding and are justified by promising empirical results.	4
I appreciate the formal description of the label trick and the associated theoretical analysis. However, the novel use-cases motivated by that analysis are weak and empirical analysis on the role of the label trick is incomplete.  	3
Some fundamental issues in the paper are preventing me from accepting this paper: the experimental validation is insufficient and unconvincing. The motivation for proposing the deterministic alternative to the label trick is not discussed.	2
The paper has strengths and tackles an important problem, but needs further work on the points mentioned above. 	3
"Overall while having justifications of what people do in practice is nice, I think this paper fall short of providing a satisfactory analysis of the label tricks, especially since it does not rigorously explained it superiority compared to other methods.
I think this stream of research is valuable but this is too early for publication.
"	3
I have a positive view of this paper and the line of research the authors are working on. I believe there is a lot of value in revisiting earlier work such as real-time recurrent learning (RTRL) (Williams & Zipser, 1989), which the authors correctly identify as a related work and explain in their manuscript.	3
"Essentially, the paper is applying the known ""sketch and project"" trick
to recurrent learning, so the novelty is rather limited. The experimental
comparison is a bit lacking in my opinion and there is a big question mark in terms of the added computational cost
(and whether that is worth it in practice). I am recommending rejection
for now, but would encourage the authors to respond to my concerns in the rebuttal."	1
The idea is new, simple, relatively easy to implement, and seems to improve the estimation of the gradient. But the paper lacks empirical evidence, in particular consistency of the method over several datasets *with the same setup*. 	3
The paper discusses the possibility of applying direction derivatives to improve the gradient descent method. There is no concrete calculations and analytical examples to support the idea.  	1
The work is interesting, as it explores the idea of knowledge distillation from a Graph transformer to a SMILES transformer for performant molecular prediction with fast inference. However, the idea of cross-modal knowledge distillation is not novel. While it is well understood that the SMILES transformer after KD provides the advantage of fast inference, it is not clear if knowledge distillation is the way to get to SOTA performance with fast inference, as recent pre-trained SMILES-based efficient transformer models are providing competitive performance.  Also, the work does not provide any strong evidence of the  generalizability of the proposed approach.	3
"The authors provide an interesting knowledge distillation framework to learn molecule embedding, however, the reviewer is not fully onboard with the motivation of doing the knowledge distillation (e.g. learning SMILES based transformer from graph-based transformer), since smiles-based transformer has a competitive performance. 

"	3
This paper is based on an interesting idea of building a fast and high-quality SMILES-based molecular fingerprint generator with knowledge distillation from state-of-the-art graph-based models. The authors have described the proposed methods clearly, with their claims being supported by experimental results. Although more experiments can be performed to give a thorough evaluation to the model’s performance, I believe the paper should be recommended due to its novel idea and convincing results.	3
I am concerned about the over-claim of efficiency and the correctness of tokenization.	3
The idea is interesting but the terminology and experiment results are confusing.	2
"There is relatively little research on Explanation by manipulating weights. So the direction of the paper is interesting and relevant.
Unfortunately the paper is lacking clarity in many aspects, for example how attribution maps are extracted or the description of experiments, especially the ""model treatment"".
It is unclear how the attribution maps give better insight in model behaviour than other explanation methods.
There is no quantitative evaluation of results.


"	3
Overall, I think that the underlying idea of the manuscript is promising but that the draft is immature and requires a significant rewrite to more clearly explain the underlying notation, background, and theoretical assumptions. Furthermore, the experiments need to be extended and more clearly explained. Please refer to the list of feedback above for details. 	2
"In weighing the pros and cons of the paper, I believe that the current version is not ready for formal publication. 
As well-motivated as the paper can be, a significant revision needs to be done before the paper can bring useful insights to the community.
"	2
The authors claim to show a link between robust RL and MaxEnt RL, however, their main result is only about robust MaxEnt RL and MaxEnt RL(with modified reward). The relevance of the paper is, thus, difficult to state, since it is not clear how the result relates with any un-regularized formulation. The clarity of the exposition can also be improved, together with the experimental analysis which struggles to effectively shedding light on the theoretical results. For these reasons I recommend a weak reject.	2
Well-written paper with interesting results related max entropy RL and robust RL. Some technical results should be clarified and the experimental results could be strengthened.	3
In summary, the paper is a nice step forward in obtaining robustness guarantees for RL algorithms. 	4
interesting paper connecting robustness and maximum entropy.	3
Interesting design with potential applications.	3
"Correctness: I did not spot any obvious errors in the paper.
Novelty and significance: this is my major concern. It is unclear to me whether the proposed method is original and significant enough.
"	2
"In summary, the reviewer found applying deep RL to portfolio management interesting. However, the reviewer has some concern about the clarity of the presentation. Clarification for some technical contents are needed. 
"	2
In general the paper is well-written, the motivation is clear, and the numerical result is convincing. I recommend acceptance for this paper.	3
This paper, while tackling an interesting computational linguistic problem, does not bring anything new, and there are detected issues with its thematic fit with ICLR, its lack of novelty and methodological, theoretical or empirical improvements, some problems with presentation (a stronger motivation is needed, the results are presented as snapshots of figures!), etc.	1
The paper is technically below ICLR threshold.	1
Several methodological problems in how data was preprocessed. Motivation is weak, and models are out-dated. 	1
This paper is an empirical study of standard methods on the problem of classifying a writer's country of origin. It has limited technical contributions or insights. 	1
Besides the few minor typos, I think the paper meets the quality bar for ICLR. The technical analysis has some valuable novel aspects that justify well why adversarially trained models are good generative models. 	4
The proposed unified probabilistic framework CEM is fundamentally novel and interesting. It gives explanation on the generative capability of adversarial trained model. It also gives probabilistic understanding of ST and AT in both supervised and unsupervised learnings. The derived adversarial sampling method show a better sample quality. However, more experiments are needed to further justify the correctness of the proposed CEM, given that there are a lot of approximation in the derivations.	3
The problem studied is interesting, however, more investigation should be conducted to show the effectiveness and importance of the proposed probabilistic framework. Please also check the weakness part for more explanation. 	2
Overall, I like the insights provided in the paper and their experiments also proved their points well. Solid accept.	4
There is a mismatch between the theoretical part of the paper and the experiments.	3
This paper provides a through discussion on limitations of existing PE approaches and proposes the criterions of PE-equivariance and PE-stable. Mathematically, this paper proves the proposed PEG layer is able to preserve permutation equivariance as well as to achieve PE-stability. The proposed technique is solid and novel. There are some minor concerns as raised in weaknesses, but overall it is a paper worth of accept. 	3
Overall, I feel this work is interesting and important in this direction of *designing PEs for GNNs* that do not have any issues violating the beauty of using GNNs in the first place; and critical for link prediction tasks as known from the literature [2]. However, it seems the empirical results may not be adequate to reflect on the technical contributions of the paper. In summary, my evaluation of ‘correctness’ and ‘technical novelty and significance’ measures of this paper are positive, while the evaluation of ‘empirical significance’ does not seem to align to the technical contribution.	3
I am generally satisfied with the content of the paper.	3
"Overall the paper is very well written, has some interesting theoretical insights and very practical results. It will be very helpful if the authors can further address some of the  questions and weaknesses as pointed out in the previous subsection of this review. I believe that the authors should at least perform more diverse experiments to completely support their arguments regarding generalization and higher stability. After addressing these minor pointers, I believe that the contributions will be worth presenting at the conference. I additionally support the simplicity of the proposed approach, both analytically and computationally.

"	3
This paper has some interesting concepts but falls short on uncovering novel insight.  They are able to recapitulate other papers' results, but even this seems a bit tenuous. The experimental support for any conclusions is too weak in this paper to draw any real conclusions.	1
Unfortunately, it seems this paper was rushed into submission. The idea sounds interesting, however, the proposed manuscript has several serious issues as raised in my main review. I therefore recommend rejection of this paper.	2
The paper is very interesting but would benefit substantially from 1) medium scale experiments (e.g., ResNet on ImageNet) and 2) a more thorough discussion and comparison with related work.	3
The perspective of the paper is interesting, but some claims might need correction. Moreover, the observation is straightforward, especially from the view of network quantization.	2
As the extendibility to any n-dimensional data is its big advantage over those complex data augmentation methods, I am interested in seeing some results in other domains.  Given how well AugMix performs against the standard training procedure in data corruptions, I may increase my score if additional experiments are provided to reflect its ease of use/extendibility over methods such as AugMix.  	4
"The usage of input-jacobian-fourier (which is highest input sensitivity) to measure spatial frequency sensitivity is intuitive and well explained. However, the paper neither justifies its practical applicability, nor does it develop the methods into a fleshed out model analysis to yield insightful conclusions.
"	2
This paper introduces a new idea and demonstrates the empirical strength of the method, while the method has some empirical virtue, the way the results are presented makes it very hard to appreciate the results. 	2
"The proposed SFS measures are clearly motivated, and provide interesting insights from the frequency-perspective on how existing methods improves model’s robustness. My major concern lies in the insufficient experiments and unconvincing results as stated in the weakness section.

**Disclaimer**: I am unfamiliar with robustness in CV, it's possible I have some misunderstandings. My rating is highly subject to change based on other reviewers’ comments."	3
"The paper is interesting and uses a suite of analytical tools to evaluate the representations learned in different multi-path network settings.

However, in the current format, it is hard to see how the results of the experiments generalise to networks used in the literature, and which aren't directly consistent with the network used for experiments. Furthermore, the analysis is very similar to Richter et al. and the main novelty is its application to multi-path network.

For this reason, I am currently recommending Rejection"	2
The paper seems like a good first attempt towards an important and difficult problem. However, most of the conclusions seemed obvious or inconclusive (e.g., since the paper does not account for “how difficult a given problem is”, it is not easy to conclude how much receptive field size we really need over depth, etc.). More comprehensive evaluation on difficult datasets (not necessarily large datasets) is needed for a better contribution. Note that, most statements made in the paper themselves are correct and well-supported. The reviewer has concerns about the claims being obvious.	1
Although the motivation of the paper is interesting, the analysis results of the paper is weak and not that surprising. In addition, part of the experiments are redundant since it is mentioned in the previous paper. There's little new insights or takeaways from the analysis. As a result, I recommend to reject the paper.	2
"Strengths:
1. This paper is overall well written and the method is simple and easy to follow
2. The task-agnostic backdoor attack is an interesting and less explored problem, which can be a real practical threat.

Weakness:
1. Lack of a comprehensive comparison with related work
2. More experimental details are missing.
3. Some experimental results are a bit concerning and unconvincing. 
"	3
Overall an interesting paper and a relevant contribution. I recommend acceptance to the conference.	3
While the problem considered by the paper: injecting task-agnostic backdoors in pretrained models that sustain after finetuning is very interesting and important, this approach proposed in the paper has minimal technical novelty with missing crucial technical details which makes the paper tough to understand and reason. The empirical evaluation of this paper has several weakness on the fronts of missing baselines, missing details about backdoor poisoning data, and stealthiness evaluation. 	1
Contributions in terms of achieving a successful backdoor technique are well explored. Apart from some cosmetic changes mentioned above, along with the issue of a more vigorous evaluation of potential defenses, I feel the paper is in good shape and would make an valuable contribution to the conference proceedings. 	1
"I only have one major concerns towards the experiment. The authors propose several different approaches in the paper, including class-specific distillation and margin based distillation, and corresponding delegation methods. However in the experiment section it looks like they only focus on class-specific distillation and some related methods which are introduced in the appendix. 

I would actually suggest the authors to make the paper more self-contained, if the methods are introduced in the main text they should be evaluated as well. And some alternative methods can be included in the appendix. Also, I feel like it would be better to have a high level summary in terms of the pros and cons for each method, along with the recommendation for users in practice. 

Minor issues: line 4 of Introduction, upto -> up tp"	2
The paper attempts to solve and important problem, but is very heuristic and does not validate that it is better than the prior art.	2
Even though the performance of the two-stage framework seems to be higher than the standard distillation, I am not sure yet if the gains of the proposed approach compared to standard distillation justify the acceptance of this paper. 	3
This paper fails to demonstrate the motivation and advantage of the proposed method for efficient inference of large model. It does not meet the acceptance bar of the conference.	1
I am positive on the paper, due to its clarity and technical depth. I think the paper can be rated a 7, yet there is no rating between 6 and 8. 	3
This paper is well written, and provides good theoretical insights about the representation power of both shallow and deep neural networks.	3
"
Because of concerns in the significance and technical novelty in this paper, I would recommend weak reject.

**After Rebuttal:** Thank the authors again for a detailed reply. The authors have addressed my main concerns. I would like to raise my recommendation to weak accept and increase the correctness and novelty scores, and confidence. I suggest the authors to incorporate their answers, especially 1.(a) and 2.(a), in their next revision."	3
Except several relatively minor issues with presentation and clarification of the results, the reviewer thinks this paper to be a good paper on the topic of approximating Korobov functions. More work is needed for presentation of the paper, and fixing a few things here and there (see above as well)	2
"My vote is - reject.
I mentioned my concerns. It is possible that I did not assess the contributions correctly, I am willing to read the other reviews and reconsider my score if they convince me otherwise. "	2
"
The proposed method combines an online learning algorithm and an adversarial training approach, which is intuitive and new. However, the motivation for protecting the weak in an adversarial setting is not well-explained and justified in my perspective. In addition, the baseline results presented in the paper are far below the state-of-the-art and there does not seem to be a clear advantage of the proposed method over existing approach. Therefore, I suggest a reject for this paper given the abovementioned concerns.
"	2
The paper is easy to understand, presents a straightforward/well-motivated method, and backs claims with corresponding experimental results. The presented methods also outperform the baselines. However, the paper has relatively minor issues with clarity and incomplete related work.	3
"The paper has several advantages 
(i) points out an overlooked problem in AT (to my knowledge), 
(ii) proposes a novel method for the problem with guarantees
(iii) does a nice trade-off between theoretical analyses and empirical evaluation, and
(iv) it is well written.
There are two main directions for improvement theoretically and empirically, e.g. better connection between the theoretical result and the empirical analyses, more exhaustive empirical evaluation on par with AT literature. Nonetheless, I found the paper very interesting and I think it is relevant for the ICLR audience.
I am happy to raise my score if I misunderstood and the above comments are arguable, or if the authors provide improvements (see questions and comments above)."	3
I think CSQ is a simple but effective method that can be helpful for ultra-low bit quantization. However, the novelty and application scope are limited and the experiments have room for improvement. Consequently, I think the paper is marginally below the acceptance threshold.	2
The analysis and experiments have some issues (see weakness). I will give a borderline due to those concerns and change it accordingly based on rebuttal.	2
This paper seems to provide a better understanding of the low-precision (= 2-bit) quantizers, but the discovery itself seems to be marginally significant. Therefore, I am inclined toward rejection. 	2
To sum up, this paper seems to have several technical flaws for the claims to be well supported. 	2
In summary, I cannot recommend this paper for acceptance at its current state. Although the work introduces a potentially exciting solution to an interesting problem it has a number of ambiguous statements and lacks clarity on the otherwise extensive evaluation. In addition, some of the used assumptions in this work may prevent if from being actually applicable to full object 3D printing. Upon clearly and convincingly rebutting my review, I will be inclined to reconsider my recommendation.  	2
Given that this paper is application-focused and light on algorithmic contributions, I think there is should be a higher burden for it to be clear and readable. At the moment, I think the discussion about modeling is not clear enough. For this reason, I will recommend rejection, but will remain open-minded to the author rebuttal and the opinion of other reviewers.	2
Overall, I found this to be interesting in both its technical execution and presentation. However, at the moment, it reads more as a technical report than a scientific paper. Learning how to extend the proposed technical innovation to other systems or tasks could strengthen the contribution, in my opinion.	3
Although, I believe the proposed method has the potential for a good publication, I do not recommend the acceptance of the paper in the current form.	2
This is an overall well-executed paper, with good novelty and solid experiments. Some points should be clarified and stregnthened in the revision. 	4
Limited technical contribution, lack of evaluations against more defenses.	2
"### Strengths: 
- Tries to improve efficiency of adversarial attacks

### Weaknesses:
- the paper is missing some definition that should not be taken for granted
- the evaluation is not entirely convincing and might be unfair
- results should be presented better, as they are very difficult to read and understand"	2
Although the approach to learn an augmentation policy using the adversarial and similarity objectives is studied in the image domain, the proposed method contributes to the adaptation of learnable augmentation in the NLP domain and shows good experimental results. Overall, I tend to recommend for acceptance for the initial rating.	2
This paper proposed a simple and effective reward function for learning-based augmentation selection in NLP. Extensive experiments and analysis demonstrated its effectiveness on text classification and entailment, especially on low-resource and class-imbalanced regimes.	2
The paper applies existing differentiable data augmentation methods (Hataya et al., 2020) to text classification. Overall, the technical novelty is low. There are several additional experiments (in particular ablation studies) needed to justify the design choices and the performance gain. 	2
Overall, I vote for accepting. I think the paper proposes a very interesting approach to improve data augmentation in NLP tasks by learning a policy that decides how to combine different augmentation techniques in a task dependent manner to generate samples that are informative while retaining the semantic information from the original sample. Since this research area is relatively new, I think this paper provides a really good baseline, and a framework for other papers to improve upon, as its easy to implement and allows adding more augmentation techniques in the policy pool, improving the loss/regularizer functions, etc. I have a few minor issues regarding the paper, and I described them in detail below. Hopefully the authors can address my concerns in the rebuttal period.	4
"Built upon a recent study on generalizing EG for nonconvex-nonconcave setting, this paper provides a nice unified framework of EG using the projection onto a hyperplane. Although the improvement in terms of the convergence guarantee is not significant, this paper is theoretically inspiring, and the toyish experiments are informative, which I believe has potential for further research both theoretically and practically.
"	4
"The paper provides several contributions for different variants of the extragradient method, which in general are of interest to the community. 
It lacks stochastic counterpart convergence guarantees of the full-batch results, and it considers several variants of the method rather than focusing mainly on the method with best convergence properties and to consistently discuss its convergence in full-batch setting, then stochastic, and finally verifying these results and ideally discussing/showing if these are relevant for challenging real-world setups.
In summary, the paper is somewhat rushed, and although it is in a good and valuable direction, at this stage, it requires major modifications (see comments above) and additional stochastic counterpart results."	2
This paper provides approach to avoid lower bound of Weak MVI parameter and provide a lower bound perspective on the fundamental difference compared with existing literature.	3
Due to the aforementioned pros, I think this is an excellent paper and would like to see it accepted. Some points need to be clarified as aforementioned in the concerns and minor comments. 	3
"The paper uses a questionable notion of privacy. Also, key results and presentation of experiments is not clear.
"	1
This work studies the connection between OOD generalization, stable features and privacy (particularly robustness against membership attacks) through extensive experiments. It is mainly an empirical study, so technical contribution is limited. However, it provides evidence on the relationship between the three factors. A particularly interesting insight is that robustness against MI attacks can be an indicator of stable features. The main concern is that the experiments do not control the factor of the algorithms being used. 	2
This paper makes an extensive evaluation to show the connection between out-of-distribution (OOD) generalization and privacy. It is important for privacy community, but I'm not sure if it is complete without any theoretical claim. Can the authors provide a theoretical analysis to show the connection and support their claims?	3
The paper is well motivated. However, the empirical studies have mixed results (especially on the real-world datasets) while the explanations of the observerations are neither not in-depth or comprehensive.	2
This is a neat and well-thought-out idea. However, as I've expressed in the main review, some of the claims need further justification and the paper clarity needs to be improved. I am willing to update my score if the authors are able to provide a convincing response!	3
The paper tackles an important problem and is well and clearly written (but clarity can be improved). The proposed idea is valid and interesting. The experiment clearly shows the superiority of the proposed method in rapid memory read/wright, memory capacity, and de-noising, and generation, etc. The proposed method is also supported by a theoretic analysis on error bound.	3
"I believe that the theoretical contributions of the paper with respect to DKMs is good if perhaps a bit incremental, however, I found the empirical results questionable.
If the authors can address my questions I will consider raising my score.
"	2
Overall, the proposed work is an incremental step forward for survival analysis. While the impact may not be huge, I believe the proposed approach is a reasonable, novel view on the problem. The experimental results are not overwhelming, but they show that the proposed approach does succeed at clustering patients. This is key for precision medicine, so other researchers could also build on this formulation and approach.	3
This manuscript proposes an interesting latent mixture survival model that makes a useful contribution to the field.	3
The paper which proposed a novel survival clustering algorithm is well-written with thorough literature review, clear analysis assumptions, comprehensive experiment design and results comparison with state-of-art baselines models and closely related benchmark datasets, which is a good example for dedicated research work. 	3
An interesting work, well-described and with in-depth experiments. However, the technical contribution has to be described in greater details.	2
I am not convinced that this paper is contributive (against the bar of ICLR) to either the machine learning or numerical analysis communities. Hence I would oppose its acceptance.	1
The lack of comparison to prior work, the usage of non-standard metrics, and lack of novelty (beyond the general framing, which I found poorly motivated) make it impossible for me to recommend acceptance of this paper in the present form.	1
The idea is interesting, but the execution of the paper is poor. The experiments lack rigor, there is likely massive overfitting, and the baselines are incomplete and weak. The writing in the paper could also use some work.	2
"This paper uses a standard implementation of a cGAN on a standard benchmark task for physics prediction. There is no significant innovation, comparison to any of the many other ML approaches solving this problem, or interesting analysis and discussion.
I don't think this paper brings any new insights to the community."	1
"The idea is reasonable and moderately novel. The conducted experiments demonstrate the better performance of the proposed model in two different settings.
The paper misses some details when describing the proposed method, making readers difficult to fully understand its idea.
Although the experiments already include a lot of baselines, there are still some results to be included for fair comparison (simple data augmentation method)."	3
The proposed algorithm is simple and seems effective. However, some important details are missing and the novelty is not very clear.	2
The motivation (tackling the response generation diverging issue from human language) of this work is clear, and the solution is also good, by adding a critic on the top of existing pre-trained GPT-2 dialogue agent, and shows promising empirical results on the automatic evaluation of two datasets. However, the main concern for this work, is is quite incremental with limited novelty, and also lacks human evaluation to verify its effectiveness, may recommend for a workshop paper. 	1
"1. Good method to incorporate policy network and q network into a GPT-2 model to handle large scale natural language action space.
2. Have concern on offline q-value estimation and sampling of response candidates. "	3
"This paper has an interesting observation and the proposed method seems to be original. However, as mentioned before, it would have been better to have additional training setups and baselines to emphasize the efficacy of the proposed method. I would like to rate this paper to be borderline, but I am open to changing the score after the rebuttal.

=======================

After rebuttal:

After reading the authors' response, I think my concerns both on the compared baselines (FixMatch) and the evaluated datasets (ImageNet) are somewhat addressed. Therefore, I would like to increase the score to be 6."	4
I like this paper in general, but I have some concerns on the effectiveness of this method in large-scale benchmarks. Also using the expensive and time-consuming clustering method on large-scale benchmark like ImageNet may be of a big challenge to computational efficiency. 	3
"This paper is well presented and organized. The proposed ideas are interesting. However, there are several main issues as follows:
1. the motivation is not clear and strong. The authors state that "" imbalance of pseudo-labels harms the performance”. It lacks justifications for this statement. Fig 3 (b) shows that imbalanced pseudo-labels of OOD would result in many of data with class 1-5 into class 0. However, if the pseudo-labels of OOD is not imbalanced, it may still force the ID data into the incorrect class.
2. Fig 7 (c) shows that Υ-Model is sensitive to the Number of Extra Classes K. However, this paper does not provide a good strategy to choose K.  In the real case, we don’t know the actual number of OOD classes. 

3.  The Υ-Model may not work well in a large-scale dataset since the performance of the proposed SEC technical relies on the deep clustering algorithm. When the dataset becomes more complex or the number of OOD classes is large, then SEC may not work at all. It is better to show some results in large-scale datasets.
4. Can Υ-Model extend to other SSL methods? If not, it could be a limation of the proposed method. It is better to show some empirical results.
5. Since 100% Class Mismatch ratio is an extreme case, it is better to show a 50% ratio result for Fig 7 (c).

6. What is the purpose of Fig 2 (a)?
"	2
The paper investigates the issues of using pseudo-labels in class-mismatch semi-supervised learning, and shows the benefits of the proposed method empirically. It performs better than recent class-mismatch semi-supervised learning methods.	4
"
The paper unfortunately suffers from some unclear writing as well as some lengthy unimportant descriptions and some important aspects that are missing

PRO
* interesting domain with lots of problems
* main claim of language emergence from simple to complex is illustrated by one example. But this is such a general claim - it would require a lot more experiments
* the main claim of the paper is not really a technical claim 

CONS
* some key explanations are unclear.
* many less important explanations are lengthy and convoluted making it difficult to follow
* somewhat trivial domain (that is pretty common though in this line of research)
"	1
While the paper shows that curriculum learning can help compositional language emergence, which is interesting, I'd want to see more extensive experiments and ablations to recommend acceptance.	2
The authors propose a novel module that helps in training better compositional and symmetric language using a curriculum from simple to complex tasks. The paper is well-written and the experiments show that the proposed method indeed supports the hypothesis although the experiments are only performed on a simple environment. The complexity of the tasks can be varied along different axis but only a few options are explored to extend this.	4
The current version of the paper greatly overstates the novelty of its technical contributions (SM, refdis, task transfer). To me the primary contribution is therefore the synthesis of prior technical contributions, the strong experimental result, and the case for further research into symbolic representations. Only on the condition that the authors substantially revise their discussions of SM, refdis, and task transfer to credit prior work, I would recommend the paper for acceptance. 	2
"The paper presents a novel and motivated setting of task transfer and has the right start for experiments.
On the other hand, the explanation of symbolic mapping, central to linking the setting and experiments, is very unclear.
I intend to change my ""reject"" to an ""accept"" if the symbolic mapping is sufficiently clarified.
"	3
A plausible proposal for the promising direction of learning subword tokenization end-to-end. However, some important design and implementation details are missing or confusing which are not helped by the lack of source code. The extensive experimental results themselves would benefit the community. However, they don't seem to strongly support the advantage of the proposed module in learning subword representation either in terms of end-task accuracy or explainability. If the authors could clarify what they did exactly with their GBST module and thus their novelty, I would be inclined to accept for the merit of extensive evaluation results and one more alternative that lies between subword-level and plain-byte-level models.	3
The paper focuses on an interesting problem and has many experiments with state-of-the-art results in certain settings. My main concerns are regarding its novelty, framing, and evaluation.  Even though some of the results are competitive, the experiments do not provide enough evidence that the learned  tokenization is crucial for achieving the results. 	2
"To summarize,

Pro

1. Excellent paper writing.
2. Extensive experiments.
3. Simple but effective token-free module.

Con

1. Lack of experiment on different sizes of the model, especially performance when with a larger scale.
2. The comparison seems not between the best setting (deep encoder, shallow decoder) of both baseline and this model.

"	3
"This work first implements an end-to-end trainable byte-level (character) tokenizer, while limiting computational overhead via a learned token weighting. It is compatible with common Transformer stacks, but can reduce their width for computational gains, without sacrificing performance, and in some places gaining performance. The evaluation on multiple NLU and noisy NLP tasks demonstrates only slighly less to slightly stronger performance compared to byte and sub-word tokenization models. Ease of use, being conceivably language agnostic, training more efficiently and allowing easy reimplementation (pytorch), make this work a welcome contribution to the field.

Therefore, I recommend accepting this work."	4
This work is well-motivated. The overall design of the model is technically sound, and it provides moderate improvements on several datasets, while being memory efficient, faster, with fewer parameters. It is not clear if the advance of the method can generalize to other languages or larger models.	3
"My biggest concern is the ""imaginary"" agent freely collecting information, making decisions, and delivering those decisions to all the agents at both the training and execution times. Comparison against the chosen baselines is not fair, even when the chosen evaluation task is a simple enough one, in which SOP-CG's limited representational capacity would not appear that pronounced."	2
State-dependent coordination graph is important. However, the paper currently has several weaknesses as mentioned above. It seems clearly below the bar of ICLR.  	2
The work presented is indeed interesting and relevant to the real scenarios. I recommend that this paper could be accepted.	3
This work proposes two methods for improving the efficiency of large-scale deep learning. While the methods are conceptually very simple and thus compelling, the paper might benefit from a more thorough validation of claims about these methods, both in terms of compaing with prior work and more detailed empirical evaluation.	3
This paper proposes an empirical method for fast training of models with repeated layers. The M6-10T experiments warrant scalability. However, I believe a more comprehensive set of experiments is needed to demonstrate the efficacy of the proposed training paradigm. Please see detailed comments in the main review.	3
It is very impressive to be able to train a 10-trillion parameter model. However, the benefit of training such a large model is unknown, since critical evaluations are missing. Overall, it appears to me that the experiments on very large models are not ready, given the lack of a reasonably sized dataset or downstream evaluations. Thus, it is not clear how impactful the proposed method can be.	2
Although the problem is important and the proposed P2R approach is promising, the submission does not demonstrate the effectiveness of the proposal on the problem. Specifically, the paper lacks downstream task results of a 10T model trained with P2R. It is difficult to fairly judge this paper without these qualitative results, especially given the claims. 	3
The paper proposed an interesting idea to reduce training time for models with identically repeated layers. The empirical evidence provided in the experiments are not convincing enough to make a standard practice.  However, it's still worth a try for practitioners in the model pretraining community. 	2
"The problem of zero-shot transfer in time series forecasting is important but it is also necessary to define its scope. The method presented by the authors is novel but incremental. There are concerns regarding the reporting of results (see Weaknesses) which cast doubt on the model performance, which is already lower than NBEATS on some tasks. Overall the paper presents an interesting approach to performing meta-forecasting but it needs more detailed analysis in terms of its applicability as well as completeness in terms of reporting its results. I would be willing to consider an improvement in the score if these issues are addressed.  

**Post-rebuttal:** My concerns regarding scope and cherry-picking have been sufficiently addressed in the revised submission and the authors' comments. However, I am still concerned about the novelty of the work as well as the extent of ablations. I have increased my overall score to 5 (marginally below the acceptance threshold) and provided additional feedback to the authors in comments.
"	2
While the raw idea has promise, many corrections/clarifications need to be made before it is ready for publication -- specifically related to the terminology used, whether inputs between datasets can differ, and the complexity claims of their proposed layer. In addition, more suitable benchmarks are required to fully evaluate performance claims.	2
The method proposed is interesting (but somewhat incremental) and the problem is important. However, there are several issues that need to be addressed. The method typically doesn’t perform well compared to the state-of-the-art as shown in Table 1. It is also strange why different metrics are used for different datasets, for instance, of the four datasets used, there are 3 different metrics used in the evaluation ND for Electr/Traff, SMAPE for M3, and MAPE for Tourism. It would have been better to report all 3 metrics for each dataset (or just show one of the metrics, and the others in the appendix). It currently seems to be cherry picked a bit. Furthermore, the training time for each baseline and dataset is missing, please include a table like Table 1 but with the runtime for each baseline and dataset. Please see above for other points. Overall, the contribution and novelty of this work is limited.	2
Overall, the paper addresses an important problem but there are many concerns with the experiments and the formulation. 	3
"
The proposed method is novel and convincing.
The paper is well organized and clear.
Experiments shown that the proposed approach clearly overcome other methods.  
"	3
The paper follows a solid theoretical foundation using (scaled) isometry between input and latent space as a principle for designing a regularizer to learn meaningful representations with autoencoders. Experimental results show improvements over relevant baseline methods in an unsupervised retrieval task and as tradeoffs between reconstruction accuracy and surrogate metrics for how close the learned latent space is to an (scaled) isometry. These positive aspects, however, are overshadowed by key major issues: (i) results are not averaged over multiple random seeds, which makes it difficult to assess the stability of the regularizer in training; (ii) and missing discussion of conflicting requirements of the KL divergence term with the one introduced, making the method analysis seem too superficial.	2
To summarize, the authors address an important problem and propose a new method for learning a low-dimensional representation from high-dimensional data. They rely on the manifold assumption and propose a new regularized to a variational autoencoder. The results demonstrate that the method leads to improvement in retrieval tasks. However, the method has some limitations (namely the low dimensional embedding dimension), and the experiments are not backed up by a rigorous scheme for hyperparameter tuning. For these reasons and the marginal improvements demonstrated in figures 1-3, I recommend a weak rejection of the paper.	3
A well-written manuscript with a practical implementation of an isometric representation learning pipeline, which is nonetheless counterbalanced by unclear motivations and not very impressive results. Might still have an impact on a certain community, but a potentially broader impact is not clear at this stage.	3
A very nice paper, both well justified and nicely outlined. I can't see any weaknesses.	4
"I lean to accept this paper. I think they present a solid contribution to a relevant problem. Even though, there are some limitations that should be discussed by the authors. 
"	3
This paper introduces an interesting problem, studies it well and comes up with a first algorithm. However, I have main concern about the usability of this algorithm, which only adjusts the location of the posterior while leaving its shape untouched. I am worried that under this restriction, it may be impossible to achieve the intended goal, namely forgetting part of the data. Furthermore, I think the paper should have discussed these issues with the proposed approach.	3
"The problem of removing the effect of some training examples on a trained model is well motivated by the paper.  In addition, the simplicity and the effectiveness of the proposed method should be contributory to the community.

At the same time, the reviewer is not able to verify the proofs of the theorems in 4.3 and 4.4. "	4
The paper is nicely written and provides an interesting alternative to analysing a linear MDP with an actor critic setup. I have some doubts about how much insight can be gained from this analysis for future work, especially algorithmically and therefore I am currently advocating for a weak accept. 	3
A highly theoretical paper underlying an interesting result on the bias of actor critic toward high entropy policies. The proofs are long and technical and assessing their correctness is behind my reach. But my bet is that it is a good and correct paper.	4
"I think the paper makes reasonable contribution but the presentation of the proofs can be improved.

==============

The authors successfully addressed my concerns and I increased my score accordingly."	4
"I recommend to accept the paper.  

It is a good addition to the set of methods on an important topic - OOD. The approach is novel, reasonably principled and the code has been made publicly available at [https://github.com/igeood/Igeood](https://github.com/igeood/Igeood) which is good for the community to able to put the above methods to test.  The validation is comprehensive though the results are somewhat inconclusive though promising. There are some concerns regarding the validation of assumptions and the experimental evaluation. My preliminary assessment is that the submission passes the criteria for acceptance to this venue. "	3
This paper presents a novel idea of using Fisher-Rao distance for confidence scoring in OOD detection. The main idea is interesting, the paper is well structured in terms of methodological description and the problem/experimental setups, but the empirical results do not appear to be sufficient to validate the intended purpose of this study. 	3
In summary, I recommend a weak accept for this paper. The empirical results are good, and the method is generally novel. I am open to increasing my score, if the authors address the concerns I raised in my review.	3
The paper proposed a new OOD detection framework, IGEGOOD. My biggest concern is that the experiment results are fairly weak given the complex procedure of obtaining the detection scores. Therefore, I am leaning towards the rejection of the paper.	3
The paper proposes to utilize TV distance in the task of fair representation learning. The empirical experiments suggest a strong performance. It would be very helpful if the authors can kindly comment on the questions in the `Main Review`, so that the significance of the results can be better appreciated.	2
This paper is not ready to go under review. I think this paper can be further improved in terms of writing. Once, it is written at an academic level it can go under review. I would suggest the authors improve the presentation of their paper. 	2
"The topic handled in this paper is important, and the contribution is interesting. There are some remaining questions I have regarding the paper in general (listed in my main review above). Given that there are some points the authors have to clarify further, concerning their method as well as related work, I believe a score of 5 is correct at this stage. 

However, I do believe the points I raise can be handled during a rebuttal period. Hence, I look forward to discussing more with other reviewers and the authors of this work. "	3
The definition of fairness adopted in this paper needs further discussion.	2
The paper presents an interesting method and good supporting experiments. I would like to see the few points of clarification I raised above addressed in the rebuttal.	3
"While the method is clear and has an interesting premise, I am left wondering if the rationale behind the methodology is the right one. Experimental evidence is not very strong, and there are some clear aspects lacking in the evaluation. The most important issues are the lack of proper validation on downstream tasks, and the lack of variety of student architectures. (the score of 2 on ""Empirical Novelty And Significance"" is due to lack of completeness of experiments but can be raised with stronger evidence).

(updated to 6 after rebuttal)"	3
"- The paper is well-constructed with clear motivation and comprehensive quantitative experiments. 
- Experiments show that the proposed BINGO learning schema is valid and bring incremental improvement over previous SEED and Compress. 
- My main concern is the technical contribution of BINGO somehow overlaps with [1] as previously mentioned. And some experimental settings are not consistent or clearly explained. 
I'd like to adjust my initial ratings after reading the response afterwards."	3
The paper is well written and easy to follow. As I have mentioned, I'm a bit concerned about the main result. Interestingly, I have reviewed this paper in another venue, but my questions are still not addressed well. However, I would like to increase my score if they can be handled well here. 	4
"If it were a purely theory paper, I am not sure that the idea of leveraging the low-rank structure of the GGN makes for a novel enough contribution by itself to be published at ICLR.

It if were a pure implementation paper, I think ICLR would not be the most appropriate venue.

Hence my low rating, for a paper that is otherwise very pleasant to read.

I think that the proposed adaptive damping scheme can however make for a nice follow-up."	1
The proposed curvature approximation method is novel and practical and appears to have the potential of being useful to future work. Though there are a few things to be improved, in general I find this work well written, and my current recommendation is marginally above the acceptance threshold.	3
This paper proposes to use the GGN’s low-rank structure for fast and sound model learning. The empirical performance in promising. However, the technical novelty of using low-rankness of GGN's seems limited and the technical significance seems not sufficiently strong. 	2
The paper relies heavily on different forms of approximations to make the technique feasible, however  discussion on the accuracy of the approximated quantities is lacking, therefore it is difficult to judge how useful the obtained quantities are. Moreover, the scalability of the method to contemporary, practical architectures and datasets is not well-supported, thus the significance of the results is limited. Therefore, I recommend rejection of the paper in its current form. However, I believe that the paper has merit and some interesting contributions, and I am open to increasing my score if the authors address my concerns.	2
Overall I think the technique is interesting, but I also feel the overall contribution of the submission is somewhat limited. 	2
"The proposed stability regularization has nice theoretical foundation and shows good performance across different problems involving discrete stochastic variables. But I am lean to reject mainly due to the following reasons: 1) the lack of theoretical foundation for the extension to the categorical variable, 2) the insufficient quantitative results supporting the ""hassle-free"" characteristic of the proposed method."	3
A very nice idea, probably a bit expansive on the computational side.	3
Overall this is a nice contribution to what is an important problem, learning discrete (stochastic) variables. The idea is simple and the authors demonstrate it in a variety of datasets. The only thing that I would have liked to see is a comparison with other methods that are meant to also work in such discrete settings, e.g. straight-through, score-based estimators. 	3
"In summary, the paper proposes an extension to LISTA, called PALM, which unrolls an alternating minimization to solve a bilinear inverse problem, i.e. sparse coding. The experiments indicate that the proposed approach is an improvement over conventional alternating minimization, but more experimental work is needed as well as proper contextualization with other unrolled methods related to unrolled blind deconvolution, dictionary learning, and other bilinear inverse problems.
"	3
"The paper proposes an interesting method with promising results, but there are several problems in the presentation of the algorithm, its motivation, and the numerical results. The main issue is a lack of clarity, which undermines the results presented. As such, I do not recommend that this paper be accepted for publication.
"	2
"
1) The reason I gave a 2/4 in ""Correctness"" is that, while you do solve the sensitivity problem w.r.t. PALM's hyperparmeters, you introduce a slew of new hyperparameters associated with training. So the real question is: How does the sensitivity to those new hyperparameters compare? (I'm not saying you're incorrect, but saying it's not well-supported that the sensitivity problem is solved; you even mention that learning rate schedule was a ""difficult"" hyperparameter to set in Sec 3.3)
2) In terms of technical novelty or significance, I think this is standard unrolling. The thing I have not seen before is incorporating priors for both A* and S* both into the loss function.
3)  To be clear, unrolling is fully understood to give improvements over a non-learned counterpart (although it is good to rehash it in specificity in Sec 3.1,3.2, they are not sufficient for publication in ML). So when it comes to Section 3.2, the main problem I have is that it seems the other methods were trained very differently. My main takeaway is that the overall procedure gives an improvement, which is good (or even great), but in terms of the significance I have to defer to someone who knows about this application.

Edit: I want to thank the authors for giving thorough feedback to each comment and adding/updating the appendix comprehensively. I am satisfied with the responses and have raised my score accordingly. I see now that the learning-rate-schedule difficulty is a comment on the other methods."	3
"The paper provides an unrolling version of the well-known PALM algorithm with application to blind source separation for unmixing of hyperspectral astronomical data. The authors provide empirical results that show the advantages of the proposed approach over other relevant state-of-the-art unrolling algorithms. Given the existing state-of-the-art algorithms the technical  novelty of the current work is limited. Yet, the work presented could be of practical interest when it comes to the BSS application that is studied. However, the result are not always convincing (see Weaknesses) and could be significantly improved. The paper also lacks any theoretical justification and the evaluation of the method in only through the empirical results provided.  The paper could be also better organized since I feel the authors use a lot of subsections (especially in the experimental part), which disrupt the flow of the paper. 

------------------------------------------
Post-rebuttal update

The authors have addressed my comments and concerns adding further experiments and re-organizing the paper. I, therefore, raise my score."	2
Reasonably good paper. There are few potential improvements for evaluation procedure.	3
"Overall the paper is well motivated, and the problem statement is clear. However the technical novelty is limited and more baseliens on improving adversarial transferability need to be compared with.
It would be interesting to see if the proposed method can boost the adversarial transferability for different tasks such as object detections as well.
"	2
Overall, the paper proposes a novel adversarial attack, which is effective and easy to follow. I think the paper is marginally above the acceptance threshold, although there are some weaknesses.	4
I think this is an Okay submission, but several technical and experimental questions remain. 	2
I think the work proposes several interesting ideas; these ideas may be based on prior literature and may seem straightforward in hindsight, but I think they involved creative thinking to propose in the first place. Although I am not convinced by the computational efficiency of the proposed UCB algorithm and think a more computationally efficient baseline is missing from the comparisons, I think the idea of taking a multi-armed bandits approach to model interpretation is still clever, and the UCB algorithm could likely be extended/modified to leverage a more computationally efficient way of evaluating interactions. I also think the ParaACE approach to compressing models and combatting overfitting is a novel take on how feature interactions can be made useful (prior work has been heavily focused only on interpretability). However, at least for model compression, I think there is at least one baseline method worth including. Overall, I rate the paper above the acceptance threshold as-is.	3
Overall this is a very nice paper. Identification of interaction terms can lead to very novel analysis of the black-box models and can even lead to knowledge discovery e.g. discovering drug-drug interactions. The paper also seems to have 2 strong contributions where the proposed ParACE model leads to a very novel compressible architecture. The paper can be improved upon by addressing some of the aspects but even in its current state it will be of interest to the larger AI/ML community. 	3
"The authors propose a more efficient method for hessian matrix estimation with little novelty and not comprehensive experimental results.
If experimental issues are resolved with experiments on real datasets of high dimension and comparison with a proper baseline, then there is a change for the acceptance."	3
This paper is recommended for acceptance at ICLR. The paper is thoroughly written with sound technical backing - especially the code sharing and the content will help in improving the state of art in recent advances in explainable deep learning approaches for others to build on. The applicability in practical use needs more justification.	4
Overall, I think the paper is well-written and the benchmark is valuble as testbed for uncertainty estimation. But the uncertainty measurement part lacks some important uncertainty estimation methods, which are highly encouraged to be included in the paper.	2
New empirical benchmark with reproducible software but with unclear research novelty and value to community. 	2
In summary, this study has a large number of design flaws. However, given the benchmark, the authors have done a great job in thoroughly benchmarking existing algorithms.	1
"With the above strengths and weaknesses, I tend to give ""marginally below the acceptance threshold"". But I'm open change my rating after seeing the rebuttal from the authors to clarify my concerns."	3
"I think this paper deserves credit for training a superior performance agent by designing a multi-stage learning framework combining several techniques. However, this paper has some flaws:
- The proposed HPPO algorithm may suffer from high variance, and it is better to compare it with other methods.
- The relationship between strategy style and the target area is unclear. 
- Some methods in Stage 3 are unclear.
- The experimental setup in evaluating strategy diversity is unfair.
- The paper should have more experiments to evaluate the strategic diversity of the agent."	2
The paper improves the performance over the previously top-ranked solutions of the ViZDoom competition. Their approach consists of a combination of known technics well-engineered together. As such, the work lacks originality and novel contributions. In addition, many pieces of information that would enable a complete understanding of the method and the reproducibility of the results are missing. Consequently, I consider that the current version of the work does not meet the required standard for acceptance.	1
"
Strengths: 

Experiments are well-designed, and ablation studies are also presented. Visualizations make the paper much more interesting and understandable. This paper shows that instead of using larger NN, assembling existing techniques are also able to achieve SOTA performance. 

 

Weaknesses: 

Although the network is trained end-to-end, but they do use some hand-crafted domain specific rules in stage 2 and stage 3. Also, in this multi-stage learning system, each stage is manually selected/formed.  
Only a single game is used for empirical validation, the proposed method is quite complex, consisting of the combination of a number of existing methods,  the clarity of the contributed method is not sufficient, and no explanation is provided to explain how the method works and why the performance is better.
Overall the work is somewhat interesting, but not significant enough to warrant publication. Reproducibility is problematic."	2
The paper is well-motivated and interesting. The provided guidelines are practical. But I am concerned about the generalization of the proposed metrics in other MARL environments.	3
"
--- motivation --- 
I’m not convinced that better understanding roles will lead to improvements in multiagent coordination. In fact, this is not really the investigation the paper conducts: The paper demonstrates there are certain correlations between their role definitions and coordination performance, but not that this understanding results in coordination improvements. What concrete challenges currently exist in multiagent optimization --- e.g. what do the authors mean by `roles can avoid the issue of “policy degradation”` in the abstract? --- and how can these challenges be addressed by understanding how groups/teams can be factorized into roles? 

I’m confused why the authors break down MARL cooperative algorithms by “parameter sharing”, “communication” and “credit assignment”. These are components that don’t seem directly comparable to each other, and it’s not clear how roles are related to the components. Parameter sharing is a technical means to learning policies, communication is an assumption by which we allow additional agent interaction / information passing, and credit assignment is a general challenge when optimizing in long-horizon settings for reinforcement learning. 

The authors directly propose their three different role definitions in the second to last paragraph of the introduction, but these alternative definitions need more context and motivation. It’s not clear what challenges they address and why these definitions are ones we should use for characterizing multiagent behavior. Additionally, without context or grounding in prior work, it’s unclear what “policy-based,...” roles should even refer to. 


--- method --- 

How does this proposal show that agents take on the same role? If I’m finding a good position just like all other agents, shouldn’t this indicate that we have the same role? This doesn’t seem reflected in this metric. The policy-based role also seems to have a lot of overlap with the trajectory-based role --- in some sense, the trajectory based role feels quite redundant: If I can infer what role the agent has, I should be able to have a good generative model on what the agents’ most likely trajectories should look like. 

In Equation 1, a sum over the policy conditioned on an action is an unnormalized probability distribution. It’s not clear to me how this is being used in the KL metric. 

In Equation 2, the normalization should happen over A^2, and not A. 

It seems weird that the trajectory-based role is defined as the observation overlap. Observation overlap seems to be a measurement of … pairwise observability (i.e. how much information do I have about what you see, and you about what I see?), and less about roles. 

I disagree with the authors on their interpretation of Q values on “contribution-based roles”: in the setting where agents are performing individual policy optimization, I don’t see how Q values [optimized to reflect the agents’ expected return, and not a ratio between the outcomes of an agent’s actions and all other teammates] should not be interpreted as the agent’s contribution to the team. If there’s a misunderstanding or some related work that asserts this, the authors should elaborate. 

--- theoretical analysis --- 

The notation is not self-contained in this section, e.g. “Please refer to Sec. B.1 for the detailed definitions”. 

The second term in the error is the irreducible error after we find a best fit for relating the optimal joint Q values and the concatenation of independent Q values. How do we go from this line to line (5)? On a related note, how are we going from line (5) to (6)? 

“The first term on the RHS of (5)...” I don’t see how this difference is related to the contribution based role which is based on an agent’s Q values. How can this be related to the linear weights? 

“The first term on the RHS of (6)...” Minimizing the bias results in minimizing the policy-based metric, which intuitively leads to agents _not_ aligning their actions. Does it make sense to think about this minimization as reducing the error we incur when approximating a joint policy with independent policies? This seems contradictory to what the authors want to propose in their methods section. The bias term should also normalized over the number of samples too. 


--- claims --- 
“These definitions are intuitive and cannot accurately describe the role difference.” Why not? What is insufficient with their definitions? It would be nice if these definitions could also be used in the experiments to compare and contrast the success/failure modes.

“As common sense would indicate, actions taken at the same time step can indicate different roles” - Do actions do this? This is arguable. Actions can be considered coordinated or uncoordinated, joint vs. independent. They can indicate different or shared roles.

--- figure ---
Figure 1 is quite confusing for the following reasons 
> What is the blue vs. green color schema supposed to refer to? 
> I don’t understand what MARL alg 1 and 2 should represent. I thought the agents are trained using the same algorithm? 
> Related to my comment on “why the authors break down MARL cooperative algorithms by ... ”, I don’t understand why these axes are being used as input parameters for the algorithm in Fig 1a, and then as outputs from Role Diversity.
If the point is that role diversity should inform our assumptions in developing algorithms, then I’d recommend rephrasing the figure caption, omitting the performance graphs, and improving the figure pipeline. The figure doesn’t elucidate the authors claim that “role diversity [avoids] possible bottleneck of a MARL algorithm … ” What is this bottleneck? 

Figure 2 onwards refer to the scenarios with hard-to-parse names, eg. “1s1m1h1M_vs_5z”. These figure names should be updated for clarity.



--- writing nits ---
Introduction
> “lack the study of the question of why” - too wordy and awk. 
> “In other word” → “In ”
> “Current researches are more focusing”
> “The reason maybe:” - informal. 

Related works
> “adopt q-learning” - note capitalization
> “In this study, we contended that” - note tense 

"	2
This paper is timely and interesting, but there are still some concerns, especially the the first one in the Main Review. The reviewer thinks this paper should be further improved.	3
For the stated reasons, the proposed diversity metric does not manage to convince me of its potential usefulness, although I still believe that the research direction is promising.	2
This paper provides some insights to the design of a three-part network for fine-grained vision tasks. A concrete design is presented and performance gains are demonstrated in the object detection task. However, the experiments are not comprehensive, lacking the evaluation on a relevant task and necessary ablation. The writing of the paper also has room for improvement. 	2
"The limited novelty is my main concern to accept this paper, and the module design is too heuristic. I recommend ""Reject"" as the pre-rebuttal score.
"	2
"The paper is well-written, easy to follow, and the literature review is adequate. The proposed method is well-motivated in the introduction.  Figures describing the proposed method help in better visualization.  Of the two main claims of the paper, one is empirically well-supported, while the other lacks convincing evidence (see main review above). Experimental studies compare the proposed TPN to existing baselines like PANet and FPN, among others, on the COCO dataset. Additionally, the paper  demonstrates empirically that TPN underperforms compared to state-of-the-art transformer based architectures like DETR and Deformable DETR, and in future work, the authors intend to incorporate transformer based processing in the proposed TPN core to further enhance detection performance. The paper may be improved if the cons (see above) are addressed. 
"	3
Although TPN shows some interesting empirical results and solid improvement over existing works on smaller networks, it is not technically interesting, and the novelty is limited.	2
"-----
**Post-rebuttal comments**
The authors have addressed my concerns and engaged with other reviewers concerns. Their rebuttal includes several complementary analysis and experiments which helps to better validate the quality of the approach. Even though the proposed approach is straightforward, the idea behind RevIN is technically sound and the thorough experimental evaluation is conclusive. Thus, I maintain my rate in favor of an acceptance. 
------

While instance normalization is a simple and well-known method in the distribution shift literature, its application for time-series forecasting is sound and empirically validated in this paper. Overall, I tend to vote for accepting but authors should provide complementary metrics raised in weaknesses to further analyze the effect of their approach."	2
"My main hesitance to accept the paper is the lack of novelty - as sliding window normalization is already commonly used, and the method is a very minor change. I feel this could possibly be improved with the ablation study comparing the results of the proposed method without the learnable parameters gamma and beta.  

I would also like to see more analyses and discussion around the impact of the window length used, and fixes / additional details around method and experiment details.   It would also be best to see the performance on a larger variety of datasets - in case the method is only beneficial for the type of data used here - and there are more datasets from the base method papers that could be used for this, that are left out here.  Additionally it would also be useful to see the results of a hybrid approach with the baselines as a commonly used alternative to the proposed dynamic normalization approach (e.g., fit a linear AR model + the baseline model).

Overall, I feel the paper provides an extensive study showcasing the benefit of the proposed normalization approach / sliding window normalization so it also provides value to the community even if lack of novelty - and highlights this approach to the community as an alternative to the global normalization that has become the standard in recent work in ML-based forecasting.  The method is simple and effective, and this brings attention to a potentially useful tool to have in the forecasting toolbox.  


***Update:***
I feel the authors adequately addressed most of my comments, and provided a lot of useful follow-on study and analyses and additional experiments which are much appreciated and really provide a lot more useful information.  Some of it is useful even for providing more info. on the performance of the base models such as varying the input window length to see the impact on the forecast errors.  It would have been good to see even smaller input window lengths however, as I would expect a behavior where at some point if the input window is too small the method is not beneficial, and if the window becomes too big, it should not be much different from the baselines.

Overall, I feel the set of results are impressive and useful.  I agree with the authors that it's useful to bring the approach to the attention of the community (as there are some similar approaches used in some communities but not well known or used in this community), and also the motivation and perspective comes from very different places compared to the similar approaches, and the idea of how it affects the data distribution and showing results along that direction is also new compared to the prior similar approaches.  It provides a widely applicable and effective method, with thorough experiment results and study.  I also feel it could motivate future work in a direction not too well considered before.  Therefore I think it is worthy of acceptance despite the actual algorithmic approach contributed itself being incremental and simple, and given the responses and revision, decide to change my rating.

 
"	2
The paper is easy to follow as the main idea is straightforward. The technical contribution however seems ad-hoc and incremental. A proof for a simple case would make the paper more convincing.	2
The analysis of the logit Hessian spectrum, in itself, is very interesting. But other than that, most of the other contributions are not so far-fetched or novel, the claim about implicit regularization is not motivated theoretically, many of the empirical aspects are unexplained and questionable, the approach is very specific to cross-entropy loss and thus likely not a general explanation if so. As a result, overall it is unclear if their offered explanations are conclusive and not speculative.	2
The paper is poorly written and too informal in a number of places. There are many claims which are presented as facts but are in reality hypotheses. I have expected an analysis based on the Hessian of a neural network and it turns out that the work focuses only on the Hessian matrix of the exponential family model, completely ignoring the network parameters up to the sofmax operator. I also fail to grasp the relevance of the Jacobian regularization and do not see why the drop in the magnitude of Jacobian norm would be of interest. 	1
This paper provides a new view that connects three notions in optimization and generalization (flat minima, oscillation, gradient regularization) with theoretical and empirical supports. However, the current manuscript lacks clear descriptions; without these, I cannot evaluate the correctness. Furthermore, the proposed method (Jacobian regularization) is not significantly evaluated. Also, the motivation of using the proposed decomposition is not grounded. 	3
"The paper proposes to study an approximate Hessian operator of the loss function. This is helpful in understanding the fundamental non-convexity questions in learning. 

A more analytical example is needed to improve the current paper. The example should address how the simplified Hessian matrix depends on the neural network functions and architectures. "	2
Authors have done a great job identifying a critical missing piece in the existing ML methods for CVRP. However, the proposed method relies on heavy heuristic engineering to enforce the constraint, and authors' modeling contributions seem orthogonal to the main problem (bounded fleet size) they are solving.	2
The paper propose incremental improvements to improve the work of Kaempfer and Wolf and adapt it to tackle the CVRP with bounded fleet size problem. The evaluation of this works needs improvement to trully demonstrate the value of each of these improvements.	2
"I currently am recommending a borderline rejection for this paper. I believe the work is important since it is the first to propose a deep learning solution for the VRP with fixed fleet size. However, I have concerns related to the underlying motivation for the proposed supervised deep learning approach, the validity of a few of the claims and contributions, and have made significant suggestions to strengthen the experiments section. Altogether, this makes me feel the paper is not yet ready for publication. I welcome a response from the authors, particularly pertaining to my concerns related to the motivation and contributions.

================================================================================================

Update after rebuttal: While some of the minor concerns I raised were addressed by the paper revisions, the major concerns about the use of SL, the claim of superiority of the proposed end-to-end SL approach compared to RL approaches (particularly with regards to the efficiency experiment shown in Table 3), and the lack of clarity with respect to the precise technical contributions were not addressed. Therefore, I am maintaining my initial score of 5."	2
"My decision is weak reject. 

The paper proposes a supervised learning algorithm to solve CVRPs with fleet size constraints and vehicle costs. The authors design a novel permutation invariant network to deal with the combinatorial structure. The experiment results show that the model has comparable performance for standard CVRP and outperforms other learning-based methods accounting for fixed vehicle costs.

However, my major concern is the generalization ability of this model. For large-scale CVRP (for example N > 500), simple google’s OR-Tools usually cannot provide near-optimal solutions, while LKH3 is very time-consuming. Therefore, generating a training set will be a big problem. I would change my mind if authors could come up with some ideas to improve or solve this problem. 

"	3
This paper derives a family of top-k cross entropy losses which is a novel practice. The experimental analysis on ImageNet including the impact of the distribution and ranking set size m, etc, is concrete and sufficient.	3
This paper proposes a flexible loss function for top-k classification, providing useful insights for image classification. So it is worth of reading for the researchers in this area.	3
 Although the paper brings several novel perspectives, there remain several ambiguities as well. Some additional experiments, clarifications can also strengthen the draft. Overall, in the current form, the paper is a borderline one and the final decision will depend a lot on the discussion during the rebuttal phase. 	3
"An interesting attempt to improve the top-K classification but consistent limitations:  
(I) an incremental contribution and no clear justification of considering k as a random variable
(ii) no significant improvement of the proposed loss over cross-entropy

"	2
"I do not believe the paper is a good fit for ICLR for the following reasons:

1. The authors' model is highly stylized and there is no connection to the type of applications and/or theoretical questions that are relevant to ICLR.

1. The paper suffers from imprecise writing, to the extent that the main theorems are impossible to parse.

1. The authors make no attempt to explain their results and the connections of the required conditions to the underlying game.

This paper could be a good fit to a control theory conference like CDC, but not ICLR.

---------------
Post-rebuttal
---------------
I have read the answers of the authors.  Unfortunately, I am not convinced and I will retain my original score."	2
Application of concepts of control theory in learning the structure of a game with linear-quadratic payoff may be interesting, especially in the case of partially observable actions, however the setting of this paper seems specific without being very well motivated, there are some technical errors and not well supported arguments and the paper reads somewhat incoherent. 	2
Seems like a very nice paper, but not accessible enough for me to understand all the details.	3
The proposed method seems to have some nice benefits, but I feel there are a few weaknesses that should be addressed. I also have a few questions and it would be helpful if the authors can take a look at the previous section (main review).	3
Overall, this paper uses a novel idea to improve the state of the art for semi-supervised training.	4
This is an interesting paper from technique perspective. But it definitely needs more empirical studies to demonstrate practical value. 	3
Interesting idea, poor experiments and confusing derivation.	2
Although the empirical setup is well designed and the paper is easy to follow, the lack of novelty in the contribution and the apparent lack of clarity in the explanations/analysis led to my non-acceptance for this paper at this point. 	2
I would recommend rejection since the authors only investigate few-shot image classification tasks. More analysis and experiments are needed for such an analysis paper.	2
The paper provides an extensive set of empirical evidence to demonstrate that task diversity (during training) is not beneficial for meta-learning. The insights and conclusions drawn from these empirical experiments, however, are not so convincing and helpful, i.e., it does not tell other researchers how to rectify the problem or how to design better meta-learning algorithms.	2
Although the paper is somewhat difficult to read (see suggestions), it provides a useful new approach to single-neuron probing and summarizes some weaknesses in recent existing work.	3
"This paper identifies issues with existing methods for ranking neurons containing linguistic attributes and proposes an effective solution.
"	3
"This paper is an in-depth analysis with new perspectives on how we should think about neuron ranking. It does not discuss much the difference between their philosophy towards the purpose of neuron ranking, compared to the implicit philosophy in the prior works which they compare. Overall it seems valuable but may not be entirely fair to the existing literature.
"	3
"Overall I found the paper to be quite illuminating on highlighting the different aspects of probing with individual neurons. However, the paper requires more polish in presenting the results and discussions of their work. I believe the finding is important for the community, especially the intervention analysis. However, I'm a bit skeptical of the claims (as highlighted in my review). Improving the writing of the paper and performing an additional model experiment could be useful to make the paper strong for acceptance.

Edit: Following the rebuttal, I'm increasing my original score of 5 to 8
"	3
   	3
The paper is well written and addresses an important direction is handling more complex representations of knowledge graphs. The work seems solid however lacks some aspects of evaluation. It is unable to justify that it is backward compatible to the original datasets which would be of great value even though not being at par with the state of the art approaches. In other words, it is ok to show drop in performance on Freebase datasets but claim generalizability over both traditional representations and KGs with reification. 	3
"The current version of this paper is marginally below the acceptance threshold. I believe some improvements can be made to strengthen the paper.

"	2
This paper propose a method to answer such queries and demonstrate in their experiments that qualifiers improve QA on a diverse set of query patterns. The logic is clear. The argument is complete and the grammatical expression is very standard. However, in the process of demonstration, there is a lack of description of the challenge and thinking about how to solve the problem. I think the authors needs to improve the writing.	2
"I think this is a solid piece of work extending the problem space solved by modern embedding techniques. The presentation is mostly solid, and easy to read and understand. There is some very minor presentation issues that can be improved such as adding the best hyperparameters in the main paper, or adding the missing node color in the legend in Figure 4 or maybe using ""Oracle-"" instead of just ""Oracle"" which can be a bit misleading. But otherwise, this paper is well written with a clear goal and contributions, and evaluations. Since in case of limited set of qualifier values, the edge+qualifiers can be easily represented and embedded as altogether, it would be interesting to see the comparison with such a baseline. But I don't think this is necessary to show the value of this paper."	3
Overall, the paper proposes to use hypersolvers to speed up ODE solving for numerical control. The idea is promising but it needs more experiment results comparisons, especially in terms of runtime, to justify the efficacy of using hypersolvers for optimal control. Also, the paper could consider expanding the explanation on how the optimal control problem (2) is solved in their experiments. 	3
I thought this paper was a sound, enjoyable, and clear extension of the previous work on hypersolvers to the setting of controlled dynamical systems. However, providing experiments on larger-dimensional systems would in my view significantly strengthen the paper by more fully demonstrating the potential benefits of the proposed method. That said, I am marginally inclined to accept the paper nonetheless rather than rejecting it solely on that basis.	2
The application of hypersolves to optimal control is somewhat incremental and not of great relevance to the ICLR community. I woudl be willing to change my score if the reviewers and AC disagree. 	2
"Although the paper is clearly written and many experiments are presented, the paper does not meet the bar of the top conference such as ICLR because of it being a very direct application of a previous paper.


My review is rather short for this paper because based based on the lack of novelty of this paper, I do not have many questions to ask or suggestions to make."	1
I think that sufficiently many of my previous concerns have been addressed, and I am now leaning on the side of acceptance. The authors have presented a useful extension of Barlow Twins into the graph domain, and now have experiments in support of the industrial relevance of their method. The novelty is somewhat limited (as is the case for most of the recent graph SSL papers that adapt image domain techniques) but it is useful in and of itself that the gains observed in images transfer well to the irregular domains.	3
This paper adapted the recent Barlow Twins to self-supervised graph representation learning and provided some informative empirical experiment results. With such interesting trials, the reviewer expected to see the concerns are well addressed.	2
Overall a solid piece of work. Results are not surprising but are reasonable. 	3
Overall I do not think this paper made sufficient novel contributions to the field of WDRO and FL. Several theoretical results are already well-known by the WDRO community, and the proposed algorithm is not very different from FedAvg. The numerical experiments also do not look very convincing. 	1
In general, the paper is well-written. It provides a comprehensive study of the proposed framework. However, in my humble view, the motivation of the paper is not well-supported, and the comparison against existing methods is insufficient. Therefore, I rate this paper as a marginal paper. 	3
I think this paper continues an interesting line of research. The goal and the motivation of the paper, and the connection to the existing prior work could be made more clear, but once those are addressed I am willing to change my score. 	3
The paper gives an in-depth theoretical analysis of neural networks. But some of its claims are not well-supported or well explained. The writing of the paper is ok, but the prerequisites of the paper can be more comprehensive.	2
The problem of this paper deal with is important, and the results show the effectiveness of their proposed method.	3
The idea of using deep linearly gated networks (DLGN) with dual view looks novel and interesting. I think this paper should be above the acceptance threshold.	3
This paper proposes a modification to the transformer's attention mechanism, which shows improvement on multiple evaluation benchmarks of various types. I find this proposal to be convincingly useful, although evaluation on more o.o.d splits and experiments with non-shared parameters could have made results stronger.	3
The paper focuses on improving parameter redundancy and generalizability in multi-head attention. The compositional attention is well motivated to mitigate these issues. The paper is well written and has shown improvement of compositional attention. That said, it's lacking analysis of the constraints on computation overhead of compositional attention on more real-world tasks under both sufficient data and low data scenario. It's a bit unclear of the rule-of-thumb to replace the multi-head attention with compositional attention.	3
"The paper provides a somewhat novel insight into the shortcomings of standard multi-head attention, and also proposed a compositional attention mechanism. However, there are still some concerns about the evaluation.
"	3
"The paper presents an interesting idea, based on the promising property of compositionality, to increase the power of a single MHA computation and a resulting transformer architecture.  However, I am not quite convinced by the baselines and the analysis of the computational complexity.  I am happy to raise my score if my above concerns are addressed.


### Update after author comments

Thank you for the clear, detailed response and the additional experiments.  You have addressed my concerns, and I will raise my score.  Please consider including the results of the additional experiments and the related discussion of compositionality vs depth in the paper itself, even if just in an appendix.    "	3
"I think this is a nice paper, suggesting the logic program provides strong background data for the network and that the gradient based search can  provide good prob  estimates.

The one flaw in this paper is that you only compare with DeepProblog, You drop NeurASP after the first table In general. it is hard for me to fully accept your claims :("	3
A new ASP-based deep probabilistic programming language. Using generative probabilistic circuits allow to cope with missing data. This seems to be a novel approach and SOTA results are achieved. I think the paper can be safely accepted.	3
"- Unifying PCs with neuro-symbolic approaches is an interesting research direction
- The technical description of SLASH could be improved, possibly by shortening the introduction and motivation for neuro-symbolic approaches
- The preliminary results are promising although the empirical evaluation could be improved. Most importantly, the experiments and the following discussion seem to oversell the SLASH framework rather than rigorously investigating the use of PCs in neuro-symbolic models."	3
The paper presents an incremental advance over the SotA that seems to have important practical consequences but the experiments do not clearly show them and the theoretical treatment is incomplete and imprecise.	2
The idea is interesting. But the novelty and experiments are not good enough for ICLR. 	2
The proposed model outperforms the baselines and the proposed FID score for time series looks interesting. However, there are a few unclarities in the description of the model. I hope that these things can be explained during the rebuttal.	3
This paper proposes a novel approach for generating realistic time-series data. It also introduces a metric for evaluating the quality of synthetic time series data which has been lacking in this domain. The paper also provides comprehensive experiments to show the effectiveness of the proposed framework.	3
"Overall, the paper has strong experimental results and solid contributions. But the novelty is not very high, and also several aspects of the paper need improvements. I am willing to increase my score if the authors address the points above. 

---------------------------

I am updating my score as the reviewers have addressed some of my concerns with extra experiments. I believe the paper still needs significant edits to reflect the points in the reviewer responses. "	2
The approach that combines MARL and Theory of Mind is interesting. The proposed TOM2C can be applied to multiple MARL problems which need cooperation between agents. But the proposed algorithms should be explained more in detail. Also, more experiments are needed to persuade the reviewers regarding its performance.	3
The use of a TOM model to facilitate communication for cooperation was impressive. I believe the paper is slightly below the acceptance threshold in its current state due to a lack of tests on more complex benchmarks that test for decentralized cooperation (SMAC/ Hanabi) and limited applicability because of access to unrealistic privileged information during training.	2
"Technical and experimental work is impressive but the organization of writing and lack of clarity in certain key parts makes this paper fall under the necessary standards.

During rebuttal: authors have provided detailed answers to my questions and revised the manuscript to improve clarity."	3
The contribution is based on intuitions that do not seem very solid and should be better studied. It does not really perform better than mean imputation.	3
"In summary, the proposal of this paper is interesting and promising, but it lacks clarification of the problem setting and justifications w.r.t. the choice of the representation of the missing values as well as the positioning of this approach w.r.t. other methods such as NeuMiss (Le Morvan et al., 2020). Importantly as well, a more extensive simulation study would be helpful to assess the behavior of the proposed strategy, especially looking at other missingness mechanisms than MCAR alone. I would encourage the authors to at least either add more justifications and/or results in the analytical part or to extend their simulation study to a wider range of possible settings to allow for an appropriate assessment of their proposal.
I will read the rebuttal carefully and am willing to increase the score if the authors address the raised concerns."	4
"I vote for accepting the paper subject to some additional experiments, based on my review above.
"	4
The method presented in this paper is novel and presents an interesting handling of the missing data problem. It can even be tested on DNN-based imputation such as generative models. Given the versatility of the model, it has so much potential for many uses. The main weakness of the model is the lack of significant improvement in performance in comparison to the baseline models but this should also be commended as the author(s) did not cherry pick the results where the model is well-performing and rather opted to report all their results. The other weakness is the small number of baseline models the new method is compared to. I believe the authors should summarize where their method shines vs where it does not in the discussion or summary because the way the method is pitched in the current format is that it leads to better performance than the imputation-based methods in the basic tasks of classification/regression while the results do not reflect that. Additionally more baseline methods should be added to the comparison to enable proper comparison.	4
"Overall, this paper proposes the (m)PROMISSING methods for handling missing data in neural networks. The proposal is in fact a simplified version of the well-known indicator variable method for handling missing data, as shown above. The experiments carried out do not show any significant advantage over standard methods of data imputation.
"	1
*The paper presents a method that is a hybrid of 2 major previous state-of-art methods. Although heavily inspired, the paper proposes solutions to reduce the drawbacks of the previous approaches - which is the highlight of the contribution. Additionally, the biggest take-away is that the method is unsupervised. Given the breadth of the experiments to validate each of the proposed solutions, and substantial ablation experiments to justify each proposal, the paper overall is a good contribution.*	3
Overall, this paper proposes a straightforward pipeline to synthesize and cluster images in fine-grained classes. A contrastive loss is used in place of the regular softmax loss in the InfoGAN framework. The performance seems to be very solid compared to SOTA methods on both synthesis and clustering tasks. There are some unclear or confusing parts in the paper, but I think given the simplicity and good performance of the method, it might be worth being seen by the community to inspire similar works. 	2
The paper combines several techniques to achieve unsupervised fine-grained clustering of semantic classes. Their representations' high quality is able to drive GAN generation without mode collapse, thereby achieving great two-fold contributions.	3
I tend to vote for accepting this paper as I think it proposes a great approach and presents a convincing comparative performance. 	3
"Overall this work has enough novel and significant contributions (single-parameter noise model and ICE). But it has a strong assumption (argmax preserving) and lacks theoretical justifications. More importantly, the experiment section lacks detailed descriptions and did not compare with Zhang et al. (2021a). These shortcomings make me skeptical about the effectiveness of the proposed method. Therefore, I vote for a weak reject.

---

After rebuttal:

I raised my score to 6. See my reply below for more details."	3
Overall, the proposed method is too heuristic without enough justification. The experiments are not sufficient.	2
This is an overall novel and interesting paper. While I still have some concerns regarding the single-scale confidence parameter and the intuition of the instance-confidence embedding function $g$. Besides, I am not fully convinced of the effectiveness of ICE by referring to the baseline selections. I am willing to increase my score if some of my above concerns are well addressed.	4
This paper has a strong motivation. However, it seems that the proposed approach (ICE) does not strongly connect to IDN and the performance gain is minor compared to existing approaches. Authors should provide more explanations of why embedding on the index can approximate IDN and perform more comparisons. 	2
In summary, I would encourage the authors to discuss the generalizability of the approach,before the paper can be recommended for acceptance at the conference. 	3
The proposed technique is well described and simple to implement. Its benefits and weaknesses are left unclear by evaluation that includes key challengers such as Mixup, AugMix, and RandAugment only in scattered combinations in different network setups. However, the obtained mCE of 44.8 in Imagenet-C is a very good result, and the paper should emphasize this by showing that no previous methods or their combinations reach that when using the same network architecture.	3
"1. Pre-rebuttal score.

    Despite the interesting findings in the paper, currently, I rate the submission below the acceptance threshold. The reasons are twofold: Firstly, the idea is not completely novel. Secondly, the proper ablation study for the proposed augmentation was not conducted. I ask the authors to address this weakness during the rebuttal period.

2. Post-rebuttal update.

    After reading the authors' feedback, I keep the initial assessment. The paper misses a proper ablation study while presenting method is of limited novelty."	2
"Overall, I believe there are solid contributions in this submission which can be further demonstrated with the suggested experiments. Hopefully, the authors will be able to address the weaknesses I have outlined. 

For now, I am giving this a 6: marginal acceptance. I am happy to increase the rating based on the authors' response to the next level.

UPDATE AFTER AUTHOR RESPONSE:

I thank the authors for a clear response to my suggestions and the additional experiments. I am satisfied with the additions and modifications and I am increasing my score. Please try to add ImageNet results if possible, as it may help others in the field who use it as a common benchmark.

I have also read the other reviews and the authors have also addressed them well too. 
"	4
This paper is well-written, novel, tackles an important question, and contains potentially interesting ideas. However, the evidence provided is not quite sufficient to validate the claims made. The suggestion that zig-zag in the learning trajectory is important to the success of distillation on real (non-noisy) data is not well-demonstrated, and the benefits of the proposed FilterKD technique seem marginal on real-world data.	3
"I am still not sure the proposed method is justified well in theory. It is based on NTK theory but it is applicable only for a sufficiently wide network. Also, several ""zig-zag"" patterns are shown in figures, but we are not sure of their definition and how often it appears in the real dataset. The improvement in accuracy is rather minor."	3
"The paper is easy to follow and has contributions that are sufficiently motivated. Empirical evaluation shows that the proposed method is more robust to noisy labels. However, the problem definition (therefore the scope of the paper) is not clear, and the core contribution Filter-KD has not been evaluated to an extent that justifies its simplicity.

**UPDATE: After discussing with the authors (see thread below) I have updated my review to reflect my view of the paper.**"	3
I vote for rejection mainly due to the questionable validity of the proposed model as a stochastic processes, as the parameterization can violate marginal consistency required by the Kolmogorov extension theorem.	2
Despite my lack of knowledge of the relevant literature, I still lean towards acceptance, because I like the idea of decoupling the mean and covariance function and believe this idea may lead to more advancements in the future. 	2
The motivation is interesting, but the model somewhat lacks novelty.	3
I think the model is novel and has a significant contribution to the community. The connection between DKNP and GP is interesting. 	4
This paper presents an exciting approach to improving the prosodic diversity of generated speech; however, more work is needed before bringing this research to the publication stage. The paper needs a complete rewrite to integrate the technical approach and the problem of interest and use unified symbols across different sections.	3
The work is well-motivated and novel. The method also achieves diverse prosody which is clear in the samples provided and the MOS scores. However, some of the claims are a little exaggerated. The naturalness does indeed suffer quite a bit. Additionally, there are some lingering confusions that I raise in the review. Thus I am giving a score of 5.	4
"The current writeup lacks clarity, ablation studies, and proper evaluations. The way of how conditional DDP is applied also seems problematic.
"	2
Potentially great paper with unclear motivation/applications	2
"- Overall, I quite like the idea of the paper due to the generality of the approach, namely learning an interpreter from observation. 

- “proving” the discovered rule is indeed the secret one by examining it in practice (i.e., being used to tag unseen structure) as opposed to revealing the rule directly in text is also a very neat idea."	3
Although I am all for explanation based learning solutions, I don't think the components introduced in this paper warrants introduction of a new learning framework. I think it will be better to place it as an improvement to existing approaches for cross task generalization. I also found the experimental results to be weak. It was not clear if the proposed techniques will perform better than other cross task generalization methods.  	2
"I find Odeen to be a useful contribution, and one that would raise awareness on the need of certain underused techniques in the machine learning literature. But, the rest of the paper needs to be more clearly and properly placed in the context of existing work in the literature.
"	2
Overall, I think the paper is below the acceptance threshold, and my main question is really about the fundamental assumption the authors have made, why networks of different architecture can be combined in that manner ?	2
The extension of model fusion (OTFusion) to heterogeneous settings (in the sense of different depths) seems to be well-executed. There is still room for many of the choices to be more streamlined or automated, but nevertheless does show fairly satisfactory results across various settings. The other important drawback is that the current empirical demonstration is limited to the case of 2 networks. The lack of clarity in the presentation, at some places, does not help either.  	2
"(1) Model fusion is a challenging problem in deep learning. The topic is interesting.
(2) The explanations and analyses of the proposed model fusion are not convincing.
(3) More expensive experiments are needed.
"	2
"1. The practical value is questionable.
2. The description of the approach is not complete.
3. Experiments are only performed on very small datasets.

Overall, this paper is not well prepared for publication."	2
Since this paper misses important existing works and baseline comparisons, I don't think the current version can be accepted to ICLR. 	2
"I am not convinced that it makes significant contribution or overcomes unique
challenges. Moreover, similar works have been proposed, yet not compared by this
paper."	2
1) The attack may be impractical in realistic settings and 2) comparison with existing work is not sufficient. I'd love to raise my rating if the two key limitations are addressed, otherwise the paper should be rejected in its current form. 	2
This paper is trying to deal with a quite interesting and practical problem, and extensive experiments (both NLP and CV tasks) have been conducted to demonstrate the threat of the pre-trained models backdoor attacks.	3
"
The paper provides an extension of the work in [Monteiro et al. 2020] to the case of VAEs. The paper provides adequate references but it would greatly benefit for more downstream experiments and detailed explanation as suggested in the main review."	2
The paper is sound, well written and tackles an interesting problem. However, more work is needed to demonstrate improvement upon state of the art methods (specially, Dorta et al, 2020, and potentially Hou et al, 2017).	2
The contribution of this paper is limited, it merely takes an existing method to capture covariances of model outputs and incoroporates it into the decoder of a variational autoencoder. More quantitative results are needed to demonstrate possible advantages of the proposed method and calibration of the estimated covariances. Further details on parameter optimization and convergence are needed.	2
While I think this paper proposes a sensible idea and shows improvements over using diagonal covariances in Gaussian VAE decoders, I have concerns about the stability of the proposed method; and I believe that the authors should compare against a stronger baseline than the one they use for the experiments to be fully convincing.	3
Overall, the paper is clear and the author propose an efficient and practical approach for video understanding. So I would like to provide a positive score in the initial stage of the review and keep tuning during the discussion period.	3
I believe that this work presents a novel idea of weight calibration that has benefits in terms of performance and model efficiency that for video understanding tasks. Its use as a drop-in replacement shows that it improves performance for existing architectures. It also shows that there may still be unexplored aspects for improving these model types. Despite the issues that I see in the paper, which I believe could be straightforwardly improved, I think this paper is at an acceptable threshold.	3
The paper is well-motivated and easy to understand. There are a few things I would like to point out: 1) Comparison with dynamic modules and TANet, 2) Scalability for longer input clips, 3) Clearly show the efficiency of TAda2D.	2
Overall, the paper makes an incremental contribution by essentially making (2+1)D convolution adaptive by generating the 1D conv weights within the network. The experimental claims need clarifications and revisions, as they are currently omitting stronger performing models. The ablation experiments could be strengthened by including experiments on the effect of dynamic vs. static weights. Overall the paper makes an incremental contribution and the experiments are not currently strong enough to show the value of it. With revisions, the paper could be much stronger.	2
Overall, this is a strong paper presenting a significant improvement in unsupervised pretraining for images, that should be presented at ICLR. More analysis (e.g. linear probing) could make the paper stronger. The limits of the paper are fair to have, and can potentially be addressed in follow-ups.	3
The paper is well written, the idea is somewhat novel (novel in computer vision, less novel in general because of BERT). Experiments are good but improvable. 	3
"- Consider adding a VLP subsection to the related work section.
- There are several ways to exploit the discrete VAE tokens.
- The reviewer thinks the paper's results are solid."	3
"1. The idea is good, going through an important direction of self-supervised learning.
2. Downstream tests show good performance.
3. BUT, whether the improvement comes from the designed framework is questionable.

I need additional details from the authors to make the final decision."	3
The paper presents a promising method for an agent to learn opponent models in a multi-agent environment. The method is compared empirically with similar algorithms which are outperformed.  Unfortunately, the paper does not provide theoretical guarantees, lacks details in some of the explanations, as listed above, and shows only examples with two agents, an agent and an opponent.  In conclusion, the method seem promising and well thought out,  but it is hard to be understand what are the requirements to make the method applicable in practice. 	3
An interesting model-based opponent modeling method in MARL while approximation errors in recursive reasoning and more explanatory experimental results should be considered.	3
An interesting line of research but theoretical analysis and empirical evaluation should be improved in a revised version.	3
I recommend rejecting the paper in its current state. While the authors identify a problem and provide a nice and simple solution to the problem, these are undermined by experiments that are underspecified and a lack of analytical experiments into the interworkings of the algorithm. 	3
"The paper suggests a straight forward application of ML to solve persuasion optimisation with a reduced set of assumptions. Which is ok, but rather week, contribution. If the overall approach of using learning to supplant unknown elements was novel -- would be a publishable paper -- however, there's a lot of previous work where this has been used before, but authors missed it.
"	1
A paper with a good motivation but falls short in fulfilling the proposed objective. The contribution is not significant.  	2
To summarize, the paper could be strengthened both from theoretical understanding of the assumption relaxations and experiments. Given the current stand, I think it not reaches the bar of ICLR. Hence, my rejection score.	2
Overall I think the paper proposes a novel differentiable approach to Bayesian persuasion, which is then validated experimentally.  The theoretical results are good to know, but I don't think they are particularly helpful for practical purposes; the experimental results overall support the claims, but may or may not scale well.	3
Overall I think the authors made interesting observations, but the mathematical analysis is weak and does not fully explain the observations. 	2
"(Copied from above) The problem is interesting, and the proposed method is simple and apparently easy to tune; however, I am concerned about whether or not this method will scale to more challenging environments than the Gym benchmark tasks because of various qualities of the method I describe below. While this is a promising direction of research, I think that this paper requires some iteration before acceptance.

-- Post Rebuttal -- 

Given the new experiments and analyses, I am increasing my score to a 6. "	3
"The paper is generally well-written and easy to follow. The problem formulation proposed by the authors is both relevant and important and the proposed method is simple to implement while showing promising results; however, the paper could benefit from additional analysis and more comprehensive literature review covering robust reinforcement learning.
"	3
Nice paper, some questions about the proposed idea needs answering and some of the experimental results need further clarification. 	2
Although the solution proposed in this paper is not fancy, the reasoning, intuition, and experiments are well written. 	2
"This paper discovers that vanilla fine-tuning performs worse than linear probing for the OOD tests and then develops a new method combining these two techniques sequentially. Results on several datasets verify its effectiveness. Even there exists some minor problems, this paper is interesting and easy to read. Thus, I tend to give the ""weak accept"" score.

-----POST REBUTTAL-----

The authors have addressed my concerns. Thus, I increase my score from 6 to 8."	3
Overall, while there are several points of concern that could strengthen the paper, I believe the results and theory presented by this work would still be of interest to many in the community, so I recommend its acceptance.	3
"This paper is very interesting and will be an important contribution if concerns are properly addressed. An essential concern is the contextualization of the method LP-FT --- is this method (or a modification) something that people have explored previously and if so in what context. In the current framing of the abstract and introduction, LP-FT seems to be introduced by this paper. In addition, this papers empirical verification of the theory (Sec 4.3) appears very promising but could benefit from additional detail and experiments (e.g., more than one distribution shift in the euclidean distance experiments).

Edit: authors have addressed many concerns and I have changed my score to 6."	3
Lacking motivation, contribution weak, evaluation not solid.	2
The advantage of the proposed Bi-InfoNCE loss is neither clear demonstrated nor well supported by the experimental results. Thus I tend to reject this paper.	2
"While this paper addresses an interesting problem, the significance is not sufficiently discussed in the paper. The novelty and effectiveness of the proposed method will be quite weak. The clarity needs to be improved.
"	2
"The technical novelty is limited. Moreover, the improvements and universality of TR are not perfectly verified.

In conclusion, I will vote for Reject."	2
"The authors introduce an interesting and important problem but given that the paper relies purely on empirical evidence for its claims the current evaluation is quite limited and not enough for acceptance. I find the claims fairly intuitive, however, and so would be willing to increase my score if more empirical evidence (in line with my comments above) is presented to back them.

Comments after rebutal : I appreciate the added experiments which certainly demonstrate that the proposed schemes are able to improve performance across different datasets. I have increased my score to reflect the same. "	2
"Even though the reviewer is quite happy about the problem addressed in this paper, it lacks a solid experimental section. I would be willing to increase the score if the authors can present a more comprehensive results section.

Post rebuttal:
I have read the author response and other reviews. Thanks to the authors for a detailed rebuttal and new experiments. It addressed my concerns and so I have increased the rating from 5 to 6. "	3
Overall, while ZeroFL is simple, it does seem to be a straightforward extension of SWAT to the federated setting with a very minor modification. The experimental evaluation is rather weak (experiments on a single dataset) and there is an absence of other critical baselines (such as federated dropout). These are the primary issues that lead me to recommending rejection. 	2
I believe the contributions of this paper are not significant enough.	2
"To summarize, I believe that despite the potential A-PRIORI interest by some specialised readers, the paper will not convince these specialist readers either. And it will noot bring much to non-specialists. It is badly written, and no real insights are provided. Many computations are just adaptations of things present elsewhere. Many known facts and equivalences are not cited nor even alluded to (again the equivalence between the AMP state evolution and LOO = cavity method is known to any physicist with some experience in spin glasses). The relevant literature is hugely missing. 

I suggest the authors to completely re-write the paper, and consider instead a direct journal submission, I really do not see the point too submit this work to ICLR. And, importantly, to cite much more carefully and expmain more thoroughly what is new or not in the paper (as such, I do not believe that much is actually new)."	1
While the paper focuses on a very interesting subject, unfortunately it does not seem to offer much insight into why the different state evolutions lead to the same fixed point. As a result, the contribution of this work appears to be clearly below the acceptance bar. 	2
"The claims made in the paper are correct to the best of my knowledge. 

The paper deserves to be published even though it lacks in several other ways, mainly because it is the first of its kind in establishing an equivalence between 3 different derivations for the asymptotic analysis of estimators.

This body of work can serve as a way for researchers using one method to check their predictions with another method using a parameter transformation suggested in this paper.

However, there is serious room for improvement in the presentation of the material discussed. The lack of insight provided is also a concern but perhaps can be clarified in future works."	4
The method presented in this paper is new, but more detailed analysis and evaluation are needed to make this work more solid.	3
"I am a bit torn about the paper in its current form. On the one hand it proposes a new and potentially very useful approach. On the other hand, the execution leaves some things to be desired. This makes it a bit difficult to properly assess the benefits and limitations of the method compared to other approaches, in particular more recent ones than PointNet/++. If the authors can clarify the issues mentioned above and provide a convincing argument that the method might scale also to more complex use cases, I'm willing to increase my score.

### Update after rebuttal/discussions
I am still a bit torn, since the intuitions behind e.g. the dense fitting step are still not clear and the role of negative weights. Since some points were clarified, I would give the authors the benefit of the doubt, though."	4
"The paper is interesting and novel as it rethink neural networks  for learning functions. 
"	4
"
When describing the TreeHEM approach, for reducing/merging  (or killing some Gaussian components) I found it rather cumbersome as other methodologies can be used to prune the mixture components. Next, I leave several questions for the discussion:

-it is known that the EM suffers from the problem of the initialization, that is, how many components (per node)  should we represent the data. This often leads to a model selection type of problem. So, how can we deal with this issue automatically in the proposed architecture ?

-Also, it is mentioned that reduction of  the number to Gaussians must be performed. Here, it would be welcome to introduce some of the statistical metrics that are also available to perform this task. In this way it could be possible to have more than one approach, from which a comparison could be performed, and thus, providing more insight into how this could be properly tackled.

-It is not clear the mapping between the proposed context and the clustering hierarchy in Vasconcelos & Lippman (1999) work (that is, levels, blocks etc…).

It is mentioned that a value of T = 2 Gaussian is the best choice. However, this is difficult to follow …

-For completeness, please specify the meaning of the operation in (8).

-When performing the pooling, the fitting process is also performed.Here, it is necessary to enlarge the receptive field. To accomplish this,  the reduction of the covariance diagonal must be decreased. The question is, how is it possible to find the best scaling factor for this purpose ? receptive field . Is it possible to act on the covariance uncertainty for this purpose ?

-it is said that the k-means is used to estimate the initial center’s position. What happens if this procedure does not provide the best location for the centroids ?It is well known that this algorithm may have some limitations regarding this issue.

-Since the proposal is based on the EM algorithm, a clear mapping with this should be done. Specifically, to describe explicitly the E-step and M-step
"	3
Overall I think this is a paper worthy of publication, with 1) its additional experiments to find flaws in the original hypothesis of SimSiam; and 2) explanations by decomposing the vector into two parts. However, I do have some concerns about the writing of the paper, so it would be good if the paper could have a higher quality (with proof-reading) and better organization so that it presents a more clear, unified message.	3
Overall, I think the idea of analyzing the extra gradient component is novel. The writing quality of the paper is not very good.	2
I think the paper needs to improve descriptions about their framework and provide more experimental evidences.	2
While the addressed phenomenon of preventing collapsing representations without the usage of negative samples is interesting, the presented analyis provides insufficient novel insights about how collapse is prevented. Although some novel framework based on vector decomposition is presented, the reasoning is often fuzzy, not convincing and lacks backup by clear theoretical derivations. Thus, I vote to reject this submission in its present form.	3
Based on the abovementioned strength of this paper and the interesting conclusion, I recommend to accept this paper. 	4
Overall, I think the paper proposed a nice interesting model on an important problem, motivates the goal of the model in a compelling manner, presents the model with good detail and clarity, provides a nice mix of experiments of different types and on different datasets, and can be of benefit to the research community, and therefore I recommend its acceptance and wish best of luck to the authors.	3
Although the paper is interesting, it's considered that the proposed technique is somewhat incremental. 	2
A reasonable first draft of a promising line of work but more analysis is needed to make it interesting.	3
"As it is now presented the paper seems only to be a minor technical extension of Concept Bottleneck Models and I would recommend that the authors focus more on presenting as well as evaluating the benefit of their ConceptTransformer. 
The authors state that the explanations by their introduced approach are plausible and faithful. However, no quantitive evaluation in this regard is performed (cf. Margeloiu et al. Do Concept Bottleneck Models Learn as Intended?). The introduced ConceptTransformers seems to be a technical extension to „simple“ linear classification heads of Concept Bottleneck Models. The advantage is not clearly described and evaluated. 
Therefore, the paper seems to me to be only a minor technical contribution. 
"	2
"A solid, well-motivated, and well-presented work. Although the method would suffer from poor robustness, it is a meaningful attempt to connect logic/subgraph patterns and graph representation learning for explainable reasoning.

--- after rebuttal ---

The authors' response addressed my concerns. I would raise the score to 8."	3
"Authors propose a new relation prediction framework *GraphANGEL* which is generalizable and combines GNNs and graph pattern matching. The experimentation done in the paper is extensive and shows the effectiveness of the method. 
Overall I feel the authors make a significant contribution to the relation prediction literature and I would like to recommend **accepting** this work. "	4
"8: accept, Good Paper

Even though the proposed approach is simple, inductive, and is shown to be better than considered baselines, the gains seem to be marginal especially without error bars. Even in the inductive setting (i.e. section 5.4), the proposed approach doesn’t provide a significant boost compared to the baselines especially compared to CompEx-N3* baseline. For other reasons for my score please refer to the Cons section above. 

But, I think the proposed approach is interesting because of its simplicity and the problem it is trying to solve and I am on the fence leaning towards a weak acceptance. But, I am willing to raise my score if the authors can provide error bars that will validate that the results are statistically significant. I also think that the authors should add a discussion about how to scale the method when the graph changes especially in the recommendation setting. Is there a need to resample the subgraphs which means that this computation cost is no longer one-time and should be reported as a part of at least the training time imho (i.e in fig. A5)? If not, then this should strengthen the contributions of the paper and discussed (in Appendix perhaps if there is no space). 

Another interesting analysis to have is to see the performance of GraphANGEL based on the count of relations that are added for generalization study in section 5.4. Maybe, there is an interesting pattern there.  Even going a step further, instead of splitting the relations randomly split them based on the frequency of occurrence of the relation. I think that these additions will make this paper more impactful. 

Edit: Updated after reading response from the authors."	3
The idea of the paper is different. Experiments show improved results. However, I have concerns about the methodology that majorly considers the feature embeddings without exploiting the graph structure. More experiments (mentioned in the main review) could be conducted to back the idea. Also, the points under limitations can severely affect the method in different scenarios.	3
"Overall the paper is very well written, contains extensive experiments supporting their claims and proofs of correctness as well as runtime complexity analysis for all proposed algorithms, ensuring completeness. It will be very helpful if the authors can further address some of the  questions and weaknesses as pointed out in the previous subsection of this review, particularly the inclusion of some recent and strong knowledge graph completion baselines. I look forward to the rebuttal and am willing to improve the score if the authors can provide a satisfactory response. 

After addressing these minor pointers, I believe that the contributions will be worth presenting at the conference. I additionally support the simplicity of the proposed approach and given its versatility, it can be plugged with various types of  deep learning based components used for subgraph embedding computation and similarity computation.
"	2
"The paper studies an interesting problem and attempts to present corresponding solutions. The presentation could be improved. Authors should also try to address the issue regarding the correctness of Lemma 1. I am willing to change my mind if the authors could present a convincing argument of why it's true (how my counter argument is wrong), or a fix to the theory. 
"	3
This is a very interesting work that studies the effect of how the sampling steps at different $t$ affect the performance of score-based diffusion models. It also proposes a method to train SDE-based models with smaller $\sigma_\text{min}$ so that samples corresponding to $\sigma_\text{min}$ would ideally have better local fidelity. Empirical results show that the proposed method can improve the sample quality when applied to existing SDE-based models. However, the experimental setup is not very clear and I also have some concerns about the problem formulation (see Main Review). 	2
"The paper presents three practical problems and solutions to training / parameterizing score-based diffusion models. Overall the direction is quite relevant, but the presentation of the work is subpar (I’ve pointed at specific sections/paragraphs for future improvement in detailed comments), which makes me doubt it would benefit the ICLR audience *in its current form*. 
"	2
I think this is an interesting paper.  It is technical and I did not go through all the mathematical details carefully.  However, I think the results are potentially useful and lasting. The paper itself can be cleaned up. The ST-trick appears to be quite useful, but I am less convinced about the significance of the problem with the loss parameterization as discussed above.	3
"The paper outlines four problems and one fix for each problem. In my opinion, three fixes are either only incremental or not sufficiently tested. The ""soft truncation"" trick, on the other hand, seems like a good idea, however, this contribution is buried under the three other problemx/fixes and not enough space/thought is devoted to it. In my opinion, the weaknesses of the paper clearly outperform its strengths."	2
"It is interesting that the flow transformation allows for a formal definition of a non-linear diffusion in data space via Ito's Lemma, but it seems this cannot be leveraged for improved modeling power, as directly training this non-linear diffusion is not possible, unfortunately. Hence, the paper resorts to training a regular flow together with a regular ""linear"" diffusion, separating the two components. With this in mind, I think methodologically the paper is fairly incremental compared to LSGM [1]. It is basically the same with the encoder/decoder replaced with invertible neural networks and the invertibility does not provide any practical advantages. The paper's experimental results are not impressive and I think some relevant baselines are missing. Therefore, in conclusion I recommend rejection."	2
The paper proposes an interesting way to model nonlinear diffusion using normalizing flows, and presents convincing experiments results to support the main claims, although the clarity of the writing can be improved.	3
very interesting paper but with a very questionable english usage	3
"The paper is clearly written and I am also interested in the actual advantages of the proposed method compared to other diffusion models. If the author successfully addresses the above concerns, I can improve my rating of the paper. 

---
I have read the revised version of the paper and the author's responses. It addressed most of my concerns. However, although I understand the difference between PDM and LSGM from a theoretical point of view, I think demonstrating it empirically is also important because the inter-changeability between latent space and data space during inference is mainly due to the merit of normalizing flow, not the PDM framework. Since concatenating the flow and diffusion model is straightforward, the resulting model (from a structure point of view) has limited novelty. On the other hand, I think the argument of reducing the variational gap is interesting, and along with extensive empirical evaluations, I will raise my score to 6. But I still suggest the author to further argue about the novelty of PDM from both theoretical and empirical points of view. "	3
This well-written paper presents a Bayesian learning algorithm for RL in CMDPs, which is an interesting and challenging task, and of high relevance to ICLR.	3
The proposed method is based on an elegant approach with Dirichlet Processes, and performs well on tasks where the benchmark algorithms chosen by the authors struggle or even fail. Additional experiments, metrics and explanations would be informative though.	4
The paper studies a challenging problem setting, and demonstrates the effectiveness of the HDP to model this particular setting. The experimental section presents a lot of varied analysis of their model already, but is missing results that demonstrate its scalability to more complex environments. Since the main argument of this work is that the HDP is best suited here, I think there are aspects (described in the main review) that still need to be evaluated.	2
"This paper addresses a very interesting problem where there exists unknown nonstationarity in RL problems. The proposed method takes the Hierarchical Dirichlet Process as the transition prior. Overall the idea of using HDP is original. However, this paper still lacks sufficient experimental results that make the proposed method substantially stand out. I will give a 6 but and I am still on the boardline. 
"	3
This paper is an interesting look at domain shift from a bayesian probabilistic modeling perspective. The experiments do a good job of taking apart the design choices, in particular results A and B. I have a few lingering questions about this paper's relationship to prior work in Meta Learning and Domain Shift. 	3
"Overall, the idea from OT to deal with long-tailed problems is novel. However, the writing of this paper makes it not very easy to understand or follow. Especially for people who don't have much background on OT. I thus give a score of 5.

================================update===================================

The author addresses most of my concerns. Also, the latest version has admitted its limitations and put a more clear clarification on its advantages. I thus decide to raise the score to boardline accept.
"	3
"Despite some drawbacks on the assumption made and experiment evaluations, the paper would still benefit the community. I therefore lean towards a positive direction at this moment.

Update:  As mentioned by reviewer uP2E, the strong assumption of knowing the marginal distribution of labels is not desirable in real-world scenarios. The proposed approach can be only applied to offline applications. I would recommend the authors include the following items in the future version:

1. The performance of the proposed approach with an estimated marginal distribution;
2. How can one apply the proposed method to online scenarios? (e.g. store evaluated samples to form a large batch)
"	3
"I recommend a borderline reject. Although using OT is a novel idea, my major concern is that it could be infeasible to employ the OT-based method in practice. Thus, I am unsure if the new direction is worth further exploration. 

=========== after reading author's latest revisions======

I find the limitations of the methods adequately addressed in the latest revision. I agree with the authors that this paper's setting is consistent with some existing literature and the proposed method can be useful in some offline applications after reading the author's feedback. I would like to raise my recommendation from borderline reject to borderline accept."	3
"Overall, I find the paper to be lacking background analysis, and thus also in novelty. 

The autoencoder with the Fisher information metric as the metric of the latent space is an intriguing proposal. However, as this is the only contribution of the paper, the authors are required to thoroughly compare the geometric and statistical approaches. There has been decades of research that analyzed the advantages of the two representations, especially in the field of image and point set registration. Therefore, such knowledge had to be used in the analysis. As the paper lacks such analysis, I have to lean towards rejection."	2
I believe the paper presents a novel take on analysis of point clouds.	3
A rather theoretical paper, the proposed view is in some sense elegant, but not as revolutionary as claimed. Interesting theoretical or practical developments could perhaps build on the proposed view, although the paper does not push far enough in any directions to get to a real impact. Still, I have no objections against putting the idea out to the community, if the authors tone down their claims and acknowledge that they are throwing out a cute, raw idea; while neither exploring the theoretical implications in much depth nor engineering it into a strong algorithmic tool.	3
I like the overall idea, and some qualitative results seem promising. However, some aspects require better experimentation and remain a bit obscure, particularly the effectiveness of the method, the positioning among other SOTA approaches, and the behavior in different conditions (e.g., different domains, probability distribution). 	3
"Interesting approach. One major concern is that the learned representation from the proposed approach might only capture ""global"" properties of protein structures and might not be able to capture finer-grain features of protein structures. Proposed approach shows improvements on fold and enzyme classification tasks, but these tasks may have only limited practical utility.
"	3
I recommend 'weak accept' due to the concerns raised in the above section.	3
"This is a complicated model or set of models, coupled with results that do not clearly support the central hypothesis of the paper, namely, that using contrastive learning outperforms other methods for learning informative embeddigs of protein structures.

"	2
Overall while the problem that the paper addresses is important and the method is reasonable, unfortunately, as it stands the proposed embedding method does not yield very convincing results compared to existing baselines in downstream tasks. Added with limited novelty of the method the paper in my opinion is marginally below acceptance; although I remain open to authors discussions in the rebuttal.	2
"While the problem setting seems interesting, I think that there are a number of issues indicating that this work is not yet ready for publication. Indeed, the problem setting should be more concisely described, and the relation of this work to prior works on IRL should be discussed more thoroughly. Additionally, I think that the authors should more clearly outline the goals of the interpretability of their method, and outline concrete solutions others could use to interpret the belief trajectories in practice.
"	2
Based on strengths and weaknesses as I mentioned above, I think this paper is on borderline. Now I am leaning towards acceptance.	2
This is a good and interesting paper. But it is questionable if this could not be achieved with non-stationary MABs or indeed; if past organ allocations have no impact on present allocation, simply running one MAB after another (though it is not clear why we cannot do this). Simply, their novel question in the second paragraph of the introduction is not novel and has been asked many times before.	2
Overall, the paper is well-written; the motivation, related work, and the experimental part are all easy to follow. I do not find critical flaw in the paper at this moment, but it is very likely that I might miss something. I would recommend weak accept, but I will pay attention to the other reviewer's opinion.	3
"Paper provides quite nice theoretical justifications for their decision to use informative prior instead of classic non-informative one. However, I feel that modification itself is too simple to warrant an ICLR paper. I hope authors can provide a nice rebuttal to this point in assessing the level of contribution of this paper. 

The other point that lowers my score is that the final selection of noise variance is based on ad-hoc technique and not justified theoretically, whereas other parts of the paper are.  "	3
"The basic idea of using a better initial prior distribution to improve inference speed and training convergence is nice, although it lacks some mathematical justification. The experimental section is lacking too much.
"	3
"Interesting and relevant idea, to improve the behavior of diffusion models. Experimental results indicate the idea has a positive effect on metrics used in specific application domains, including training performance and parameter efficiency.
However, the theoretical justification of the proposed method is somehow weak, and does not provide sufficiently compelling arguments on the observed behavior of the proposed method in practice.


*** Post-rebuttal / discussions remarks ***

Thanks for the useful discussions and for the additional work to improve the paper. I have increased my ranking for the paper from 5 to 6.
The main reason why I didn't rank the paper higher is that the theoretical contribution of this work, despite being very promising, is still not mature and sufficiently mathematically grounded."	3
"The paper is technically correct and shows some improvements in the empirical evaluation. However, I think that moving from standard to non-standard Gaussian priors is rather obvious. In addition, I am concerned about the generality of the approach, as it only focuses on two specific speech generation tasks and deriving appropriate means and variances from certain conditioning signals can be problematic or may not yield to an improvement. One of the three main claims is not well supported.

[Update: After authors' rebuttal I'm changing my score from 5 to 6]"	2
This paper proposed a simple and novel target-side data augmentation technique that is at least helpful for some natural language generation tasks. I'm going with a weak-accept for now citing some doubts mentioned in the main review, but am happy to update with clarifications from the authors.	3
"Overall, this work tries to tackle the important problem of language generation and proposes a data augmentation method to give soft pseudo decoder input.
As what has been mentioned in the weaknesses part of Main Review, there still exist problems to solve and need to prove the effectiveness of such method. Therefore, I'm leaning towards a weak rejection of this paper."	2
"The work proposes a data-augmentation strategy using soft labels during training instead of hard labels as in scheduled sampling.  One of the key highlights is that this work doesn't require any external model for performing data augmentation. However, the need for consistency loss is unclear, and using this model is not observable. While the paper reports incremental results, the lack of significance testing makes it difficult to comprehend if the results are not by-chance. The ablation study section is not an ablation study but hyper-parameter tuning. Additionally, the lack of experimental details regarding the number of examples seen by each model during training (number of epochs * samples) makes it unclear if the incremental results are due to more extended training or the overall augmentation strategy. 

In its current form, and based on the reasons stated above, I would recommend a weak reject for this paper."	3
While I think their idea of augmenting the dataset is interesting and it does not introduce any inference burden, based on the weaknesses I mentioned above, I'm leaning towards a rejection of the paper.	2
The paper does not provide deep analysis of offensive content in any visual dataset. Technically, the paper combines currently trendy and exciting techniques in fairly straightforward ways to create a classifier from little data for morality judgements. This technical contribution is not fully explored, and in itself does not appear novel. 	2
In general I think it's a solid paper with proper experiments as proof-of-concept, but the lack of technical contribution and missing the actual analysis on the detection of offensive contents are major weakness of the submission. For top-tier AI conference like ICLR this paper is not good enough.	2
Overall, the paper puts forward a number of interesting empirical experiments, but because the problem statement (“offensiveness”) to begin with is not well-scoped, it then becomes difficult to assess the contributions of the proposed methodology. In addition, the ultimate impact of the work is only shown qualitatively with example concepts (and occurrences) that it’s identified in ImageNet1k, but the results of these are not validated and its impact on say models trained on ImageNet1k without these “offensive” examples is not quantified.	1
I'd be willing to raise my score if the above points are addressed. Otherwise, I feel the paper proves something interesting, but the translation of these ideas into either experiments or downstream theory is lacking.	2
In summary, this work indeed showed some progress regarding the nonlinear Q-function evaluation, but the quality of presenting the significance and challenges of having these results is not enough to be accepted at this stage.	2
"I recommend ""6: marginally above the acceptance threshold"" by assuming the technical weakness (2) is not flaws of the paper but rather imperfectness of presentation.  However, I strongly suggest the authors conduct certain clarifications in the final version if this paper is accepted. "	3
This paper proposes an interesting idea based on Koopman theory that augments the limited dataset in offline RL but it is not clear from the current manuscript whether the performance of the method comes from the proposed innovations. The authors should present results of ablation studies that convince the reader that Koopman-operator is essential and a similar performance boost cannot be achieved by other augmentation methods (I have suggested a few above).	3
Weighing the strengths and concerns, I am recommending weak accept for now.	3
This paper needs to be written in a more precise way. Especially, some important assumptions are not so clear and require more clarification.	3
Overall, my concerns over the clarity of the paper, the gap between theory and experimental results, and the limited comparison to existing methods lead me to recommend that the paper be rejected.	2
"I think the novelty outweighs the weakness, and I would recommend weak accept.

**After Discussion:** I read other reviewers' reviews. Weakness 1 pointed out by Reviewer o8hT mentioned a paper that seems to contradict this paper [Zhu and Rigotti, 2021], and the authors seem not responding to this concern. Also, Reviewer uzFB pointed out several other baselines, including Boltzmann exploration and inverse-gap, that the authors did not compare with. I think the first concern downgrades my evaluation on the theoretical strength of this paper, and the second concern downgrades the empirical strength. Thus, I would like to recommend weak reject.
"	2
The idea of incorporating SAU into RL exploration is promising. Nevertheless, the presence of context and transition dynamics makes it unclear from the theoretical perspective whether SAU can be directly adopted into the exploration of RL. Hence, the work would be stronger if the authors could provide additional quantitative arguments supporting the validity of SAU for RL. In addition, the comparison with other baselines can be made more straightforward if the authors could provide charts of evaluation scores. 	3
"Based on the above issues, I am inclined to given a negative score. I will increase the score if the above problems are well-tackled in the rebuttal:
1. Discussions on the comparison with SAU method by Rigotti & Zhu.
2. Explanation of the insight behind the exploration strategy.
3. More experiments to support their conclusion."	1
Overall, while I think the idea is interesting, it is necessary to provide a more detailed experimental evaluation of the method to support the claims. I think the paper is not currently ready for publication.	2
This paper proposes a novel computational module that is elegant conceptually and effective empirically. Conceptually, the proposed block bridges the design choices between the 3D CNN and transformer block, which can be influential to future works on designing better structures in the attention module. Empirically, this paper achieves SOTA results in both accuracy and efficiency. Thus, I r	3
Solid paper with clean formulation, experimental analysis, and well written. 	4
"This paper explores an interesting direction on how to enable transformer-like architecture aware of both local and global information.
The experiments are carefully designed and results are encouraging. The reviewer would appreciate it if the authors address the above concerns."	3
Despite the shortcomings listed above, I'm still largely in favor of accepting this paper. It provides a very valuable empirical study of combining 3D convolution-based operators with self-attention. Furthermore, the authors obtain very impressive results at a relatively small computational cost (especially compared to prior video transformer papers). The presented ablation studies would also be highly beneficial for the research community.	2
Although the idea is interesting, the novelty is minimal. Besides that, more work has to be done in the experimental section to prove this new algorithm can achieve state-of-the-art results in dynamic pruning.	1
The motivations in this paper are weakly established. Further the results in this paper have not been compared against relevant benchmarks or on more established benchmarks. Given these issues, I recommend rejecting this paper. 	1
Its a paper which is a bit below the perceived acceptance level. It is an empirical paper, therefore for a high ranked conference it should come with a larger experimental evaluation and clearer details on how the XAI was used and how much was pruned. Most problems have higher complexity than CIFAR-10/100.	2
"The contribution needs to be made clearer. In addition, the experiment part that supports the contribution needs to be thoroughly supplemented. Please address my concerns in ""Main Review"" through the rebuttal process."	2
"The paper proposes a method for voice conversion which aims to use 2 classification tasks to learn content and speaker embeddings. The idea is simple and makes sense. However, the paper is not clear on several fronts. The claims are not justified properly through experiments and key details are missing as well. I think the paper in the current form is not ready for publication even though it appears to be a meaningful approach for VC. 
"	2
"Based on the above main review (especially the weaknesses of the paper), the method proposed by the paper is actually very simple and straightforward.  The novelty of the proposed method is limited.  There might also be some errors for the technical part.  The English writing of the paper needs improvement.  Hence, I do NOT think the paper should be accepted.
"	2
The paper does not argue why the proposed approach is novel. The experiments do not show any insight. The theory in the paper is weak, if not wrong. Several weakness of the approach are not mentioned. The presentation has a lot of room for improvement.	2
"In general, the author has mixed feeling about this work. On the one hand, the proposed framework is decent with clear mathematical proof (apart from Theorem 3.2, which is not valid but essentially trivial in the whole proof). However on the other hand, more than several statements are questionable, along with wording and grammatical mistakes. Besides, the speaker embedding background information is missed, along with Theorem 3.2 which is wrong in reviewer's personal knowledge.

Therefore, the reviewer is recommend this paper to be on the margin. Even if it got accepted, significant work is expected to be done on re-phrasing, minor additional experiments, and speaker embedding related theoretical coverage."	3
"Overall I believe the presentation of the technical problem is confusing, could be significantly more clear, cost function/inference better connected to the problem and validation improved. I strongly believe the validation should include some form of real misspecification and then evaluate how the learned policy improves over time. 


Post-discussion update - Thank you to the authors for their clarifications. I have updated my score based on their response."	2
Overall, I like the paper but it can improve on clarity and experiments. 	3
The paper identifies an interesting problem that takes a different perspective than past work. It covers a lot of related literature but is a bit tunnel-visioned on the problem's perceived newness. The algorithmic approach introduced is a pretty basic tweak on existing methods, but the performance is promising.	3
Overall, the paper is a theoretical paper and the bound analysis helps to understand the gap between classical linear factorization and non-linear factorization. 	4
"The proposed models built on top of meaningful assumptions for matrix and tensor factorization tasks which aim to address the gap between deep learning and factorization. However, there are several aspects in this paper that require improvement:

(1) The experimental evaluation is not very strong: 
(1-1)I recommend the authors to provide in depth-analysis of the latent factors using visualization methods for downstream task of matrix and tensor completion.
(1-2)The authors need to explain why the proposed models do not achieve the best performance on some datasets.
(1-3)Please include more baselines for experimental evaluation
(2) Please include more details on the optimization procedures in the paper or appendix.
(3) I suggest the authors to put computational complexity analysis of the proposed models in the paper.
"	3
This well-written paper presents some interesting and important theoretical results and proposes a promising multi-mode deep matrix and tensor factorization model. Its theoretical aspect seems solid to me, yet some of the experimental settings need to be clarified and evaluation against missing baselines is needed.	3
Overall, I vote for acceptance. The multi-mode generalization of deep matrix factorization and its extension to tensors are insightful with detailed theoretical discussions. My major concern is about the clarity of the paper and some notational confusion. Hopefully, the authors can address my concern in the rebuttal period. 	3
The ideas presented in the manuscript are novel and well presented, and the method seems to drastically cut down on overhead, and will be useful for both classification and forecasting.	3
The LORD-NRDE model proposed by the paper is novel, intuitive and shows impressive performance across a wide varying of experiments. In particular, I find the use of a signature-based autoencoder for time series particularly interesting (in its own right). For these reasons, I would recommend the paper for acceptance into ICLR.	4
"The paper proposes an interesting extension to neural rough differential equations which shows good results in practice and is in principle a reasonable contribution to ICLR. However, the writing lacks detail in some instances (see comments) to an extent that the claims cannot be verified. 
For this reason, I will stop short of recommending accepting at this point
"	3
The main idea is sound and the empirical justification is impressively thorough. I have some concerns about clarity/novelty but these are not sufficient to detract from my overall positive impression of this paper.	3
"MARL is a very challenging task, and I agree that entropy regularization introduces problematic biases, so they address a relevant and challenging problem. The paper is strong in presenting both theory (I took a brief look at the proofs, the authors seem to know what they are doing, but I did not go through details) and experiments. 

What I'm not so sure about is how deep the contribution is compared to previous work, in particular Wang et al. 2019, or, for instance, (Haarnoja et al., 2018), which is cited in the proof of Lem 2. Additionally, it seems the guarantees are only for fixed rho, while the full strength (nonbiasedness) of the method seems to come into play only when updating rho as well."	3
Initially, I vote for a score of 6. While some parts of the theoretical studies are based on related works (e.g., SAC, DOP), DMAC shows the appealing benefit that the framework can be easily combined with prior MARL works, so it has much applicability. After reading the authors' responses to my questions, I am open to raising my score.	2
"DMAC does provide improvements over existing algorithmic benchmarks, but the level of novelty seems limited as the DMAC method resembles SAC and several components of DMAC has questionable motivation. I’d be happy to change my score, if the authors address these issues accordingly. 
"	2
This paper presents a new algorithmic framework with sufficient mathematical discussions, although I'm a bit concerned about its current form. However, the paper can be improved if more novelty discussions can be presented and more thorough experiments can be conducted. 	2
This paper proposes the network pruning method using simulated annealing. The basic framework of the proposed method is the same as the usual pruning methods. The experimental comparison with other pruning methods is not performed. Therefore, the reviewer can recognize neither the novelty and effectiveness of the proposed method.	2
The proposed SA method is interesting, however, I am not convinced it can scale well, and the experiments are using small networks. Need more baseline comparison to show its advantages.  	2
I recommend rejection of the paper. Simulated annealing is not a new technique, and the limited scale of the experiments and lack of comparisons to baselines limit the utility of this work in its current form. 	2
 Although the idea seems interesting, this paper is not technically sound. Experimental results cannot fully support the main conclusion of this paper. Without extensive performance comparison, it is also hard to judge whether this new method can make a real contribution to the related community.	2
Overall, this paper makes an interesting observation about the HCC game setting and proposes a reasonable NHG method, reducing the non-convex non-concave problem to a convex-concave problem. However, I am concerned with the preconditioning in the NHG method, making it not scalable and potentially not possible to be used in other non-convex settings. Therefore, I make a borderline decision for the current moment. 	2
Overall, the authors study a very interesting emerging setting, covering many important applications. However, the progress appears to be only incremental, and comparisons with existing results appear to be missing.	3
Interesting results while it would be better to add more discussions.	2
The ideas driving this paper are interesting, but the mathematical rigor is somewhat questionable. There are several points, partially detailed below, where either the presentation is unclear because of many flaws and typos in the writing, or at least not rigorously established. Overall I appreciated the contribution, but consider it as a borderline. 	3
"While the empirical results of the paper are quite strong, the paper doesn't compare to a number of more recent methods that also use ensembles of Q functions. Moreover, the method is rather complex, and it doesn't provide a guiding explanation for why this particular combination of ideas is good. 

------------------------
**After rebuttal**: Thanks to the authors for responding to some of the points raised in the review, and for running additional experiments. My two main concerns with the paper are (1) whether most of the empirical benefits are coming from the ensemble and (2) the clarity of the writing. While I am not convinced that the revised version addressed these concerns, I would encourage the authors to continue revising the paper and submit to a future conference. "	2
The paper is built on the observation that  achieving different goals may need different pseduo success trajectories which are unfortunately not provided by the naive HER algorithm. The paper then provides a straightforward solution to this by adding a Boostrapped DQN onto HER so as to allow it explore deeper and be able to evaluate the goodness of a pseduo trajectory for different goals.  Although this idea is shown work well compared to the naive HER, I think that further more principled solution may be needed.	2
"# Final Remarks
* The authors summarize their contributions in Section 6. While their fifth point is true, I would respond to the other points as (1) there is actually no use or examination of the statistical bootstrap in this work as the ensembles rely solely on random initializations, BootDQN also does not distinguish data sources, and there are other principled means of addressing exploration through ensembles such as UCB (2) while the proposed prioritization is new, there are many others that also do not depend on the environment (TD-error) which have not been assesssed (3) the combination is largely uninteresting based on ablation studies in this work which show near negligible impact of the proposed prioritization scheme and (4) I don't believe DDPG (with or without HER) holds state-of-the-art for these Mujoco domains; I would suspect that lies with either Soft Actor Critic or TD3.

Taken together, I don't believe this paper is ready for publication at this time.



======= Post Rebuttal =======

I thank the authors for their response but the justifications for the utility of representing epistemic uncertainty and lack of baselines are shallow;  it's clear that substantial revisions are needed before the submission is ready for publication."	1
"The idea of enhancing exploration in goal-conditioned RL using bootstrapped DQN is reasonable. Also using the multiple heads to prioritize samples for replay is a good extension, which leads to the investigation in the paper.
The idea of counterintuitive prioritization is investigated using an empirical investigation that gives a good insight into the reasons why this approach can be expected to work.

However, the paper does not sufficiently tease apart the effect of prioritization versus the enhanced exploration.
The comparison to other techniques that prioritize their samples does not take into account the enhanced exploration either.
And the paper does not compare to other techniques that enhance exploration."	3
My recommendation is mainly based on the theoretical novelty and the applicability of the theory. 	2
Overall I think this is a good paper with a straightforward idea of combining pessimism and neural bandits for offline policy learning. I am open to revise my rating if the authors could address the concerns.	3
Overall the paper studies a new problem and presents a good analysis. Since the technique is quite similar to existing works, it would be much better to present necessary discussions to claim their contributions.	2
The technical contribution of this paper is OK but not strong. The assumptions and theory of the paper need substantial clarifications. 	2
"While the overall idea of iterative hierarchical heatmap refinement is interesting, there are some significant areas of improvement that make it difficult to argue for the acceptance of this work as-is. For instance:
- Writing errors in key areas make it difficult to parse what is going on internally in the model.
- The novelty with respect to the original GOHOME model should be expanded upon, perhaps with a concrete sentence stating the core differences near the beginning of Section 3 and stating the relationship of this work to GOHOME in the related work.
- The experiments are good at showing that the model works, but there are a bit of key insights missing. For example, why are only two rounds of heatmap upsampling done? What do the performance curves look like with increased points? What does the Pareto front look like with respect to computation time or required FLOPs (at least to indicate to a reader why stopping at two levels makes sense)?
- Finally, another dataset would really strengthen this work and aid in convincing readers that the method's performance extends beyond the Interaction dataset.

Addressing these would certainly ready this paper for publication in this or a similar venue.

### Post-Rebuttal

The added results and comments in the author rebuttal address most of my concerns. I share the same overall sentiment as Reviewer PgCY, the paper may not have a single ""major"" novelty, but the set of presented incremental contributions are sufficient to convince me of the performance and utility of this work over prior approaches, and I believe this paper is now more suitable for publication."	2
"Despite good results on the leaderboard, given the limited technical novelty without solid supporting evidences, I do not recommend this paper for acceptance at current status.

**Updated review after rebuttal:**

Thanks to the authors for the revision that clarifies the technical novelity much better, and the additional experiments that compare with the GOHOME baseline thoroughly on more benchmarks. These address most of my concerns and therefore I raise my rating."	2
"
The paper provides an incremental solution to heatmap based motion
predictions that improves on scene level predictions.
Even though this idea is interesting, the experiments were not able
to convince me as this being an impactful approach for scene level prediction.

# Post rebuttal review 

After carefully reading the authors rebuttal, I decided to increase my score.
The changes proposed by the authors and the explanations provided would increase the quality of the paper.
The two main problems: The fact that the contributions in comparison with GOHOME were not clear,
and the lack of results in comparable benchmark were both carefully addressed.

I am changing the score to a 6 provided that the authors proofread the paper as well recommended
by reviewer 4LEw, add the other benchmark results and better compare with GOHOME.

The paper does not qualify as of containing major new contributions. However, I think
the incremental contributions shown are sufficient for me to lean towards acceptance."	2
"I think this paper has significant contributions in terms of the presented method. However, my main concerns are related to the lack of details in the writing as well as the thoroughness of the experiments and the insights that can be extracted from them. For these reasons, I am leaning towards rejection, but I look forward to the authors' response during the rebuttal period.

### Post-rebuttal

After careful consideration of the rebuttal, I am happy to increase my score and I lean towards acceptance. My doubts were clarified, and the authors have timely adapted the write-up. Since my main issue was with the lack of clarity in some parts and that has been improved I raise my score to a 6. I am in between 6 and 8, but I think 8 is a bit too much since the improvements are incremental. I would rate it as a 7 if I had the option given that the proposed linear recombination of marginal predictions is very flexible and could be easily adapted to multiple prior works in the literature, thus raising the future impact of this work."	3
"
 The method addresses an important problem, and is well motivated and evaluated. In all this is a well executed piece of work and I am cautiously happy to recommend its acceptance at this point. I do, however, have a couple of questions I would like to see some answers to (see above). Depending on the answers to these questions, and discussion with other reviewers, I am happy to consider raising my score. 
"	2
I have novelty, significance and experiment rigor related concerns as detailed in my review. 	2
"Overall, the observation of the paper is not novel and the theoretical analysis is rather weak. Though the proposed method leads to improved worst-group performance, it is not well developed and supported with sufficient empirical evidence. Thus, I recommend a ‘reject’ for the paper. 


++++++++ Post-rebuttal ++++++++

I will increase my score to 5, but I still think the paper can be improved a lot before acceptance as detailed [here](https://openreview.net/forum?id=cVak2hs06z&noteId=rfOvGo4VyL9).
"	2
The paper can use some cleaning up but the idea is interesting and clearly communicted enough to be of value to the conference.	4
Overall I think this is an interesting paper. However, my main concern is the accuracy of the claims and the presentation of the paper. 	2
Despite its novelty and dense theoretical analysis, the empirical evaluation can be further improved to better support the robustness of MD-AIRL. I also hope to see more discussion between MD-AIRL and the related RAIRL in the rebuttal. I read through both authors' responses and other reviews and decided to recommend a marginal reject. I have thought the submission is interesting and has its own contribution in some extent, but it seems like there haven't been signficant improvement during the rebuttal period. The performance improvement is not significant and the written clarity is not ready to be acceoted by ICLR. 	3
Overall I think the paper proposes a novel approach to IRL. But the most obvious drawback is that the paper is not clearly written, which has many significant issues leading readers to misunderstand/cannot understand the paper. I think the current revision is not sufficient to be published in ICLR, thus I recommend a marginal reject and tend to reject if there is no significant revision. 	3
While I find the paper well written and potentially relevant for this field, there is a major flaw in how the policy gradient is computed. It is not clear to me how the theoretical results can be fixed in the off-policy case, while for on-policy learning some additional work might be able to keep the results valid. Unfortunately, the experiments conducted are in the off-policy setting, hence, it is not clear what we can conclude from them. Given the issues above, I propose to reject this paper in the current version.	2
To my knowledge, the method proposed by this paper is novel and tackles an important problem. I recommend acceptance.	3
In all, although this paper is clear to follow and nicely presented, I believe the theoretical analysis part is weak. The fast convergence rate somehow lacks significant novelty. The stability of the proposed method is less convincing. 	2
"a. The Stackelberg view of actor-critic seems not well motivated. 

b. There exists a small issue in the implicit gradient involving the sampling distribution. 

c. The theoretical arguments depend on strong and ungrounded assumptions and the proofs are hard to follow.

"	2
I think this is an interesting paper, and I would like to understand better the technical novelty.	2
This paper is significant, in the sense that it extends LC3[1] and provides a theoretical guarantee for a setting that is more complicated than [1]. It also uses a novel technique to utilize the noise in transition to remove the dependability on the global Lipschitz constant. Although the dependency on noise might have room for improvement, I think it is a good paper and could be accepted.	3
Overall, I think this paper makes interesting contributions on both the theoretical and empirical end of representation learning within RL. However, significant work needs to be done in order to clarify its problem setting, results, and position within related work. 	3
"I believe the clarity of the writing can be improved. It might not be a good idea to emphasize the algorithm is conducting representation learning and using the model-based learning might be more suitable.

The theoretical contribution is rather limited, and most significant contribution seems to be on the empirical side (I have to say I'm not familiar with the empirical work). The authors make a strong assumption on the true model. The algorithm and the analysis seems to be directly adapted from prior work, e.g., Russo & Van Roy (2013; 2014); Osband & Van Roy (2014). 

It seems that there is a gap between the theoretical part and the experiment and it’s unclear how the experiment is conducted."	2
"I believe it is timely to allow review papers into main ML conferences, so on balance I vote for acceptance. It is worth mentioning that several position papers cited in the text, e.g. Pineau et al. 2021, Mitches et al. 2019, Bender & Friedman, 2018 have all been published in journals and/or conference proceedings. 
"	3
"This is an interesting approach to evaluating continual learning. The transparent and multi-faceted approach is interesting and unique, and can be truly useful for those working in the community.
I feel that there is, however, a gap to be addressed between the more theoretical (and informative) usage of CLEVA and the more applied usage of it as a tool in guiding ML research. While section 4 of the paper does address this to some extent, the fact that the authors do not recommend using CLEVA to compare models, or alongside other works cited in that paragraph -- this would merit some additional thought and development."	3
The main motivation and the benefits of the proposed compass is not clear. As an assessment tool the main advantages that this work can provide to the community is not clear to me. The better comparison of four works in terms of this compass in the text could be one way to show the superiority of the world of using CLEVA versus not using it.	3
The paper addresses a crucial topic in Continual Learning, and I think it may interest many people. Evaluating and comparing different methods transparently is relevant not only for Continual Learning but for all areas of Machine Learning, as mentioned by the authors. My only concern is the questions I leave to the authors, which I hope they can answer.	3
The paper tackles an important problem of fairness in recommender systems, and it defines a fairness notion that contains both the user groups and items groups. The metric defined is pretty straightforward to understand. The kernel-based probability density estimation to compute the difference between conditional and marginal probabilities is not a very standard method but it seems to work for the purposes of the experiment. Overall, the fairness notions are not very strongly motivated (as highlighted above), and the paper is missing the guidance around what groupings of items and users are meaningful for such a recommendation task.	3
The new fairness notion introduced by the presented study can provide certain new knowledge and insights on how to construct a fair recommender system, and the effectiveness of the proposed recommendation method seemed to be supported by the experimental results. However, the study can be further strengthened if more in-depth analysis on the recommendation results can be provided.	2
While the paper introduces a new meaningful notion of fairness. The notion closely follows the previous work. The optimization methods used also follow the previous work. Overall, this makes the contribution of the paper very incremental. 	2
"- Since I am not very familiar with recent work on fairness, the proposed notion ""equal experience"" seems novel to me. 
- The authors provide the optimization method for the ""equal experience""  based on both traditional ML and deep learning techniques.
- The results prove that the proposed method can receive better ""equal experience"" optimization performance compared to other baselines."	2
Overall I like this paper, which is solid, complete, with sufficient experiments and evaluations. I am okay with acceptance, but I am a bit concerned that the contribution might not be significant enough for acceptance, since it is essentially the sinGAN setting plus content/layout separation.	2
In summary, the statement of learning from a single video is basically the same as learning from hundreds of images, and is thus much easier than learning from a single image. The proposed contributions, i.e., the two-branch discriminator and diversity loss, lack technical novelty (see above). So I think the paper is not good enough for acceptance.	2
" 
Overall, I vote for rejecting. I am especially concerned about the novelty of the two-branch-like discriminator, the effectiveness of the discriminator, and the motivation of learning from a single video. Please address the concerns listed in Cons during the rebuttal period.
"	2
The proposed novleties make sense and seem to lead to clear improvements. However, the evaluation is only based on unconditional image synthesis and ignores many more practical applications for single-image models such as animation, harmonization, retargeting, etc. A more broad evaluation on some of these tasks coupled with a user study would be more appropriate.	3
"My assessment is that the paper has a nice application to MSSO problems (which could be impactful due to their usage in OR models), but conceptually the novelty is low. The empirical work shows that the algorithms are generally impressive, but some details are missing. The paper could also use more clarity in writing -- see some of the comments that I brought up above.
"	3
The paper is well motivated. The proposed algorithm is novel and interesting. Thus, I recommend acceptance for the paper.	3
"I believe the paper presents a sound and functional method that improves algorithmic performance on the MSSO problem. It is well-written and evaluated.

I remain unconvinced about the wider significance of MSSO but I am willing to listen to new arguments.

Overall, I have a mild tendency to primarily appreciate the technical quality and lean towards acceptance. However, at this point, I do not see myself championing the paper in case of a large variance of opinions."	2
The paper develops an intuitive and natural idea, with promising results on two benchmarks. However, the paper (i) lacks connection with closely related literature (batch learning in SDDPs), and, in my view, (ii) it does not deeply investigate trade-offs of the methodology and underlying structural properties, besides a limited study on the cut generation procedure.	3
This work proposes a new setting of MDP models with stochastic delays. But the assumption of the MDP model is not well-justified -- it is unclear what signal interval distribution justifies the assumption. Also, the policy class might be restricted and it is unclear how to conduct policy improvement within the policy class. 	2
Paper falls a little short on both theoretical/methodological novelty and rigorous experimentation.	2
I suggest rejecting the paper because of the weaknesses listed in my main review. I like the idea of the paper and would encourage the authors to keep working on it. But the paper in its current form is not ready to be published from my point of view.	3
The paper makes strong assumptions on MDPs with delayed reward that do not align well with knowledge from previous works. Instead of trying to compare and articulate the exact differences and conclusions of their work and previous literature, the authors briefly list some existing works at the end of the paper in an independent section without such warranted explanations. While I appreciate the vast experimental part of the paper with comparison to several other methods, the rest of the paper is not well justifying many of its statements and does not convince that the overall approach makes sense.	2
The MEME proposed by the authors in this study achieves high performance in both partial and complete cases. It also shows interesting results in terms of relatedness. However, there is a lack of discussion on why MEME is better than existing methods and an explanation of its limitations.	3
The paper is about the new approach to solve the missing unpaired data problem in the multimodal setting. Although the approach is solid and interesting, it is incremental to the prior work. Some of the experiments demonstrating the superiority of the proposed approach are also unclear.   	2
In summary, this paper presents an interesting and intuitive approach for modeling the joint distributions across modalities. In particular, it appears to work well in the partially observed setting. Given the nature of real-world data which is usually very noisy, this is significant. Coupled with informative figures and extensive experiments (including those in the supplementary material), this paper could serve as a very useful reference for modeling the joint distribution of all modalities concurrently. Hence, I recommend acceptance.	3
This paper presents a method extending CCVAE from label and data to multiple modalities. The way of CCVAE handling semi-supervised can be used for training with partially-observed data. Even though the novelty is somewhat limited in VAE literature, it's still a contribution to multi-modal generative modeling. Thus I vote for a borderline acceptance at this point.	3
"Overall the paper is of good quality and it studies an important problem. The proposed method formulation makes sense. However, the experiments part need improvement and clarification. 

Post rebuttal: I read the authors' rebuttal and appreciate all the feedbacks. I hope the author could incorporate the suggestions by the reviewers to improve the overall quality of the paper. I raised my rating for the rebuttal and the paper has a nontrivial contribution to the field. "	3
"I overall like the idea and execution in the paper. I rate the paper as acceptance. Still, there are issues that need to be addressed (related work, baseline methods, and clarity). Nonetheless, these issues are minor and should be straightforward to address. 

** Update after rebuttal **
I read the rebuttal and the other reviews. The rebuttal and the revision address my questions and concerns. I think the paper should be accepted."	2
Overall the paper is a good submission, and I would recommend accepting it. I hope the authors can incorporate my feedback into the final version.	2
I'm on the fence about accepting this paper. On one hand I think it is good that the authors are exploring a new and important area and the ideas are interesting. On the other hand, this work still feels preliminary and the benefits of intra-episode exploration are not yet convincingly demonstrated.  I am not strongly opposed to accepting this paper since it could at least be a starting point for research in this area. However I think that if the authors could introduce new tasks where intra-episodic exploration convincingly helps, then I think this would be a very strong submission to a later conference.	3
"The paper studies the relatively under-explored question of when agents should explore, introduces a novel exploration trigger called ""value promise discrepancy"", and performs a thorough empirical analysis in the domain of seven Atari games. Please see the main review for detailed comments and suggestions for improvement."	3
The paper brings an in-depth study, with interesting results and very well written. It is worth being divulged to remind everyone who works in the RL area that there is still a lot to study, investigate and evaluate in order to have robust, efficient and effective systems.	3
I believe that the idea presented in this paper is interesting but the results are lacking. The related work section briefly covers some similar methods, and I think comparisons are still needed with these other methods. E.g. GoExplore also focuses on the *when* question of exploration and should be included as a baseline as well as works with monolithic behaviour policies where the mode-switching is replaced by a weighting problem of external and intrinsic rewards of a single behaviour policy.	4
The paper has a very strong contribution to the field of multi-norm adversarial robustness. However, I am concerned if this phenomenon generalizes to other datasets -- based on some observations noted by the authors. I would be happy to raise my scores if the authors can show results on MNIST as well.	2
"Overall, the paper could use some more work to separate the effects of the components of E-AT and motivate the combination of it with the fine-tuning contribution, which seems orthogonal. Thus, I do not recommend acceptance, although I am open to raising my score during the discussion period.

**After discussion period:** My overall position is that I would be more supportive of accepting the paper if it focused on fine-tuning over E-AT. Currently, the extensive focus on E-AT detracts from what seems like the more important contribution and could lead that contribution to be overlooked. However, I believe changing the focus would require a major revision and thus I maintain my weak reject score. Nonetheless, I respect that some of the other reviewers have different opinions on whether the paper should be published and I hope the AC weighs all our points to make the final decision.

"	2
The motivation and the theoretical effect of the proposed defense should be clarified and demonstrated, and some experimental comparisons need to be improved. 	2
I think this paper tackles an important problem and conducts an exhaustive experimental evaluation. However, I would like to clarify the points raised in my review and would be happy to raise my score if the authors can successfully address my concerns in the discussion period.	2
There are significant drawbacks both from a theoretical and empirical point of view. I am not sure how to strengthen the paper theoretically, but I think from an empirical point of view a more thorough comparison with various DP mechanism (Abadi et al. for instance) and including a baseline would strengthen the paper.	2
Overall, the paper is well-written and the proofs are easy to follow. The contribution is solid since the result directly improves the previous DDG scheme. However, there are some minor issues that the authors should clarify or address.	3
Compared to other approaches to address privacy issues in joint learning, the Skellam mechanism is very compatible with MPC. However, a similar work has already been done.	2
Overall I think this is a nice paper. However, I would like to hear the author response (questions 1 and 2) before recommending acceptance.	3
The authors explicitly address the similarity of their work to prior contributions from Jing et al. How significant the differences are hinges in part on the quantitative comparisons. I would welcome if the authors could address my questions with respect to Figure 2 and potentially include additional comparisons. 	2
"Reasons to accept: reasonable designed training objective for the application of drug design, good generation quality, new method of constructing molecule fragment vocabulary.

Reason to reject: unclear description of method, limited novelty of model, questionable metric used in the evaluation, missing baselines. "	1
"I find the work overall convincing. It takes a pragmatic approach that uses relatively sophisticated (but not wildly innovative tools) to improve one obvious flaw in motif-based models which is their inability to invent new motifs. I find the addition of per-atom sampling a convincing escape route for that failure mode.

The results are relatively convincing in the innovation is practically useful, although it is hard to quantify the size of the gains, because hyperparameter optimization strategies were different for the new method and the baselines. Overall, I think it is acceptable and I would be interested in using a model like this in the practice. "	3
The proposed molecule graph generation method is flexible and achieves good experimentals results. 	3
"Overall, despite the good empirical performance, I think the lack of novelty is a significant weakness of the paper. As such, I recommend weakly rejecting the paper for now. 

After Rebuttal:
I would like to thank the authors of the paper for all clarifications and additional experiments. After reading the response and other reviewers' comments, I am raising my score to 6. 
"	2
"This paper provides a simple way to extend label smoothing to instance-specific label smoothing with an adaptive smoothing parameter. The benefits of the method is shown empirically, with an ablation study, and through gradient analysis.

After rebuttal: Thank you for the additional experiments. It is good to see that the proposed method tend to perform better than the additional baselines. I do not have further comments/questions."	4
This paper proposes a label soothing scheme that brings dynamic nature into the smoothing parameter and the prior label distribution from the distilled knowledge. More theoretical results and empirical results should be reported to validate the effectiveness of the proposed adaptive label smoothing. The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.	3
"A theoretically simple yet empirically effective approach to label smoothing.

+++++++++++++++++

Update based on authors' response:
Thanks for the clarifications. I will maintain my current score."	3
"The paper contribution is minor. Their experiments are limited in terms of test environments and benchmark methods.
"	2
Weak experimental results and lack of comparisons to prior methods	2
"The paper explores an interesting direction for RL safety and shielding and even though it cannot provide performance guarantees, it is helpful to the agent's performance and reduces some violations.
The contribution will be helpful to the community."	3
This paper modifies a prior model-based RL framework, Dreamer to incorporate a safety violation predictor based on latent shielding. While this idea is simple, easy to implement, there are several issues with the paper with regard to severe safety violations during training, compounding errors due to model error and policy error, and lack of experiments + comparisons. As such, I don't think the paper is ready for publication, and does not provide concrete reliable takeaways. 	2
The paper is well-written, but I am concern about the applicability of the bounds due to its assumptions. The authors claim to analyze an unsupervised algorithm, but the assumptions do require the access to full supervision.	3
The paper is well written with many great figures to explain ideas. The improved upper bound and lower bound are studied, which I think it's good to the community.  However, I don't closely follow the theory development of representation learning, which I can only follow the derivations in the paper, and judge from a practitioner perspective.	3
Overall, although this work provides new tighter bound for contrastive learning, it does not provide new insights and new proof technique/frameworks.  	1
This work proposes a sharp loss upperbound for contrastive learning and its downstream supervised learning. It is validated by the empirical experiments conducted by the authors on different datasets, but lacks the interaction between the number of classes and the number of negative samples. Because it’s indeed lower than the earlier upperbound, I think it would be useful for the community and would recommend for acceptance.	3
I am recommending weak rej at this stage. My main arguments are that (i) the author(s) actually analyzed supervised contrastive learning, not unsupervised contrastive learning; (2) need evidence to show the gains from large K are really from the variance reduction of sandwich bound, not from the variance reduction + bound tightening from the MI estimation perspective. 	2
"This paper presents an interesting observation that adversarial training might decrease inter-class similarity, which might in turn hurt the robustness-accuracy trade-off. The proposed method that combines label smoothing, or maximum entropy, with both AT and TRADES, seems to improve the accuracy-tradeoff by quite a margin. 

On the other hand, the authors should present more results on other datasets to show the results hold across scenarios. Also more discussion on related works would be useful to clarify the benefits of label smoothing in terms of adversarial robustness (it's known that LS improves accuracy)."	3
While the paper solves a timely and important problem, it can also be improved by shortening its introduction to the problem, better explaining why LS works and why we should focus on ME and TRADES, citing and comparing with more related work, and making its evaluations more extensive.	3
"Given the lack of technical contributions and the discussion to a closely related line of works in the fairness literature, I would vote for rejection.
"	1
Overall, I think the paper is on borderline in its current version. The main weakness of the paper is its technical novelty. But on the other hand, the paper provides extensive experimental results and a (potentially) reasonable explanation for the big thing in adversarial robustness: trade-off between robustness and accuracy and robust fairness. Extra theoretical analysis in support of the empirical discovery will significantly improve the paper and weight my rate towards recommending full acceptance.	2
"The view-consistency modeling does not give a supervise or somehow it is not new enough. 
The diversity loss is not well addressed."	2
Overall, this submission's empirical contribution is more appreciated than its unclear theoretical justification. However, the arguments made in the intro were too vague and also misleading. Therefore, I do not vote for accepting this paper.	2
This paper concerns the semi-supervised problem. Different from SOTA deep semi-supervised methods, the proposed DiCom employs a diversity measure on the labeled multi-view data, and combines diversity with consistency based on underlying probabilistic graphical assumptions. Experiments verify the effectiveness. However, there exist several shortcomings for this paper, so I recommend weak rejection.	2
"This paper proposed a technically sound method based on an undirected graphical model for semi-supervised regression.
However, compared to an existing formulation [Yu et al., 2011], its novelty and contribution are a little questionable and need to be further highlighted.
Hence, I think it's on the borderline for now.
"	2
"Although the paper presents interesting empirical observations and analysis, mainly, it is dataset centric. The theoretical justification of uniform distribution is weak -- again, using uniform distribution comes down to data dependency.
"	2
The novelty and difference with existing works need to be emphasized. I'll consider raise my score if the authors can address my concerns properly.	3
Overall, I vote for accepting. The paper well presents their proposed method with motivation.	3
Overall, I think that this paper is not ready to be accepted. The details are elaborated in main review.	2
This paper deals with the important topic of active transfer learning and is considered to be a contribution to both theory and experiment.　Although there are some shortcomings in the experiments and some questions, I can generally support the acceptance of the paper.	3
While the formalizes and motivates the proposed solution, I don't see any novelty in the final algorithm used for domain adaptation. There are also many loose ends that I feel that the authors should address. 	2
While the proposed methodology is of limited novelty and not described in a self-contained manner, when considered along with the proposed theoretical framework, the contributions of the paper may be worth sharing with the community. 	3
Based on the strengths and weaknesses listed above, I think overall the paper is marginally above the threshold. 	2
The paper presents gradient alignment to induce feature invariance. Several comparisons are presented but paper lacks explanation on the central idea, that why invariance is achieved. Also, some clarifications mentioned in the main review are need. 	2
"Overall, I think this is a well-written paper with good empirical results. I did not observe obvious flaw in the paper.

"	3
This paper proposes to maximize the gradient similarity to learn invariant features across domains. But the improvment is minor and some critical explanations to the results are lacking.	3
Literature could be improved. For better understanding slow but exact algorithm could be tested on small dataset. 	3
As stated in the main reviews, the motivation of using a matching gradient is not particularly impressive, but overall the paper does provide some interesting analysis that may encourage new ways of thinking about domain generalization problems. As such, I think this paper may be of interest to the ICLR community.	3
This addresses an important area in large language models, of better encoding knowledge in multilingual models. I believe this is a useful direction and will be of interest to the NLP community at ICLR. I think the two core techniques proposed here (code-switching using KG triples and using KG cycles to teach models about transitive relations) are new and interesting. The main drawback of this work is that the improvements on the tasks chosen are quite modest. It is also unclear whether a simpler version of the proposed technique (discussed above, of training using monolingual triples) would have been sufficient for these modest gains. I think the work would benefit from more clearly defining the main hypotheses in the study and more carefully evaluating (with also qualitative analysis) the benefits of the proposed techniques.	2
The paper is having really good performance across multiple tasks and is simple to implement and follow. The multilingual is a nice addition to existing knowledge-based LMs, but there are some concerns regarding the pretraining data generation and low-resource setting.	2
An interesting idea, but with an evaluation that does not demonstrate the true impact of the proposed knowledge-enhanced model.	3
"The paper deals with Knowledge-based Multilingual representation which is relevant and a valuable contribution to the field of language understanding.

The authors provide good and clear motivation and introduction to their research work. In general, the paper is well structured.
 
The discussion of related work is adequate with a number of recent citations. It could be extended in the final version, especially regarding the future directions.

The evaluation part contains interesting implementation details, used technology is state-of-the-art. The evaluation is convincing and nicely shows the merit of the approach.

Although the proposed method looks incremental, it has the following advantages:

1. The proposed model is explicitly trained to derive new knowledge through logical reasoning in addition to memorizing knowledge facts.

2. It does not require a separate encoder for knowledge graph encoding. And also doesn't rely on any entity linker to link the text to the corresponding entities, as done in existing methods.

3. It also keeps the model structure of the multilingual pre-trained language model without introducing any additional component during both training and inference stages.

Recommendation 

The technical contributions could be explicitly highlighted, the description made more sound, and or easy to follow with either algorithm or figure/workflow.

Section 3.1 Reasoning Based Training Data: The authors mention that “Fig. 2 (a) the cycles of length 3 can be viewed as the basic components of complex logical reasoning process”. This is however not clear, a more clear explanation would be helpful. 

English needs some proofreading.




"	3
"This work proposed to solve interesting problems of multilingual knowledge pretraining. The method is straight-forward and the pre-training task or logical patterns are not clearly written. Some ablation studies should be further investigated such as with/without normal sentences from CC100 corpus. More importantly, the author should think more about logical reasoning.
"	2
"The paper is well motivated while it needs to be improved in clarity. (1) The dataset used in evaluation should be unified. (2) The experimental setup in Table 1 and Table 2 should be explained more clearly. (3) Additional ablation or explanation of Table 3 is suggested to be added. (4) Some discussion about the epistemic and aleatoric uncertainty.
"	3
Overall, the paper presents a large amount of empirical results, but is lacking a well supported, clear actionable message beyond using longer rollout horizon and ensemble standard deviation as an uncertainty penalty. Even this claim is lacking empirical support, as there is no evaluation of a fixed, guiding principle for hyperparameter selection that is held constant over a wide range of domains. As such, I'm hesitant to recommend acceptance. If the manuscript is updated to have clearer empirical evidence for the main claims in the paper, including a fair evaluation of the recommended strategy across a variety of domains without optimizing hyperparameters separately for each environment, I would increase my score.	2
I recommend accepting this paper. I think that more rigorous evaluations of the design and choices made in RL and particularly offline RL methods are important contributions to help drive future research. I think the analysis of the uncertainty quantification methods is particularly interesting.	2
I recommend rejecting this paper because the technical contribution is not strong enough. 	2
I like the paper and the work done by the authors. I think it is valuable for the community (researchers and practitioners). I would just like to have some clarifications about the points I raised in my review. I will change my score accordingly.	3
"1. Interesting approach and impressive results.
2. Need more clarifications for a few points."	2
I agree to accept this paper. The model is novel, the idea is well motivated, and the experiments are compelling.	3
All in all, the paper makes an important contribution to the area of security in machine learning. The proposed approach is able to successfully extract neural architectures and even their parameters, allowing for piracy of such models. Hopefully, this work will foster defense mechanisms in order to prevent such attacks. Therefore, I recommend to accept the paper.	3
"The paper presents an approach to ’steal’ a DNN from a NAS training process. There are strong claims as to the applicability of the approach across all NAS approaches. However, the authors make an awful lot of assumptions as to how NAS works and the sorts of networks they can produce. If you factor in the NAS approaches they are not considering and the fact that the work seems to be focused on CPU training (which is not the norm) then the work has far more limited applicability.

Although the work may work for GPU, there is no discussion in the text of this therefore it is not possible to assess if it would work or not.

As someone who reviews for security venues too, I’d imagine this work would score higher at one of those venues.
"	3
"The approach is interesting but it is hard to understand what aspect is novel compared to other competitive approaches. Perhaps the authors should describe other methods that have been used to solve this task. This would help make it easier for the reader to understand the novel contribution of the paper and also makes the paper more complete by itself. 
"	3
"The model seems nice and the results seem positive, but I think the experiments are weak. We don't really learn much from this paper about the strengths and limitations of the proposed model, and it seems plausible that the experimental results are not statistically robust, due to a combination of the small test set size and the limited experimental reporting that is done in the paper.

---

EDIT: Some of my concerns were addressed, particularly the issues regarding reporting more details on the model's performance (number of runs, variance, etc.) which I think were most critical. I am still not satisfied with the discussion of interpretability (which I think should be removed; see discussion below) and I think the paper could have done a better job with experiments demonstrating the relative strengths and weaknesses of the model, but the results that are present seem strong. So I wouldn't say I'm particularly eager to accept the paper but I won't gatekeep it either. Raising my score from a 5 to 6."	4
"FocalReasoner exhibits a strong experimental result and it is corroborated in the new submission with added experiment results in $LReasoner_{RoBERTa}$ , $LReasoner_{DeBERTa}$ , as well as $FocalReasoner_{DeBERTa}$ in Table 1. 

Albeit its high-performance, as the previous review pointed out, I am not sure what is the major factor in the improvement when many parts of the neural machinery are used together. I view the main contribution as using the Levi graph from dependency trees as  Beck et al. 2018 and connecting them with coreference and entity linking.

Overall, I'm borderline about this paper. I think this paper tests whether fine-grained pieces of information in the paragraph are useful or not (intuitively they should, and this paper shows that it does). However, I am not sure whether the usage graphs are novel or significantly better than previous work. 



"	3
Novel concept of fact units to answer logic-driven questions. More discussions are needed.	3
In summary, this paper applies HDP to CGMM, which is intuitive and theoretically sound. However, the idea of using HDP is not new in unsupervised generative models, and the improvement of iCGMM over CGMM seems insignificant. Thus, I would lean towards a weak reject.	2
The authors propose an unsupervised Bayesian Non-Parametric method for GNN, which learns the GNN structure in an unsupervised fashion. For scaling-up, the authors propose a faster Gibbs Sampling based inference method. Experimental results indicate the strong performance of the method as compared to supervised learning methods. Overall, the direction is very interesting and could inspire future work at the intersection of BNP and Neural Networks. 	4
"This work investigates automatic model size selection without compromising model performance, which is a very important and timely research question. 

However, the proposed solution revolves around a specific architecture that limits its applicability. Also, it does not consider the edge properties of graphs, which are extremely important for different graphs like crystal (CGCNN - Phys. Rev. Lett.). 

More model ablation studies for e.g graph size (or structural complexity like wiener index/diameter/density) vs learned latent variable size are required."	3
Interesting idea but not very impressive experimental results.	3
The manuscript has a certain contribution that it points out potential problems when naively using BNNs for ODD detection. However, I think that this contribution is not enough due to the same reasons I listed as the weaknesses of this study above.	2
"In summary, I found that this paper represents interesting and timely work that should inform future discussion as to how we think about OOD detection and generalization with BNNs.
However, I found the experimental evaluation somewhat lacking, and therefore am only tentatively recommending accept at the moment.
I look forward to reading the other reviews and to hearing the author responses."	3
"A well written paper showing interesting concepts that are mostly well known in the machine learning community. The paper lack novelty.
"	2
The paper proposes to optimize the state-action stationary distribution directly without considering the unstable triple intertwined optimization porblems in actor-critic-based constrained RL algorithms. However, the effectiveness of the proposed method in obtaining a trade-off between reward maximization and constraint satisfaction will be more convincing if 1) more straightforward adaptation of state-of-the-art offline policy learning algorithms are considered as baeslines; 2) more ablation study to analyze the sensitivity of the performance against different cost thresholds.	3
"The paper is well-founded and feels like a ""correct"" solution to this problem, and the empirical results show the method performs well in practice. I"	3
The offline safe RL is a very challenging problem, but this paper provide a very promising approach to address several issues in this setting. Specifically, the approach proposed in this paper not only addresses the policy optimization issue in the behavioral agnostic setting but also guarantees the constraints satisfication, which are both significant contributions.	3
Overall, the paper is well-written and motivated. However, due to the lack of discussion on the coverage of the dataset, I think the paper is below the borderline of acceptance. I would be happy to raise my evaluation if the paper could address my concerns in the main review part.	2
This is a novel approach to a problem that does not have feasible exact solutions, and I think it is an interesting use of neural networks. The proposed approach is relatively light-weight and demonstrates improvement in both run-time and performance quality in most cases. It has been evaluated quite rigorously, further evaluation can improve the quality of the paper, but even as it is, the proposed approach demonstrates non-trivial improvement for real-world scenario datasets, which I believe warrants acceptance of this paper. 	3
"The paper presents a practical and neat idea for RegEx synthesis that can leverage existing synthesis tools as subroutines, based on the proposed neural-split model. However the neural splitter is not technically solid, and there can be more experimental studies for the sake of completeness and the understanding of current limitations. 
"	2
The paper has less technical novelty, weak experimental results, lack of clarity, missing related works and narrow scope in terms of applicability to other programs that are not regex. I will suggest improvement in writing in terms of clarity and a thorough evaluation across other DSLs.	2
Overall, I think that the proposed idea is itself simple, which means it should be easy to use, and at the same time it could potentially be useful for the task. However, I have several comments/questions, which limit my understanding and make the potential impact less clear. Resolving these would greatly strengthen the paper. The issues lie at the intersection of clarity, positioning, and evaluation.	3
The paper is well motivated and the technical part is straightforward to follow. But the actual gain with this wavelet-based compression of feature maps is not convincing, mainly the added complexity is not reflected in the Bits-Operation metric, the results on experiments are not consistently performing well.	2
"Overall the paper's main idea is novel and interesting. Many minor issues make the description of the method harder to understand than necessary. I am recommending weak acceptance, for now, assuming that the most important questions will be answered properly during the rebuttal phase. Depending on the answers I am willing to reconsider my recommendation.
"	3
"The motivation and contribution need to be made clearer. In addition, the experiment part that supports the contribution needs to be thoroughly supplemented. Please address my concerns in ""Main Review"" through the rebuttal process."	2
This paper applies the traditional image compression method to compress the activation map and conduct convolution on the transformation domain. This idea is trivial. I do not find the novelty of this paper.	1
While some design choices made in the study could be clearer, and I would have preferred to have more benchmark environments, the paper is well written and the experiments appear carefully designed. I believe ablative studies like this are very useful for the research community and practitioners alike.	2
"Overall, I think the paper is not good enough to get accepted yet. On one hand, the paper focuses on understanding and comparing different settings of designing predictive models in the literature, which offers the community a better understanding of planning. On the other hand, there remain several concerns. My major concern is that the experiment setting may affect the accuracy of the conclusion provided in the paper. Focusing too much on deterministic environments makes me wonder if the conclusion still holds when there exists stochasticity in the environment. This may cause an unfair comparison between the deterministic model and stochastic model, which is a major part of this work. Furthermore, the overlapping area when showing the 20/80 percentile suggests more than 5 random seeds are needed. Moreover, using different network architectures can affect the learning efficiency thus causing inaccuracy when comparing these models.
"	3
"--- methodology ---

I find the question of the paper interesting, but the methodology doesn’t feel satisfactory to me. A few things appear odd to me:
* Running a stochastic model in a deterministic setting seems like a weird setting for evaluating the benefits of stochastic models. Feels like an unfair comparison. Could the authors try running this on environments that elicit stochasticity, and where this choice might seem more natural? 
* The setup of the paper seems odd: The data collection assumes that we already have a well-trained model-free agent that already does the task we care about...We then learn a dynamics model from this policy and deploy the model-based version. It seems that this setup doesn’t even need a model-based method in the first place. So why should we care about these domains that the authors evaluate on? Plus, the most challenging domain - humanoid - doesn’t seem to lend itself to model analysis in this work.

--- paper narrative & motivation --- 

In some sense, the takeaway of the paper feels negative to me: there isn’t really a consistent way of learning accurate / useful dynamics models, so future researchers might just need to empirically test all the design choices for themselves and see what works best. I wish the work could do more to explore a bit more into what kinds of errors these dynamics models incur, and why these approximation errors are fatal - e.g. humanoid completely fails and so all of its results are omitted, but … why are those model errors that bad compared to other domains? How can this failure case inform us in how to better learn dynamics models? 

There are several areas in the paper that come unmotivated or are not well contextualized. For example, I don’t really understand the dynamics manifold argument in Section 4.5 when the authors say that long time horizon predictions lead to inputs being outside the desired manifold, yet claim that adding noise also “causes the inputs to lie outside the manifold” but the model is more robust to predict solutions returning to the manifold. It seems like noise will only, up to a certain extent, help you in the short run but long horizon settings will still be difficult to predict accurately. 

Why would we consider having a model-based method in the first place? It would have been nice to see explicit sample gains in having model-based methods - e.g. number of data samples saved for these settings [or considering some form of computational budget]. 

The work also seems to assume the best case scenario of having an optimal (model-free) policy which is oftentimes not available. It would have been interesting to investigate the approximation tradeoffs when a well-trained policy is not available as a data source for the to-be-learned dynamics model. 

--- experiments ---

* What does the MSE graph look like for Humanoid? 
* Observation that MSE is not a good predictor of planning performance when using a learned model seems non intuitive and potentially an interesting thing to further investigate. 
E.g. Figure 3: Why is it that for longer horizons, MSE goes down and reward goes down? Since you only execute the first action anyway, how frequently does the plan deviate from what an optimal policy would on average do? 
If you were to enforce the proper physical constraints in the prediction, does that resolve the exploitation problem? Since you have access to the simulator and you’re only predicting observation joints, this feels like a conjecture that can be easily verified. This would also disentangle whether physical constraints are the issue vs. the prediction being wrong at critical states of the trajectory.
* Why did the authors choose to use MPO policy as the base policy? 
* Could the authors elaborate why the multi-step loss helps up to a certain number of steps for those domains? 
* Is state dimensionality the core issue in preventing humanoid from working? Could this hypothesis be testing on 4k+1 observation-space swimmer environment where k varies? 
* It’s impossible to have complete coverage of previous methods, but I feel like this work is missing a key class of dynamics models which are a function of history. E.g. in robotics settings (already cited by the authors), this includes concatenating states or having a recurrent neural network. This helps in settings where the environment is partially observable. Another thing is that it’s common practice to predict the difference in states. Authors should consider including experiments there.
* Related work is tied to robotics (and this is where a good dynamics model is super critical, not the synthetic simulated environments used here), but no robotics-like environment is used for evaluation.
* It’s surprising to see that ball-in-cup can (a) tolerate that much noise and (b) improves with increasing noise. Why is that the case for this domain, and not for the others? Eg. I could imagine Finger-Spin to also somewhat tolerate noise, but the fact that it’s sensitive makes me think that the observation ranges matter a lot in this study. Could the authors elaborate on this? 

Nits: 
* Title is not reflective of the paper goal. Currently reads quite broad - doesn’t indicate that this paper is about understanding the design choices for dynamics models. "	2
This paper has positive and negative sides for me. As mentioned above, I like a comparison/benchmarking effort, which I believe the field indeed needs, and the authors have taken a nice, systematic approach, and written a well-structured paper. On the downside, I kind of doubt whether I learned too much from the results, since 1) the results vary quite a bit over tasks, which I think might be due to the relatively small set of tasks, 2) the results on stochasticity are limited by the set-up, which excludes stochastic environments, and 3) some other results were already partially known (like the multi-step prediction effects and poor performance of MSE). I am therefore a bit in doubt, because the paper does still have its merit in systematically comparing different element of model learning in deterministic control tasks. 	1
I like the key question and the experimental setup of this paper and would recommend an accept. It would have been better had the authors considered another AL method, along with uncertainty sampling to investigate the relative impact of pre-training and the active learning methodology in data savings. 	3
The authors make some interesting and potentially impressive observations regarding the performance of ‘simple’ active learning (i.e., uncertainty sampling) in the context of pre-trained models — showing positive results both on image and text classification problems. Additionally, they show that biasing toward difficult examples for selection may be correlated with being able to ignore spurious features (e.g., background in images). However, the experiments are limited to ‘least confidence’ uncertainty sampling, there is non-negligible missing contextualization wrt related work, and there isn’t a strong justification for the observed performance improvements. Thus, my assessment is that this submission should be rejected in its current form.	2
It's an interesting paper but the authors need more empirical validation to establish some of the claims. See detailed comments in main review.	2
Overall, I think this paper is on track to make a good contribution, but would need a more rounded-out experimental execution. Therefore, I am giving a reject recommendation for now, but I'd be happy to increase my score if the authors provide the requested revision. 	2
The technical results seems non-trivial but the implications are not clear.	2
"The paper has interesting findings on implicit bias theory. While the findings themselves are presented in a rather clear manner, the paper is lacking a sound conclusion especially so since it is ""challenging"" existing beliefs. I would encourage the authors to attempt clarifying their view on how these results are expected to affect further research on implicit bias."	3
"This paper studies an interesting topic and presents a lot of positive and negative results, which could be potentially used by other theoretical works later. But this paper lucks a key result to highlight. I appreciate the authors' efforts in analyzing the various settings in the paper, but I have to vote for weak rejection because of the lack of a key result and all the other weaknesses I mentioned above.

-----------------

I was mainly concerned that this paper lacks a key result to highlight, but after reading the rebuttal, I am now satisfied with the authors' response to this major concern. While my other concerns remain (and the authors did not respond to most of them), I am happy to increase my score to 6, assuming that the authors will add the short conclusion section as promised. I look forward to the next revision of the paper."	3
The paper studies an important question and is quite thorough. However it suffers from being a collection of many rather small results.	3
The presented work is solid. The main concern is with its limited audience in the scope of the conference and potential applications beyond those presented in the paper.	3
While the problem setup is reasonably well-motivated and some of the empirical results are interesting, it is not clear to me how practically relevant the empirical results are for the problems being studied. The very general framework is also only discussed in the restricted case of linear system solving for PDEs. As a result I tend to lean against acceptance.	2
"The paper proposes a general framework for efficiently finding solutions to numerical problems, but only evaluates the framework on PDE problems. Furthermore, additional tricks, such as data augmentations and using the previous two timesteps of the solution, are required to make the method work well empirically.

I’m not very familiar with meta-learning or PDE solvers, so I’m not very confident in my assessment.
"	3
The paper introduced a new terminology and is more like a perspective paper, instead of a comprehensive research article.	2
The paper does not properly discuss, compare to, and improve upon related work that also uses OOD scores of the form $\frac{\partial}{\partial \ \cdot} \log p_{\theta}(\mathbf{y} | \mathbf{x})$.	1
This submission lacks a proper justification for the designed method and lacks an appropriate comparison to related works. These issues signal a rejection.	1
This is an interesting topic and I hope to see more discussions of the proposed method. Currently I am not 100% convinced by the assumption and I wonder whether there are even better solutions. 	2
I recommend rejecting this paper, mainly because there are severe flaws in its evaluation.	2
"In summary, the paper proposes a nice combination of implicit and explicit regularization for matrix completion using a deep matrix factorization and penalty on the DE of the rows and columns. The results are compelling, but some of the choices made are not fully justified or analyzed.
"	3
Overall, based on the detailed description above, the main contribution of this paper seems limited as the gradient flow of deep matrix factorization has already been studied. Also, the form of the adaptive regularize is not clear and the main results lack sufficient explanation. 	3
Overall, while the proposed work shows empirical promise, it seems to be an incremental improvement over previous work. The readability of the manuscript can also be significantly improved. Based on these factors, my current score is a 5. 	2
This paper presents interesting results in both theory and experiments. Howeever the presentation of the paper is too confused for publication. Too many important details are in the appendix.	3
Overall, there are certain merits for the proposed architectures, but the paper is hard to read due to some imprecise math, lack of clarity, proofs, etc. I initially suggest rejection.	2
Overall, I think this paper is not ready to be accepted at this time. 	2
The main claim of the paper is that there is no information lost in the representations learned. As far as I can tell, this claim is not supported by the theory or experiments. Moreover, it does not seem that the learned representations are equivariant.	2
The proposed transformation has the flaws outlined above which might prevent generalization across local environments. More experiments are required to judge how severe these issues are in practice. Therefore, I can not recommend acceptance at this stage.	2
"The proposed approach is significant and relevant in open world object detection, proposing an end-to-end approach that learns incrementally and can detect unknown objects. A key promise of the proposed approach over prior work seems to be the improved ability in detecting unknown objects. However, I have concern regarding the main results reported (see weakness 1), and some other questions. I am willing to raise my rating if the authors can address these.

**[Edit after discussion period: ]** I thank the authors and the other reviewers for active engagement in the discussion. Important related work on zero shot learning was not cited in the preliminary version, which the authors included later on. While there is one point of concern in the paper claims (see below), I think this work provided a solid contribution to open-world detection problem. Hence I would like to raise my rating assuming the authors can make the necessary updates to fix the concern below.

* It was brought to notice by a reviewer the discrepancy between the problem definition in Section 3.1 and the evaluation protocol. While the problem is defined to involve ""human user.. annotate (unknown instances) and return back to the model"" at each stage, the evaluation protocol involves a different training set involving the unknown instances at each stage. I believe both settings are equally relevant for open-world detection problem, and I would encourage the authors to update the claims to match with the actual evaluation protocol."	2
While the proposed semantic topology is simple yet effective, outperforming the state of the art, this work has marginal novelty, clear limitations, and missing analyses. I'm on the borderline, slightly leaning toward rejection. I will make a final decision based on the authors' rebuttal responding to my critiques. 	2
"This paper describes a nice way of leveraging knowledge about various visual concepts, distilled in modern language models, to improve over incremental learning method by [A]. 

However, overall I, unfortunately, need to point out that the proposed (valuable and effective!) extension of ORE is almost exactly what was proposed before in the scope of zero-shot object detection [C], just leveraging the most recent developments in natural language processing. In case my interpretation is wrong I would like to see a very thorough discussion in relation to the aforementioned (that should already have been in the original manuscript), with a focus on what exactly the difference is.  


I still think this paper still carries an interesting message for the community, but (i) link to prior work on ZSL must be clearly established, and (ii) I do not think that ICLR is the right venue for this work in terms of novelty.  


**Post rebuttal** 


I would like to thank the authors for their comments and colleagues for the discussion. First, I would like to clarify that the paper was updated to include a discussion on the closely related topic of ZSL. I take back my comments that the usage for semantic anchors reduces this problem from open-world detection to zero-shot detection. Those were based on my initial miss-conception on how semantic anchors were used. 

However, I still think that the evaluation protocol studies only incremental learning and provides no evidence whether the proposed method can be actually applied to the (significantly more challenging!) problem of open-world detection, and authors actually do acknowledge that in one of their responses. 

As a reviewer, I cannot recommend accepting a paper that (in my view) has a mismatch between paper claims and premise (open world detection) and the actual delivery (incremental learning). My colleagues do not find this point as concerning (esp. given the fact that this issue was inherited from the prior work), therefore I will not strongly argue against acceptance, but I nevertheless wanted to bring this issue to AC's attention. 
"	2
Overall, I think the paper presents a solid contribution to the task of open-world object detection. Some aspects of the paper need more clarification and some related work is missing that needs to be discussed.	3
"The novelty of the proposed method is limited, since IaBN is an existing method and the method on one-sample adaptation is straight-forward. Although the effectiveness of the proposed method regarding performance was shown by the experiments, the required processing time was too much. From these points, this paper is regarded as being around the border.
"	2
I think the overall approach is interesting, but ad-hoc in its design. It would be great to see some comparisons to existing pseudo-labeling approaches as I think that the setting with only a single image from the target domain artificial.	3
Overall, the paper has some merits (e.g., good writing, promising results). However, the technical contributions seems to be minor. I am concerning that the paper may be still under the level of ICLR.	3
This paper presents a new model and pipeline for domain generalization in semantic segmentation. Although this work shows better results than previous work. Lacking of novelty and insights is the main drawback of this work. Besides, the experiments can be improved, that more comparisons with previous work is helpful. Therefore, the reviewer rates this paper marginally below the acceptance threshold. 	2
The addressed problem is interesting and the proposed model is elegant but I think that the experimental part is too weak to assess the quality of the proposed approach and that the novelty is not sufficient wrt previous works.  	2
This work seems very promising but the interest of the invariant representation is not clearly demonstrated according to me. Some experiments and critical information are missing so that I have to reject this paper. I strongly encourage the author to write & submit an improved version of this article in another conference.	3
The paper studies a relatively new topic and the design contains new ideas in the specific area. However, some details of the model are not presented clearly and the experimental design could be improved. 	3
"Overall, the proposed method claims to be designed for environments that are dynamically changing over time, but actually assumes the latent variable to be fixed throughout the entire episode (not just locally for a small part of the episode). As such it contradicts the main claim.

Let me know if my understanding of the work is not accurate and I will be happy to reconsider the score."	2
"I think the idea presented in the paper is great and well-motivated. The approach is elegant and the empirical results are compelling. Most of my concerns are in--important--missing details and details that are swept in the appendix, which I think can be easily fixed with some rewriting. For this reason, I will give a marginal acceptance. I am more than happy to increase my score if the authors address my concerns. 
Also, if I misunderstood/missed something please let me know!"	2
While I do believe that the paper has some merits, the technical contribution appears limited and I am unable to recommend an accept. It would be helpful if the authors clarify differences to existing models, especially KVAE and switching state-space models.	1
This paper provides plausible and moderate extensions to the existing model to solve problems in a new setting. However, some necessary clarifications and important baselines are missing. Therefore, I'd like to put it borderline and am willing to adjust my scores if more information from the rebuttal and discussion phases is provided.	3
The paper is well-written and presents an interesting approach to solving TSP that can outperform some other existing approaches. The structure is appropriate, there is a very good review of related works, good description of the method, experiments, and results. There are also extensive supplementary materials. The introduced method is novel, might be significant, and the quality of this article seems to be on-par with other papers applying ML techniques to solve TSP published at top-tier conferences (which are also cited in this paper). The only weakness I see is the way of presenting the results in Fig. 3. All the percentages of optimally solved problems are relatively low, so the plots for some algorithms are not clearly visible (however, it is clear that the introduced algorithm outperforms other approaches). I recommend acceptance of this article.	3
In my view, the submission fails to compare against the large body of existing literature for the TSP problem and ignores standard benchmark sets. I do not share the submissions view that the Concorde solver and various heuristics are geared towards finding optimum solutions only. I would also like to point out that there are algorithms that do provide quality guarantees. That being said, I am not sure what ML brings to the table here.	3
"I recommend that this paper is currently **marginally below the acceptance threshold**. Overall,  I found the methodological ideas (learning-guided metaheuristics + GNNs on line graphs to respect the symmetry of combinatorial problems) to be both novel and interesting. However, I have major issues with the **lack of extensive experimental evaluation**. In particular, the experiments are insufficient to convince the reader that the proposed methodology leads to significant empirical improvements over recent works, or can generalize to real-time routing problems beyond the TSP. It is also difficult to contextualize this paper’s results as the evaluation setup is not conventional and comparissons to classical baselines like Concorde and LKH3 are absent. While the new setup may be a step in a positive direction, additional results **comparing to the latest published work** [6,7] in a conventional format or generalization to real-world and **larger scale TSPLib instances** would be needed for me to be convinced to improve my score. If the major motivation for the work is to tackle large-scale routing problems in real-time, demonstrating that the proposed techniques work on TSPs larger than just 100 cities and on non-random distributions is also important.

---

# Update after rebuttal

I appreciate the updates to the evaluation setup and the reported results. I also appreciate the inclusion of TSPLib experiments and discussions on recent literature [5,6,7,9]. Most of my concerns are addressed, and I am happy to **raise my score to an accept** after the authors provided clarifications to the newly introduced changes (discussed below in the comments).

I believe this work is **bringing new methodological ideas** to the literature on learning-driven combinatorial optimization and will be of interest to the community. However, I am still lukewarm because the motivation of the work is to tackle real-time routing problems with non-standard constraints, but the **empirical results are only shown for TSP**. In the paper's present state, it is not clear to me whether this approach will generalize to other routing problems, especially those problems with more challenging constraints for which generating labelled training datasets is an issue."	3
The contribution of the paper is significant and somewhat new with insights into the TSP problem by GNN.	3
"Although the paper shows some improvements over several learning-based methods on small TSP instances, more comparison between the proposed method and effective traditional algorithms on large-scale TSP instances is required in order to support the claim of the paper.
"	2
Considering the lack of originality, inadequate match to the theme of this conference and lack of rigorousness in experimental procedures, I will have to recommend a rejection for this paper. 	2
"Overall, the idea is novel and neat, and the discussion of the complexity/challenge and motivation are well presented. My main concern is about the experimental results, as they are not as good as expected, especially the Walker 2D example. 

It might be because of the implementation details or other reasons, but the idea seems promising. 

Therefore, my overall recommendation is towards accept. "	3
The proposed method is not techincally novel enought and the experiment result does not fully support the effectiveness of the proposed method.	2
The paper seems like it has promise in terms of the core idea (i.e. throw out low correlations between certain parameters), but does not execute properly on showing where or why this modified CMA-ES variant is needed (outside of just BBOB benchmarking). I would suggest that the authors think strongly about this question, and strengthen the motivation more.	2
This paper proposes a non-linear extension of subspace identification and model predictive control. The idea is interesting and theoretically sound. I like this study. The case study using a simple non-linear system is not convincing enough. 	3
"It is not technically correct to assume obtaining an observer, i.e., a dynamical system, from a static DNN mapping. The technical point of robustness w.r.t. changes in the state initial conditions is at the core of observers theory and cannot be ignored. 
Theoretically the paper empty, and the empirical validation is too simple."	1
Overall, the paper has some interesting results yet lacks technical and experimental arguments to justify the benefits of the approach.	2
I think this paper has some interesting points, but it could be improved by making the motivation clear. 	2
The core idea of the paper is interesting and novel. I am concerned by the limited experiments, but this does not cause me to doubt the author's main claims. I lean very slightly towards accept but I think the paper might be better suited to a workshop.	4
The paper is well-motivated, but many details are not introduced clearly, and experimental results are not convincing. Some claims are not supported by their experimental results. Thus, I think the paper should be rejected.	2
The paper is well written and the ideas are novel. The main strength of the paper is the general applicability of the attack. However, as mentioned in the previous section, my main concern comes from how the method would perform in the presence of object detection and anomaly detection. 	3
"The paper looks good. Some aspects of the presentation can be improved. Some of the results rely on approximations or numerical evidence which might not be universal, but I find the method quite interesting, and it paves the road to new possibilities in deep active learning.
"	3
The theory in this paper appears good, though I am not very familiar with NTK analyses which makes me hesitate to say more. The empirical evaluation lacks quality however.	3
I found the paper's experimental section to be insufficient. 	4
"A paper that continues the investigation of active learning using gradient/influence function signal (Ash et al., 2020; Liu et al., 2021). It builts a theoretical bridge between these objectives with the concepts of training dynamics and generalization, and supplied a algorithm that is practically effective. I personally find this work to be a nice value add to the community, both in terms of the theoretical contribution and the practical utility of the algorithm. Hence recommend accept.
"	3
"**Recommendation**

I like aspects of this paper, but there are some major outstanding questions. Currently, it does not meet the bar for acceptance."	3
"I think this paper offers a fresh perspective on NLL loss failures as I mentioned in my main review. However, their only support for this a contrived toy example over a single run. The paper would be much stronger if:
- The author(s) could apply their analysis to identify either of the two conditions from section 3 in a real world data set.
- Their proposed methods offered comparable (using simpler methods) or superior performance to related works.
Unfortunately, without either of these, the paper's potential significance remains limited."	3
"The authors provide a very nice discussion and analysis of the known phenomenon of variance weighting arising in SGD on NLL loss of Gaussian likelihoods. While I judge this to be an interesting contribution on its own, I would like to see an improved experimental evaluation. Indeed, I am not yet fully convinced that the proposed $\beta$-NLL is effective. In its current state I rate the submission as borderline but I will raise my score if the authors improve upon the mentioned points during the rebuttal.

------------------------------

**Update after rebuttal:**
I thank the authors for their effort to improve the experimental evaluation. I raise my score to 6."	3
Overall its a weak accept. Authors should improve the automation in choosing appropriate stimuli for the training, validation and test sets as well as optimal parameter selection and perform a systematic analysis of compression possibilities of the learning-based models with error control.	3
"The paper effectively shows that a tiny GRU network can reproduce a few time-series generated from a larger simulation. The general research direction of building reduced-order models for such simulations is interesting, but the very limited empirical results presented here and lack of details about the simulation make it impossible for me to recommend acceptance.
"	1
"This is a solid paper that addresses a question which will be of
interest to part of the ICLR community. The research is somewhat
limited in scope, but very well executed and written up, and overall a
worthy contribution. It is also a good demonstration of the usefulness
of open databases of neuroscience models and data.

My recommendation is to **accept**."	2
"The paper presents a novel idea consisting in using an auto-regressive approach to estimate the condition marginal densities which allows the to use a more efficient application of the Knothe-Rosenblatt optimal transport (OT). This is new and interesting.
However, the comparison both in terms expressiveness and experiments with respect to other OT is rather limited and should be extended.
The experimental setup is not completely clear and thus not clearly convincing.
Some benchmarks in vision are not considered.
There is no theoretical results supporting the paper.

Overall, there is the beginning of an interesting idea but that is not enough developed for ICLR."	2
It seems that the contribution is quite incremental. The usage of optimal transport-based approaches for domain adaptation is not new. The proposed OT-based approach is not well-motivated, its advantage is not cleared yet (although it has better performances, there are no discussions why such results are expected)	2
See comments above. 	2
I consider the paper is a methodology paper, so I believe that more experiments are needed e.g. comparison with deep DA methods including OT-based deep DA, computational speed comparison between these methods. Also, some discussion is needed as mentioned in the review part.	2
While I enjoyed reading the paper and liked how the authors operationalized modularity, I don't think the paper is ready for publication yet as the results and experiment don't seem to add value or strongly support the practical utility of the proposed metrics and means to quantify them in studying DNNs.	2
The approach provided in this work is nice (albeit somewhat simple or even straightforward) and provides a way to cluster together neurons that have some redundancy in their captured information, and therefore distil important information pathways or features in the network. The results are intriguing, but in my opinion do not lead to a clear conclusion, and I don't see how to leverage them towards better understanding of neural networks, their properties, or their design. As such, the impact and significance of this work are rather minor. Most importantly, while the paper claims to establish some quantitative approach to measure modularity in neural networks, the results mostly establish the redundancy of information without relating it in a clear way to modularity per se. Therefore, I believe this work is unfortunately slightly below the bar for acceptance to ICLR.	2
"Detecting modularity in networks is surely an important and timely problem in deep learning. The authors proposal to address it contains some interesting elements of novelty, but I am uncertain that the method really measures modularity and I believe that the arbitrariness and the many heuristics involved in the definition of modularity make the methodology difficult to trust, reproduce, or extend. 
"	2
A good paper to explore the explainability of neuron networks with graph clustering methods. 	3
The proposed method seems original, but in my view the theoretical analysis and experimental support are not strong enough and need improvement. 	3
An intellectually stimulating take on network pruning, with possible applications elsewhere.	2
The paper proposes a regularization loss that penalizes L1 distances between consecutive feature maps that are projected to the same dimension. This loss leads to smooth features and this characteristic is unclear to the target of classification.	4
"This is a well written paper with an interesting result on the use of DDPMs for representation learning.  This is particularly interesting because there is widespread interest in diffusion models at present, and the question of DDPMs for representation learning has not been addressed to the best of my knowledge.  That said, the approach taken in the paper is relatively straightforward, and only one downstream task is considered, albeit with extensive baselines.  It would have been great to see results on another task.

I also want to thanks for the reviewers for their extensive responses to questions raised by the reviewers. Most of my questions have been addressed, and the new results reported support the results reported in the original submission.  I continue to believe that this paper is above the bar and would make a good addition to the conference.  My rating of 6 should be interpreted as 6+ or 7."	3
The paper presents a new use of DDPMs as unsupervised representation learners. Although I see the small-scale experiments as more exploratory than competitive, it contributes to our understanding of an increasingly popular modeling approach.	3
"The paper certainly draws a very interesting question, ""Can DDPM serve as a representation learner for high-level computer vision tasks?"", and provides insightful analyses and experiments.
However, I have a concern if the conclusions drawn from the provided small-scale experiment can be valid for a large-scale/general setup.
For now, I want to be a bit conservative in the rating and possibly increase based on the discussion and other reviews. 

----

**After the author response**:
The rebuttal resolved my main concerns (small-scale experiments). The other reviewers also claimed valid concerns, and they seem resolved/answered well (providing ablation studies, more details, and justifications). Thus, I raise my rating to Accept."	3
"I believe this paper made some interesting observations towards understanding and mitigating catastrophic overfitting in adversarial training. However, I am not fully convinced as the support analyses cannot provide sufficient evidence of why the proposed method could work. I am willing to increase my score if the authors can better clarify the proposed method and provide more intuition.
"	3
"In all, although the method proposed in this paper beats the baselines, it may not be a fair comparison and there are some fundamental errors in the experimental settings.

Post-rebuttal adjustment:

Thanks for the additional clarification. I decide to increase my score by a point due to the authors' effortful response.

I did not increase the score to 6 since I am not fully convinced on why noise augmentation + with adequate strength and without clipping is a principled solution to improve the effectiveness of fast adversarial training. I did not penalize its simplicity (this is fine to me), however, its effectiveness should be further justified either empirically or theoretically, e.g., whether or not the proposed design is applicable to fast adversarial training using TRADES-type loss, or other similar and in-depth studies."	2
"Overall, I vote for accepting. This paper find that not clipping the perturbation around the clean sample and using a stronger noise is highly effective in avoiding CO for large perturbation radii, which is interesting. Based on this, it proposes a simple but effective method called N-FGSM, which achieves the SOTA results while achieving a 3x speed-up. One major concern is about the theoretical analysis on the observations. Hopefully the author can address my concern in the rebuttal period.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
UPDATE

I have carafully read your response. My main concern has not been well addressed. The main drawback of this paper is the lack of theoretical analysis. The reason why it works is not clear enough. I believe that the paper may have potential but as its current form has some weakness. I turn to the rating of marginally above the acceptance threshold.

By the way, the performance improvement refers to accuracy rather than efficiency.

"	3
To summary, this paper is more like an experiments report instead of a research paper. The contribution is limited, and the evaluation metric seems to be unfair. I suggest a clear rejection for this paper. 	1
It's a model architecture paper for neural abstractive text summarization. It works super well on many widely used datasets. Despite the empirical success, it lacks more principled insights and justification about what works behind it. 	2
The top-down and bottom-up framework proposed in this paper achieves good performance on long document summarization tasks. However, despite the results, the novelty of this design is somehow limited since the bottom-up and top-down idea is not new. The experiment part of this paper is also not clear, some details are missing, and some issues need to resolve (please see the weakness part).	2
"1、It is recommended to explain the specific calculation process of the model in detail, especially for the top-down inference process;
2、It is recommended to further verify the computational efficiency and memory usage of this method through experiments."	2
"I like the premise of this paper and it is clear that this is a work of depth and quality. However, I feel there are some details missing that I would hope can be fixed to make the work clearer and to address some of its weaknesses. 
If the authors can address the points I made about the paper (probably the first is hard to do now given the narrative of the manuscript, but the rest may be feasible) and the method is clarified to have clearly identifiable empirical benefits over the baselines in Sec. A, I would be happy to revise my opinion positively and strongly support publication.

Update post-review:
I thank the authors for their measured responses. My sense is this work could still benefit from some more iterations, such as the disconnected gradient and the somewhat dissatisfactory empirical comparisons. As such, while I still like the paper, I cannot clearly recommend acceptance because I would have these questions open as it stands. I suggest working on fixing these aspects to have a more complete version of this manuscript and hope to see it again as a reviewer when that is the case so I can strongly support it."	3
"Overall, I like the approach of the paper, and think there is potential. Therefore, although I am borderline, I am leaning towards accept. My main review outlines my biggest issues (especially points 1-4), these tend to be about the practical implementation not aligning with what I view as the biggest benefits of the theory (and the biggest differences to past works), as well as questions about experimental results.

------------
Post-rebuttal: Please see discussion. I am sticking with my score for now, despite some of my initial concerns remaining (and additional valide points raised by Reviewer rF5E). "	3
Given the technical quality of the paper, I am leaning toward recommending acceptance of the paper. However, given some concerns around novelty and empirical evaluations detailed in my review, I am disinclined to recommend a clear acceptance.	2
In summary, I raise several concerns about the theoretical soundness of the proposed method, which currently prevents me from rating this paper higher. I will consider raising my evaluation if the authors address my concerns or point out my misunderstandings.	2
I suggest substantially revising the paper and submitting to another venue. 	2
Though the numerical exeriments show that the proposed model SALT marginally improves over the sota deep learning model SAINT, it barely beats the tree-based LightGBM model. The architectural choices of SALT have no motivation/justification. 	2
"The proposed SALT, in my view, should be technically correct. 
But I think the novelty is limit. 
Consequently, although both the problem and the method is okay, this manuscript is a borderline work.
I recommend rejecting it."	2
Overall, I feel it is quite difficult to understand the potential impact of this paper. The idea of sharing attention matrices looks interesting but there are some questions about the scalability of the proposed approach and if it can work on any tabular datasets. 	2
The paper proposes a centroid-based approximation to evaluating the likelihood function of latent models -- a rather incremental approach. The main problem is that the final choice of the clustering procedure -- which is the crux of this paper -- is not clearly described. The authors describe more in detail other appoaches -- like KD-trees -- that fail, whereas for the actual choice -- balanced binary tree?-- we know little to nothing. Moreover, the clustering approach itself, and the need for the clusters to be balanced, is one that I have objections based on reasons described before. Under the assumptions that we use O(long) balanced clusters, then (n/logn) nodes will fall under the same cluster. Hence, almost all nodes! Maybe then the relevant term that gives this O(N^2) burden is not relevant to optimize? For these reasons, and despite the claimed superior performance in downstream tasks, I am not in favor of recommending this paper for acceptance in its current form.	2
Interesting paper, but lots of decisions/choices that appear arbitrary, insufficiently motivated/justified, and over-engineered. 	3
Overall, this paper seems unfit for publication. The novelty is limited, and the methods of validation/ experiments do not make a strong case for the usefulness of such a data processing piepline.	2
Although the proposed method performs well in experiments under the low embedding dimension setting, more explorations on different settings are expected to understand this model comprehensively. Other concerns are listed above in weaknesses.	2
Overall, it appears that this paper presents a simple method in an interesting direction, and can server as a useful baseline for further research.	3
"Overall, the paper is well-written and outlines good directions for future work. The motivations are described clearly and thoughtfully. However, as is, I do not think it represents a significant enough contribution. The approach described is already present in most systems that use a large pre-trained model to provide embeddings for the input. That is, it's common for most existing approaches to take an off-the-shelf model, freeze the weights, and then use the produced embeddings for some down-stream task (see for example [2]). Finally, I'm not sure if the proposed solution can properly be called ""continual learning"" since it's unclear how it should scale with the addition of new tasks."	2
"### Original Summary
This paper proposes a simple and effective method for a unique problem setting. On the other hand, the novelty of the method is much less than the authors may think, and a similar method is not cited. The rationality of the design of the proposed method is also questionable, and its versatility has not been experimentally demonstrated. The reviewer recommends that the authors expand the experiments and resubmit the revised paper to another international conference.

### Final Rating
The reviewer has read the other reviews, the responses from the authors, and the revised manuscript. The first score (3) has been upgraded a little to (5), but the reviewer still leans to reject the paper. The reasons are described in the last post of this thread."	2
"I think the idea of the difference vector is similar to some previous work. For the experiment, the data is somewhat constructed and simple for the method to find the relative object or description. However, It does not show that if the method can also be applied to the more complex case, which may be more common in the real life. And the comparison is not so sufficient. The author may compare with more few-shot methods to support their contributions. 

In general, I think the paper still needs to be improved and I recommend rejecting it this time."	2
"The papers attempts to develop a theory to simultaneously address heterophily and oversmoothing issues in graph neural networks, which is a plus. However, despite making many simplifying assumptions, the ""theoretical"" developments are sloppy and hard to follow. As such, I have NOT confidence on the theoretical results obtained in the paper. I have NO confidence in the theoretical results obtained in the paper."	2
"Strength:
+ Comprehensive summary on existing literature
+ Good paper structure

Weakness:
- Novelty needs to be justified
- Evaluation results need to be further discussed
- Missing scalability analysis"	3
"This approach to study this topic definitely deserves the attention of the community. The paper is overall well-written. I do not recommend full acceptance at the current point because I think it is not fair scientific practice to omit recent and very closely related work altogether.

[Note] I reviewed the paper before. While the previous version of the paper was basically the same in the technical and experimental contents, the presentation was lacking clarity a lot. Great parts have been rewritten now, and I appreciate the efforts of the authors in addressing these issues. Since there was already a lot of discussion of various aspects of the approach, I only have few comments above. I am fairly confident about the paper itself, but I did not verify the proofs in the appendix in more detail.

"	3
"Overall, I recommend rejecting this paper. It is actually exciting to see a paper that tries to link the two widely existing problems of GCNs together theoretically. However, although the theorems presented in the paper are actually correct, they don't support the conclusion that the authors claim.

After reading the other reviews, I would choose to maintain my opinion, and the lack of innovation is my primary concern."	3
"The paper provides a simple method to reduce (modestly) the size of an already trained language model, without the need for costly re-training. The evidence in favor of the method is empirical, with little in terms of theoretical or experimental exploration of the motivating phenomenon: the overlap between ""important parameters"" and ""parameters poorly captured by SVD""."	2
"The proposed idea is interesting and seems to lead to improvements over truncated SVD, however these improvements 
are not major. Moreover, the proposed algorithm is less general than truncated SVD."	2
"The paper introduces a new approach to weighting the low-rank compression of neural nets, and empirically demonstrates that it outperforms other methods. The approach isn't too surprising, but it does appear to be novel (though I would recommend doing a more thorough lit review and make the related work section larger, since I feel certain that there are many more related papers doing similar things (I mention one in the main review). I think the experiments are overall good and demonstrate the point nicely, which is essentially that the important thing in compressing networks is not the parameter compression -- but rather, the compression of the function specifically on the relevant distribution. Overall I vote to accept.

UPDATE:
I read the comments, my opinion is unchanged. "	3
"I like the direction this paper is going: combining visual and proprioceptive information to train RL agents for quadrupedal locomotion. I also love that the authors include real-world demos of the learned policy on a physical robot. However, my main concern is that, although there are extensive evaluations in the simulation, the current set of real-world examples may not be sufficient to show the benefit of visual inputs. Examples like climbing stairs or scenarios that require more precise footstep planning would make the paper much stronger.

I'm generally excited about the progress in this direction, thus I'm currently leaning towards the acceptance side, but I hope the authors can address the issues mentioned above."	3
The authors proposed to use transformer architecture to solve visual locomotion + navigation tasks. The proposed approach is compared with a few end-to-end trained baselines including HRL and has demonstrated advantages. The authors also deployed the trained policy successfully to the real robot. 	2
"Overall this paper is written well and has a sound idea that is supported well by an extensive evaluation. There are some minor details that may further improve the quality of the paper but I see this as a strong submission which I can recommend for acceptance.
"	3
"Good paper on relevance and experimental evidence, on a topic that is very much of interest to the robot learning community today.
Novelty limited due to combination of known techniques.
EDIT: bumped confidence to a 5 based on comments and rebuttal. This is a solid contribution."	3
Though the direction that this paper is heading towards is interesting, the framework only works on a toy case. The experiments are very weak. This paper does not reach of the bar of the conference.	2
The paper has a variety of issues which do not (in my opinion) make it fit for presentation at this time.	2
As it stands currently, my feeling is that the evidence for the contribution and improvement of the proposed method over prior work is lacking.	2
"Overall, the paper describes a method which uses a coordinate-based network and ODE solver in conjunction to learn physical parameters (and their initial conditions) from a single video. This method is shown to outperform the “baseline” significantly, and seems to generate good results in terms of physical parameter estimation and video synthesis. Thus, I believe that this contribution is impactful, as I haven’t seen coordinate-based networks used in this way before. However, the amount of evaluation seems limited (only on the case of a pendulum), the comparisons to competing methods seem limited, and it’s not immediately clear to me why the coordinate-based network needs to be used for learning from a single video. With some clarification of this, I would be happy to raise my score, but based on my limited understanding I believe that the paper is borderline or slightly above.

**Update after Author Response**:
See response comment, I have chosen to retain my score."	4
The proposed approach is novel and interesting. But there need to be more experiments to validate the approach.	3
"This paper proposes a novel approach for neural multi-objective combinatorial optimization. The novelty of the proposed approach is limited as it reuses several ideas from single-objective neural combinatorial optimization. However, it is technically sound and exhibits a very competitive performance in a broad range of problems. The description of the method is hard to follow but, in general, the paper is well-written. The use of the term ""preference"" is confusing, so I would recommend the authors explain more clearly that their approach does not incorporate preference information."	3
"The paper is well-written and presents an interesting approach to solving MOCO problems. The structure is appropriate, there is a very good review of related works, clear problem formulation, a good description of the method, experiments, and results. There are also extensive supplementary materials. The introduced method is novel, might be significant, and the quality of this article seems to be on-par with other papers applying ML techniques to solve TSP published at top-tier conferences (which are also cited in this paper). I don't see significant weaknesses. There are some minor typos (e.g., ""a exceptionally"" -> ""an exceptionally"", p. 4), so I recommend revising the paper before the final publication, but from the methodological point of view, the paper seems to be good enough to be accepted."	3
I feel this is an interesting contribution concerning a difficult and relevant problem, whose well-made practical part makes up for a more limited theoretical study. 	4
In general, this paper is novel and sound, and the experiment results are convincing. I am suggesting a borderline accept of this paper, and it will be better if the authors could update the theoretical part of this paper and rearrange some materials to make this paper self-contained.	3
I think this is a fine paper, well executed, and with decent results. However, I think the novelty is minimal compared to prior work that introduced the key methods that are used, and the empirical gains are also not that large. I therefore think this paper is slightly below the threshold of typical ICLR papers in terms of value to the community, although it is above the threshold in terms of sound science.	2
I enjoyed reading the paper and feel it has value to the community. The experimental study justifies the usefulness of the paper.	3
"This paper is well written and experiments are done thoroughly. The main concern is the novelty as it combines existing techniques without substantial findings.
"	3
This paper proposes to combine knowledge distillation and contrastive learning for distillation tasks, which is also well-motivated. Extensive experiments on benchmarks validate its effectiveness. It's a nice paper with solid, but somewhat incremental, technical contributions. 	3
see above. 	2
The authors shows that the proposed method performs better under their proposed training regime but don't provide any supporting evidence for this training regime. This raises some concerns about the significance of the empirical results, and the usefulness of the proposed regime. These concerns prevent me from recommending acceptance.	2
Overall, though the proposed algorithm seems like an incremental change over existing ones, it clearly is preferable to use over existing methods that simply fine-tune on the online data. I believe that the realization that using the same training objective for both offline and online data yields overly pessimistic policies is an important contribution, and opens the door for future work that considers different objectives for offline and online data as this paper does. Because of this, I recommend that the paper be accepted. 	2
The problem the paper tackles is interesting, offline RL with limited online interactions. I think this is a nice direction of research that merits further investigation. The proposed approach makes intuitive sense to me and the algorithm, CGQL, has good performance on the benchmark environments. I do have a number of clarification questions and the paper would require some editing to make these clear. Overall, I am currently leaning towards acceptance.	3
Great paper connecting causality and adversarial robustness	4
"The paper undertakes an original approach to studying the important problem of adversarial vulnerability. However, the description of the method, its design choices, and evaluation requires improvement. That said, the results are quite encouraging and authors should provide more evidence for their hypothesis on reasons for adversarial vulnerability and test their method against stronger baselines.

---
After the response

Most of my concerns have been adequately addressed. The description of the method needs further clarification. I would encourage authors to clarify the choices made in Appendix A. Given that the approach to adversarial robustness is novel, empirically useful, and potentially will inspire further work, I have substantially improved the score and leaning towards a recommendation to Accept."	3
I enjoy reading the analysis of the paper. This new perspective is novel to me and I would tend to accept this paper. If the author can address my concerns on the experiments. I would be more convinced.	4
Given the strengths listed, I tend to accept this paper. I suggest the authors concern the questions listed to be addressed.	3
I feel not fully confident to assess the quality of the work as I am non-expert in federated learning.	3
This paper is well-written and provides extensive experiments which show the efficiency of PrivHFL and accuracy gains compared to a baseline, which uses models trained on the local datasets. Some aspects of the dataset expansion method and  the secure querying protocol are unclear, which require further clarifications. 	2
The paper shows much better performance (e.g., much faster execution of the private inference) but this is at the cost of lowering the security level. Additionally, the aggregation through logits makes the method more vulnerable to attacks than other proposed protocols, such as, CaPC. MixUp and Active Learning methods were already used in federated learning (FedMix, CaPC) so this is cannot be classified as a technical novelty.	2
The motivation of investigating the effect of modern architecture Transformer, without using spatial convolution or pooling, on image generation is interesting and valuable. Transformer-like architectures have shown remarkable performance on a wide range of tasks, including image recognition, while exploring it on GANs for image generation is a novel and challenging direction. The current framework (without significant modification to the original ViT) has shown promising results compared to CNN benchmarks. On the other hand, the paper can to be improved by providing more discussion and analysis about the relation between locality that convolution intrinsically has and global context that self-attention captures. 	3
"I recommend the paper would be worthy with the rating 6.
If all my concerns could be well-addressed, a score of 8 is deserved. And this works would be one of best ViT+GAN proposal."	3
"Concerns about the method and experiment results have been presented in Weaknesses. 
I think the authors need more important experiments to support the advantages of ViT-based GAN over CNN-based GAN. "	3
"This work is an empirical study and of interest to the community. The novelty of the idea and the technical contributions are somewhat limited.  As for empirical insights, there are several interesting findings, but one of the main contributions (ISN) needs to be compared to a stronger baseline, ie., a sweep over gamma values. Further, the comparisons to other baselines are currently incomplete, and the limitations will need more discussion. I rate this paper at 5, but I am happy to move up the score once the raised issues are addressed.
"	2
The paper is overall well written and presents a solid study of using ViT models to build image GANs. However, the lack of computation/model size statistics may lead to unfair comparison; the novelty might be limited since most of the techniques are already proposed. I would like to hear from the authors for the final decision.	2
"See ""Main Review"" section for more details."	1
"The first couple of paragraph (before ""Detail"") of the review above offer a summary."	1
Overall, it is a good starting point to explore the vision transformers in the medical imaging task. However, some preliminary conclusions are not well-supported. What's more, analysis on 3D imaging and lesion detection are also missing in this work, which are recommended to cover as they are very common for the medical community. So I would like to recommend a weak reject as of now.	2
I think the paper has some issues regarding representation for both CNN and transformer models to study the difference. As the main contribution of the paper is empirical, this becomes a main issue. The empirical findings of paper like paper self-supervised pre-training on domain-specific medical images followed by supervised fine-tuning of ViTs gives the best performance could be useful to the research community.	1
I think this paper is not ready for publication in a top venue like ICLR. This paper does not present any technical novelty since it is an empirical paper. I agree that empirical papers can contribute a lot to the community as well, but it requires deep analysis of the results. The experiments are very coarse only comparing the average performance of two models in classification and segmentation of medical imaging. In my opinion, the findings of these experiments are somewhat expected and even the difference between the two models are too small to conclude something. Due to these reasons, I suggest rejecting this paper.	1
"There are a lot of clarifying question that I do not understand in this paper. 

The authors have answered most of the clarification questions I had during the author's response phase. I still feel the paper is cluttered and not ready for ICLR and at maximum is a borderline accept mainly due to the strength of the problem studied. "	2
Overall, the paper provides an effective technique for improved utility for RR. However, I have a few confusions regarding the solution setting (see comments above). 	3
An interesting LDP randomiser that takes into account the significance of bits in binary encoding of floating point numbers (and vectors). However, I think the presentation should be improved to meet the bar of ICLR. Currently, I think this paper will not be easily understandable to anyone else than those working close to this i.e. practical algorithms for LDP and federated learning.	3
In general, the proposed privatization scheme is not novel, and the analysis seems not optimal. I appreciate the authors' efforts in conducting extensive experiments, but there might be some issues when comparing the proposed algorithm with previous works, which I hope the authors can clarify.	2
"Justification:  The technical content is interesting, although in its current form, it is hard to follow. For example, why is problem (4)  infinite dimensional? As far as I can see, the decision variables in optimization problem (4) is a d by k matrix which is finite dimensional, although the zeroth order and first order oracles maybe noisy. This is evident in their convergence analysis, where they can only bound the gradient norm of the reformulated variable U (which in its original formulation is V). The paper contains some technical insights in the formulation as a SDP which happens to have a nice connection to Riemannian geometry as illustrated. However, at times, the paper reads as if one has to refer multiple textbooks just to make sure that paper indeed is technically correct. For example, many quantities appear without definitions (\hat{p}_a is not defined), and sometimes with somewhat unclear notations (Q(dx\times da). To me, the biggest technical question that the paper fails to answer is the generalization to nonbinary features. This is important in the context of the paper's contribution because we know that standard SDP solvers do not scale to settings with big or high dimensional data. The convergence rate in this setting is also not clear since a naive extension would indicate at least polynomial slowdown. The experiments are also inconclusive since the paper focuses exclusively on the binary sensitive attribute setting, and with just one metric. While the approach here may not be directly extendable to other metrics, the paper completely misses on discussing when the chosen formulation is appropriate for practitioners. 
"	2
This paper proposes a new formulation for PCA and derives a complete set of results with regard to the computational issues of the new results. My only reservation is that most theory are relatively straightforward compared to existing literature on distributionally robust optimization and Riemannian optimization.	2
It's a good paper, but is missing some interpretation of its theoretical results and justifications. It is likely that I will change my recommendation after the discussion period, if the authors provide more details.	3
"
Based on the aforementioned strengths and weakness, I would consider the proposed method contain interest merits, with the conservation of the sufficiency on the methodology novelty, framework rigor, and the applicability in practice.
"	3
This paper introduces some nice ideas for uncertainty estimation with distillation, but it fails to make comparison to many more related existing works and it has some clarity issues. It is hard to know how it stands when it sufficiently considers the existing methods for comparison.	3
I do think this is an interesting approach of using just one model for ensembling and distillation. However, at this stage, I found the paper lacks in clarity, significance and thorough evaluation (see weaknesses above for details). Thus I am currently recommending a weak reject.	3
I am not able to recommend acceptance. I think the paper could use a more careful grounding in prior work which investigates the contributions and their theoretical motivations more carefully. I also have questions about the implementation of the baselines and whether there might be a reason the performance of baselines falls below those reported for the architecture.	2
This paper proposed an efficient uncertainty prediction. Experiments are done on two tasks to do evaluations. The paper is well written. However, since the proposed method is like a combination of existing techniques. The main concern is the novelty of the proposed method and whether it is fairly compared with SOTA. 	3
Good motivation and extensive ablation study. But the method should be described in more details with more insightful explanation in a better organizing way. In addition, it should be compared with more rehearsal-free methods on larger datasets.	3
I believe the proposed method has its flaws but overall I believe it is an illustrating study and I lean towards acceptance.	2
This paper studies an interesting problem of knowledge transfer in the class-IL setting. Existing work only did it in the task-IL setting. However, this work is incomplete as it assumes that class similarities are known, which significantly decreases the value of this work at this stage. There are also a lot of issues in the experimental section. 	2
The proposed method is based on an existing method and is very incremental. The presentation of the method could be improved. Experiments are not sufficiently convincing.	2
"The study of the feature drift is important, otherwise, the study has to rely on the pre-trained feature model, which makes the setting impractical in many cases.  
The paper provides some summarization of multiple tracks to deal with the weights normalization and preserving in the last layer but the insights brought from the work are limited. The analysis is strait-forward on the observation of weights but there is not much deeper and richer analysis on the causes of such observation.  
The experiment results do not support most hypotheses made, but the cause of the failures remains unclear. "	2
The validity and novelty of the results and observations in the paper are somewhat dubious. 	2
"Please refer before sections

Changes:
In the Sec 5.2 (Median Layer...): Third line ""MeanLayer Could Show some more advantages against MeanLayer"""	1
I have major concerns about the clarity of messages conveyed in the paper, and also the insights that this paper could potentially provide on continual learning if the paper was accepted. Therefore, I do not recommend accepting this submission.	2
"In my opinion, this work is original and has the potential to be an impactful contribution. However, in its current state, it is not ready yet. My main issues are the ambiguity in the model construction exposition and the lack of experiments that explore the model’s behaviour and properties.

**********
Edit: My score was updated to 6 after the rebuttal.
**********"	3
This paper proposes a multi-task Bayesian model. Compared with existing multi-task Bayesian models, I don't know why the proposed model is better. The proposed model is not suitable for classification tasks. Important baselines are missing in experiments.	2
The paper is poorly written. The motivation of the proposed formulation is not clear. The empirical study is not adequate. 	2
The paper is clearly organized and written. It is easy to follow. The results are very impressive. Although some important discussions are missed, this contribution of this work is good. 	2
The conclusions of the paper add no value, and the experiments are not performed well.	1
- Draft is incomplete in multiple aspects. It promises much in the abstract (or goals) but fails to deliver it. It is supposed to be a comprehensive empirical analysis, but the provided experiments fall short of it.	1
In general, this paper raised an interesting problem and studied the obstacle when people want to apply ML methods to CPSs data. The authors conducted experiments with existing methods on OT, deep learning, and adversarial attack. However, I feel the results and analysis are still in their preliminary form. This paper could meet the acceptance requirements with a clear problem formulation, more extensive experimental results with CPSs data, and theoretical analysis. 	1
The paper proposes an interesting approach for embedding hierarchical data into complex hyperbolic space. By choosing complex hyperbolic space as the target embedding space, the approach tries to gain additional representation power. I have some conceptual concerns and also think that a more comprehensive experimental comparison with existing approaches would improve the paper (see comments above).	3
The idea looks interesting but the paper lacks providing the intuition of the learned representations, and baselines are lacking.	2
The unit ball model-based embeddings have a more powerful representation capacity to capture a variety of hierarchical structures. Through experiments on synthetic and real-world data, the author's work shows promising empirical results. However, there are two key problems causing my concerns.	2
Despite the slight concerns mentioned above, I still think the idea and the methodology of the paper is good enough for ICLR. The proposed complex hyperbolic spaces can potentially inspire more advanced method in the field. Hence, I lean to recommend accept of this paper.	3
"Strengths:
- An interesting mixture model perspective of multihead attention
- It is nice to reduce compute cost with the same or better accuracy

Weaknesses:
- It can be confusing to introduce concepts that are never used in the model
- Several key details of the implementation need to be clarified.
- Weak LM baseline. Would be great to include more experiments. "	3
As mentioned in the top of the weaknesses section, the main reason for my recommendation is that there is not sufficient evidence showing an improvement in performance with the proposed method.	2
"Overall, I vote for weak acceptance. I like the probabilistic interpretation of self-attention and the empirical results are strong. 
My major concern is about the clarity of the paper (please see weak point #1 below).  
"	3
"Overall, I vote for ""marginally below the acceptance threshold"". I think the extension of attention keys to the mixture of Gaussian and exploring the learning methods with EM algorithms are somewhat technically sound. However, experimentally demonstrating its practical usefulness is required further. Hopefully, the authors can address my concern in the rebuttal period."	3
This paper builds the connection between self-attention and Gaussian distribution with some assumptions, which is novel for Transfomer model. The extension from original self-attention to mixture Gaussian model is natural and convincing to me. The experiment results are sufficient to illustrate the improvement of their models.	3
" 
Overall, I vote for accepting. I like the analysis of aligned GANs’ models. The paper also demonstrates impressive experimental results by leveraging the properties of aligned models for image translation, image morphing, and zero-shot classification. My major concern is about the clarity of the paper and some missing details. Hopefully, the authors can address my concern in the rebuttal period. 
"	4
This paper investigates an important topic of the semantic alignment of the parent model and finetuned child model. Despite the application part could be further enriched to better match the analyzed properties, the comprehensive analyses on the properties provides many insights and might arouse following researches. Therefore, I am positive.	3
As can be inferred from the balance in strengths/weaknesses, my preliminary rating for this submission is borderline accept. While the interesting results are shown in the paper and supplementary material, the key contribution for models, techniques or theoretical insights is unclear. Furthermore, some comparisons seem unfair in the paper. The authors should clearly interpret them during the rebuttal. 	2
The paper presents some novel analysis in a popular field of research, with implications that can help drive the field further, with minimal changes.	2
This paper proposed a novel approach to jointly model text and stock price information and fuse them for stock market forecasting. While the proposed approach is solid, this paper still needs more work on its writing and experiments.	3
This paper solves the interesting text-based stock prediction problem with a well-motivated method. However, it is hard to identify the novelty and understand the method in detail based on the current manuscripts. I think this paper can be greatly improved when more details and comparisons with existing methods are provided.	2
Overall, the quality of this paper in terms of both technical contributions and presentation does not meet the bar of publication of ICLR. (see the above comments)	2
The proposed CIL strategy, which exploits distillation on unlabeled placebo data, seems novel and effective. However, several SOTA methods are missing in the related work, and the experimental settings and the efficacy of RL need to be clarified. 	3
The idea of sampling data from a free image stream for class-incremental learning is not new, but the RL-based sampling algorithm looks new. The paper is well-written, but some parts could be improved.	2
Regretfully the authors missed very important related work already showing the potential of external data. This drastically reduces the novelty of the paper. This work also proposes a method to select data from the data stream which could be compared to the proposed RL algorithm. Given that comparison with the most relevant related work is missing, I recommend rejection.	2
Given the strength and the weaknesses, the paper has decent novelty of using extra free data stream for improving continual learning methods but may be less fairly evaluated (i.e., the evaluation protocol is designed favorably towards the proposed method) with other compared methods.	2
"The paper presents an interesting new challenge that could give rise to a number of new research ideas. The challenge is well motivated from the literature

Authors explore the new task with existing methods with some obvious expansions.

The proposed baselines should be explored more and specific tests outside of reward should be part of actualy analysis

PRO
* paper is written well
* the challenge is well described  
* paper presents some sensible baselines

CONS
* no technical contribution
* potentially shaky results (not clear if these results are significant)
* evaluation relies on reward only. other evaluations are discussed but not really explored in detail
"	2
As of right now I won't argue for accepting this paper, as the paper doesn’t make a convincing argument for the novelty or significance of the current results.	2
I think this paper has a lot of potentials but is incomplete in its current form due to 1) a lack of interactive review, 2) insufficient evaluation (missing systematic evaluation of the proposed tests), and 3) a lack of discussion of how scalable and generalizable SymmToM and the approach are. Please see specific concerns in my main review.	2
The multi-agent reinforcement learning with the Theory of Mind is an interesting topic to be discussed. The proposed SymmTom aims at the researchers in the Theory of Mind and MARL field. But overall problem setups and experimental results are not described enough.	3
"In sum, the paper puts forth a compelling benchmark: one that is simple and well-motivated but appears to demonstrate a considerable delta between an heuristic ""upper bound"" (not necessarily actually the upper bound) and reasonable baselines. I feel that these merits outweigh the critique that this might not be isolating critical aspects of ToM capacity in people. As such, I think it would be worthwhile to get this to the broader community."	2
This paper advances the field of graph adversarial attack by promoting homophily unnoticeability. The experimental results are impressive while there are still some concerns regarding some analysis.	3
The claims and proofs are solid. The problem addressed is very interesting in terms of pointing out a major deficiency in robustness of neural graph embedding models against GIA attacks. I believe the contribution of this work is significant in that further work can be built on their results to address the discovered vulnerability in deep graph models. There is possibility of improvement (such as adding discussion on intuition behind the method, limitations of it, and in-depth discussion of implications and future directions). But, overall,  I evaluate this paper as “accept”.	4
"I am leaning towards weakly rejecting the paper because 1) node injection attacks are only shown to be stronger than indirect node modification attacks, which becomes less interesting. In addition, the equivalence proof between the node injection and modification attacks cannot always hold in practice, 2) the proposed harmonious adversarial objective is of some limited novelty.
 "	3
In summary, this paper studies an important problem, provides theoretically analysis to understand GIA and further propose a new regualrizer to improve GIA. Extensive experimental results also show the effectiveness of the proposed method.	2
Interesting idea and application, results are promising, but the write-up could be improved to understand the approach clearly.	3
Although the problem studies by this submission is interesting, this paper is very difficult to follow. There are some details missing and some technical errors in the paper. There are also some unclear details and missing experiments mentioned in the above section.	1
The authors propose a new method for capturing object representation directly from images and used for object manipulation. The idea of each component is not very novel. More importantly, the experiments miss baseline algorithms. I am not very convinced of the practical contribution or benefit of the proposed method. Can it solve more problems or outperform the existing method? 	2
"While I like the direction the paper is investigating, I do not think the work is mature enough yet to warrant publication at ICLR. More experiments, comparisons, and clearer exposition would definitely make this paper a better candidate for publication.

---- UPDATE -----
Raised score from 3 to 5.

---- UPDATE -----
Raised score from 5 to 6 in light of comparison to hand-engineered methods."	3
The core idea of the paper is very interesting for the problem of mobility forecasting but as it is the description of the model is too broad to reproduce the architecture and the experiments.  Results show some surprising effects that are not analyzed. 	3
In summary, this paper is interesting to read and easy to follow, but the work needs to be improved before being ready for publication. There are several key issues related to the presentation (e.g. clarity and level of detail of explanations), experimentation, empirical validation of several important claims, state-of-the-art references, and baselines, that should be addressed. Unfortunately, the level of novelty is also rather low.	2
Overall, the paper studies a topic in an important domain. However, the motivation of the work is questionable and unjustified. It does not seem to be beneficial to disentangle spatial and temporal information from an ST dataset. The idea of using generative models for ST data modeling has been explored before, which was ignored by the authors. 	2
"The authors approach an interesting and relevant problem and propose an interesting avenue for improving mobility forecasting techniques. However, I find the main issue of this paper to be the weakness of the empirical results, making the contribution, in my opinion, rather marginal. The methodological section can also be improved, as it leaves many open questions to the reader on a number of design choices. Because of this, I believe the current version of the paper is not yet worthy of publication. 

In my opinion, in order to improve the current version of the manuscript, I believe the authors should focus on:
- Additional experiments to justify/prove the claims on “spatio-temporal disentanglement”
- Clarify experimental design (e.g. settings of hyper-parameters)
- Clarify the significance of results in Table 1
- Improve/Revise the presentation of the math describing the proposed architecture (i.e., Section 3)
"	2
Overall, the analysis in this work is novel and interesting, although there exists a weakness in supporting the claim. In this concern, I recommend a score marginally above the acceptance threshold	4
This paper studied the cause of robust overfitting and proposed a method to mitigate it. Both theoretical investigation and experiments on realistic datasets are included for demonstrating the effectiveness of the method. The paper is overall well written. However, the presentation of key concepts needs to be modified, and overstated claims should be rephrased. The key idea of implicit label noise is novel and insightful and could inspire works that connect the fields of adversarial robustness and learning with noisy data. 	2
This paper offers some novel understanding about robust overfitting, but the findings are not surprising as similar observations have been made in the previous literature on the model-wise double descent. I'm also not fully convinced by the equivalent impact of label noise and implicit label noise on the overfitting. The proposed method seems also not significant and the comparison is not very clearly stated in the paper. For these reasons, I'm currently on the negative side.	2
"Pros:- This paper demonstrates that ""robust overfitting"" during adversarial training is an early part of an epoch-wise double descent phenomenon for relatively large models. Proposed a novel technique motivated by this analysis.

Cons:- Comparative results are not convincing enough. 

Overall, I liked the paper. I am giving an initial score of 5 due to my concerns about comparative models in experiments.  If the authors can clarify my concerns, I shall increase my score towards acceptance."	3
Although the motivation of the paper is valid, it is not clear if the conclusion is interesting due to strong assumptions.	2
Interesting problem, solid numerical and analytical results in the linear regression setting, need more experimental results in the case where DNNs are used	3
The conclusions of this paper are interesting, and some of the analyses come from a novel perspective. However, the contribution of this paper still remains in doubt as it is unclear how the insights can prove to be useful in practical applications.	3
"I believe that the overall direction is interesting and there is room to study which kind of adaptive learning method is better suitable for a different set of tasks (having many similar tasks/multiple hard and multiple easy tasks/etc). In addition, the paper makes an effort for laying some definitions of task hardness and tries to demonstrate different tradeoffs between adaptivity and hardness. 

However, I believe that the comparison itself is not in the right place and MAML should be compared within a set of adaptive methods. I do not think it is impressive that MAML is more adaptive than the non-adaptive learning method, which does not seem to be practical. I also think that the definition of hardness is a bit oversimplistic (or at least not sufficiently motivated) as it measures the variance of the samples in the task.    "	2
This paper studies an interesting question about the effect of data quality on robustness. Unfortunately, I find the conclusions for robustness as emphasized in the paper either unsubstantiated (removing low quality data improves robustness) or difficult to make actionable. Nevertheless, there are some interesting empirical observations in the paper such as the difference between important points for standard and robust accuracy, the effect of data quality on robustness-accuracy tradeoffs which were not known previously. I apologize for the late review - there was some issue at my end and I realized the review didn't get submitted. I would really appreciate if the authors could respond to my questions above. Thanks!	2
Empirical study paper with an interesting message that adversarial training and nominal training do not benefit from the same type of samples. The paper could have gone more in depth by studying the extra data case which is commonly used in the literature and gets the SOTA results. Especially, this extra data case normally fits the message of the paper so it is a missing element in the paper.	3
Overall, this paper systematically study the influences of data quality on the three problems in adversarial training. However, I am skeptical about the practical values of the proposed metric for data quality.	2
I vote to accept this paper since I find the empirical contributions significant; the authors rigorously study the impact of data quality on various properties of adversarially trained models including robustness, robust overfitting and robustness overestimation, and robustness-accuracy tradeoff.  I think the discussion of related works can be improved though.	2
My overall disposition towards the paper is indifferent although the paper proposes an interesting idea. It is a bit difficult to follow the method due to a lack of information. More details are needed especially on experimental settings, and methods.	2
I think the architecture has novelty when it comes to NLP tasks. And this work could benefit the explanation community. However, I found the experiment results are confusing. To the best degree, it offers marginal improvement over the best baselines in terms of task F1. When it comes performance of explanation, I only see confusing numbers, thus no conclusion can be made. Analysis on generated explanation is another weak point since it's absent.	3
The paper addresses an important issue in explainable natural language processing models. The proposed model is reasonable, and the results seem to be OK. However, many issues are left unclear, such as its efficiency, hyper-parameter tuning, etc. I tend to lean towards weak rejection at the moment.	2
While i think this work combines many interesting threads in interpretability, using user feedback and case based reasoning, the paper in its current state is not ready. I’m happy to increase my score / recommendation if the weaknesses are addressed, however. 	3
The paper is clear and the proposed method is reasonable. It has the weaknesses on the use of pre-trained models for PDE solving. However, I tend to accept the paper for its effectiveness.  	3
"I find that the paper is well-written and introduces a method combining two known techniques for learning and solving PDEs from data. The authors present numerical results that outperform PDE benchmark used in the literature, taking advantage of the PDE constraint from the PINN approach or the neural operator learning from the FNO technique. Yet, there are some weaknesses in the study that I discuss in the main review. However, most weaknesses are inherent from the current challenges faced by the field of PDE discovery (such as choice of training data, practical applications, limitations on the domain geometry, boundary conditions).

Therefore, I believe that the paper could motivate further explorations and research between deep learning PDE solvers and PDE learning methods and recommend acceptance of the paper to ICLR provided that the authors address the points raised in the review."	2
I think the technical and algorithmic contribution is solid ,the emperical results are good, and (if the novelty is good) I think this is a publishable paper.	3
This seems to be too incremental given the previous work and the experimental setups are not sufficient to support the claims of the paper. There is no theory provided.	1
"In general, I believe this is a well-written, well-motivated paper that draws some great ideas from Experimental Semiotics. Furthermore, the discussion and analysis of the learning dynamics of the various components of the approach should be a mainstay of future systems driven work in ML.

However, I am deeply concerned with the assumptions made with this approach, stemming from the architect’s access to the ground-truth state transition function. This coupled with a missing discussion of related work in hierarchical RL and Feudal RL specifically, and a more thorough evaluation including comparison to these methods and other relevant ablations inform my decision to lean towards rejecting this paper."	2
The paper proposes an interesting and relevant framework for interactive learning / teaching between two agents. The model choice and solution is sensible, and the empirical evaluation can shows that the initial approach for the problem works sufficiently well for the toy problem. The learning challenges with respect to other works in MAS could be clarified in more detail.	3
The paper presents a novel and interesting setting and algorithm. They nicely implement ideas from cognitive science in an MDP setting. Even though the formulation needs to be more rigorous and the experiment environments are simplistic, I think the contributions are interesting enough to draw attention of the community in the future. I recommend acceptance. 	4
This paper presents an interesting setting; however, it appears to rely too heavily on strong assumptions, and the communication protocols that emerge do not seem to exhibit any interesting properties (i.e. the communicating agent simply learns to output messages that correspond to the desired action for the acting agent).	2
Interesting paper on improving test-time adaptation but I have some concerns (see weaknesses). I will be willing to reconsider my rating based on the author response.	3
Primarily due to my concerns above, I am initially recommending a weak accept of this paper. I am happy to engage in discussion with the authors and other reviewers in order to reach a more confident final recommendation.	2
Overall, the paper presents an information maximization (IM) loss for TTA on unlabeled target data. My main concerns are that the effectiveness of the proposed $L_{div}$ on mini-batch data, and the scale-normalization problem of the proposed logits. Hopefully, the authors can address my concern in the rebuttal period. 	2
"This work makes a reasonable but minor contribution that can inform further extensions of test-time adaptation. A large part of this work is the double-checking of elements of test-time adaptation methods, with only marginal empirical novelty and significance, or the importing of techniques from other scopes, with either no or only marginal technical novelty and significance. It is a pity that the most new part, the test-time input transformer, is not further studied and improved to give this work a more independent dimension of contribution. At the same time, the harm to accuracy on normal data is cause for hesitation to accept the proposed changes. While there is value in this work, rejection is recommended so that (1) the input transformer can be more fully covered and (2) the issue of improving out-of-distribution accuracy at the cost of in-distribution accuracy can be resolved.

**Final Review** The rebuttal thoroughly clarified results and offered additional experiments to empirically justify the contributions and reduce worries about potential issues. In particular, the results with the baselines of TENT/TENT+ now make sense with the clarification of online/offline results (please underline this in the paper), and the proposed method is not so sensitive to the choice of frozen layers. I have raised my score to 6, and I would have considered a 7 if there were such a rating. I did not go higher because the trade-off of lower accuracy on unshifted data for higher accuracy on shifted data remains. Nevertheless there is informative material here, in the main paper and appendix, and so I vote for acceptance so that this work can inform the burgeoning direction of test-time adaptation. Along with the more novel parts, such as input transformation during testing, this work also helpfully confirms and tunes other parts like the diversity regularizer in ways that future work can simply adopt."	2
The paper mainly questions the effect of sparsity and soft-thresholding introduced by ReLU. The authors show that the classification improvements by eliminating spatial within-class variabilities come from a phase collapse. This counts as a contribution compared to related works. The hypothesis is clearly defined and supported using theory (and assumptions) and experiments. However, there are pitfalls that need to be addressed. 	2
Based on the novelty and the strong results, I vote for acceptance. I have one remark that I couldn’t completely understand and it would be nice if the authors could help me there. All in all, I find that I learned something from this paper, which I think is a great result for any publication.	4
"Overall I found this an interesting paper. It is well written, and has a fairly solid experimental section in support of some not so well known theoretical ideas as to why deep neural nets achieve good classification
Overall here is my summarized assessment:

originality: The authors present some theoretical results to justify their experimental framework. The paper is not just blindly attempting different DNN architectures. From this point of view I consider this a fairly original piece of work

quality: I believe the exposition and experimental results are of sufficient quality for ICLR. The paper is in general well written, though as I indicate elsewhere certain parts need to be improved. 

clarity: I mentioned above in the ""Weaknesses"" section some clarity issues that I noticed. The authors should address these issues. I was ambivalent as to whether the paper should be rejected because of these clarity issues, but since they can all be addressed I think I decided to be a bit lenient. But for the final ICLR submission they should be addressed.

significance: Unless I missed it, are the authors providing source code? The paper's significance and clarity will be improved if source code is provided, to make the results reproducible"	3
"I think this is a potentially interesting paper but I feel it leaves quite a bit to be desired. 

post author response update:

I have read the other reviewers comments as well as the author responses - it would seem that I have been harsh in my scoring and this has been revised. I think this is a valuable contribution to our understanding of the field and a nice demonstration that fixed wavelet filters can, under the right circumstances, perform as well as learned networks. My confidence in the score still remains low as I feel was not the right person to review this."	3
This is a compelling paper. it's a well motivated and technically sound perspective on self-supervised training.  The performance on Librispeech 960 is quite strong.  The performance in noise conditions is strong, but would be more convincing if shown on more actual rather than synthetic noise.	3
Based on the above main review (especially the strengths of the paper), although some of the ideas have been borrowed from previous work (e.g. MT, BYOL, contrastive loss, etc), the paper has proposed extensions to these work by considering the sequential applications in speech processing.  And the relations with the previous work have been clearly discussed in section 2.  Hence, I think the paper could be accepted for publication on ICLR.	3
Making self-supervised speech representations easier to train is a significant contribution, and this paper presents a viable approach to doing so. Some results feel a bit preliminary, but the paper is well written and the authors may have updated results but the time of the conference, and they will release code (plus presumably models?). My recommendation is thus for accept.	3
The paper has an interesting premise and some promising experiments to justify it. On the whole, the method is somewhat comparable to wav2vec 2.0, although it falls short in several cases. This can be made clearer in the abstract and introduction, while highlighting the reduced training cost. I am not convinced that the comparison to wav2vec 2.0 is complete in terms of denoising uses, as there is no discussion of specaug and additive noise in wav2vec 2.0 during pre-training or fine-tuning. Also, some additional ablation studies to understand the benefit of additive noise and positional randomization would strengthen the paper.	3
While there are some clear limitations in the empirical sense, particularly in the claim that the model learns denoised high-level representations, the paper advances self-supervised learning for speech tasks by adapting the popular teacher-student framework popular in image representation learning. The model achieves WERs similar/better than the popular wav2vec 2.0 architecture while reducing training time and model size. The ablation experiments raise questions about whether the modeling/training strategies deemed essential in that modality are also needed for sequential tasks, although the authors have not addressed these questions directly in this work. Overall, this work (and the promised code release) is likely to lead to new explorations in this direction for speech self-supervised learning.	2
"The proposed method is well-motivated: it is a small modification to recent related work that seems to be justified mathematically by the constraint argument they give for tying together the optimization of the weights and parameters of the network. The paper is generally well-written but I missed a better setting of the problem in section 2 -- which I find confusing -- especially in the use of the ""biased""/""unbiased"" terms when referring to parameters and datasets. The experimental section describes the experiments with enough details that they can be reproduced, the experiments are well structured and the accuracy results show a small but consistent improvement in most experiments over the state-of-the-art. Therefore, I believe the contribution of this paper is just enough to warrant acceptance."	3
In a word, the idea of the proposed model is useful to obtain a better performance, however, several detail issues are not clear in the proposed model, and the experimental results are not sufficient to verify the effectiveness of the proposed method.  	3
As many real-world datasets naturally forms in an imbalanced form, the solution provided in this paper motivated from theoretical side and is strongly proved empirically by comparing with other methods on different datasets. 	3
The paper is relatively smooth to read. However, the descriptions can sometimes be very verbose (e.g., introduction) and thus there is some room for improvement there. Overall the approach seems justified and the results are promising. However, the improvements can be minimal in some cases from the baselines as shown by the experimental results. Another potential issue can be the low volume of training data. I would have liked to see more larger datasets being used here as in practice much bigger datasets are the use cases where the class imbalance problem pops up.	3
This is a promising, but immature work. It needs to be focused on only one of the identified problems and the evaluation needs to be significantly improved. It seems like there is potential for multiple papers in this work, but it needs to be split up and experiments carried out more carefully. This paper would also benefit from a more direct connection to the identified use case, as well as some justification of why that use case is realistic. 	2
This paper provides an interesting application of using ML.	1
In summary, although the paper tackles a novel problem and designs an FL-based framework for the target, it shows significant defects in its motivation, innovation, and experiments. As a result, I do not recommend accepting the paper.	2
"While the presented concept of collaborative learning for predictive maintenance can be a key contribution to the domain of PdM, this idea is not well developed by means of related datasets. One suggestion is, the authors use several bearing datasets that are available publicly, i.e. Case Western Reserve University dataset, University of Cincinnati’s dataset and many more available bearing datasets and use them as different vendors and then train the global model. Then the model can be tested on an unseen different dataset. The output should be compared with ground truth if the authors are solving a prognostics problem or be compared to a known fault type if they are solving a diagnostics classification problem. 
With the above said enhancements needed for the paper, I cannot recommend the paper for the publication. "	2
Overall I believe this paper provides an interesting contribution for interpretable policy learning particularly for clinical decision-making. I do have a few followup questions that will make the contribution regarding some aspects clear. 	3
Overall, this is an interesting paper. The novelty and significance are positive, if not overwhelming. The utility for practitioners and potential impact in areas outside of machine learning, however, is nontrivial. The exposition of the method and its interpretation would benefit from additional refinement. Nevertheless, in its current state, I can recommend a marginal accept.	2
"I'd give a rejection based on the comments above. And I would like to suggest the authors revise the paper for re-submission to another conference.

----Post Rebuttal---
To Reviewer kv5i. To be honest, I'm still not convinced that the issues raised in this paper are more artificially fabricated than they are actually present. Clearly, if the goal of this paper is to discover machine learning models that can be used to explain, then decision tree models and support vector machine models are superior choices when compared to other benchmark models, and in fact, the interpretability of decision tree models would not lag behind the methods posed by the article. It is possible, however, that the assumption made about the problem encountered in the clinical data (a Markov process with partial observability) is ill-posed, i.e., that the requirement to take into account the impact brought by the medical regimen is incorrect.

---- Post Post-Rebuttal ---
I appreciate the author's thorough and precise arguments. After several days of consideration, I reviewed the entire paper and decided to increase my score. Thanks for your effort."	2
"*Initial summary*: I think this is a very interesting research on an important topic. Experiments are well done. My main concern is that the simplification of the model might have some bad impact. 

*After discussion*: Concerns have been mostly addressed. I think the paper is worth publishing."	3
"The considered problem is a bit too simple to have meaningful insights, and the paper provides almost no discussion on how the analysis on this simple problem could be useful for general POMDP problems. Even for the simple problem setting, the paper doesn't have performance guarantees for the proposed policy in the non-asymptotic region. 

"	3
The paper presents an interesting algorithm for two-armed bandit problems with finite history, but the ideas do not scale up to larger problems, and the empirical evaluation is not enough to draw any conclusion on more general POMDPs.	2
An interesting yet very limited paper.	2
This work points out potential issues with current empirical approaches to learning memory allocations and suggests that restricted memory architectures could be easier to use. While the authors do show this to be the case in one very simple environment, they do not show that this concept scales to other larger or more complex environments, such as visual navigation, that are currently used. As-is, the work is simply too narrow in scope and experiments to be a significant contribution to modern techniques and understanding.	2
Interesting empirical work but needs improvement and clarification.	3
"The analysis of the interaction between the robustness of ML models and the Populated Region Set (PRS) ratio seems promising in understanding the geometrical interpretation of robustness. Although having an extension to the current experiments would make the use of such a metric more convincing, I am slightly leaning towards acceptance.





"	2
"While the concept is interesting the paper doesn't provide enough details about how the models were trained or about the adversarial attacks. I'm not convinced the newly introduced metric will be useful without a deeper evaluation, especially explaining why similar models differ drastically in PRS size. The paper is hard to read and understand as there are a lot of grammatical errors throughout.

Given all these factors I don't think this paper should be accepted."	2
"Video generation is a very hard problem and the authors try to introduce a more efficient approach for this problem. I believe INR-based video generation is a valuable extension of the image generator but this is not significant. Also, the experiments cannot fully support the design choices for the discriminator.  Additionally, the paper misses some related works (and discussion on differences) and details on computational complexity and training time. I don't think the paper is ready to be accepted to ICLR. However, I'm open to discussions and will consider the author's response.

Update:
The author's response address almost all my questions, however, I'm not convinced about the discriminator choice and the author's explanation is not clear and convincing. This is also confirmed by their qualitative results on long videos such as the UCF101 video (64 frames). The result is much worse than DVD-GAN (Clark et al 2019). 
I believe this work has positive aspects in terms of efficiency and ability to generate frames in parallel. Therefore, I'm increasing my rating from reject to ""marginally below the acceptance threshold""."	2
I believe, the paper is a good piece of work and is above the bar.	3
"UPDATE: The provided rebuttal answers most of my concerns, and my confidence about the strength of the paper has increased. I am updating my recommendation to 8 - accept.

The paper discusses a very relevant concept in video generation: utilizing INRs to improve performance and efficiency. The authors show strong quantitative results and a few architectural novelties. The work is solid, but there is a worrying lack of compelling qualitative examples to back up certain claims. If the authors can provide more examples, I believe this could be a strong addition to ICLR."	3
"This is good work, a simple idea extended from the image to the video domain. It suffers a fatal flaw that prevents me from recommending it for acceptance. 

In its current form, this paper is ""marginally below the acceptance threshold"". If the issues with benchmarking are resolved, then I will recommend it for acceptance. If additional experiments exploring the efficiency of the proposed approach are provided, then I will strongly recommend it for acceptance.


------

***EDIT****

The authors have sufficiently addressed all of my concerns (and those of the other reviewers it seems).
I strongly recommend this paper for acceptance to ICLR22, it is a meaningful step in a promising direction for long-horizon GAN-based video generation."	3
Overall, the novel method presented by this paper, HACO, seems to be able to greatly outperform safe RL, IL, and Offline RL baselines – leading it to being able to achieve relatively similar safety and performance metrics while incurring in many less unsafe situations during training time, and while using 2 orders of magnitude less data than RL approaches. While some portions of the paper and motivations for the method lack clarity and baselining is limited to non human-in-the-loop methods, the strength of the empirical results still suggests that the submission is potentially worth accepting.	3
Missing crucial comparison to prior method	3
The paper is solid and clear, with enough experimental support. I think it is higher than the bar. 	3
"This paper presents a simple yet very effective human-in-the-loop learning algorithm. The idea of minimizing human intervention is nice, and the presented approach has strong experimental performance. The novel simulator on which the presented approach is evaluated will be released, which is another plus. I recommend acceptance for this paper.
"	3
Overall the paper considers an interesting problem and tries to unify the detection and classifier defenses. However, the theorem is only correct without considering the computational complexity and poses another question about the relation between robust classifier training and computational complexity. Also, the experiment part should include approximate results of the reduction steps to verify the feasibility of the theorems in practice. 	4
"The paper proves a natural theoretical result that is at the heart of robust learning. The result is information theoretic, but I still find it quite natural.

I think one should be very cautious to not overly interpret the implications of this paper, but I think the mere theoretical observation that testing and decoding in the context of adversarial learning are equivalent has a merit, that at least puts this paper on the border for ICLR.
"	3
"The paper is based on assumptions and findings of one single paper most of the time. Without explicit showcase that the existing defense will not work, the analysis seems misleading and unfair to the parts of the defenders as well. 

The authors believe that the existing defenses do not resemble the ""worst-case attack"". Please generate such an attack and showcase without explicitly touching the defenses that defense is not working.

In my knowledge the true and fair concept to both attacker and defender needs to be there to make serious progress in the field, else this can be just another paper reflecting the singularities of the defenses. "	2
In general, I think this submission can provide beneficial insights and evaluations for the field of adversarial detection. However, it lacks practical solutions, which implies limited contributions. I hope my concerns about Theorem 4 and 5 can be considered (or pointed out my misunderstanding) in the discussion stage. Though I cannot recommend acceptance at this stage, I will increase my score if my concerns are solved properly.	3
"This paper revisits the original WMD paper and offers a detailed evaluation showing what contributes to the performance gain: normalization instead of WMD.
"	3
This paper re-evaluates the Word Mover's Distance by well-designed experiments. They reveal the true performance of WMD and draw its relationship with L1-normalized BOW. I am more inclined to accept this paper.	4
On the whole, while I appreciate the work to bring to the community's attention some extra information and analyses of the Word Mover's Distance paper, the paper is not very informative. I think a much more interesting paper would be to survey the evaluation of multiple distance metric learning approaches and discuss common pitfalls more broadly, rather than pick on one paper in the context of a task that it wouldn't be considered a standard method for (fine-tuning BERT style pre-trained models is the go-to method for text classification and is much better than word embeddings from 2015 with kNN).	1
The paper corrects some errors made in an earlier paper by (Kusner et al. 2015) and presents a careful evaluation of baselines for document classification applications. However, I believe that the paper is more suited for an NLP conference than ICLR.	1
The authors propose x-mixup a technique which leverages parallel data and forces cross-lingual representations to be aligned. The proposed method leverages translation data (translate-train and translate-test) during training. This leads to improvements in several XTREME downstream tasks over strong baselines. The authors perform some ablations which showcase the importance of the different components in the proposed method.	3
The paper presents *X-mixup*, a method for cross-lingual transfer based on translate-train, which improves performance over multiple strong baselines. However, I have doubts regarding the framing of the paper as well as regarding the conducted experiments. Thus, I think the paper should be revised before being published.	3
The proposed idea is interesting. They show good and promising experimental results and ablation studies. Some questions need to be clarified and some more analysis is suggested.	4
I score this paper a 6. I think the manifold mixup is very interesting for this use-case and made my reading and evaluation of this paper more interesting. I look forward to the responses from the authors.	3
Overall, an interesting and well thought through investigation into the role of clustering when selecting in evolutionary strategies for reinforcement learning. The results give a clear indication that clustering can greatly improve results under certain settings, with good attention given to the impact on changes method parameters such as clustering algorithm and size. However, more needs to be done to show explicitly that those selected policies do end up being more diverse in behaviours. 	3
"- Interesting approach to maintaining the policy clusters and adapting the objective function
- Needs to improve the context in the literature, w.r.t. the use of diversity of behaviours
- An ablation study to show the contribution of the MAB for objective function adaptation, would be beneficial

EDIT: I have increased my Empirical Novelty And Significance score, and conditioned on the additional results added to the camera ready I would be willing to increase my recommendation."	3
"Overall the paper is very clear and easy to read and understand. It is well structured and nicely illustrated. 
Unfortunately, while it is a great paper, it misses some key references in the literature which are making the novelty claims less obvious and inviting additional experiments.

---- 
Updated score after discussion below. 
----"	1
"While the paper produces experimentally strong results, it lacks motivation (especially for the clustering heuristic) as well as certain components such as the use of multi-armed bandits for adaptively tuning $\lambda$. One danger to this lack of motivation is the introduction to potential confounding factors (e.g. what if the clustering heuristic is implicitly optimizing an ideal diversity objective, and it's better to optimize this ideal diversity objective instead?). 

There is also a nontrivial amount of presentation issues (not just small grammar mistakes or typos) that affect the paper's scientific quality as well.

I am willing to increase my score if the authors fixed the core issues outlined above.

EDIT: I have increased my score to a 6 as well as my confidence scores. The authors have addressed my questions and concerns."	3
"*** For the initial review***
I am concerned about the effectiveness of the main claim of this paper because it is not confirmed through the experiment, i.e., it is not compared with the most important existing method. 

***As for the final rating,***
I acknowledged the comparison with the most important method and confirmed the advantage of the proposed method."	3
"I recommend Accept for this paper since the strengths outweigh the weaknesses. The paper is generally written well with good performance. One thing that can be improved as I mentioned above is more ablation experiments.
"	3
"*** Summary and justification of the initial rating.

Overall, the paper presents very interesting and relatively new ingredients to address the problem of Multi-View Stereo using supervized learning. The presentation of the contents, despite a few substantial issues, also lies in the list of positives. 

However, as it currently stands, the proposed package suffers from major drawbacks in the evaluation department, as detailed above in the main review. In particular:

(i) The proposed experimental comparisons are performed in a fair setup, for the most part and regarding the most important dataset / set of experiments.

(ii) As an extension to (i), there are key claims, references and justifications that are inaccurate to say the least, that could easily mislead the reader.

(iii) Given the nature of the proposed contributions forming a sequential pipeline, (ie: feature extraction, approximate curvature priors, 3D cost volume aggregation and regularization) it is highly expected to provide a comprehensive ablative study to analyze, motivate and qualtify the need of each individual ingredient. Such experiments have not been proposed.

As a result, the proposed package is very interesting and the results promising, but the execution of the evaluation part do not allow to validate the main claims and novelties so far. "	3
"The paper is novel, well written, and provides nice SOTA results. 
More justification of the design choices can turn it from weak to strong accept."	3
I do not think without experiments, the work has enough technical and theoretical novelty (in terms of proof details) for acceptance. Simply improving the bound, by re-weighting the update and following similar analysis as other offline RL works do not, in my opinion, pass the criterion. I may be wrong, as I am not an expert in following the technical proofs of the paper. 	2
In general, this paper is clearly written and elegantly presented. The combination of rigorous theory and high-level intuitions make it fluent to follow. This work provides a refined treatment of pessimistic penalty functions, which leads to tighter upper bounds and nearly matching lower bounds. Although complete for the current setting, I think the paper can be improved by adding some broader considerations to more general cases to bring more insights beyond the restrictions of fixed behavior policy and well-behaved covariance matrix. 	3
Overall, the paper is well motivated, aiming to solve an important problem in the area of offline reinforcement learning. The work is theoretically well grounded, studying offline reinforcement learning for time-inhomogeneous episodic linear Markov decision processes. While the paper is reasonably well organized, it's quite hard to grasp the fundamental concepts especially for those not an expert in this sub-field.	3
The overall idea is clear and easy to follow, but it can be much better if the authors can polish the paper again and provide more discussion.	3
My main concern is regarding the practicality of the proposed method (see Weakness #1 above). I also feel that discussion of prior work and overall paper presentation could be improved.	2
In my opinion the assumptions made in the paper contradict the premise of the unsupervised skill discovery problem. Further, the approach is not clearly described, the experimental evaluation is not fair due to differing assumptions and the overall writing of the paper needs improvement. Thus, I do not recommend acceptance of the submission.	2
In my view, the experiments in the paper fail to justify the complexity of the method that was introduced. With the amount of supervision required, simpler and potentially more effective approaches come to mind but are missing in the comparison.	2
Given the lack of clarity and the multiple algorithm design decisions that are not sufficiently justified, I cannot recommend acceptation. I cannot comment on the novelty of the contribution as I am not well versed into hierarchical reinforcement learning.	3
The paper presents extreme compression on pre-trained language models. Though the introduced methods are not new, the adaptation to the Transformer layers and the analysis are interesting, and the experiments are convincing. Though there exist some weaknesses, I think the paper is of good quality, if the authors could mitigate them.	3
The paper would be better with more experiments on larger models and ablation studies. Also, the presentation and the rationale behind the idea are not clear to me.	3
Overall, I think this paper is an incremental improvement to previous state-of-the-art. Tucker decomposition has been used extensively in NN to compress RNNs, CNNs and Embeddings. Thus use of tucker decomposition and its ability to compress BERT MHA and FFN layers is incremental improvement over the previous results, especially given the fact that prior work has also shown that MHA and FFN layers can be decomposed in a low rank structure and talked about the redundancy in the parameters in those layers.  However, the results of the paper are interesting from an engineering point of view. 	3
A new approach to model compression with strong-ish empirical results. Some reservations about the soundness of the method. And it is furthermore not clear that the improvements are coming from the proposed approach as opposed to something else, since an ablation study across the different decomposition methods is missing.	2
"The empirical evaluations of the paper are thorough and informative. Although the experimental results are not the strongest (marginal improvements over baselines), they do reflect some effectiveness of the approach despite its simplicity. Because of that, I believe the paper deserves a spot in the conference.

=======================================================================
UPDATE:

Thanks the authors for their response. All of my concerns have been thoroughly addressed. Even though I still think that the improvements of CLOP over previous baselines are not that large, the insights provided by additional experiments and empirical analyses can be quite valuable to the community. Because of that, I have raised my score to 8."	3
"Overall, the paper is clearly written and easy to follow and I find the idea behind CLOP simple and effective. However, concerns regarding scalability and robustness of the proposed method should be addressed by the authors. I am happy to raise my score and vote for the inclusion of this work at ICLR provided that authors report evidence of a deeper empirical analysis.


===============

UPDATE: I raised my score from 5 to 8. I think that the analysis conducted by the authors have drastically improved the paper. I believe it meets the criteria to be included in the conference. The main contributions of this work are the introduction of a new regularization method (CLOP) for image-based RL and supervised learning tasks as well as a detailed empirical analysis and evaluation on several standard benchmarks."	2
An interesting and neat approach to regularize CNNs, motivated by the need to learn task-relevant representations invariant to background and other distractions for visual RL tasks. Strong results, well-designed experiments, and well presented. I am in favor of accepting this paper, but await the author responses and other reviewers' comments to form a stronger opinion.	4
Overall, I think the proposed augmentation technique is simple and effective when compared to original PPO and other data augmentation techniques, but it does not significantly outperforms state-of-the-art IDAAC. I have some concerns on its connections to RL and some experiment details. Therefore, at this moment I lean towards rejection.	2
"The proposed CLOP method is simple and effective, although I'm afraid it can (to some readers) seem like a (somewhat motivated) ""hack"", which isn't a huge flaw, but might be limited in impact scope. It is strong to the eyes of an experimentalist, but raises many questions to someone who may be more theoretical.

I am willing to increase the score if the authors answer my main question about the policy head, which made me somewhat alert/suspicious.


EDIT: I have increased my score to an 8 after the rebuttal. The authors have answered my questions thoroughly. Previously, I was concerned that the CLOP method might be a bit too ""hacky"" (for lack of a better word), but I think the paper's simplicity and strong empirical results may also lead to how we redefine RL generalization. Perhaps Procgen tests too much on ""visual generalization"" and thus this paper might be providing some evidence for new benchmarks."	3
The innovation is insufficient.	1
"The paper could contain interesting material but requires additional work on order to be published.
"	2
This is work proposes an organized toolbox and has very limited novelty. Therefore, I’m suggesting a rejection. However, since I’m not a connoisseur of high energy physics, I am not able to exactly gauge the utility of this toolbox to the specific demographic.	1
The paper mainly describes the ZüNIS package but does not contain much original research. 	2
"The paper proposes an interesting direction for open-vocabulary open detection. The authors have described most things about the proposed approach clearly and have written an easy-to-understand paper. 

But the paper, in its current form, is not ready to be accepted. There are several concerns about the validity of the experimental set-up and comparisons against prior works. Further, there are several decisions taken by the authors which need clarification or more empirical evidence.  



Edit: 
After reading the other reviews and the authors' responses to all the reviews, I have increased my rating to 6."	3
Overall, I think this is a solid and interesting work. There are several aspects of the paper that need improvement, though, see comments above. I encourage the authors to consider those comments to further improve the manuscript.	3
The paper presents a new solution for an important, yet unexplored, research problem. It achieves impressive SOTA performance and provides valuable empirical insights. Hence, I believe this paper would significantly contribute to the field.	2
The ex-post maximin fairness problem is interesting and well-motivated. The authors made an effort to develop an algorithm to solve the problem, which looks somewhat incremental from a technical point of view given an existing result by Zahavy et al. (Reward is Enough for Convex MDPs). There is a potential technical issue (see Main Review) which I hope the authors could clarify.	3
Overall the paper is well written and easy to read. However, given the concerns above, I tend to reject it.	2
Overall, as mentioned above, I think the paper is very elegant. I would look forward to hearing the author(s) comments on my two concerns above. 	2
The key issue in the paper is the in-efficient regret bound, and the approach novelty needs better explanations. 	2
The paper is well written and interesting. The authors combine different components to design CLOOB, a learning algorithm that shows performance gains compared to the chosen baseline. Despite this, the components used in the approach are not novel, they are the result of previous work, meaning that the technical contributions mainly consist in finding ways to adequately combine Hopfield networks and the InfoLOOB objective. I would still recommend acceptance as I do not feel lack of technical novelty alone should detract from the empirical merits of this paper.	2
While empirical results are good, the overall contribution of the paper seems fall below the bar. It simply applies a combination of two existing techniques to image-text contrastive learning, and doesn't show how it broadly applies to other contrastive learning tasks.	2
In summary, this paper proposes a novel method for CLIP-like model training, and results demonstrate the effectiveness of the proposed method on zero-shot transfer learning. However, the reviewer also thinks that this paper can be made stronger (see details in my comments), therefore, the reviewer decides to give a Borderline Accept recommendation at this moment.  	3
I appreciate the effectiveness of the proposed method. However, considering the limited theoretical contribution of this work, and insufficient novelty of the proposed method, I could only give a borderline score (incline to rejection) for this paper.	2
"Strong: 
1.	Using of bert-gat-crf for argument unit detection and achieved improvements in small-scale datsets;

Weak:
1.	The method of bert-gat-crf is less novel for this paper;
2.	The target question is a pure NLP task and also related to other tasks such as semantic role labeling that better mention and compare with their baselines as well.
"	2
"Although the paper has a novel and interesting idea, I argue it is not ready for publication due to the aforementioned weaknesses above. The critical problems are incomparable results with baseline as well as its incompleteness issues. 
"	2
I recommend to reject this paper for the following reasons: a) it does not sufficiently motivate the use of syntax, and does not make it clear relation between ADU and constituency; b) it is haphazard in its use and description of its main feature, constituency trees; c) the writing is lacking, it does not tell a particularly compelling research story, nor cover related work particularly well. In summary, it is not particularly strong in any dimension, and the main result does beat the baseline, but can easily be explained by the connection between constituency and ADU.	2
"The paper lacks many details that, in my opinion, preclude it from publication at this stage. It makes bold claims that are not supported by citations or results (see Main review section). 

The results show improvements over a BERT model that does not use the syntactic information nor CRF. I would recommend the authors to train multiple models (e.g. 5) with different random seeds and show average scores and standard deviation, it would help to ensure that your improvements are not just for the initialization you chose."	2
The paper presents a somewhat novel combination of biologically plausible features of deep nets. However, the overall approach is too simple to scale to hard tasks (STDP in hiddden layers + R-STDP in the last layer), and it performs poorly even on MNIST. The paper should be rejected. 	2
"The main concern on my end is the proposed approach lacks clear motivation why the developed SNN network and corresponding training approach are meaningful. As an SNN researcher, I did not really see the significant value. The validation is only with MNIST, and I think it is too limited. 

But, my rating is not firmed at the moment. Based on the authors' feedback, if I realize I indeed miss something important, I am flexible to change my recommendation. "	2
"This paper presents a novel combination of existing approaches. It is
in my opinion incremental in nature, but reasonably consistent in
achieving its goals. In its current state I can't recommend it for
acceptance, but I may revise my score if the confusing biological
plausibility angle is softened and if better connections are made with
related work, especially gradient based methods which at the moment
seem unjustly penalized.


---
Update after discussion: I am updating my recommendation to ""marginally above the acceptance threshold"" to reflect the effort made by the authors to engage at least with some of my concerns. See my message downthread for more details."	3
In summary, the empirical results of the paper show advantages of the proposed training objective in producing better certified smoothed classifiers. However, given the current low clarity of the paper, especially when introducing the design of the proposed loss function, I suggest a weak reject for this work.	2
My current assessment of this paper is slightly below the threshold because I am not very satisfied with the high-confidence part. I would like to hear from the authors about the mathematical intuition behind the current loss.	3
The studied problem is important and paper is easy to follow. However, the novelty is limited and the evaluation is insufficient.	1
I appreciate the analysis and the novelty of the proposed training method. However, the current loss design seems to be not effective enough, and there are a few important details missing in the experimental evaluation. Therefore, I do not lean towards acceptance. The authors may try to follow the proposed principle to improve the detail design of the current training approach.	3
Overall, the paper provides valuable theoretical contributions in the form of their analysis of the perturbed distributional bellman optimality operator (PDBOO) and its successful application in toy environments (N-chain) as well as some non-trivial ones (LunarLander, Atari). However, the paper lacks in quality of writing and empirical analysis.	3
"Pros
- Important problem

Cons
- Unclear whether their approach solves the stated problem
- Limited theoretical guarantee
- Weak experiments
"	2
Overall, I think the main idea is clearly articulated. While there is room to improve, the theoretical and empirical contributions seem to be relevant and of interest. 	3
The paper presented a novel architecture for style control in abstractive summarization. However, without evaluating the model on a dataset where each input is associated with multiple references of different styles or running human evaluation, the effectiveness of the style control and the quality of generated summaries are not clear. 	2
The novelty of the proposed method is thin. However, the paper presents rigorous experiments and evaluation to validate the main claim. Performing human evaluation to judge the quality of the generated summaries would be beneficial.	3
"Overall this is an interesting paper proposing a new approach to stylistic controllability for neural text summarization. To the best of my knowledge, this approach is novel. I believe this work is interesting as is, but were surprised by some of the experimental settings that I would like to see clarified by the authors.

"	3
A valuable contribution to controlled generation. The evaluation focuses on the strong point of the models (more diversity) and is somehow unfair (against non-controllable models that are smaller). In this sense it is not clear if it is better or just different than existing methods.	3
Like I mentioned in the previous section, I think the overall idea of the work is interesting. But the maths behind the proposed approach is not rigorous and confused. I find a lot of confusion reading Section 3, which is the main section of the paper. I have several questions to the authors listed in the previous sections, hopefully, the answers from the authors will be able to clear my doubt.	3
Given the lack of literature review and comparison experiments, plus the bound for $\epsilon_{BO}$ lacks theoretical guarantee. This paper needs a major revision. 	2
This paper proposes a novel early stopping criterion for Bayesian optimization. It is well-written and well-motivated. The proposed idea is simple and technically sound. The claims are well-supported by theoretical analyses and extensive experimental results.	3
The paper tackles a vital problem (OOD generalization) with a novel adversarial training approach. The paper also illustrates an interesting connection to Invariant Risk Minimization. The method is supported by good performance on four OOD benchmark datasets, where it outperforms both IRM and standard adversarial training. Additional insights into the benefits of DAT compared to IRM (given the similarity) and a more extensive quantitative evaluation would strengthen the paper. 	4
"Although the inquired relationship between IRM and AT is interesting, the proposed approach is only marginally novel, as it is a simple adaptation of UAT to Domain Generalization settings. UAT acts as data augmentation, so it is not surprising that it is beneficial in DG settings. 

Furthermore, I am not convinced by experimental results: there is only a limited insight on favorable scenarios, as domain split results are not presented, and improvements are mostly due do very good performance on a single dataset consisting of synthetic digits.

Still I am looking forward for the authors' response, and I hope that they can address my concerns (see Weaknesses)."	2
"I think the idea of treating every domain independently while performing AT for DG is very interesting. It is indeed reasonable to assume that each domain will rely on different nuisances that are not relevant for the task at hand, and removing those at domain level seem to be indeed effective.

Yet, I'm not fully convinced by the experimental analysis and the connection with IRM (in order of importance, in my assessment). (Gulrajani and Lopez-Paz, ICLR 2021) provide clear guidelines to compare DG methods in a fair fashion, and this benchmark has been widely accepted in the community (cf. recent works on DG). Since this benchmark is embraced in this submission, I do not understand why Table 1 i) provides worse performing baselines and ii) does not include results related to all the benchmarks.

I look forward reading the Author response and participating in the follow-up discussions."	3
"Well motivated idea that derives connections between IRM and AT; experiments are found lacking.
During discussions with authors, more empirical evidence was provided and most of my questions were addressed."	2
The paper is clearly written and motivated but lacks on the experimental side, both in results of current experiments, and also the scope of the experiments is rather limited.	3
"This paper is a potentially significant contribution to the area of graph ML.
At the moment, however, I have some concerns regarding the training mechanism proposed, specifically regarding the utilization of ""discrete gradients"". Moreover, the experimental section could be improved."	4
Overall, this is an interesting paper which studies a topic that has not been fully explored yet.  Even though the originality of the paper is not stellar, the paper introduces some new ideas. However, there are several issues that the authors need to address (e.g., running time, investigate whether clustering is consistent, ablation study on some other dataset, etc). Furthermore, the empirical results are not bad, but they are also not impressive. Thus, the paper does not seem ready for publication in its current state. 	3
I think this line of work has great potential in its current state, I do not believe this paper is ready for publication. 	2
I recommend rejecting this paper as important baselines are missed. 	3
The paper proposes an interesting approach, but the writing and presentation were incomplete / unclear. 	3
"I do not believe this paper is ready to be published. There are a number of fundamental problems with the presentation and technical results. It may be that there are interesting or novel ideas here, as the experiments would seem to suggest, but unfortunately I believe it would require significant revisions.

I have the following high-level suggestions to improve the paper: The algorithm and basic method should be formally stated early on in a clear and concise way. When an approximation is made, it would be helpful to state it explicitly and justify it (if possible). It would be helpful to remove confusing claims and limit unnecessary discussion of prior work. "	2
**Overall evaluation:**  I think the paper addresses a very important problem. Generally speaking, non-memory-replay-based methods for overcoming catastrophic forgetting could suffer from intransigence, which is the inability to learn new tasks due to increased stiffness/rigidity of the network. This reduces both forward and backward transfer, especially when the tasks are similar (e.g., revisiting an old task). This paper provides a rational solution to this problem, which provides consistent numerical improvement over the state-of-the-art. From an editorial point of view, the paper is well-written, easy to follow, and it provides a good overview of the recent literature on the topic. Hence, I think this is a good paper and vote for its acceptance.  	4
This paper put forward the main problem of existing methods and provides a detailed discussion. The author proposes a novel solution to this problem, which achieves substantial performance improvement on all benchmarks and overwhelms its baseline method. But some of the claims seem not convincing and some seem lack illustration.	4
"Current works in continual learning focus mostly on tackling catastrophic forgetting and overlook accumulative learning. Hence, I think the authors have selected a good area and the proposed idea also seems to be sound. However, I have some reservations about this work:

1. Given the expectation in the recent literature on continual learning, only relatively simple datasets are considered in the experiments. I think results on more complex datasets such as split-Sub-ImageNet and Split-ImageNet should be added. 

2. I was wondering why forward transfer metric is not used for comparison? I think it is as informative as backward transfer metric.

3.  An area of interest is to analyze the learning curves and dynamics of learning, i.e., performance vs learning epoch. It is helpful to see if the proposed method enables better jumpstart performance when a new task is learned.

4. Comparison with more works is missing. For example, EWC and HAT are both outdated for regularization-based methods and many recent follow-ups exist. For a realistic comparison against prior works, state-of-the-art methods for each group of methods should be included. Please check the recent literature.

5. Computational complexity is overlooked. Whereas it is an important factor for continual learning. I think it is necessary to include how much additional computational load should be performed to benefit from TRGP.

6. For a more informative comparison, the standard deviation in results should be reported. Also, BWT metric can be reported more accurately by including more decimals.

 In conclusion, I think this work is in a good direction but further improvement is necessary to make it suitable for a venue similar to ICLR."	3
"This paper proposes a heuristic to improve GPM (Saha et al.).
However, the proposed method lacks theoretical justification, useful insights, and some essential experiments.
Also, the overall writing should be improved.
Therefore, I do not think this paper meets the ICLR standards."	2
This paper is interesting and has some promising results. But I have concern about its application in the real world and its robustness in more complex scenarios, which I hope the authors can address. Therefore, I incline to reject this paper.	3
As a result, even though I liked the ideas in the paper a lot, I am going to vote for rejection unless I can see the experiments added to the paper.	4
Overall, the paper comes up with an interesting approach to learn from very few samples. The approach is plausible, backed up by theory and seems to work for simple tasks. However, it seems to be questionable if this would also work for more realistic datasets. Moreover, the experimental section would need to be overworked. In this way, the paper would be an interesting contribution, but there are still a few open points hampering a publication as it is.	4
The main idea is novel and interesting, and the experimental results on limited training data demonstrate the superior performance of the proposed method. However, it may be not easy to apply this approach to other more complex tasks. In summary, the reviewer thinks this paper is marginally above the acceptance threshold.	3
A paper with a novel method to reduce biases in text encoders and also reasonable empirical results. I would recommend to accept this paper.	3
"Overall, while there are merits in proposing attention based debiasing, the evaluation methodology is weak. It is another paper treating fairness issues as benchmarking without questioning on the actual effect, differences, and whether the datasets are meaningful in the first place. 
"	2
"The method presented in the paper is intuitive and simple, with some,
potentially considerable computation overhead. While the semantic
performance is preserved and there are modest improvements on
intrinsic measures of bias, the effects of the model on the bias
results of a downstream, application task are not addressed and,
hence, not understood."	3
The main contribution of this paper is well claimed and verified. With a small revision, the algorithm can achieve better performance on the examined tasks in this paper.	2
"I recommend acceptance of this paper. Main strengths of the paper are:

+ Compelling motivation of algorithm
+ Careful analysis of its design
+ Competitive performance

Main weaknesses are
- Overly complicated presentation of the algorithm

Post rebuttal: 

I've read other reviews and the author's rebuttal and maintain my recommendation."	3
"The paper does not provide compelling evidence that their method addressed a critical problem, and the experiments make it difficult to know if the method proposed really outperforms current meta-offline RL method since details on tuning and important comparisons were not included. Moreover, the overall method is rather complex and potentially limited to the pure-offline RL (unless I am mistaken) evaluation scenario, making the practicality of the method questionable.

Edit: Based on the rebuttal, I've increased my score up to a 6."	2
The paper has a good motivation and provides a reasonable solution to the problem in offline meta-RL. Theoretical and empirical results support the advantage of the proposed algorithm over recent baseline algorithms.	3
A strong paper lacking some discussion on complexity. 	3
"The contributions featured in this paper provide a valuable and non-trivial extension to the literature on NNGPs. Given that this is a fairly new area of research, such work is especially timely. Even so, I still believe that the paper’s writing could be heavily improved in order to make the exposition more clear. I also have some remaining concerns on the experimental evaluation that I would like to see addressed in either the rebuttal or a future revision of the paper before tending more heavily towards acceptance.

** Rebuttal Update **

Raised score to 6 following clarifications and additional experiment results provided by authors."	3
Overall, this is a well written paper that studies an interesting extension of NNGPs to a scale mixture version, which allows realization of a richer class of stochastic processes such as those with heavy tails. Rigorous results are provided and they seem to be technically correct (although I have not checked all the details). These are the key strengths of the paper and the main factors behind my rating (an acceptance). However, no intuition or theory is provided to justify the empirical findings for the robustness, which seems to be an important practical utility of the proposed approach. 	2
Nice theoretical results that lead to already known methods in the machine learning community which questions the novelty of the paper.	1
I think the originality and evaluation of the paper need to be strengthened, including comprehensive comparison with state-of-the-art randomized algorithms as well as empirical evaluation on real-life data.	2
The problem is well motivated and of considerable interest to the community, the numerical experiments appear promising as well. On the other hand, formal proofs are missing as is any discussion of time complexity results which seems to be a central contribution. At the moment, I believe the paper is below the acceptance threshold. 	3
"It is a well written paper, but the contribution seems to be incremental.
"	2
"This is an interesting paper that combines theory and empirical insights to
highlight differences between angle propagation in random CNNs and feedforward
networks. The results should inspire interesting follow-up work in this
setting. I hope the authors will clarify the relationship to the theorems in
this work to any possible prior art in e.g. CNTK papers, as mentioned above.
The concentration results are basic, but probably enough as first-cuts (it
seems important to acknowledge limitations, however).
"	3
Good, self-consistent and easy to ready paper. Some proofs are not needed.	3
"My major concern is the technical significance. At the current stage, I think this paper may not be strong enough for ICLR.

**** After seeing the author's comments. I agree the JL-type results for CNNs are different from those in the literature. I change my score accordingly."	2
Overall a strong paper, tackling important and underexplored problem of knowledge-grounded open-ended generation, proposing novel objective, significant empirical improvements on two datasets in multiple metrics. There are a few concerns like absence of a potentially stronger baseline, justification of evaluation metrics, choice of terms, choice of implementation details.	4
This is a simple and interesting idea which shows promise in performance. However, the experimental results are a little in weak -- in particular there is no study about hallucination/memorization in these models and some of the modeling choices aren't well studied.  	2
"The authors show improvements in two downstream tasks when improving retrieval via posterior-guided supervision of a retriever in a retrieval-augmented generation setup. However, the datasets considered are not explored to their maximum potential, with appropriate baselines missing from the paper (training the retrievers directly on the target passages); and, datasets where this solution appears to be most applicable are not considered at all (those without target passages). The paper indeed supports its claims, as mentioned in the summary, though some work is missing that could bolster the proposed efficacy of the method. 
"	3
"This work proposes a novel approach to the knowledge grounded text generation tasks via the design of a new ""guide-retriever"".
The methodology is reasonable and the experiments show significant improvement over one baseline. However, this work can be improved if comparison with more baselines is provided."	3
The paper proposes and  two graph structures to model EEG sensors and brain connectivity and a pre-training method to improve model performance. Both of these are well demonstrated and the results demonstrate how one structure is superior to another for a a particular type of rare seizure. It also includes an occlusion map based seizure localization method. Overall, the paper clearly states its goal and does a good job of reaching those goals using well demonstrated experimental results.	3
The authors present a graph neural network method for seizure detection and classification. The results are shown to improve current state-of-the-art. However some of the design choices and interpretability analysis needs more background and motivation. In particular, the details of the self supervised implementation needs more elaboration. 	3
Overall, the paper covers a research field of interest from theoretical (Spatio-temporal coding of time series using graph and DCRNN) and practical (EEG-based seizure detection and localization) perspectives. The experiments are exhaustive and convincing.	3
"### Detailed comments

1. The neuron masking model augmentation has been proposed in CL4SRec, but claimed as a contribution in the paper. In this way, this model augmentation can not be also considered as a contribution. 
2. Even though overclaimed contributions are included, the contributions may not sufficient. The proposed methods look a little straightforward and motivations are not supported by analysis. 
3. The detailed setting of baselinesa are not given.
4. Regarding encoder complements, the authors should compare the baselines with GRU encoders as additional module. This is for illustrating the effectiveness of self supervised learning compared to simple ensemble .
5. ""‘SRMA w/o D’ outperforms other baselines on the Sports dataset and has comparable performance to ‘CL4S.’, which indicates the model augmentation is of more impact in the SSL paradigm compared with data augmentation"". The authors should discuss more about why the superority of SRMA w/o D to CL4S can indicate that the model augmentation is of more impact in the SSL paradigm compared with data augmentation. "	2
This paper proposes three levels of model augmentation methods: neuron masking, layer dropping, and encoder complementing. But the novelty and contributions are limited. Besides, it fails to explain the motivation of the proposed augmentation methods.	2
The paper addresses an important research problem and shows that a few model augmentation techniques can help with the sequential recommendation performances. My main concerns about the paper is the lack of explanation for the differences between model augmentation and regularization techniques. Addressing the concerns will help us understand us better if the proposed methods are general enough to be applied to other research problems.	3
This paper proposes a novel method to improve the performance of unsupervised denoising methods. But I still hope the authors address my concerns above.	3
This work proposed an interesing self-supervised denoising method and its improvements are significant. But it has several issues to be addressed listed above. 	3
The case for this paper is quite clear to me. The paper present a minor generalization of the noise2self, which is conceptually more general, but not constructive scheme is developed to make it more applicable that noise2self. There is no concrete example to show its advantage over noise2self in terms of generalization to different problems or different noise. This paper is not a theoretical paper and its value largely lies in its empirical performance.  There have been many recent works on the unbiased estimator of the supervised loss function, the same as the proposed one. e.g.  noiser2noise[CVPR'20], self2self [CVPR'20], Noise-as-clean [TIP'21], partially-linear denoiser [PAMI'21], R2R [CVPR'21]. These methods provide competitive performance to the representative supervised denoising network, such as DnCNN, for various noise types including Gaussian, Poisson and real-world noise. The experiments conducted in this paper does not provide any comparison to these methods on standard benchmarking dataset, and the available experiments seems that the gap between the proposed one and the supervised one is quite big. In other words, the practical benefit of the proposed DDSL is not impressive.	1
WPipe is basically a memory efficiency optimization of PipeDream-2BW, but the paper does not report throughput benefits on real world models. So it is not clear how practical the optimizations are. 	2
The novelty factor in the proposed work is incremental and I also feel the paper can benefit from some more polishing of the draft due to the presentation issues I outlined.	2
"This paper introduces a novel pipeline training strategy and comprehensively evaluates its performance.
Although the writing can be improved, I recommend accepting this paper."	4
The paper proposes a new model-parallel pipeline called WPipe, for DNN training. A careful construction is proposed, based on two groups of weights. A key feature of the new scheme is the placement of the forward pass of the next step for one of the groups (G0) at the front of the backward pass of G0 of the current step. This elegant construction, allows group G1 to only maintain one weight version and G0 to maintain only two weight versions, thus significantly reducing the memory overhead. Theoretical analysis and an extensive experimental evaluation on text and image data show that WPipe outperforms state-of-the-art schemes, being 1.4 times faster while using 36% less memory.	2
Overall an interesting and elegant approach, that shows some gains. There are weaknesses, discussed above; looking forward to the rebuttal from the authors.	3
The paper technical contribution is interesting, however the experimental section show the performance gains can be marginal and claims regarding results should be revised. Additional references and experiments are needed to draw conclusions regarding the actual empirical contributions of the method. 	3
"The idea of this paper is interesting and makes sense.
Although the performance improvements against existing methods are not that significant, the stable improvements on multiple benchmarks demonstrate the effectiveness of the proposed method. 
"	3
Theoretical analysis is limited to toy models. Empirical studies should be improved demonstrate that square loss leads to more robust and calibrated models. 	1
See above	3
In a word, I agree with the argument put forward in this paper and think its theoretical conclusion is correct. If the author can provide the theoretical comparison between SL and CL and supplement some missing citations of related work, I will more recognize the integrity of the paper.	3
"my main two concern:
- missing discussion of the relationship between line of research of using NN as a non-parametric estimator.
- I don't see any discussion of the specialty of the l2 loss. The paper don't answer the question of how l2 loss and CE loss is different. "	3
I am not very familiar with the related work and thus cannot really evaluate the papers significance and potential impact. That being said, the paper is difficult to navigate, as crucial information lies in the appendix and the experiments do, from my perspective, not really corroborate the theoretical contribution. Therefore I cautiously give a score of 5.	3
The theory in the paper utilizes polynomially-sized Boolean circuit theory, which is an interesting connection. The paper does not have obvious contribution to practically trained GAN models, however, this hardness result provides revealing insights of GANs. In fact, the paper opens some directions to investigate and should be highly important to GANs. For example, if we change the architecture of the generator, does the discriminator in the paper still lack power? Maybe an easier (somewhat orthogonal) question is if the generator network is powerful in representing the data distribution in Wasserstein distance, does there exists good choice of discriminator (poly size for example) to guarantee the distribution recovery in Wasserstein distance. Overall, I am positive on the paper.	4
Overall, I think this is an interesting paper and currently, I tend to accept it if all of the concerns are well addressed.	3
This is a solid work and proves an interesting result. The presentation can be improved, and in general the paper is not easy to read. The experiments are quite weak.	3
The paper is highly theoretical. My major concern is the significance of the result. It seems to be well-known and intuitive that fooling a weak class of discriminators does not imply exactly learning the target distribution.	3
The paper provides a new perspective on the important problem if GAN is able to achieve distributional learning. In general, the paper is well-written, and to the best of my knowledge, the proposed methodology is novel to this type of problem. Thus I recommend acceptance.	4
"The algorithmic idea is interesting, well executed for RL and probably useful for optimisation in general, but comparison with other ensemble approaches is missing in the related work to better understand its novelty.
Simulation results are promising but not fully convincing as it shows similar training performance to previous methods, but it is not clear the final performance of the proposed approach for many of the most interesting environments. 
I lack a clear practical motivation for the CEM approach.
 "	3
I find it really hard to judge this paper, since it has strong and weak points. As mentioned above, I like the idea (it is straightforward and easy to grasp, yet well motivated), the paper is very clearly written, has clear notation, and gives good intuitive illustration of the idea. On the downside, I do not think the results are really convincing (although there is some signal), I lack hyperparameters on the number of mixtures in the toy experiment (and the sensitivity of results to varying it), I completely miss related work from the CEM literature, there is no Discussion and Future work, and the graphs are not easy to read. Individually, all these downsides can be overlooked, but with all of them together I get in doubt. 	2
"The advantage of the proposed algorithm is unclear and there are no theoretical insights within the paper. The algorithm is a trivial extension of POPLIN and has little relevance. Therefore, I recommend rejecting the paper.
"	1
The paper contributes to practically very important yet relatively little explored area of research - that of predictive modelling with missing data. The proposed method is rather complex, composed of multiple steps adding onto each other to solve a problem arising in the previous steps. These are all well motivated, however, overall the current presentation of the method is difficult to follow and should be improved to help the reader (see main review). Moreover, the documented performance benefits seem to be rather little to justify the use of such a complex method over simpler baseline. These two (lack of clarity, low performance given the complexity of method) are for me the reasons not to consider the paper for this conference. 	3
The conceptual part, i.e. the model and the proposed learning approach are in my view concise and sufficiently novel. This outweighs the missing scalability analysis in the experimental part. I would however expect the authors to clearly address the raised conceptual questions.	4
Given the strengths of the paper listed above, I would recommend acceptance for this paper, if the authors can figure out a clear feedback for the questions I summarized when reading the paper.	3
The author propose an interesting discriminative learning approach with generative modeling to solve the missing data modeling problem, by extending the traditional variational lower bound (ELBO), with a novel and stable upper bound that can be estimated without bias with Monte Carlo estimation. It is better if the author can study more on the missing data pattern (both empirically and theoretically) and the optimal solution preservation (under sub-optimal cases). Also, the empirical performance still has some space to improve.	3
The paper identifies an interesting shortcoming of prior personalization approaches. However, given the various weaknesses outlined above (e.g., insignificant results, misrepresentation of results) and the limited novelty, I do not believe that the paper meets the bar for publication in its current form.	2
I think this work tackles an interesting hypothesis that can limit generalization in case of personalised FL. The proposed solution is principled and well motivated with appropriate ablation study. The only drawback would be lack of experimentation on large scale problems which would certainly make this a valuable piece of work.	3
The idea is good and novel, however, the presentation is disappointing and the experiments are weak. I recommend rejection.	2
Though the idea of balancing between behavior cloning and optimal value learning through the Bellman expectile operator is interesting, the clarity of the current manuscript still require some improvement. If the authors could address my concerns and questions, I'd be happy to adjust the score.	2
"Overall, I think this is a good paper. But I am a little concerned about the novelty of this work. So I will give weak acceptance.

_____________________________________________________________

I have read the author's responses. My major concern has been solved. I decide to raise my score to 8.
"	3
See above. 	3
The Expectile V-learning (EVL) is new, the Value-based Episodic Memory is simple yet efficient, the theoretical analysis is convincing, and the experiments are supportive. However, my major concern is that none of the contributions are significant. I am looking forward to the authors' response, especially justifications on the novelty and effectiveness of  EVL.	2
Weighing the strengths and concerns of the paper, I am recommending weak accept as of now.	3
"Given the comments in the previous section, I think it would be good if the authors can address the raised issues. Based on its current status, I would vote for a weak reject.

_____________________________
post rebuttal update: I've read the response and most of my concerns are addressed. I will raise my score to a 6."	3
Incremental but sound idea. Presentation could be improved.	2
While technically sound, I don't advocate for this work to be accepted at ICLR as the contribution is too marginal.	2
"The paper is well written and the idea is simple and straightforward. The novelty is limited. My main concern is about the design of the known kinetic energy (mass) and the claim of generalizability to unseen system sizes. The known kinetic energy design does not make sense. The generalizability to unseen system sizes is only valid for homogeneous pairwise potential energy. The whole framework seems to only work for a specific system - interacting particles. This limited applicability also weakens the contribution. I think this submission does not meet the ICLR standard, thus I recommend rejection. 

To meet the standard of major AI conferences, the paper should at least learn masses simultaneously and explore more application scenarios other than interacting particles. "	2
"The manuscript is well-organized, and the experiments are reliable. The main concern is the novelty. Many similar approaches have been investigated in the community of molecular dynamics, and the improvement from them is unclear.
"	1
"I recommend this paper to be accepted for the ICLR conference (score: 8 good paper). Overall, the paper is well written and does extensive ablation studies for their design choices. In this paper authors proposed a monocular Image-based 3D object detection method that distills the knowledge from the LiDAR-based teacher. At the inference time  this method predicts at 25 FPS on GPU without any intermediate depth-prediction, which is critical for the real-time application such as autonomous driving. 

However, there are some minor issues in some of the claims (please see above). I would like to see the authors addressing “Weaknesses” in the rebuttal. 
"	2
The basic motivation is quite good and is worth studying. The current ablation studies also demonstrate the good point of cross-modal distillation. The performance seems good enough in the related works. But it’s known that cross-modal KD is helpful as shown in the Main Review (1), which is not a new enough idea. And the technical improvement of the proposed KD method over the baseline KD approach should be strengthened more. Overall, I would like to give the rating of borderline reject.	3
"While the paper has minor weaknesses, I believe that they can be corrected in the course of the review process.
The strenghts (in particulat S1-3) make this work worth sharing.
"	2
This paper applies knowledge distillation to image-based monocular 3D detection and achieves state-of-the-art results. It can provide a potential path to better leverage LiDAR signals to boost the image-based 3D detectors while not much influencing the original design and efficiency. However, there are some concerns in terms of novelty and generalization ability. So I would vote for borderline accept temporarily.	2
This paper has many problems in experiments, especially the authenticity of the experimental results, and the completeness and richness of the experiments. And there are many writing errors in this paper.	3
Given the above issues, my current review leans towards negative. My major concern is the lacking of novelties and the technique correctness of some design choices. I will appreciate it if authors can address my concerns in the rebuttal and I will change my reviews accordingly. 	2
In summary, the proposed method using lighting transportation function is a novel approach to represent an object and extends previous works on learning BRDF of an object, considering the missing comparisons and the limitations, I vote for the board line initially, and I'm happy to listen to the authors and other reviewers.	2
"This paper introduces object-centric neural scattering functions for modeling light transport of an object, allowing novel lighting rendering and scene composition. However, the data requires to optimize the OSFs is restricted and the rendering time complexity is high. Also, the experiment is not very convincing due to the lack of comparisons. I would like to give a negative rating at the current stage.
"	3
"The paper proposes a well-designed method and is promising. However, experiments are not yet quite sufficient for acceptance. That's why I will go with borderline reject for now. 

Experiments on more real scenes, for example from the same dataset as currently, should be added.

Furthermore, the experiments suggested under ""Runtime"" above should be added, though possibly in a reduced form since the method might be too slow to feasibly obtain these numbers. In that case, I'd prefer an experimental setup wherer the number of pixels is reduced (e.g. randomly picking 1000 rays from a full image, and doing that for, say, 100 images, ideally across different scenes in terms of objects and object positions) rather than fewer settings tried. 

Demonstrating plausible consistency while smoothly changing light position/object positions also strikes me as beneficial but I don't believe that acceptance should hinge on it. "	3
Good theoretical insights, can have better connection to practice	4
Overall, this is an interesting paper and a valuable contribution to ICLR. I think that the lack of empirical results and experiments is justified by the theoretical contribution. Subject to some of the further improvements in aspects of the presentation, as well as discussion of additional related work suggested in my main review, I recommend acceptance. 	3
I think this is an exciting direction of work that runs orthogonally to a lot of the recent advances in disentanglement, by assuming knowledge of the underlying mechanism rather than independence and distribution assumptions on the latent variables. Presentation of an algorithm or some basic experiments to justify the feasibility of the new setting would substantially strengthen the paper. However, I think the identifiability results are useful enough for follow-up work that they have merit on their own. 	3
The paper studies an interesting setting, but should explain more about the potential problem of the method (please check the main review) and compare it with a more recent MA-ICQ method.	2
This paper studies an interesting problem in offline MARL. A transition modification technique, OTC is proposed to correct the transition bias during the learning. However, the experimental evaluation is not convincing as important baselines are missing, and the proposed algorithm seems to be too straightforward due to lack of theoretical analysis. Thus, I recommend the rejection.	2
Although the paper is interesting, I still have some concerns on multiple aspects. Three important issues are: 1. The details of the algorithms. 2. The unique design for multi-agent settings. 3. The high-level idea of using similar samples for update.	3
"Though the proposed work is sound on the technical side, I feel not surprising about the proposed method. The novelty is rather limited in the sense that the proposed method is a way to shift the offline data for updates in the context of BCQ. The experiment also seems rather limited; showing its effectiveness over BCQ is also not surprising either. Given the novel setting of offline training with online fine-tuning, the author maybe want to consider transfer learning cases like few-shot settings, and experiments on other multi-agent environment such as StarCraftII will also be favoured.  

"	2
A marginally above the acceptance threshold is given. Overall, this paper addresses the over-smoothing problem in BERT. Theoretical analysis on the over-smoothing problem in BERT from the perspective of graph is new and provides interesting findings. However, proposed fusion methods are not that novel and not well theoretically and empirically proved to be able to address the over-smoothing problem.  	3
The analysis of oversmoothing in transformer is a good contribution. The limited empirical evidence and lack strong improvement over oversmoothing by the proposed method are the main limitations of this paper.	3
Overall, I vote for accepting this paper. I like the idea of establishing an analogy between self-attention and the adjacency matrix of a graph model, and I like the theory that establishes the relationship between layer normalization and dimensionality reduction of token representation. My major concern is about the clarity of some parts of the paper and some additional improvement. I hope the authors could address my concerns in the rebuttal. 	3
An accept has been recommended for this paper for the interestingness of the topic and the novel perspective of theoretical study of BERT by graph neural networks. This will inspire some new directions for researchers and hence an accept is recommended. 	3
This paper studies an interesting offline decentralized MARL setting. However, relevant works are not cited and discussed, and experimental evaluation can be improved.	2
The topic is interesting, but the setting lacks a strong applicative example and the formalism of the paper should be improved before it is worthed for publication. Moreover, I think that the statements sometimes are hard to understand. Therefore, I suggest the author should try their best to improve the readability of the paper. Moreover, the experiments should provide some statistical significance that the proposed method is better than the state of the art.	3
A lot of descriptions in the submission are very vague and the statements are mathematically imprecise. I do not think the submission is ready for acceptance with its current presentation.	2
This paper studies a novel problem in MARL area. However, the proposed value deviation method is not well motivated, the experimental evaluation is only done in replay datasets, and other marl offline paper are not cited and discussed. Thus I recommend the rejection.	2
This paper explores a novel direction and gets impressive results. I strongly recommend accepting this paper.	3
"I am leaning towards a reject for now, but I am happy to be convinced. I am aware of how crushing it can be to authors to read these things -- and I would like to say that I think the work has strong value to the community -- but it is not necessarily a strong piece of novel research. 

Post rebuttal:

I am not convinced that my original view has changed regarding novelty. I strongly recommend that the AC rejects the paper since I do not believe the research contribution is sufficient."	2
"This paper introduces a simple yet effective technique to reduce the memory usage in GNN training, which demonstrates promising results. However, I still several questions/concerns for evaluation metric, scalability, design rationale, and key insight. I'd ask the authors to address them in their rebuttal.
"	3
The method seems reasonable but the approach is very complicated and there are some lacking experiments that would make the claims more compelling. If the community is to be convinced to embrace such a complicated approach experiments need to be more compelling i should think.	3
The authors propose an approach to self-supervised learning for reinforcement learning which is very similar to SPR. The key contribution is the addition of a flow-based approach to predicting future latent vectors. Because of this, the generality of the approach is limited to image inputs. However, the results are promising and hold across different domains and agent setups. 	2
The paper combines existing self-supervised architectures (self-supervised flow estimation and InfoNCE-like similarity loss) to learn visual features in a rather novel and intriguing combination, but the overall increment in RL performance does not seem to justify the added complexity compared to much simpler counterparts.	2
I think the paper provides an interesting new objective for encouraging the representation by self-supervised learning also captures important local information for efficient learning of RL tasks. 	4
This authors propose to use PMI for label sorting in the set generation. However, the experiment only demonstrates the importance of sorted labels is better than random permutation, but not how effective it is when compared with other sorting methods. 	2
The paper proposes a simple, interesting idea, but needs additional baselines to show the proposed approach is useful.	3
This work provides an interesting data augmentation approach to training set generation models using seq2seq formulations. I would like to see comparisons against more fair baselines as well as non-seq2seq approaches to better contextualize the applicability of this work, especially with regards to methods for seq2seq set generation that do not rely on data augmentation.	2
"The results seem promising; however, the paper could be improved in the presentation and more detailed descriptions with comparisons to the SOTA models. 
"	2
The idea of  applying low-precision optimization to SGLD is a direction worth exploring, but the ambiguity between applying SGLD as a sampling algorithm versus an optimization (training) algorithm in this paper has made the current results in this work quite questionable. Some mathematical statements and notation in this paper are also unclear. 	2
A novel paper with reasonable techniques. The results might be improved by tightening the assumptions. 	4
In light of the specifics in the main review, the discussion of low-precision SGLD in this submission is not significant enough and there are some places in the argument that are not clear.	2
The paper proposed some analysis for low-precision SGLD but the novelties or depths in terms of methodology and theory are both limited. In addition, the experiments don't match the theoretical analysis and don't have the results for uncertainty estimations, which leads me to suspect why to bother to use SGLD (I believe SGD with fine-tuning parameters can also achieve the claimed results). Based on these evaluations, I tend to reject this paper.	2
Due to the lack of precision and weaknesses in the experiments, I cannot recommend this paper for acceptance, but I do acknowledge that the ideas may be helpful. 	3
This paper wants to propose a new deep RL method, which makes use of their decoupled objectives. However, I can see slight improvement over their methods. And the proposed EWMA algorithm contributes little to the batch size invariance.	2
The paper does a good job in explaining the proposed method, in running appropriate experiments, and in terms of writing. However, it fails to motivate a single objective or problem which is addressed by the proposed method. I do not see what the main contribution is and so am currently recommending a weak reject. I am happy to revise the review if the authors can address these concerns.  	2
"This paper presented some interesting ideas of batch size-invariance of policy gradient. 
Although the experimental results well demonstrated the effectiveness of their method. 
The method lacks theoretical justification, and the claim of the batch size-invariant property is vague and weak.  "	2
This paper can be much improved by conducting a more extensive numerical experiments, basic idea is obvious and not fundamentally novel. 	3
Overall the paper is good, demonstrating a novel method to quickly produce adversarial attacks in hard cases. It could be improved with the inclusion of more extensive experiments.	3
Overall, I think this paper is marginally below the acceptance threshold.	3
"Since it is hard to capture the main point of the paper and the problem it is trying to solve, I tend to be on the rejection side.
"	3
This paper proposes a novel neural network architecture based on MLP. The neural network architecture in speech processing becomes more complicated, and the proposed method can provide simple and alternative solutions. The effectiveness is also shown with command recognition, keyword search, and speech enhancement. My concern of this paper is that the application is rather simple. The method has a lot of potentials and may have more attention if they are applied to sequence-to-sequence tasks like ASR and TTS.	3
Overall, this is an interesting study, the paper is clearly written and motivated and presents a novel architecture which leads to state-of-the-art results for keyword spotting and speech enhancement.	3
The proposed architecture is similar to a convolutional neural network. The context expansion and chunk-wise processing is analogous to the concept of separable convolution. The experiments however, show the effectiveness of the model on keyword spotting and speech enhancement tasks. The proposed approach appears to take advantage of the structure of speech signals, specifically, temporal continuity. The authors have also conducted ablation studies to show how splitting the model into multiple frequency bands has desirable effects. It would be interesting to see how the 'split and glue' output performs on the relevant tasks without any residual connection. Further, comparison with supervised methods and error bars on the evaluation metric for speech enhancement will provide additional insight into the proposed technique.	2
Whilst some results are persuasive, showing that the architecture can perform as well as much larger ones, the method lacks rigor and contains no clear advance over other the methods on which it is based.	2
The paper is well written and easy to understand with clear diagrams. However, I have some significant concerns about the split-and-glue layer (which seems to me to be a groupwise or separable convolution) and the keyword spotting comparisons (which omit results which outperform speech-MLP, then claim that speech-MLP surpasses the SOTA results). With these (and other more minor) concerns, it is hard to evaluate the novelty and impact of the paper, but these issues would be fixed with improved writing, literature review, and hyperparameter search / comparison.	2
"Overall, this paper provides a novel algorithm towards finding the mixed Nash equilibrium. However, the presentation of this paper does not support their result well. As a result, the paper is not ready for acceptance at this time.


----------
Post Rebuttal Comments:

The authors have addressed my concerns. Therefore I'm increasing my score from 5 to 6."	3
The proposed algorithms show its powerfulness in solving the min-max problem. Empirical study is supportive. The theoretical analysis is nicely presented with the QuasiStatic Wasserstein flow. The idea and the theoretical part both show the novelty. 	3
"The authors study a gradient flow of a particular type functional, which comes from an inf-sup problem in zero-sum games. The paper is overall well written. Some key issues on geodesic convexity and relation to literature are missed. 
"	3
Good paper. Clear writing. 	3
This paper would have a broad impact on the machine learning community to tackle noisy data training based on CTC. The formulation is straightforward and the experimental effectiveness is valid. The paper does not show the experimental impact of this method on the real problem, and this addition makes the paper stronger.	3
"The problem is interesting and the paper is clear and easy to understand. However, the contribution of how to use dynamic programming to compute the CTC loss over all sub-segments is marginal. The paper could benefit from a deeper analysis and a more mathematically sound definition.

Edit:
The additional experiments and details are helpful and indeed benefit the paper."	2
The paper is very well written with clear explanation of proposed method, well covered experimental results across domains and tasks. Proposed idea is mainly based on existing DTW algorithm (SPRING), however it is extended and applied to CTC demonstrating significantly improved results on incomplete labels. Thus, I would recommend this paper to be accepted. It could lead to a broader discussion on methods with incomplete labels and data - more real case scenario of data collection.	2
The paper is clean and it is easy to understand the content. The experiments are fair and the effectiveness of the approach is confirmed using simulated data. It is great that the results are given for three different domains. There are certain weak points in the paper, but the strengths slightly outweigh the weaknesses. I would recommend a weak accept.	2
"Although the problem setting is interesting, there are a number of concerns I have with the writing and technical results/proofs of the paper (see above for details). Since I do not think these concerns can be sufficiently addressed during the rebuttal, I do not think that this paper is ready for publication.

----

_Post-author response_: Please refer to my response to the authors. I have changed my opinion of the technical concerns, and the remaining ones seem minor. The authors have given a satisfactory response, and I have increased my score accordingly. I would still like to see the concerns mentioned in my responses addressed before publication, however.
"	3
As above I am supportive of this paper and line of work. I would like to better understand the challenges which will help me appreciate the results better. Thus, I would like the authors to address that.	3
The problem this paper studied is defined in a clear way, but authors may need to emphasize the technical contribution that is built upon the existing work and demonstrate the tightness of the regret bound.	2
I vote for 6: marginally above the acceptance threshold. The paper undertakes the ambitious goal of incorporating the joint effects of three new factors into a more realistic MAB framework. The proposed UCB-FDF policy is shown to achieve desirable logarithmic regrets without excessive incentive spending, and the theoretical results can apply to quite general delay distributions. Although I think the paper is well-written and the results are useful, I find the current version lacking in two main aspects. First, the experiments are limited: in addition to verifying the theoretical bounds, the experiments could also provide more insights about the UCB-FDF policy and/or compare with alternative MAB frameworks to demonstrate the advantages of incorporating the new factors. Second, the technical novelty and significance should be highlighted in a more clear way. 	3
This paper presents a novel method for TTS with convincing experiments and results. Although it has some weaknesses, I tend to see it marginally above the acceptance threshold.	3
Overall, this paper proposes a new architecture of utilizing both untranscribed and transcribed speech data to generate speech with good quality.	3
"The biggest advantage of the proposed framework is it was able to use large amount speech data so it potentially could modeling wider range of prosody or other aspect of speech. However, the paper's experimental results cannot well-support it. I would happy to change the score if the author can improve follows:

(1) Adding evidence such guidance framework is relative stable, e.g. faithfully reflect the text it been given. 
(2) Demonstrate this model is better than a simpler baseline: a text-to-mel model + speaker verification network + vocoder (trained on diversified mel). My bet is if here is a win, DPM model might capture better prosody. If showing prosody win is hard, at least demonstrate this model give better results on Blizzard which i assume the training data is more noisy?
(3) The readability of Table 1 and 2 can be improved. E.g. adding what data been trained for the classifier, which speaker been used etc."	2
"While the novelty is limited, this is still a very interesting direction to investigate. However, in its current form, it provides way too little experiments and analysis.
"	2
As noted in the section above, while I do think this is interesting work with interesting findings, I am not fully convinced of the merits of the metrics proposed. Further comparisons and discussion of other existing metrics, along with demonstrations of the benefits of better joint predictions in a downstream task would be helpful in making a stronger case for this work. 	3
Overall, I vote for accepting. I like the idea of using simulated settings with explicit control on variables to perform benchmark analysis and this paper enables this through a novel framework as well as open source tools. 	3
I believe the scope of the benchmark is quite limited, and the paper does not show any strong benefits provided by this benchmark over other already existing benchmarks.	2
"An overall interesting paper in a new direction (after the contribution of Lu et al. is clarified) to evaluate BDL models that tries a bit too hard to be controversial. 
"	3
Overall, the paper is clear and proposes a novel method. However, the empirical evaluation can benefit from more exhaustive comparison, for example on Domain Bed. It is also not clear if the motivation to use shallow learners in the inner loop of bi-level optimization is practically achieved even in the toy set. Analysis highlighting the characteristics of the biases identified by groups can be insightful. 	3
Overall, I vote for rejecting mainly because I think the technical novelty is limited and it lacks theoretical supports.	1
The studied problem is important and the empirical results are good. The major weaknesses are the proposed algorithm formulation, motivation, and the lack of ablation studies to verify the need/motivation of the components of the algorithm. There is no formal analysis or explanation -- most explanations are ad hoc.	3
"Strengths: A easy-to-implement and automatic method that shows some advantage when more than one spuriously correlated feature is present in the data; It outperforms IRM in most settings, hence a promising direction to explore for actually fulfilling the yet unfulfilled promise of IRM.

Weaknesses: The paper lacks (1) a discussion on the key assumptions under which BLOOD works, (2) a discussion on when BLOOD fails to work, and (3) a discussion on the potential source of biases in real data applications.

Given the above strengths and weaknesses, I evaluate the paper as ""the contributions are only marginally significant or novel"" in the empirical aspect. There is no novel contribution in the technical aspect. Taken together, I would evaluate the paper as ""marginally below the acceptance threshold"". But I do think there is room for improvement regarding the main weaknesses in my opinion."	1
This is an overall strong paper. It proposes an interesting new approach, and does a good job of exploring the behaviour of this approach with respect to alternative architectures. The weaknesses identified are largely minor, points of clarity, or easily fixable.	4
I agree with the author that the foveation module could be helpful for the computer vision task. However, it is hard for me to see the technical novelty of the proposed method. Therefore, I lean to reject this paper.	2
The method presents an interesting and unique approach that is motivated by human vision perception system including gaze and attention control as well as the foveated vision. However, experimental validation is limited, and thus difficult to understand the pure effectiveness. I think other application scenarios such as real-time video recognition may be suit for the scheme than the object recognition tasks for its low-cost computing nature.	4
This is an interesting area of research, but I do not think that the authors did a sufficient job of motivating the model and experiments. They did not motivate the use of transformer networks, or explain why adversarial attacks are an appropriate problem for comparing foveated vision systems. They also left some important questions answered or not fully answered (such as whether the fixation movements were actually benefiting the classification). Experiments provided do not do direct comparison with equivalent models with and without the predictive attention which could easily determine the contribution for this part of the model. Claims in the paper are not sufficiently supported. A priority claim made in the paper as the first work to combine foveated vision and transformer networks is false and ignores published work by Harris et al. 2019 that uses spatial transformer networks for foveated convolutions. Direct comparisons to other foveated vision systems are lacking, and without these it is not possible to assess how well their particular approach compares to the most similar prior art.	3
"Currently, I lean towards rejection due to the concerns I raised. I would like to update my evaluation after discussing with the authors and my fellow reviewers.

***
Post-rebuttal:  I appreciate the authors’ clarifications on my questions. I am happy to see the ablation study result, which can support the necessity of the proposed components more clearly.

However, the essential assumptions used in this paper ($y = A \phi(x)$ with known $\phi$ and $z_U = B z_O + \epsilon$) make the proposed model be applicable only for relatively restricted problems. It prevents me from recommending a clear acceptance for this paper. While I slightly tend to accept this paper (thus I updated the review score accordingly), I would not champion this paper for the acceptance.
"	2
will do a more proper review after my queries have been addressed. Will change my score after the proper review.	3
Technical novelty is limited, and there is no ablation study on the exact proposal of the current paper. Hence, I can hardly recommend acceptance in the current form of the paper.	2
"-The contribution of this work is primarily the empirical one. 
-The authors aim at achieving the interpretability, generalization ability, and robustness of their methods. However, due to the concerns I raised in the weakness part, I prefer to weak reject (marginally below the acceptance threshold) the paper."	3
Dealing with large action is an important research direction in RL. I find the proposed approach interesting, intuitive, and allows for better interpretability. While the results are encouraging, I'm concerned with the amount of prior knowledge that went into this, e.g. semantic grouping of the actions, and rule-based action selector. Could those priors be learned? Also, I believe this paper could be strengthened by evaluating their approach on other types of tasks (i.e., not related to cooking) from the TextWorld suite. For those reasons, I can't recommend it for acceptance.	2
The paper reports a method that decides an action (textual) based on a state (and task) described by a knowledge graph. The idea is interesting, and the results are significant, in addition to enabling the interpretation of decisions made by the system. However, the scope of the proposal seems limited and, despite being well written, some points of the proposal should be further clarified.	2
The method is interesting, but it seems very contrived for a very small and specific type of problem. There appear to be a lot of components which would not generalize even to very similar TextWorld domains. The result is only tested on one small subset of TextWorld and the state-of-the-art comparisons are not adequate.	2
The paper suggests some interesting modifications to the resnet architecture. However, the changes aren't very novel, and the paper did not convince me that these new adjustments are beneficial. The results in the paper appear to be (slightly) negative, but there is no proper ablation study for them to be informative	1
The paper introduces a novel method to initialize a ResNet-like architecture deterministically with binary weights (except for the Hadamard transform where additional rescaling is required). The paper contains some novelty in its model design, has good clarity, but does not show convincing practical significance compared with previous work. Some claims are hypothetical and may require further justification or be supported by further evidence.	3
Although the idea is somewhat novel, for the unremarkable empirical results, incongruent method contradicting the topic initialization, I regret to reject it weakly.	3
Overall the paper presents some interesting ideas. However, there are some concerns such as the space-time complexity of models and limited applicability.	3
The paper is technically correct but lack of strong motivation, theoretical advances and empirical performance.	2
"Reason to accept: new problem setup for editing based text generation, two new datasets, and evidence of multiple history helps generation. 

Reasons to reject: one metric is questionable, examples showing the method not working, missing human evaluation and a straightforward baseline. Without those, it is hard to judge the quality of the generation. "	2
Please refer to the Weaknesses in Main Review, I hope the authors could give a clear answer 	1
"This paper investigates an interesting problem of modeling the iterative process of editing the sequences. The ideas proposed seem to be technically sound and novel.

However, I am not convinced with the tasks, metrics, and comparisons with prior work on sequence editing. 

Metrics: In my opinion, metrics seem to be directly aligned with the model's design, and hence comparing with models which do not optimize these metrics could be unfair.

Tasks: Tasks like Grammatical Error Correction or paraphrasing are widely known and directly aligned with the paper's motivation of iteratively editing. The effectiveness of the proposed method on such tasks can make experiments significantly more convincing.

Comparisons with prior work: The paper compares with only one baseline, while it does acknowledge well-known and recent papers in the domain of sequence editing. The paper rightly argues that most of the prior work models sequence editing as a single-step process. However, the significance of modeling edits in multiple iterative steps will be more evident after comparisons with single-step edit models."	3
"While there are some potential merits to the idea of modeling multiple revisions, I have doubts whether the evaluation demonstrates them in a convincing way.
"	3
"Overall this paper makes an interesting contribution to the field. The technique/key idea are, in my opinion, novel, and the evaluation is thorough. My concerns are limited to the problem motivation as presented in the paper, and the extent of the ablation studies.

It is possible that my understanding of the variational inference and ELBO optimization part is insufficient. I have not checked these parts for correctness, and I am not familiar with related work in the area. I have therefore indicated that my confidence in my assessment is slightly lower."	4
The paper provides an interesting and novel solution to learn better representation with the investigation of edge formulation. However, several claims are not well-explained.	3
This paper uses the graph generation model and a soft-max transformation with temperature term to implement the idea of doing K graph edge partitions in side GNN computation. The presentation of paper is well organized and experiments results are generally favorable. However, the motivations and assumptions are not discussed clearly and verified carefully. The ablation study does not address some of the key design in this model. The overall novelty of this paper is limited. 	2
In light of the above considerations, the paper needs substantial rewriting and the empirical evaluations must be done following a proper protocol. It cannot be accepted as is, and probably there is no time to properly fix these points in the rebuttal phase.	2
While there is a question on the significance of the theoretical results, overall the paper proposed a new domain generalization algorithm that is both technically and empirically solid.	3
Paper has rigorous set of experiments and is backed by clearly explained motivation. Addition of suggested literature could further improve the paper. 	3
Though I am not an expert in this area, I have discussed my comments with my colleagues with rich experience in this area. Thus, I am still confident in my comments. 	3
This paper proposes a simple latent subspace orientation algorithm for domain generalization based on some theoretical and experimental observations. The technical parts with some complex theoretical analyses are satisfied, yet the model construction and experimental parts could be improved.	2
"This reviewer feels the theorems derived in this paper are not very useful to justify the proposed method, and cannot find clear reasons for its advantages. Also, the practical value of the method seems limited as its performance is not impressive or even inferior to previous work when it is incorporated with small networks. 

-- post-rebuttal --
I greatly appreciate the kind responses and the update of the manuscript and would like to upgrade my rating accordingly, but since my major concern has not been resolved yet, I am still leaning towards rejection."	3
I will give marginally acceptation for this paper. The idea is clear and novel but the novelty and contribution is not quite sounding. I can not decline that this is an interesting paper for this community. 	3
The topic of this paper is interesting and worth exploring. Overall, the paper is well-written and the idea is novel. However, there are some questions regarding the experiments and analyses. We would like to see the feedback of the authors to the questions above. Also, if the performance improvement of the proposed model is not significant, we would like to see some theoretical analyses on the advantages of this work.	3
I incline to give a marginal accept now given the originality and enlightening contributions. However, there are some questions towards the experiments that may affect my judgment. See main review for more details.	3
Although the target problem is important and the proposed method is novel, the empirical significances and analysis are quite insufficient currently. If the authors can provide that 1) experimental results fine-tuned from ImageNet model and 2) the empirical analysis how the augmentation parameters are actually changed by proposed method, especially in transfer scenario, I'll raise my score.	3
The contribution is solid - the problem is relevant and quite difficult. There is still so much work to be done in order to close the gap between unsupervised and supervised methods for scene semantic segmentation, but nonetheless, STEGO makes a step in the right direction. 	3
"This paper could have been outstanding, but as it stands, it's only acceptable. The best part is the unsupervised performance boost over prevoius methods (+14mIou@CocoStuff, +7mIoU@Citiscapes) and the ability to use the proposed model on top of most feature extractors. That being said, the authors have shown that the larger the transformer, the better the unsupervised segmentation, but have failed to provide a method that 'just works', regardless of the dataset. Instead, we rely on a number of tricks for performance boosting (mostly related to small objects) that could have been incorporated in the original transformer stage (fair enough, except for CRF, which is pretty much a popular solution for squeezing 2%-ish in accuracy) and some variables we need to manually set - the three-pronged loss that is at the core of the paper does not get a proper ablation study, each term is weighted empirically. Apart from the state-of-the-art performance, the training itself is kind of confusing - some classes do not benefit at all from this stage, sometimes the feature noise is happily propagated in the final segmentation, sometimes amplified - the limitations of the feature extractor and loss have not been thoroughly investigated, IMHO. This is not shown in the main paper, but I have tried the provided code on CocoStuff and those are my findings. Nevertheless, being feature extractor agnostic, future work could expand and improve the performance of this method. Furthermore, the authors suggest that label ontologies can be arbitrary - future work could benefit from a hierarchical ontology, for example. 
"	3
"I have carefully read the paper and like the idea of this paper in making self-supervised representation leanring towards dense predicton tasks like segmentation/clustering without any explicit mannual supervision.

I am aware of pros and cons in this works and would like to vote for acceptance."	3
I tend to weakly reject the paper based mainly based on the concerns about implementation and experiments mentioned above. 	2
This paper has some good empirical contributions but those are all it has to offer. For a paper like this, I wish to see a stronger version of the paper where pruning is generally applicable to all sorts of compact NAS architectures.	1
Paper presents an empirical study on a well studied problem. The setup, method and results are not significant and are well known to the community.	1
I made my recommendations mainly considering the limited contribution and limited practical significance made by this paper. 	1
"The paper is largely experimental and, in my opinion, shows improvements in FLOPS that are not practically relevant. However, the paper promises a few times that the proposed methods provide practical benefits. I am not sure, what to take away of the paper: in my opinion there are hardly any practical benefits and there is no clear takeaway message from the theoretically relevant decrease in FLOPS.
"	1
While I think there is some merit in the proposed algorithm, a lot of the claims made in the paper are not well supported and are extrapolated too much from the results of Muller et al. The theoretical results are perfunctory and the writing it very informal. I think the paper can be improved by making the claims more rigorous either empirically or analytically, especially the label smoothing embedding hypothesis.	2
While the theoretical results in the paper are interesting, these results are not connected to the implementation of the methods. Also, the method has a limited practical significance in terms of scalability. 	2
The paper is well-written. It proposes a simple method for OOD detections along with interesting theoretical analyses and promising empirical results. There are, however, several aspects that deserve to be improved/clarified. Please refer to the main review section for more details.    	3
This paper proposed a deep $k$-NN density-based method for out-of-distribution detection in an unsupervised fashion. Empirical experiments were performed to verify the effectiveness of the proposed method, and theoretical results were proven. However, I think that while this is an interesting paper, it would be better to compare more of the new OOD baselines to make the proposed method persuasive.	3
This work has solid contributions, and the writing and experiments presentation could be improved. 	3
My initial recommendation is to reject this paper.  The authors have not demonstrated that their approach provides useful explanations either via a human evaluation or for some downstream task.  They are missing a reference to at least one work addressing the same task that has claims that are problematic for this paper, and the experiments were not conducted on datasets that were designed for the motivating applications as well, making it unclear if the approach will generalize to the target setting.  The authors need address at least the first two weaknesses before I would consider recommending acceptance, although the 5th weakness is also high on my priority list for changes.	3
Overall, I think the idea is very interesting and the proposed explanation method is good contribution. But the experiment is not strong due to the concerns, and the evaluation could be improved in the future. My rating is borderline leaning to accept.	4
**Recommendation:** reject.  I like the paper and topic a lot.  But it's a bit hard to objectively evaluate the paper as it is currently written.	2
While the authors proposed a maximum deviation approach to measure model safety, the overall research question remains ambiguous and both theoretical and empirical supports for the proposed method and its claims are insufficient. 	2
"I like the idea of inspecting the safety of black-box models through approximate white-box models, and I think some ideas (Assumption 3 in particular) are interesting and novel.

However, the main theoretical results are trivial or simple modifications of the existing results.
Moreover, the experimental results showing only trivial deviations lead to a question whether it is possible to obtain non trivial maximum deviation in practice.

Overall, I think the paper is interesting in its underlying idea.
However, the technical novelty is marginal, and there remains a fundamental question when we can have non trivial maximum deviation."	3
In summary, the technical contributions and the evaluation of the approach seem solid. However, I do not see how exactly the proposed dissimilarity (the maximum deviation) can help a domain expert to retrain a safer model based in its output, which seems like the goal of such a metric. Inspecting every subset of data points on which the new model and the baseline differ substantially might take a lot of time.	3
Overall, being among the few papers that venture into the domain adaptation of graph, the paper introduces a number of new concepts and mechanisms. This framework is promising.	2
This paper proposes a new graph discriminator to replace the conventional adversarial discriminator in adversarial domain adaptation. Some theoretical and experimental analyses validate their claims, while some concerns should be clarified and improved.	3
It looks the proposed method is new, since it built an adversarial learning for this question. 	3
"The theoretical result shown is very relevant to the community and is proved under relatively weak assumptions. 
I strongly recommend accepting this paper.
"	4
The article proves a new result on the convergence to a rank one solution, in some quite specific setting and under some strong assumption. The techniques used are improvements over previous techniques, they are well presented and could be of independent interest.	3
A very interesting paper that has certain technical contributions. The assumptions may be a bit unrealistic, but the technical results can be applied in the future. I recommend acceptance and am willing to raise scores given that my concerns are adequately addressed.	3
"Strength: Well written; Novel and significant results

Weakness: Some statements might be wrong; Some proof needs clarification
"	4
"To summarise, I believe that the paper is below the acceptance threshold as:
- the proposed method do not seem to beat the state of the art (MLPeptide)
- some analyses of the generated sequences are missing, such as similarity study between the generated sequences and the real ones, and diversity of the generated sequences.  
- the value of some validation experiments (for example Table 1) or of the filtering step is unclear.

I could increase the score if the authors address these points."	3
I think this paper is on the boarder line. I tend to accept it if the secondary structure information could be proven to be useful also for other methods.	3
This paper addresses a problem for which no good gold standard is available, short of experimental synthesis and testing of the proposed sequences. 	2
The paper is well written and relatively easy to follow. It appears to introduce a novel algorithm for an important problem and should serve as an important alternative to training agents on long horizon delayed reward problems. 	3
This paper addresses the important delayed reward problem in RL. The proposed method, RRD, is simple yet somewhat novel. The theoretical analysis builds connections to existing methods and the empirical study shows promises of RRD. Thus I think this paper is above the acceptance threshold.	3
"Overall, the paper provides a new mechanism to reduce the delay in the reward and make learning faster in delayed settings. But the work is not too novel and the writing is not satisfactory. It is not clear why the optimal policy does not change after such randomized reward redistribution. 
"	2
The overall contribution is a little light, but this is nonetheless a solid well-executed paper as a whole.  However, due to the questions above about experiments, I cannot accept the paper in its current form.  Pending the answers to those questions (and the reviewer discussion), I may raise my score significantly.	2
"Paper has some novel part but the main contributions compared to OML is not significant.
"	2
Simple extension to a current method. Unclear about the significance of the contribution. 	2
The topic is intersting. However, the contribution of this paper is minor. It is a combination of several incremental improvements.	2
The technical contributions of this paper is limited compared to OML. Also, the presentation clarity, detailed analysis of the proposed method, experiments need to be improved. 	2
"The paper tries to clarify an issue, but more insights need to be developed. For instance, whether such linear separability is induced by the kind of model, input dimensionality, and if it generalizes beyond poisoning attacks – similar evidence are probably existing for backdoor poisoning and adversarial examples too.

I’m also quite concerned about the threat model, as poisoning large fractions of points (more than 20%) is not quite realistic in many practical scenarios (I don’t think it’s realistic to poison up to 50% of the dataset). This should be clarified in the paper, and more details on the practicality of the threat model should be discussed or acknowledged as specific limitations of this work (and other papers that use similar setups).
"	2
While the work does contain some interesting findings, the overall contribution is rather minor when taking into account prior work. Thus, I do not find it suitable for the general NeurIPS audience.	1
This paper made an interesting observation on the linear separability of a wide range of indiscriminate data poisoning attacks. This reveals the reason why such attacks can succuss. 	3
Overall, I feel that the finding of linear separability would be of great interest to the community, and I consider the proposed synthetic noise as a supporting evidence for the main claim about the underlying principle and as a direct application of the linear separability. Therefore, I am leaning to accept. I would like to increase my score if the concerns are well addressed.	4
Overall this is a technically sound paper. However, my major concerns are two-fold: 1. The proposed methodology is over complicated in my view, yet there is little gain in accuracy. 2. The experiment still needs to be enriched to verify the proposed solution from multiple aspects.	3
The method proposed in the work provides an alternative pruning approach that should help decrease the computational complexity. However, overheads involved in repeated formulation of groups and TMI evaluation need to be discussed and compared. The results presented in the manuscript do not possess sufficient depth and are missing analyses. 	3
"Overall, I vote for borderline rejection (score 5). Specifically, I liked the idea of introducing a new metric of ticket magnitude increase score (TMI score) to cluster the filters in the first stage based on the ""lottery ticket hypothesis"". But there are some unproven claims and inconsistencies as pointed out in the cons section."	4
In summary, I feel that although the proposed method seems to be novel, the evaluation part is not yet convincing.	3
In general, I believe this work is somehow different from existing works but the current draft makes it challenging for these ideas to reach their full potential. The authors are encouraged to address the weaknesses above. 	3
"The paper is well written and does tackle an interesting and important part of Reinforcement Learning.
The idea presented is quite simple but does show promising results in the experiments.

Authors do a good job of ablating and discussing different choices of the systems

Some information seems missing around generality of the method. "	2
The proposed method is simple and elegant, but it is unclear to me how it relates to some of the previous literature that proposed similar decomposition through the network structure. Some details of the method implementation and experiments are missing. The main experiments are convincing and well designed, but the analysis is not sufficient and does not provide enough insight into the behaviour of the method.	3
"Though the actual method proposed is fairly incremental, the interpretation of it is novel and its simplicity means it could likely be incorporated into the goal-oriented learning toolkit quite easily. The experiments were all well executed and showed improved empirical performance, and ablation studies are provided to help understand exactly which design decisions are responsible for improved performance.

***********************
POST REBUTTAL
***********************

The newly-added experiment in 4.5 strengthens the paper in my view, since it highlights the particularities of the method. I think the approach is an interesting one and could be valuable to the existing body of work on goal-oriented learning. It is also conceptually simple enough that it could easily be widely adopted and used as baselines or built upon going forward"	2
"The paper presents a valid hypothesis: that separating what the effect of an action will be in a given state and what the agent needs to do to solve a task should be separated while learning in order to aid the speed of learning as well as generalization. To do so the paper presents the separation of the value network into two networks whose vector predictions are combined to predict the value.
The downstream effects of this new architecture are validated experimentally fairly well. However, whether the new architecture actually learns the hypothesized separation is not evaluated. Additionally, the writing can be tightened up a bit."	3
A solid work with nice results and insightful analysis. It has limited originality but can benefit from more challenging experiments. 	2
The paper presents a simple approach for an important topic and demonstrates both gains over previous works as well as interesting applications that are possible with the model. However, I think it misses an important line of research on unsupervised disentangling of shape and identity based on interpretable shape representations such as keypoints and masks (e.g. [1-7]). Given these prior works, the self-supervised generation of pose-identity pairs is no new contribution. The significance of the remaining contribution with the VQSN layer is difficult to judge, since the ablations which do not use this contribution also seem to quantitatively outperform existing approaches and also do not seem very much worse than the full model. Since there are no qualitative comparisons between ablated version, its value remains unclear. So while the presented approach produces very good results, these issues make it impossible to judge where that improvement really comes from and thus it is not clear if the claims regarding the contributions and their significance is correct or not. Therefore I currently lean towards rejection.	2
This paper proposes a disentangled method for the image translation task, which is meaningful for the computer vision community. The method sounds works but the experiments are insufficient, more experiments should be conducted on high-resolution images.	3
Overall, the paper is novel in method while experiments are somewhat weak to support the claims. 	3
"Overall, I am leaning towards rejecting the paper. The current version of the paper needs to be improved, in particular: the novelty, writing, and the experiments. 
The novelty of the proposed VQSN is incremental, however, I would consider changing the rating of the paper if the authors can implement the suggested improvements. In particular, improve the clarity of the paper and show evidence of the performance of their method against other state-of-the-art methods at resolutions 1024x1024 and 512x512."	2
This paper proposed an improved variant of the Stochastic Compositional Optimization (SCO) framework to train GNNs, using a memory buffer to approximate the moving averages of all nodes. Such a fixed-size buffer solution makes the SCO algorithms practical for large graphs. However, limited intuitions are provided to understand the design of such buffer and its effectiveness in practice. Most importantly, the proposed Sparse_SCO algorithm is not superior to Adam_Sample in terms of memory and time efficiency. The performance and convergence speed are better than layer-wise sampling method (Zou et al., 2019), but it is questionable whether it can outperform other sampling strategies like GraphSAINT (Zeng et al., 2019) and ClusterGCN (Chiang et al., 2019). Moreover, the proposed SpSC algorithm is only evaluated with GCN and GraphSAGE and using one sampling algorithm in (Zou et al., 2019). It is unclear if it can be applied to other GNN backbone models and combined with other sampling strategies. Given the reasons above, I cannot recommend the current manuscript for acceptance.	2
Overall, I think the SCO perspective is interesting, and SpSC demonstrates the possibility of using SCO algorithms to improve the scalability of sampling-based training methods. However, the technical novelty may not be enough. Moreover, I think authors should compare more baselines in the experiments and the presentation could also be improved.  	2
The idea of reducing the memory cost is interesting and useful. However, there are so many flaws in the theoretical analysis. Hence, I recommend rejection. 	2
Overall, I think the current version is not ready for publishing on ICLR due to the limited novelty and unconvincing experiments. I recommend authors to strengthen its novelty and make the experiments to be more convincing.	2
The authors present a thorough evaluation into the effect of various differences in training procedures on resulting representation diversity. I have not specialized in this research area and cannot speak well to the novelty of each claim, but believe that the paper deserves acceptance due to the extensive experiments, clear presentation, and interesting findings which open questions for future research.	4
I liked the overall study and believe that it will be useful for the community. I would appreciate it if the authors could address my concerns/ questions above.	4
This paper is interesting and well-written, and it presents an impressive empirical evaluation of how training methodology affects representations and predictions. The paper does not claim novel methods, but the methods for empirical analysis themselves are interesting, and the community can learn a lot from them.	2
Overall I remain positive about this paper and vote for acceptance. My main concerns are points 2 and 3 from the main review.	3
Good literature review and motivation, but difficult to tease out the technical contributions with weak and ambiguous results.	1
The problem of learning to reason with analogies is important, and this paper makes an attempt towards doing this in a structured way, which seems like a principled move. However, as it is currently, I cannot recommend the paper for publication. The task being learned has been made too easy, so that claims about effectiveness cannot be supported. But if the authors work on a version of this study, in which the relationship types are latent/unobserved at train time, I think it would be a very promising step towards better analogical-reasoning systems. 	3
Overall,  this paper tested an interesting hypothesis based on a classic idea from cognitive science. However the results were not thoroughly analysed, and there was a  lack of clarity in the explanation of ideas that made it hard to situate this contribution of this paper in the literature.	3
"Although this paper works on an interesting and potentially impactful problem, the current design of the model and illustration does not fully support the claim the authors made and is somewhat incremental on technical novelty. Therefore, I recommend a rejection at this time with the hope that the authors can make this submission stronger in the next version.

Post rebuttal:
After reading the authors' response, I still feel that the current submission is somewhat incremental and needs more solid results for justifying the authors' claims. Scores are increased from 3 to 5 given the clarifications of the authors."	2
"I'm concerned on the novelty of the work and the evaluation performed.

I have read the rebuttal and other reviews. As the authors agree on most of the comments, I decide not to raise the score. But I do acknowledge that
1. The DRT may not fall into the modular network domain though the idea looks very similar.   
2. I cannot quite connect the model and the structural mapping theory despite other reviewers' opinion.
3. I would still encourage comparison with other models as the tasks considered are related."	2
This paper provides a new mathematical framework for analyzing the implicit regularization of SGD after attaining zero training loss. The framework is novel, and it leads to a simpler analysis of sample complexity under noisy label regularization. I am in favor of accepting this paper.	4
"(1) It would be nice if the authors can explain more and justify why you can use the SDE approximation in equation (2). $\eta$ is the stepsize, and I can understand the stochastic modified equation in the literature, but somehow I don't see why SGD can be approximated by equation (2). 

(2) In the drift term in equation (3), should it be $\nabla L$?

(3) In the paragraph after equation (3), you wrote that hopefully we can simplify the dynamics Equation (3) via choosing suitable $\Phi$. What I don't understand is that on the surface, it seems equation (3) is even simpler than equation (4) or (5). So what do you mean by simpler?

(4) On page 5, in the paragraph before Theorem 4.1., what is $\phi$?

(5) I find the assumptions in Theorem 4.6 quite strong. You need to assume that the SDE in equation (10) has a strong solution. Ideally, the assumption should be on the loss function $L$ instead of the equation (10). Another strong solution is that $Y$ never leaves $\Gamma$. This in my opinion is also quite strong. $Y(t)$ is an SDE with a tangent noise term, and because of the Brownian noise, I just don't see how this assumption can be satisfied. Even if you have degenerate noise in some direction, e.g. in the case of underdamped Langevin diffusion, it is still supported on the entire Euclidean space. If you have to assume such strong solutions for Theorem 4.6., it might be helpful to construct some toy examples to demonstrate that the limiting $Y$ is non-trivial and meaningful at least for some toy examples.

(6) I also find Remark 4.7. a bit puzzling. In Remark 4.7., you mentioned that the convergence in distribution result in Theorem 4.6. (at the time $T$) implies the convergence for the sample path on $[0,T]$. I don't understand this. Shouldn't it be the case that the latter implies the former?"	3
This paper has very good theory and is well-structured. Also, its application to label noise SGD is also very interesting.	3
As already expressed, I am very enthusiastic about the paper as they introduce the perfect mathematic tool and scale that characterizes nicely the effect of noise near the interpolation manifold. Furthermore they apply flawlessly and successfully their vision to a nice problem. In a word: Congrats ! 	4
"The main strength and contribution of the paper is the analysis of the source-domain connectivity, which was interesting to read. It was well illustrated through a toy example, and then applied to some real-life domain adaptation datasets. The whole analysis seems sound and well-though.
 However the main two weakness are **(i)** there is a lack of clarity on the data augmentation used for contrastive learning; these seem to be key to define the whole notion of connectivity so it is a bit surprising they're not discussed in more details; And **(ii)** it's not clear how significant the reported work is and what it could lead to; In particular, one of the claims of the paper is that contrastive pretraining can reach similar accuracies as other UDA techniques while not aiming to align the domains as adversarial UDA techniques do. However there is not much comparison to these adversarial techniques in the second part of the paper; For instance, Figure 5 suggest that, while domain embeddings can be easily separated with contrastive pretraining, there is still some kind of ""domain connectivity"" which is really important for final accuracy, which seems to actually match the ""domain alignment"" goal from adversarial UDA techniques."	2
Overall, I think the observations in this paper are interesting. But I think more experiments would make the paper stronger.	3
Although this paper gives an interesting new idea to solve the problem of domain adaptation from connectivity, it does not extend the idea to a specific method, nor does it conduct sufficient experimental verification.	2
"Overall, the idea is interesting and intriguing, however there are still limitations and ambiguities in the current draft. The detailed comments/questions are listed in the weaknesses / questions part.

The paper has a great potential to the CL field, and could be beneficial and inspiring for broader audience; but issues need to be addressed / made clear. I'm happy to change my score if the feedback addresses my concerns. Please refer to the points in the weaknesses / questions part. I would like to see feedbacks on these comments/questions."	3
"Correctness: I did not identify major issues in terms of correctness.
Novelty and significance: I have concerns about the significance of the proposed method in terms of modeling the RDF through non-parametric approaches. I appreciate the authors running real-world experiments with a/b test to demonstrate the performance of the proposed method."	2
The paper does not contain new methodology or theoretical result. The simulation study and real data analysis are weak.	1
"- This paper implicitly assumes that readers are familiar with the details of the causal forest algorithm. Without this prerequisite, it's impossible to understand the paper. 

- This paper doesn't motivate to use the GCF over existing estimators (Kernel DML, or the one of Kennedy et al. (2017)). 

- No theoretical analysis on the convergence or bias analysis for the GCF."	2
Overall the results in this work are interesting and possibly useful, but I feel that there are some lingering questions about applicability, particularly as the experimental results are underwhelming.	3
"Overall I think the direction of the paper is interesting, but the main paper at present is missing some key pieces (esp. how the recovered topic posterior is produced once the SSL representation f is learned). In addition, I have concerns about the disconnect in how theorems might apply to finite training sets in practice, and how little intuition was provided in the paper for the theory. 

I could be persuaded to change my mind by a strong rebuttal.
"	3
Overall, showing that both the reconstruction-based objective and the contrastive object can recover the posterior of the topic proportion vector is interesting. It would be good to demonstrate how this theoretical finds can benefit the wide topic modeling community with more experiments on real-world datasets. 	3
The theoretical analysis is interesting, but the motivation of analysis and evaluations are not entirely convincing. I suggest to focus on theoretical analysis and clarify where this analysis could be useful.	3
The reviewer thinks that the proposed method is clear enough and practical to obtain neural architecture used for resource-constrained edge devices, yet, some experiments or information that are mentioned above should be added to validate the method.	2
The main contribution of the paper is a NAS framework for multiplication-based and multiplication-free operators. The weaknesses do not affect the main contribution. I recommend accptance for it.	3
The idea of this paper is novel and the results are satisfying, but there are more details should be provided.	3
"Overall, I think this paper explores an interesting topic and potentially can benefit real-world efficient deep learning applications. However, I find the technical contributions and empirical results presented in this version are not strong enough. Thus, I recommend rejection.
"	2
I don't find the contribution of the current paper very significant compared to the Dwivedi and Mackey (2021) work. The paper adopts several critical theoretical contributions and algorithm designs from the prior work, maybe also the code. The writing is also technical and lacks intuitions and insights. I suggest the authors focus on improving the quadratic time-complexity instead, which should increase the significance/popularity of both works. 	2
Up to my knowledge, the proposed algorithms and the corresponding theoretical guarantees are new in the community. However, the manuscript is heavily technical and dense. I am afraid that only readers familiar with kernel thinning may grasp the contribution of this paper. 	3
The paper motivates the problem in a clear and convincing way even for a non-expert, and clearly outlines its contributions. The results are significant, and as far as I can see, correct and sound. I recommend this paper to be accepted. 	2
I believe this is a borderline paper. Some aspects of it were published before. A new analysis is presented (based on RKHS covering number).	3
"It’s not surprising that the CLIP-based vision backbone - after being trained on 400M image-text pairs - can outperform ImageNet-trained baselines in vision and language tasks, especially given its zero-shot performance illustrated in the original work.
The presented results can be helpful for the research community, although the answer to the question posed in the title remains largely open. The outcomes of experiments vary from task to task, with no appropriate discussion followed even for the difference between CLIP-ResNet / CLIP-ViT-B features. As it reads in the Conclusion section: “Analyses from different perspectives explain certain intriguing phenomena and offer new directions for future V&L research.”  "	2
"Not many insights besides ""switching the visual encoder to CLIP's bring the performance boost"" are given.  
To raise my recommendation, please clarify what directions the paper could suggest to the community other than the general tendency I've mentioned."	1
While I have no doubt that CLIP could offer opportunities for developing new state-of-the-art models, this paper falls short of explaining the reasons behind the successes of CLIP. I also found several experiments and analyses problematic and do not quite support the claims. Overall, this is a borderline paper to me as it does demonstrate the potential of CLIP with good results. Therefore, I am slightly leaning towards accepting it.	3
This work introduces a must-know baseline for vision-and-language research: using CLIP as a visual encoder. While this paper does not introduce a novel method, it provides important experimental information to many in the community. The results here presented would be of great interest to many, and I recommend it's acceptance to the conference.	3
I think this paper derives useful theoretical results on large learning rate optimization of matrix factorization that will be of interest to the community.	3
As both of theory and experiments do not substantiate the claims of the paper enough, I unfortunately cannot recommend acceptance. 	2
With my current understanding, I recommend a reject. However, I am expecting that the authors will answer my question and I will be able to give a higher score.	3
Although this paper raises an interesting problem of implicit regularization in matrix factorization with large learning rate, and provides evidence in special cases, it does not live up to its claims for the general case: failure to converge is a well-known issue with large learning rates, so the results for the general case which assume convergence are somewhat incomplete. It's also not clear whether the proofs in the special cases can provide any insight for the general case.	4
A good paper that studies an important problem with carefully executed control experiments.	3
Although this paper looks simple, it has actually identified an interesting, novel problem. I think the authors motivated this paper from a recent SOK paper published at the IEEE S&P'21 conference that has discussed related issues. I appreciate that the authors have done research on such an important topic. I would suggest accepting this paper as a short paper or a poster if possible.	2
The paper studies an important problem and presents many interesting findings. However, it still lacks root cause analysis on the findings to indeed “demystify” the low attack transferability in ASR.	3
"I believe the authors provide a useful beginning of understanding why ASR systems may be 
not susceptible to targeted transferability. However, I think a little further work
could have been done to improve the results as well as the explanations."	2
All in all, I think the paper attempts to tackle a very challenging problem. The method looks sound and the results might be interesting to the community in learning object-centric representations. However, there are some major concerns I have, mainly about the position and novelty of this paper with respect to a paper from last year, and the lack of results from datasets the state-of-the-art methods report on. 	2
I think the paper is not quite ready for publication. The method does not work particularly well, a part of the method (object pose estimation) seems to be not working at all, and the evaluation is only on a new toy dataset and does not include evaluation on established datasets. Also the text contains too much notation, and has some parts mixed up (with terms being used before they are introduced), but I think this can be fixed easily.	2
Overall, I think the paper is quite interesting and would be of interest to the community. However, the empirical evaluation is very limited and this makes it difficult to evaluate the full merit of the proposed approach.	3
"This is a good first step in this direction, and should inspire follow up work. The technical innovation is sensible. The results are good. 
"	3
IA-MARL solution works well, but the missing training data problem does not look scary, as presented in the paper. I would gladly buy almost all of the claims presented, except the motivation behind the problem itself. To be specific, I will up my score if I could learn more from the authors about Q5 and at least one from Q1~Q4.	4
"Based on the weaknesses mentioned in this paper, I tend to rate this paper as ""marginally below the acceptance threshold"". The main concerns are located in the experimental results and the complexity of the imputation problem itself. But I'm open to change my score after reading the rebuttal from the authors. "	2
The method currently seems fairly incremental and it's unclear to me how broadly applicable it is. This is both due to the limited /noisy experimental evaluation and the missing discussion regarding the assumptions that went into the method and limitations. 	2
"Overall, I think the submission would need to

- Do more to justify the idea that this is a problem setting in need of attention
- Address the deficiencies in writing quality
- Add additional relevant baselines / experiments
- Add additional details regarding hyperparameter tuning

to merit acceptance."	2
"Interesting new methods to tackle a very relevant problem.
Unfortunately a few too many issues remain in the presentation of the paper to make it ready for publication this time around, but it absolutely can be with a bit of additional work."	3
Overall, I believe that it is an interesting work, but the proposed is only applicable for solving a class of problems given a special structure of hypergradient. Even the authors justify the computational efficiency of the proposed algorithm numerically w.r.t. running time, the theoretical justification of the efficiency of the iteration and sample complexities should be further addressed.	2
"The paper is easy to follow and I believe that the contents of the paper are technically correct. However, the paper doesn't discuss several important related works and the empirical analysis is not rigorous: the authors conduct the empirical investigation in a limited setting and don't compare the proposed method with competitive baselines. It is still unclear if the proposed algorithm is superior to the previous approaches. I believe that there should more in-depth empirical analysis to make the proposed method more convincing. 
"	2
Overall, first-order bilevel optimization is a worthy direction to pursue, but efforts on both technical and empirical novelty are needed to make this paper competitive.	2
The authors designed a pipelined distributed GCN training algorithm to speed up large-scale GCN training and demonstrated the performance and efficiency with abundant experimental results. The paper can be accepted if the authors improve the convergence proof and modify the uncleared statements.	3
The paper is complete but lacks comparison with an important related work and a justification of the technical challenges.	3
The paper proposes an interesting framework to speed up the GCN training with the theoretical proof for the complexity. However, as mentioned in the weaknesses above, the weird setup on the largest dataset, ogbn-papers100M, and lacking the evaluation on a different number of computation nodes, it is hard to understand the scalability of the system.	2
"While the idea of stale activations/gradients is not new (as the authors have acknowledged in the related work), the convergence result seems new to the best of my limited knowledge. I am positive that (1) validation accuracy is essentially equal to the non-stale version; (2) based on the PCIe bandwidth in the experiments, the method should also work in distributed inifiniband or 100+Gbps ethernet settings. However, I do have concerns about (A) how the ROC and CAGNET results were obtained, and (B) the method only performs well in a narrow ""Goldilocks zone"" where computation time is roughly equal to communication time.

On the last point - the authors might want to consider what happens at the billion+ node scale. In my experience, the distribution of very high degree nodes increases, and it is likely that nodes with an extreme number of neighbors will end up on the boundary. This could greatly increase communication time relative to computation, and the method would no longer perform well.

Overall, it is an interesting if slightly flawed paper, and I lean towards acceptance."	3
"Personally, I like the idea using perturbed rewards to implicitly let the neural network to explore. However the proposed algorithm is still hard to be applied to practical applications. 

Detailed comments:

Equation 3.2: parameter $\lambda$ is missing?

After Lemma 4.3: ... the the neural network ...  the extra the should be removed

Lemma 4.4: it might be better if the authors could define $E_{t, 3}$ in the beginning of Lemma 4.4.

After Lemma 4.6: please resolve the cross reference error

Experiment 5.1: It is confused to me that `At each round t, only a subset of k arms out of the total K arms are sampled without replacement and discosed to all algorithms for selection.` Could the authors help explain why the algorithms are restricted to pull $k < K$ arms which seems to be different from the one discussed in this paper?

"	3
This is a good paper. It brings a solution that is much more practical than NeuralUCB and NeuralTS.  The experimental results are convincing. However, I think that the novelty is not enough. I am leaning forward to rejecting it.	2
I think the pros above outweigh the cons slightly in my mind. I am happy to raise the score if the authors can answer 1 and 6 among the cons.	2
I’m very positive about the idea behind this paper: to use combine perturbed rewards with neural contextual bandits and think that this work is timely and potentially very impactful. I have a few reservations about the extent to which experiments support the main claims of the paper, and as such have chosen a borderline score which I could increase if my concerns are addressed.	3
While the results on the test set are good, I’m not convinced that the results alone here are novel enough for acceptance. 	2
"I have some questions/suggestions;
1. Optimizing speed comparison would be a good addition because docking simulation is known to be very slow.  
2. In equation (15), the angle is sampled from a softmax funciton? Is the angle categorical?
3. In the baselines, it's known that VJTNN[1] is later model than JT-VAE. In addition, MolDQN[2] and CMG[3] are other recent models. Better to look at them.

[1] Jin, Wengong, et al. ""Learning multimodal graph-to-graph translation for molecular optimization."" arXiv preprint arXiv:1812.01070 (2018).

[2] Zhou, Zhenpeng, et al. ""Optimization of molecules via deep reinforcement learning."" Scientific Reports 9.1 (2019): 1-10.

[3] Shin, Bonggun, et al. ""Controlled molecule generator for optimizing multiple chemical properties."" Proceedings of the Conference on Health, Inference, and Learning. 2021."	2
The method contains several signs of progress to generate new drugs in 3D space. Overall it is ok, but more questions should be answered. 	2
"
I have concerns about the reproducibility of this work and I find a
lot of details lacking. The English needs some work; some figures and
tables have unexplained or mislabeled data. It is possible that with a
lot of additional work the same idea could become clearer and well
documented, but I am not confident it could make it to ICLR22.
"	2
The studied problem of this paper is interesting, while there may exist some issues in the theoretical foundation of the proposed algorithm. 	3
"The paper is well-written and clear to understand. Theoretical analysis is clean, simple and gives interesting insights. The results are also very strong with good improvements on several datasets. However, there are several weaknesses in the paper A] the motivation for the disentanglement block is unclear B] it is also unclear how simple random masking achieves disentaglement C] The motivation for T-DEMUF is unclear as well and D] a very related baseline by [Chien et. al] is missing from comparison.

In light of the above, I am inclined to rate this paper marginally below threshold. 

**References:**

Eli Chien, Jianhao Peng, Pan Li and Olgica Milenkovic. Adaptive Universal Generalized PageRank Graph Neural Network. ICLR, 2021.

---

Update post response phase:
Having gone through other reviews, it seems that there maybe a lack of clarity in the paper, particularly on the theoretical aspects. I admit it was a bit of an effort to keep up with all the notations and the paper will definitely strengthen by bringing in lot more clarity in notations. 

Having said that, I would like to add that although the conclusion of the work may not be new, but new perspectives for the same conclusion should not be discouraged. In mathematics, we have multiple proofs for a single statement like ""There are infinite number of primes"" [A], each of them brings a new perspective and has something to offer. I believe that we would be doing a disservice if we rejected papers simply on the basis that conclusion is not new, without really analyzing if the paper is offering a newer fresh perspective or not.

However, beyond these issues, I believe it is important to assess the actual model strengths as well. This is mostly where I find many things not so clear, for example, the feature disentanglement aspect is not clearly explained. Also, how does T-DEMUF work is not very well explained and what is the exact motivation for T-DEMUF is also quite lacking. 

In light of these, I am inclined to retain my current assessment of marginally below acceptance threshold.

[A] https://primes.utm.edu/notes/proofs/infinite/

---

Update post second response phase:

The authors have addressed all my issues. However, the outcome is the following:
1. The notations have to be improved in the paper
2. Explanation and motivation for disentanglement has to be added to the paper
3. Explanation and motivation for T-DEMUF has to be added to the paper.

However, I believe these should not be difficult changes to make and hence I am willing to change my rating to marginally above acceptance threshold, under the assumption that the authors would make these changes in the final revision."	3
"This paper does not live up to its claims, and is extremely poorly written.
The authors spent most of their effort on this work stating obvious things in convoluted ways, amounting to very limited statements regarding graph filters.
Despite their claims of reaching ""deep understanding"" of graph filters, this paper achieves very little.
Due to the poor writing and shallow application of this work, I strongly recommend this paper be rejected."	2
Overall the paper addresses and important problem, but the proposed methods and the experimental results need significant improvement and clarification. 	1
The problem the paper investigates is interesting, the algorithm seems like a sensible extension of unsupervised techniques to the weakly supervised setting.  The experimental study has weaknesses, but still justifies the proposed technique.	3
Even though I think the approach can be valuable, my main concern is the motivation to use it when background and target sets are already known. How does its utility compares to supervised feature selection methods? Can Contrastive Feature Selection be applicable to the cases when the target and background samples are not given (or clearly separated)?	2
The paper presents a combination of an existing feature selection method (called stochastic gating layers) and a classifier (to test the models performance). There is a lack of novelty in the paper. 	2
This is a strong paper and I recommend it for acceptance. It addresses a real problem in a manner that is both novel and pragmatic. I hope the authors will not take my constructive criticism too harshly, the paper is strong in its current form and my suggested improvements are only suggestions for ways I think it could be made even stronger. I would love to see this work see application in other areas and my comments reflect ways to speed that process. 	4
"** After the authors' response and the improvements to the paper, I have changed the score to a ""6""; marginally above the acceptance threshold. 

-----------------------------------------------------
I have recommended “marginally below the acceptance threshold”. 


I like the methodology that is presented in the paper, but I think there is a need for more extensive experimentation. Synthetic data would provided a much more controlled testing environment. An examination over more datasets would provide more compelling evidence that the proposed method is advantageous, and perhaps highlight the types of problems where the approach provides a major benefit, as opposed to those where it offers a less significant improvement (or is perhaps even detrimental due to its flexibility). More details concerning the experimental methodology are required. Results that provide a more detailed comparison between the proposed methods and baselines would be helpful (some figures rather than just a single number). Measures of variability in the results and statistical significance tests would be beneficial in understanding how dramatic an improvement has been made.  
"	3
Given my comments above, I think that there are enough strong points in the paper to put it above the acceptance threshold. I am open to being more optimistic about it after the author response.	3
Although this paper studies a relevant problem, the quality, presentation, and evaluation of this paper are not convincing.	2
This is a solid paper with a novel technical contribution that utilizes nontrivial insights for efficient matrix computations, and with a strong experiments section. The experiments demonstrate not only that the model can be an efficient alternative to transformers on tasks requiring long-range reasoning, but that it also shows promise as a generic sequence model that can be applied across a broad range of tasks. Overall, there are not many drawbacks of the paper for me, other than some unanswered questions in my mind.	3
This is theoretically and empirically a solid paper. The evaluations show that its superior performance is not limited to modeling long-term dependencies only, but it can be a strong alternative to the established sequence models. 	4
My recommendation is to accept this work.  This paper proposes a novel parameterization for solving SSM which provides computational efficiency and accuracy gains.  The approach is explained and justified theoretically and the performance is evaluated on a number of tasks across different domains.  The text is clear and well written.  The authors explicitly state the goal of finding general methods that work on a broad range of tasks and this approach offers a good step toward solving that problem under realistic computational constraints.	4
The paper is in general well written. However, there still exists some concerns towards the applicability of the proposed method as well as the uniqueness of the theoretical result. I think it is on the borderline. Depending on the authors' response, I may raise my score.	3
Although the proposed method is natural and effective and the paper has many merits, there are some concerns including implementation details, some discussion in the paper, and lack of evaluation on more model architectures. The reviewer will recommend this paper if some concerns have been addressed.	3
Although the empirical results for the method are appealing and the idea of using implicit hypergradient is interesting, the paper does not review previous work in adversarial training against backdoor attacks [1] which is extremely relevant and does not compare the proposed method with the existing algorithm. Also, the writing could be significantly improved. Because of these concerns, I vote for weak reject. 	2
"One of the main issues is that the proposed method can hardly achieve the lowest ASR. Although I-BAU seems to be more stable, one may resort to the best defense for a corresponding attack if ASR has a high priority.
"	3
The paper had a claim that initialization of different networks is not effective in proving diversity. This is not well supported. It can give a wrong message to the literature with the small and insufficient studies. this is my main concern.  	2
In general, although the paper can be still improved, the general results and suggested techniques are promising.	3
Figure 1, which is intended to demonstrates the negative correlation between representation similarity and performance, is not convincing to me. Furthermore, the experiment conditions seem to be insufficient or not explained well.	1
"I'm leaning to recommend reject. 
I acknowledge the significance of the finding that it is useful to keep the divergence among the Q-function. 
I also acknowledge the authors' effort to demonstrate the usefulness of the regularization method in various RL methods and environments. 
However, the clarity of the current version of the paper does not meet the threshold for publication, and a non-trivial revision is needed. 

----- Update after author's revision 20211125 ------------------------------  
The authors have improved the explanation of experimental setups, but the connections between the sections still have not been sufficiently improved. 
In particular, some of the other reviewers are concerned about the mismatch between the similarity criterion (CKA) used in Section 4.1 and the similarity criterion used in later sections. 
I have seen their discussions with the authors, and think that their concern about the mismatch is not sufficiently resolved. 
I think that replacing the analysis based on CKA with an analysis using criteria based on parameter values (e.g., equation (4)) would make the discussion in the paper more consistent.  

Overall, I acknowledge that the paper was improved to some extent in the revision, and thus slightly improve the score (WR->WA). 
However, as mentioned above, I have still concerns about the clarity of the paper and cannot strongly champion the paper to accept. 
"	2
The paper discusses new position embeddings and the feed-forward networks. However, the novelty of the first part is limited and the effectiveness of the second part is not properly justified.	2
In summary, I think the proposed method is this work is very practical. It seems that using a simple positional embedding help a lot. However, it seems that the motivation is not very strong, like how to explain this positional embedding. 	2
Overall, this paper tries to alleviate a significant problem in MPNNs. Still, limited experiments in a single domain and counter-intuitive results for OGBG-MOLPCBA and ZINC datasets make this paper’s contributions questionable.	3
The proposed method has strong motivation and empirical results and I vote for an acceptance.	3
"I find the proposed idea reasonable and straightforward (in a positive manner). But I'm not fully sure the experimental results well support the claims. I also have a concern that the most influential time is the PE init., not the PE embed. update formulation. 


============================================

After author feedbacks:

I feel the answers from the authors are largely satisfactory. 

Other reviewers raise a concern about the novelty of the proposed methods. I agree that the novelty is minor. 

However, the experimental results and the statistical test reports indicate the strongness of the proposed framework in practice. 
Additional experiments on other domains are a positive surprise. 
Therefore I modify the review score one step upward. "	3
This work provides interesting insights comparing the generalization capacities of DQN and QR-DQN and their evaluation method corroborates some findings in the literature (e.g. about regularization) and some somewhat surprising results on the state generalization capacities of DQN vs QR-DQN. I think the paper would be stronger by providing results for other agents and more complex environments which is why I recommend a weak reject.	2
Without presenting any analysis of the **why** question, it seems that the work is not complete in my opinion. Also, the generalizability of these findings to other domains is under question. If these two major concerns can be addressed, especially along the lines of the pointers above, I am happy to revise my score. But for now, I would vote for rejection of the paper.	2
"Overall, the paper discussed an significant topic in deep RL and their paradigm for evaluation is clearly presented and empirically investigated. However, the current study restricts itself to simple grid world envrionments with an unnatural setting to use MNIST images as observation. While the main contribution of this paper is empirical, it remains unclear how these experimental results generalize to more practical and real-world RL tasks. Also, the technical novelty and significance is limited. In sum, I consider the current paper fail to meet the acceptance criteria of ICLR and recommend a major revision, probably by performaning more comprehensice investigation on more tasks with preferrably continuous state space.


--------- Post Rebuttal -----------

The authors have addressed most of my concerns, and I increased my score by one level accordingly. However, a core limitation of the current work, ""do the results/insights also apply to more realistic environments with continuous/large state space?"", is not resolved. While I believe this work has its potential, currently I lean toward weak reject.
"	2
"Authors explore a non-standard method to measure generalisation in RL applying it empirically to two deep RL algorithms. Despite of authors multiple points against using rewards as generalisation metric, I am still not convinced that the struggles of RL with respect to supervised learning, which is the absence of the ground truth answer, is tackled by the proposed method. Authors rely on heavy assumptions of the environment, allowing them to have access to the ""ground truth"", i.e., optimal policies and value. This is not the case in most problems about RL generalisation."	3
The paper introduces a task of code editing from few support examples, and formulates the learning and inference methods to implement the composition method on top of the Graph2Edit method. While the evaluation is sound, and shows an improvement of accuracy as a function of number of support exemplars, one practical challenge in applying this technique would be to identify groups of support examples / identifying the edit intent. Which could lead to problems applying this method at scale or in the product. 	3
The paper presents a technique to combine multiple edit representations for neural code editors. The paper is difficult to understand at times. It makes some technical contribution and provides experimental evidence. However, there is a conceptual gap about the need for combining representations from multiple examples. Though at a high-level it is conceivable that multiple examples could help, the paper does not provide evidence that a systematically chosen single example or an iterative approach which goes over all examples and takes the best output would not be sufficient.	2
Task targeted is not well motivated (not clear how you apply it if you don't have the fixers, or why you would use it if you have the fixers). But the approach seems novel and interesting.	3
The results are somewhat expected. The evaluation part is unclear. Comparison with more existing work (including traditional approaches) should be performed.  	3
"This paper makes comparison with techniques used in active learning (AL), domain shift detection (DS), and multi-domain sampling to combine data from multiple sources. The experiments are conducted on datasets from questions answering and sentiment analysis. 

The paper is well organized and easy to follow. However, the contribution of this paper is not clear. Specifically, I would expect authors provide more detailed recommendation for AL, DS, and multi-domain sampling in terms of sampling techniques, and population of different sources for certain application.

My another is concern is that the motivation of the experimental design is not clear. Why authors consider questions answering and sentiment analysis as the applications? It requires more analysis about experimental results, such as Figure 1 and tables in Section D. 

"	2
"+ Good summary and thorough analysis of experiments studying 18 AL methods (2 of which are proposed variants) on 2 NLP tasks
+ Paper is well written

- Limited technical novelty (the two variants are simple modifications from existing methods)
- Similar analysis exists but not discussed"	2
The paper is well written and the experiments are useful for practitioners looking to apply active learning, however I think some of the analysis could be improved. 	2
"The paper is an exploratory study. It is well written and covers a wide range of experiments. But I question the application of the task, also in my opinion the connection between the used methods and active learning should be further discussed. Additionally, there are a few problems with the experimental setup.
"	2
The studied problem is interesting and important. However, the related work is not well discussed and the conclusion is weak for me. The authors should try to clarify and establish the novelty of the problem contradicting related robust federated learning. 	2
Overall I think the author proposes a useful framework with rigorous analysis but I am somehow concerned with the technical novelty of this paper. I think it would be better if the author can get a higher bound and make more efforts to relax some assumptions (or give more discussion on the necessity/difficulty of those assumptions)	2
This paper provides convergence analysis for federated adversarial training, which theoretically shows the feasibility of applying adversarial training to federated learning. However, it may be not easy to see some in-depth insights or huge novelty in the current submission.	3
I tend to vote for a rejection due to the lack of novelty, clear theoretical justifications and experiments. 	2
Given the above novelty concern and the lack of experiments, I would recommend rejection. I would suggest the authors rethink the practicability of the assumptions considered, rather than directly borrow these assumptions from existing literature. It would be much more interesting if the authors could give some experiments to demonstrate the proposed algorithm.	2
"I am not completely familiar with this line of research, and thus cannot properly assess the current work. IMO, paper suffers from non-informative and cryptic explanations. This issue can be solved by briefly discussing the many technical terms and concepts that paper has implicitly or explicitly utilized.

Also, it seems that this work is not completely grounded on a firm theoretical ground. A number of core ideas and intuitions have been triggered by some theoretical analysis, but have been mostly investigated through experiments.

At this moment, my vote is weak reject (with a low confidence). However, I want to see other reviewers' comments and study a number of references inside the manuscript to have a better judgement."	2
"The presentation of this paper could be substantially improved---including outlining the central hypothesis and performing experimental evaluations in more standard settings.

### Post-rebuttal update

I thank the authors for their detailed response. Unfortunately, my concerns with the paper regarding the presentation and motivation (which are also shared by other reviewers), as well as the practical significance of the proposed method still hold. In particular:

- I believe that the paper would benefit from a significant rewrite to clarify precisely the main claims, justify assumptions, and better connect the different sections. 

- Regarding the experimental findings, the canonical measure of success of black-box-attacks is the ASR for a fixed epsilon. In this regard, does not seem to be a significant improvement. Moreover, the statistical significance of the findings is hard to verify---in Tables 1 and 2, the differences w.r.t. baselines often lie within confidence intervals, and in many cases the standard deviation is larger than the mean itself. The time constraints of the rebuttal phase are not a justification for not providing results over enough samples. The authors should have verified the statistical significance of their results (by increasing the number of samples till they get reasonable confidence intervals) as part of the original submission."	1
Interesting paper, but arguments are not sufficiently supported	3
"Although this paper provides some interesting points relating to hard-label attacks on robust models, I feel the paper's arguments are scattered, which does not form a uniform and compelling viewpoint. Thereby, I hope the authors could reorganize the paper and highlight a compelling point. 



##### Post rebuttal #### 
Many thanks for authors' feedback. 
I have read other reviewers' comments and corresponding feedback. 
I agree with other reviewers' evaluations such as ""poor writing"", ""unclear points"", etc. 
I keep my score unchanged. "	3
Overall, I found the problem well motivated but the empirical study and conclusions a little underwhelming. The paper could use A) better evaluation, B) More concrete conclusions and more discussion on when a particular technique might be useful over others (maybe experimental design imitating a particular situation to demonstrate the effectiveness of one approach over another could help), C) More qualitative and quantitative analysis into the test cases generated from various models, and D) Adding text to address concern I raised in the main review (dataset origins and collection procedure, compute metric description, etc.)	1
The paper has limited technical novelty and empirical contributions. The project-wise fine-tuning strategy is not practical. There is no comparison against the few-shot learning techniques which can customize a model with a smaller number of examples and without re-training.	1
No significant insight and not thorough experiments from an analysis paper point of view.	1
The contribution is very narrow, targeting a single (far-from broadly used) task and dataset, and involving a relatively small set of projects that are not representative of the type of proprietary code the work is motivated as supporting. The results provide no new information beyond what is generally understood about fine-tuning, and lack the kind of systematic rigor seen in related work. Technical novelty is low, consisting of a single new fine-tuning approach that echoes traditional analyses, the motivation for which is also rather questionable.	1
See the main review	3
The paper is generally well written, though some paragraphs need shorten. The technical contributions are a little confusing to me, so I recommend the boardline reject.	3
"The statement of the problem is clearly stated, and good theoretical results are reported with substantial proof. As I read and check, algorithms seem to work. Then, I feel that the proposed result is valuable for the field.

A disadvantage of this paper is that most proofs are included in the appendix, and therefore it is hard to follow and validate the correctness of the paper. To be honest, I cannot validate the correctness of all proof. I feel that the paper should be published as a journal article (e.g., Theoretical Computer Science)."	3
The paper studies an interesting topic and give improved sample complexity guarantees. However, the authors fail to present their contributions, novelties, difficulties and techniques clearly.	2
I vote for accepting the paper subject to some additional experiments.	4
"This article provides a novel and interesting idea to improve flow models in multiple aspects. I think this work can inspire other researchers in the community, so I tend to accept the paper after some minor modifications.

Post-Rebuttal
----
The paper has its merits and the authors have revised the manuscript according to the comments. 

Thus, I keep the score and recommand acceptance."	4
"This paper has proposed a normalizing flow-based method to address the predictive uncertainty in regression problems. The significance of novelty can be further improved before it can be accepted.

## Update after Rebuttal

The authors have addressed most of the concerns. It would be better to clarify the difference with diffusion models in the main paper. Good luck!"	2
In my view, the paper fails to adequately explain why the proposed method achieves the results it does, and there is a relevant baseline missing. Hence, I recommend rejection. 	2
Despite the benefits of the theoretical arguments and somewhat convincing results, lack of novelty and missing details prevents this paper from being a quality to be accepted now.	2
"As explained above, my main issue with this paper is that its two main contributions are not as novel as suggested in the paper, as similar contributions were recently made by three other papers (https://arxiv.org/abs/2009.00919 and https://arxiv.org/abs/2104.10093 and https://arxiv.org/abs/1907.03799). There are some differences with these papers, but I think it is necessary that this paper clearly discusses what it contributes on top of these papers. When that is done, it should be better possible to judge the additional contribution of this paper.

My current scores are my “expectations” for this paper after a discussion of the above papers (and corresponding moderation of some novelty claims) has been incorporated. If the authors can make a more convincing case that their paper makes important contributions on top of those made by these previous papers, I’d be happy to increase my score. 
"	2
"The motivition is clear, and experiments are comprehensive. 
However, some unclear explanations and analyses are existed."	3
"Reason to accept:
Simple DP algorithm with demonstrated reduction in memory consumption and effectiveness. It enables large pre-trained NLP models to be further fine-tuned on private data under approximate DP. Clear presentation. 

Reason to reject:
Better to identify the cause of improvement and make a fair comparison to baseline models (in particular for text classification tasks). Over-statement and misleading in abstract and introduction about comparable to strong non-private baselines.
"	4
This paper presented a detailed investigation on the training configurations of fine-tuning large language models with DP-SGD through a set of extensive experiments, which are beneficial to researchers and practitioners working on private NLP, and the proposed ghost clipping trick for DP-SGD greatly reduces memory when applied to large language models, making private NLP research more feasible. I believe both the insights and the proposed method in this paper will bring value to the private NLP community.	2
The paper has a clear motivation and the main idea is also interesting, but still suffers from some issues.	3
The paper is quite good on experiments and their possible applications but faces some questions.	3
The proposed method is interesting, but the experiments can be improved.	2
Some key assumptions of the proposed method are not well justified. Also, the experimental study is quite weak in its current form. 	3
"This is an impressive paper but there is a lack of experiments.
I am willing to upraise my score if the authors successfully address my concerns regarding experiments in the rebuttal.

------------
After rebuttal: In this version, the experimental results seem convincing. I raise my score from 5 to 8."	3
As I said above, I have some questions about the necessity of the reinforcement learning-based algorithm in the framework. In addition, the comparison with the state-of-the-art algorithms should be added for the validation. I will change my score when I solve the above issues.	3
According to above analysis and concerns, I suggest borderline. If the author can solve my concerns, I will consider change the score.	2
"  The paper proposes a new way of looking at the classic
  task of image augmentation for classification. Instead of
  learning an image-level augmentation policy using RL, the
  proposed approach uses multi-agent RL to learn
  patch-specific augmentations. While the benefit of
  multi-agent RL is relatively small, it seems significant
  and the general idea of learning to apply different
  augmentations to different parts of the image is
  interesting and, to the best of my knowledge, new. The
  idea to apply image augmentations at a local level is the
  core contribution of this paper, in my opinion.

  While it would be interesting to see how this technique
  works on tasks which, unlike image-level classification,
  are themselves more ""local"", such as semantic
  segmentation or detection, overall the proposed approach
  is evaluated thoroughly.

  The writing is clear and while the authors don't
  explicitly mention an intent to publish their source
  code, it should be possible to implement the proposed
  approach just based on the description from the paper. As
  such, I recommend accepting the paper to ICLR.
"	3
"The main concern with the paper is with the motivation and missing reference/comparison with a key existing work. 
Currently, I recommend a weak reject. "	2
Overall, I think the paper presents an interesting insight (breaking down caption quality into fluency, adequacy and fidelity) and presents an approach which improves all the three aspects on the nocaps dataset. I raised a couple of concerns (use of CLIP which has been trained on human-generated alt-text data) which goes against the guidelines of the benchmark. It nullifies some of the claims in the paper and I'd be interested in hearing from the authors regarding that. I would have also liked to see comparison with other caption relevance metrics like VIFIDEL which would make the contributions stronger. Overall, I am borderline on this paper and will update my thoughts based on the author response.	3
This paper proposed a strong baseline of novel object captioning via combining BERT and CLIP models, however, the novelty is limited.	2
"The paper is well written with detailed experimental results to support the claim. The paper proposes interesting ideas to improve NOC and is marginally novel but there are still some concerns as stated in the main review. 
"	2
The major concern is about the significance of the motivation and the applied solution is not very economic by using two large scale pre-training models to solve a small task. 	2
"Decision: Accept (6)

The reviewer finds the paper of great interest to the community and the thorough experimental analysis of the proposed algorithm is the main motivation for accepting the paper. However, 

(1) several concerns/comments mentioned above, and 

(2) the fact that the major difference between the proposed algorithm and the prior work [1] is the self-validation step combined with dataset synthesis 

prevent the reviewer from giving a higher score.

[1] Yaman, Burhaneddin, et al. ""Self-supervised physics-based deep learning MRI reconstruction without fully-sampled data."" 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI). IEEE, 2020.
"	3
"The authors make a big deal of using TL, 0-shot and plug-and-play to achieve good results on MRI reconstruction. All of these elements are known and have been applied before to this problem. They do achieve better results than Senouf et al [5], thanks to a better TL regimen.

The paper is correct overall. The early stopping condition is debatable and not studied theoretically.

Overall the paper is pretty good, but I think it does not clear the bar for acceptance at ICLR due to the lack of technical novelty."	2
"Pros : 
- the paper is well written and ideas are clearly explained and stated.
- the paper is technically sound.

Cons : 
- the contribution is too incremental compared to [Yaman et al., 2020] as it consists in re-using the Fourier domain partitioning idea in order to have a validation loss allowing to detect overfitting and stop learning.
- patient specific training demands more computation time and resources than usual clinical workflow."	2
This paper establishes a connection between text generation and path consistent learning, which leads to an elegant RL-based text generation algorithm that inherits many advantages from PCL algorithm. The performance of the algorithm is encouraging for RL-based text generation, although having more large-scale experiments would be more convincing.	3
Overall, I have the experience in using RL for machine translation so I agree with the authors that those problems mentioned in the paper are indeed challenging. But I am not familiar with SQL but from the derivation provided in the paper I am skeptical on the effectiveness of SQL. Since most if not all technical stuff are existing work, I think the novelty is limited and thus justification of each component in the method is important to me. 	1
In summary, this paper is well-written but it lacks relevant analysis and experimental evidence for the problem they want to solve. Further, the method they proposed is directly from the reinforcement learning area, which significantly reduces the contribution of this paper. Last, the paper lacks the experimental comparison with the related work aiming to solve the same problem. 	2
The paper introduces a novel application of Seq2Seq and variational inference and the results show that the method's performance is superior to other state-of-the-art methods for a restricted version of Blind Source Separation where only one source is extracted from a non-linear mixture. My recommendation is to accept this paper for the conference. 	2
"The authors make a strong theoretical justification of the model approach to source extraction from time series. The writing is exceptionally clear and informative and the experimental design is exceptionally sound. I scored 3/4  Empirical significance because the method is only compared to ICA, rather than (1) another more comparable deep model, or (2) state-of-the-art method in the specific tasks you are performing on. On the other hand I greatly appreciate that the authors explore the bound of their underlying Gaussian assumption in Sec 5.4. The technical novelty and significance is more exceptional due to the provided bound justifying the approach. Overall I think this is a solid ""accept"", also because the authors did something that worked and can explain why. "	4
"While the paper presents a meaningful architecture for the task, key aspects of the nature of time-series are ignored and no meaningful baselines are used for benchmarking. Thus the paper is preliminary and needs a more thorough set of comparisons against other non-linear regression problems with and without causal restrictions on the modeling. 

-----------------------------

The authors have performed additional comparisons that help set the context. I am willing to raise my score if the text is further clarified to include the fact that is this is a non-linear multiple input and single output system identification model. 

I agree the concerns with non-causality may not be valid in all settings, and the additional testing on performance at shorter time segments is important.

Finally, I would like to encourage the revision to have careful wording regrading the non-linearity of the mixing versus non-linearity of processes. These two can get conflated. I agree that sometimes electrical recordings of linearly mixed sources result in nonlinear mixtures due to non-stationaries due to sensor/electrode movement as well as amplifier/recording nonlinearities. 

Further empricial analysis into the performance under challenging non-linearities such as clipping would be encouraged. "	3
The paper is clearly written and organized. However the paper is rather weak in depicting its contributions with the present experimental analyses and evaluations, and lacks detailed comparisons to other methods. I listed my major concerns in the main review, and would be willing to re-address my rating based on the authors’ responses and revisions.	3
My recommendation is based on the new proposed network for combining functional/structural brain data analysis in a meaningful way for processing temporal and spatial information, and allowing individualized graph structures to be learned/used within a GNN model. While the results appear good, they appear somehow incredulously higher than standard GNN methods, and important missing information about experimental setup and empirical comparisons further dampen my enthusiasm for the work. 	3
"This paper tackles an important problem in neurosience. 
Clinical motivation and the overall pipeline make sense, and extensive efforts were put on evaluation of the ideas and framework. 
The paper is mostly clear, but I do have some concerns as mentioned in the Main Review above which can be very critical. 
I am willing to change my score once I go through the rebuttal and other reviews if needed.
"	2
Based on the above assessments, I suggest rejection of this paper. The current experimental evaluations and discussion need to be stronger to be more convincing, and the presentation can be improved.	3
This article improves the network method of graph neural network to learn a better representation for brain dynamics in the application of brain images. I prefer to reject this article because this article has made some small improvements on GNN, which is more like to combine them in different ways to improve the overall effect. In addition, the baseline effects compared by the author in the experiment are relatively weak, and the effectiveness of the method cannot be demonstrated.	3
"Pros
- Important problem

Cons
- Poor writing
- Disjointed and poorly motivated results
- Non-standard formulation
"	2
"Overall, the questions that this paper wants to address are important, but the answers provided are not very satisfactory. The significance of most theoretical results in the paper is questionable due to the discrepancy with practical distributional RL algorithms (for example, most distributional RL do not use KL divergence to measure distance) and lack of empirical evidence to back up (such as Section 3, 4, 5). Some major claims are not well-supported by the rigorous evaluations (such as Section 6) and clear explanations (such as Section 4 and 5). The results section (Section 8) for the proposed method is not very trustworthy due to cherry-picking the results and lack of important details. 
"	2
Though the paper proposes a nice interpretation of distributional RL as entropy regularization and the practical property of the representation of distributional RL, the other contributions seem do not significant or meaningful to me. However, the paper could be improved with the suggestions above as the idea presented in the paper is interesting and promising if it could be investigated more deeply. I feel the paper is not yet ready for publication at the current form, and I vote for rejection. 	2
I don’t get any points the authors want to show, and the authors don’t provide any meaningful theoretical guarantees on the distributional reinforcement learning. The authors only list several unrelated thing and try to make them connected with distributional reinforcement learning, without going any real problems of distributional reinforcement learning.	1
"Overall a nice paper with technical novelty and good empirical results. I have some questions about the evaluation since some apt baselines are missing. I will raise my score if the authors can address my comments.
"	3
The idea of the paper is novel. The text of the paper is clear and easy to follow. The experiments show a significant gap in metrics values between the proposed model and baselines. Still, there are several minor drawbacks that can be fixed in the next revision of the paper.	4
 GeoDiff is an end-to-end model for generating Cartesian coordinates from a molecular graph. GeoDiff outperforms various state-of-the-art methods for generating molecular conformations. 	4
Overall, I found this paper a bit hard to follow and I don't think the key messages are clearly conveyed. I recommend the authors to carefully revise this paper in order to be published.	2
This paper studies an important and novel problem with limited works in the literature: RL agents' robustness certification. The authors made strong assumptions and borrow ideas (randomized smoothing) from existing works in similar fields. The contribution is obvious, though not very significant. I am positive about this paper and recommend a weak acceptance.	3
I find this paper to be a valuable contribution to the field, and believe that the theoretical treatment and methodology of this paper can inform future investigations in not only test-time adversarial robustness, but also the general analyses of sequential perturbations in RL.	3
"The paper presents a novel way to show robustness to perturbations in sequential decision making in the presence of an adversary.
While empirical results are not strong, the paper appears to be formally correct and would benefit the community."	3
Experiment results seem convincing, except for some minor issues. I'm not familiar with the prior literature to judge the significance of the new Neyman-Pearson Lemma. The exposition can certainly be improved to make the paper more self-contained and readable. 	3
A good summary of the state-of-the-art and highlighting some interesting relationships.	2
The main concern with this work is that the main insight seems to be a somewhat more specific variant of observations made in past work on the connection between modern unsupervised and meta-learning. While there are some interesting experimental results, I lean against accepting given the lack of any analysis concerning what the novelty is here compared to those papers.	2
"Learning self-supervised visual representations via meta-learning is new and interesting. Experimental results also indicate its potential effectiveness. But the main claim ""contrastive learning is just meta-learning"" is not well supported, as detailed in Main Review. The choice of datasets and tasks in different experiments is heuristic, making the results less convincing. Experiments on the impact of iterations within the inner loop are needed to show more in-depth comparison between the meta-learning framework and SimCLR."	3
The concept of combining self-supervised learning and meta-learning is interesting and may have a bigger impact in the future. The first trick of rotation prediction has limited novelty. But the second trick is interesting and demonstrated to be useful. Empirical results are all incremental and not convincing enough.	3
"Overall, the paper does not have enough interesting results for acceptance.
"	3
"Although the empirical results are encouraging, I believe this paper still does not make the bar in two aspects: i) conceptually - the ideas of synthesizing anomalies or using generative models for this purpose are not new, and the techniques used here are not new either - not has there been a very extensive analysis of them ii) practically - better results can be obtained using other methods such as ImageNet-pretraining, so in terms of practical utility to the engineer - this might not be the first choice.

############################

I thank the authors for the response. I do not believe it addresses my main concern i.e. lack of convincing ablation. I therefore kept my borderline reject rating."	2
"The authors proposed what seems like a simple solution (which is great in my opinion) for an important problem, however they have overcome several challenging problems and achieved good results.
The paper itself is well written, easy to follow, and is very detailed regarding the implementation and experiment settings.

====================================

Following the discussion with the other reviewers, I have decided to keep my current score. As other reviewers have pointed out, the selection of the CVAE is not justified enough, and a serious ablation study should be added to justify it and not other generative methods. "	3
"Overall, I liked the proposed algorithm and think that CGA can be a competitive baseline for supervised OOD detection. Therefore I'm leaning towards accepting the paper.
"	3
Overall, the idea of this paper is new, but I think this paper can be better organized to make reader easier to understand the technical details. If the authors can address my questions, I am willing to change my score.	4
I believe the paper contain notable contribution in designing new algorithm using dropout. Numerical experiments illustrate the practical performance of the proposed algorithm However, the proposed lacks discussion on the convergence of the algorithm while other methods like FedAvg/FedProx do have guarantee. The proposed methods appear to only work directly with neural network models, there should be more discussion on other types of model.	2
Based on the above comments, I do not believe this paper passes the acceptance threshold. I encourage the authors to address the points around: motivation, practicality, and rigorous experimental comparisons with other works.	2
While reducing client computational costs is an important task in FL, the method proposed in the paper comes at a very high communication costs, which makes it unlikely to be practically useful while communication costs are still the primary concern in FL. 	3
Because I am not fully convinced the by the claims of the authors, I rate it marginally below acceptance threshold. I am willing to update my score based on further evidence by the authors in support of their claims.	2
"A nice direction for investigation. But the approach and findings don't appear significant enough to warrant acceptance in ICLR right now. The paper is further hampered by the very poor presentation.
"	2
"Strengths:
1. Training a DP language model is a less explored problem, and the authors provide a technically sound solution along with a well-setup empirical evaluation. 
2. The paper is overall well written and the method is easy to follow. 
3. The authors propose a new evaluation metric P-N to indicate how many generated n-grams can be found in the private set, which is an interesting and useful indicator in addition to PPL on the private set.
4. The authors also provide an interesting discussion on user-level DP and n-grams-level DP for language models.

Weaknesses:
1. My main concern of this paper is whether SeqPATE can achieve satisfactory utility preservation given small epsilon (say epsilon=2). 
2. The advantage of SeqPATE vanished given a large epsilon compared with DP-SGD, which makes the algorithm less scalable. 
3. The authors discuss user-level DP and n-grams-level DP. It is better to provide some quantitative evaluation on SeqPATE on these settings.
4. Some experimental setups are a bit unclear. it is better to include more experimental details in the main paper, e.g., how the hyper-parameters are chosen (say #/ teachers) and the computational overhead of SeqPATE and DP-SGD.
"	3
Overall, I think it's a strong paper. The topic is timely and interesting. The writing is good, and the approach is clearly presented. The proposed approach contains several non-trivial changes compared to PATE. The paper provides extensive empirical evaluation, which demonstrates the practicality of SeqPATE. 	3
Unfortunately, the paper is rather incremental with respect to WaveletFlow, the contribution is not expressed well and qualitative analysis is missing. I would like to encourage authors for more extensive empirical evaluation and highlighting the novelty of the approach. Therefore, my current recommendation is to reject this work. 	2
I think that the current draft of this work makes incremental contributions to the area of normalizing flows for image synthesis. While I completely agree with the core motivations of bringing multi-resolution structures and scale-separated noise injection to CNFs, I believe the authors can make a much more significant contribution by analyzing how their core motivations play out in their proposed method. 	2
Overall, I enjoyed reading the paper and I really like the mutliresolution idea that leverages conditional probability modeling, but I think the claimed main conceptual novelties are weak and not well supported by experimental results. Therefore I cannot recommend acceptance at ICLR.	2
As the authors also point out, using multi-resolution strategies is not novel in itself. Additionally, the reported experiments indicate that the proposed method is not mature enough to be applied to larger images, limiting its usefulness. However, despite the incremental novelty of the approach and limited empirical results I believe the authors propose a method with potential to be widely adopted in the normalizing flows models. I would be willing to update my recommendation based on the authors’ response to my clarification requests listed above.	2
This paper introduces a new architecture for multi resolution continuous normalizing flows. The paper is well written and the model is sound and the authors achieve impressive results on Imagenet at high resolutions. However, as described in the weakness sections I believe there are still some aspects of the model and experiment discussion that need to be clarified. I therefore believe this paper is currently marginally above the acceptance threshold.	4
Overall, the idea in the paper is well-motivated. However, the results in the paper are mostly about overparameterized linear regression and matrix recovery, but the many of the claims in the paper are about understanding the generalization of neural networks. Besides, I am not convinced that the derived bounds in the paper really improve over the original bounds. Therefore, I am currently leaning towards a weak rejection. If the authors can address my concerns, I will consider increasing my rating. 	3
"The work has some interesting new ideas that, to the best of my knowledge, are original. However, the results are presented in a confusing manner, making it hard to understand the scope of applicability of these results to a non-expert. Therefore, I believe this work could benefit from a rewriting. 
"	3
The decomposition proposed in this paper makes sense in some specific settings like linear regression. However, it is very unclear for me to see how this decomposition can be extended to a more general regime and how it helps understand the learning dynamics of deep learning. There are also no new technical tools developed. Given that I believe the decomposition's applicability and significance are questionable, and the technical contribution seems incremental, I recommend a weak reject.	2
The idea seems to be novel. However, the advantage of the decomposing excess risk dynamics is not well justified. There are also some issues on the correctness of the deduction.	2
"The submission is clear, well-written, and it explores a simple idea with rigor. The proposed idea is theoretically grounded and the experimental results are convincing. The one thing in the way of a higher score is that the submission could be made more accessible to readers with less experience with the theory of small-sample bias in MLE and Firth's bias reduction for MLE.

---

**Post-rebuttal update**: The authors' response addresses my concerns, and I now feel comfortable making a clear acceptance recommendation."	3
I believe the proposed method is novel and interesting. 	4
Overall I am in favour of accepting this paper. The strength of this paper is on the theoretical justification of the proposed regularization term introduced associated to the general Firth bias reduction. Also extensive results are carried out to evaluate fairly the benefits of the proposed approach on SOTA solution.	4
To conclude, this paper is clearly elaborated with sufficient experimental results and promising improvement. I suggest a ‘weak accept.’	3
"This paper was an interesting read and provided insightful and clear justifications for the results. There remain some questions as to the theory and some particular loss choices made, but these are relatively minor points. 
"	2
The paper describes a new graph autoencoder approach that could encoder more information into the latent space. However, the scalability of the proposed method is questionable. Also, the proposed method did not outperform baselines on three datasets. 	3
Overall, the paper is well written and presents an interesting and efficient approach to graph representation learning. I lean towards accepting the paper if the authors address the above-mentioned concerns and questions.	3
I am satisfied with the solidness of the paper, including the methods part and experiments, while I feel it is limited in novelty.	2
This is an interesting paper, but it seems that some improvements are needed.	1
"In general the contributions of this work are significant and novel. The proposed OVD-Explorer exploration algorithm seems to be efficient on the noisy variants of the five MuJoCo environments, showing that it is able to avoid areas with high aleatoric uncertainty. Apart from that a theoretical analysis is also provided. The main weakness of this work is the fact that the OVD-Explorer has been tested only on five MuJoCo environments. I would have expected the empirical analysis to have been conducted on more environments, e.g., DeepMind control suite. Last but not least, authors should make more clear the problem of aleatoric exploration by providing some real-world cases where we encounter this type of problem. 
"	3
The paper presents a new exploration method and shows improved performance compared to state-of-the-art methods. However, the intuition behind the exploration strategy does not seem to be correct.	2
This paper proposes a novel explorer that can distinguish epistemic and aleatoric uncertainty. Although technically the theoretical motivation does not look very sound to me, the experiment results seem to be quite promising.	3
This paper proposes a new actor-critic algorithm to deal with a cooperative average reward fully decentralized MARL and provides the first finite time convergence result. The theoretical results are important. However, the differences with existing literature are not well explained and numerical experiments need some improvements.	3
The theoretical contribution of this paper on top of (Zhang et al., 2018) is good. However, there also are some issues in terms of the assumptions, comparisons with the global convergence of actor-critic methods literature and numerical experiments that need to be addressed. 	3
"Overall this is an interesting generalization from [Zhang et al 2018], but I feel many points are worth more elaboration and discussion. More simulation is needed. 
"	3
The paper has made valid contribution to the area of multi-agent RL, and is well-written. As the empirical contribution is a bit limited, so I would view the main contribution to be theoretical. However, some detailed comparison with the most related works is needed, before justifying the novelty and significance of the theoretical contribution.	2
The paper is well written and the topic is timely and of interest. Some assumptions/statements require more elaboration and the experiments should be strengthened. 	3
I like the way the authors approach the fairness constrained problem, and their study of property of feasible optimal solutions. However, I think the writing lacks justifications of the various assumptions, which makes the results very narrow. Further, there are many many results that consider the non-convex formulations resulting from minimum norm over demographic groups and composite objectives, and so though I think the algorithms proposed by the authors are nice, I suspect that these can be subsumed by existing literation in constrained optimization, and I would like the authors to comment on that. 	3
I think the problem is a natural one (finding a good classifier subject to fairness constraints), but the technical results were not compelling and the writing quality was below the bar.	2
The contribution of the paper is not good enough given the existing literature on fair machine learning. It seems to me that this paper provides an extension to (arXiv:1802.08626). There are no numerical experiments to compare their framework with the state-of-the-art fair learning algorithms such as (arXiv:1802.08626, arXiv:1610.08452).	3
The paper considers an important fairness notion EL and provides algorithms with provable guarantees. Though their technique seems not hard, this may not be the main issue in the direction of fairness. Empirically, the proposed algorithms outperform baselines. The main weakness is the missing of several important prior works.	3
Considering that the proposed bias reduction method is simialr to the idea of Fair Score Normalization, and the performance improvements are very limited, and moreover the writing of this paper is not good. Thus, I suggest to give the decision of rejection to this paper. 	2
The main novelty of the paper is applying beta-calibration on data clusters found in an unsupervised manner for face verification models. One of the selling points of FairCal is that the method allows to fairly-calibrate the classifier, i.e. the calibrated classifier will produce equal probabilities of positive matches across sensitive groups. I do not exactly understand why that is a desired property (in contrast to having equal FPRs and FNRs across subgroups) for face verification system. It would be very helpful to hear from the authors on that! Because of that (and other questions I have) I give the score of 5 for this paper, but I am willing to increase it after the rebuttal. 	3
The topic of this paper is currently very important in the CV/ML community. Although the novelty of this work is incremental,  the results justify the superiority of FairCal (and oracle). While the readability and thoroughness of this work is quite high, there are still some issues in this work. So, I vote for weak accept. I hope the authors address my concerns (especially about similarities with the BFW paper and add comparison to this paper). 	2
The paper presents a DA-inspired view on (unsupervised) cross-lingual transfer, offering some insightful analyses, but it seems as an eclectic (mostly incremental) work, with insufficient and lacking empirical validations, incomplete baselines, and inadequate positioning against previous work in this area, which would negatively affect its potential impact and its overall contributions - more work and a stronger empirical foundation are needed.	2
"The paper is mostly well organized, and provides extensive analyses and experimental results. They will be helpful to the studies in crosslingual transfer learning. However, some important descriptions on the model settings or experimental results are shown in Appendix, which makes the paper difficult to read. I would suggest to revisit and reorganize the sections for better readability. Regarding the experiments, since mBERT that the authors used as a pretrained model serves diverse language representations, they could provide deeper discussion, by moving Table 4 to the main 9 pages.
####
I read the responses from authors, and appreciate their response and showing more results. I am okay with accepting the paper, but keep the score 6. Because one concern might be that those results/analyses are mostly described in Appendix. The authors would need to reconstruct the manuscript by moving them into the main pages. "	2
"Overall, the paper first presents insightful analysis that highlights the role of feature invariance and class prior shifts on the extent of zero-shot cross-lingual transfer. The insights from the analysis are then adapted to develop Importance-weighted Domain Adaptation for zero-shot crosslingual learning, resulting in improved performance on MARC sentiment analysis and WikiANN NER; with significantly improved robustness under class-prior shifts. 

The paper produces some valuable insights and develops a well-grounded approach that improves the robustness of zero-shot crosslingual learning. However, the empirical results are limited to just two (relatively) small scale tasks. Having additional results on a wider range of tasks (for eg. additional tasks from the Xtreme benchmark) could significantly strengthen the results.

Given the thorough analysis, but limited range of tasks, I am leaning towards acceptance. If authors include additional empirical results I would be willing to update my recommendation to strong accept.


"	3
The proposed method is well motivated given that the empirical analysis reveal the influence of representation invariance and class shift. Experiments show the effectiveness of the proposed method under large class shift. However, the experimental results are not convincing enough and the paper can be improved by conducting more experiments and analysis.	4
This is a good work that fills the gap of visual reasoning of hidden attributes, but experiments could be more comprehensive.	3
"The authors provide a novel and well-thought task and corresponding dataset ComPhy which requires a model to use 4-reference videos to infer intrinsic properties of the objects and thereby apply these findings to a target video and answer corresponding factual/predictive and counterfactual questions. However, in its current form, the exact motivation of the paper is unclear, the reliance on using 4-reference videos compared to simply providing the information is not justified, and some dataset choice such as using only target video for given reference videos are not clear. More intermediate results such as finding object properties would make the paper stronger.

---
Post Rebuttal: The authors have substantially updated their paper with additional ablative studies and human evaluation results. In my opinion, this substantially improves the findings and takeaways of the paper. More experiments with physical properties in isolation, showing the effect of increasing the number of reference videos, and de-coupling the reference and target videos could further strengthen the paper.

As such, I am increasing my score to 6."	3
Both the presented dataset and model framework are minor tweaks of existing work. There's no novel insight. The dataset will not enable any new research directions. The paper also needs more work. I can't recommend accepting at all.	2
The reasons for my low rating are: (1) lack of novelty with respect to previous works, (2) dataset not being comprehensive enough, (3) simplicity of the dataset and lack of generalization to slightly richer scenarios in terms of appearance and object variability (4) lack of clarity on creation of the dataset. 	2
The paper proposes an interesting and original idea. However, I think a few important experiments missing (higher noise ratio, additional baselines such as label smoothing and mixup), and the Table 2 needs to be updated with more recent work. I would be willing to upgrade my evaluation if the authors address my concerns during the rebuttal.	4
Overall, the quality of this paper is good. The proposed approach is supported by the theorem. The experiment shows that information fusion (IF) outperforms PTD (Part-dependent transition matrix) by a large margin when the noise rate is high.	3
The idea is interesting but the paper is not well-organized or presented. The information fusion approach, which should be the key novelty of this paper, is not demonstrated sufficiently. The main concern is that the proposed information fusion approach reduces the variance at the cost of introducing bias in estimation.	3
"Generally, I feel that the claim in this paper is reasonable, the model proposed is also new. The paper is written in a clear way. I only have some minor concerns:
1. In the title, the authors use ""information fusion"", which I think is not concise. Note that ``information'' is a rather wide topic, so after reading the title, I cannot see what are fused in this paper. 
2. Since this paper studies instance-dependent label noise, I hope to see more comparisons with existing methods for handling IDN, such as ""A Second-Order Approach to Learning with Instance-Dependent Label Noise"" (CVPR 21); Tackling Instance-Dependent Label Noise via a Universal Probabilistic Model (AAAI 21);  Learning with Bounded Instance- and Label-dependent Label Noise (ICML 20). In the current experiment, only one method, namely  PDT (NeurIPS 20) is designed for IDN. Other methods such as Forward, T-revision are not specifically designed for IDN. Therefore, I feel that the comparison is not that fair.
3. Is the solution for Posterior transition matrix in Thm 3.4 guaranteed to satisfy the constraint that the sum of every column is 1?
4. From Thm 3.5 and Eq. 13, I feel that the solution is also related to statistical efficiency? See Section 2.4 and Section 3 of ""Centroid Estimation with Guaranteed Efficiency: A General Framework for Weakly Supervised Learning"" (TPAMI 20). If yes, the authors may analyze the connections or relationship between them.
5.  In Fig. 1, I suggest the authors clearly annotating PTM in the figure. Note that the authors use ""noise transition matrix (PTM)"" in the caption, but write ""posterior TM"" in the figure. Such inconsistency in the figures should be avoided.
6. Some language issues, such as ""a easy-to compute"" should be ""an easy-to-compute""; ""is the instance vector of n-th sample""->""is the instance vector of the n-th sample""."	3
Based on my concerns above, I cannot recommend acceptance at this stage.	2
Interesting application, but need the authors to justify some ambiguity.	3
The paper offer a novel perspective on Equal Risk Pricing for option hedging using reinforcement learnin and dynamic risk-measures. The contribution is mainly an algorithmical and experimental one, but the proposed approach seems to be sound and effective. I would like to propose a borderline accept to this paper.	3
Overall speaking, this is a good paper attempting to address an important topic in neural architecture search regarding search space redundancies. The message is not groundbreaking in my option given several existing works with similar implications, but the analysis and results are interesting enough to be informative.	2
"This paper focuses on an important topic of understanding the search spaces better. In addition to developing new algorithms, one also needs to investigate the characteristics of the search space and how one can improve it.
"	3
The paper points out several shortcomings of cell-based search spaces which are arguably the default in neural architectures search. In the long run, I think the paper can have a significant impact and help us to define better search spaces.	3
The findings in this paper are inspirataional. However, the analysis is limited to particular search space which doesn't mean that the cell-based approach is not valuable in general. The paper is trying to provide some insights on search space design but I would like to see more elaboration in this direction for this paper to be accepted.	2
While the discoveries in the paper are interesting and may be useful for searching CIFAR, it is entirely unclear how they extend to more practical NAS settings, e.g. tasks beyond CIFAR and constraints beyond just the DARTS search space. Indeed the findings may not be relevant if they do not hold in other settings NAS may be applied to. I view answering these questions as critical and so lean against acceptance.	2
"I would suggest the authors be careful about their claims. It appears to me that some extension from DKS motivated by the similarity between the model class-preserving transformation and Leaky ReLU is nice, but is not world-changing. 

I appreciate the detailed derivations. 

My overall rating is 6."	3
Although the authors have well-organized the paper, the current draft is hard to follow due to its technical complicity. Moreover, given the marginal improvement compared to DKS, a more extensive discussion on smooth and ReLU activations is desirable.	3
"This paper makes solid theoretical contributions on analyzing the initial conditioning of feedforward networks without shortcuts. The analysis can even be used to rescale the shortcuts of BN-free ResNets and achieve even better results than standard ResNets (with BN). However, the method also introduces an extra hyperparameter $\eta$, and results of shortcut-free networks are still slightly worse than ResNets. The clarity of the paper also needs improvements.

"	3
The paper provides sufficient discussion and experiments for improving DKS. The proposed approaches yield good results on models without residuals and normalizations.  However, when compared to existing DKS methods, the improvements seem to be somewhat marginal, thus restricting the significance of this paper.  	3
This is an interesting paper trying to answer “why” and “when” mixup works. But it is neither a theory paper providing some meaningful theoretical results (that can be applicable for general scenarios), nor an empirical paper suggesting new scheme and providing extensive experimental results. I would say this is a toy theoretical attempt, which is not suitable for ICLR acceptance.	2
I recommend 8: accept, good paper. In my opinion, this paper will be useful to the community both for theory and experiments. I have listed a few questions and suggestions in the Main Review. It is possible that I have missed some of the main arguments in the related works, but to me the contributions of the paper are novel and useful.	3
The paper contributes to the understanding of Mixup training from a training data dependency perspective, which is novel and I think it would benefit the research community. On the other hand, the paper is not clear to me in some places as detailed in my reviews, and I would like the authors to comment on them.	3
"While the paper provides insight in a very exciting area of theoretical research in machine learning, I currently do not recommend acceptance due to inaccuracies and the lack of a cohesive thesis.

Pros:
+ Novel theoretical analysis of VAEs during training and at global optima
+ Theoretical observations are corroborated by empirical results

Cons:
- Theory only considers linear encoders and linear/1-hidden layer nonlinear decoders
- The convergence behavior during training is only provided for linear VAEs
- Work feels disjointed, writing could be clearer (e.g. Theorem 4 has very little discussion)

**AFTER REBUTTAL**

My understanding of the work has been improved by the rebuttal. However, I do not feel that the stance taken in the manuscript adequately represents the stance taken in the rebuttal. I would be more willing to recommend acceptance if the misunderstandings were properly addressed in the manuscript---however, the manuscript, as it stands, has not been edited in such a way. (See my reply to the rebuttal.) I still stand by most of my points. Here is the updated summary:

Pros:
+ Novel theoretical analysis of VAEs during training and at global optima
+ Theoretical observations are corroborated by empirical results

Cons:
- Theory only considers linear encoders and linear/1-hidden layer nonlinear decoders
- Work feels disjointed, writing could be clearer (e.g. Theorem 4 has very little discussion)"	2
I'm negative on this paper. The paper needs to be revised in formal way. The motivation should be clearly introduced (rather than citing the paper), and the conclusion should be also provided. The paper seems not ready to be published, however, the work is interesting and I'm looking forward to have the paper (with additional experiment on benchmark or real-world) in the revised version.	2
The paper gives a sensible discussion, but a lack of practical aspect limits the broader importance. 	4
"(edited)

Strengths

+ The cautions against misinterpretation will be useful to the broad ML community.
+ Findings about the implicit bias are interesting.

Limitations

+ Further efforts are needed to understand the discrepancy between the results here, which do not provide rate of divergence, and the informal calculations in Dai and Wipf, which implicitly places assumptions that do not appear consistently supported by experiments."	3
"My recommendation of the paper is based on the following (more details in the main review):
- (+) I appreciate such a study because there is a lack of existing work on SSP with linear function approximation as far as I can tell (except the work of Vial et al. 2021).
- (-) My first concern is that I do not see any novelty or significant contribution on the technical front. Most of the technical tools and ideas used are borrowed from prior work with minor changes. 
- (-) My second concern is that the obtained guarantee appears to have some loose dependencies on the problem parameters and confidence parameter $\delta$. A discussion on such dependencies is lacking, especially in the low-order terms appearing in the regret upper bound (Theorem 1). 
- (-) My third concern is that the proposed algorithm requires certain inputs which might rely on unknown quantities. The authors did not clarify how these inputs should be chosen. "	2
The authors consider SSP frameork under the linear mixture models (that is particular structure for the transition kernel). Adapting ideas from recent works on linear mixture modes in other settings to SSP setting, a model based algorithm is presented and its regret bounds are provided. 	2
I think overall this is a good paper building on ideas from RL with linear function approximation to SSP setting. There are subtle issues with extending these ideas to SSP which this paper resolves. 	3
This paper proposes SAGE, an adaptive learning rate schedule to train redundant parameters more sufficiently to improve model generalization. SAGE, together with existing adaptive optimizers, shows effectiveness in a wide range of downstream tasks. The main concern is whether SAGE has adverse effects in getting efficient deployment models.	2
"For an ""optimizer"" paper, it is weak: hand-wavy motivation, no real derivation of update rule or even convergence proofs, and experiments only in one very specific domain: transfer of pre-trained transformer models. I am not convinced that it will be generally useful at all.

However, the paper raises a very important point: we should not take redundant parameters as a necessity, and it does propose a method to avoid them. I find this point important enough to still suggest acceptance."	4
A simple adaptive learning-rate formula that seems to work really well, even on top of other optimizers, as demonstrated by very thorough experiments. Downsides are that the formula isn't particularly well justified, and there are a few potential experimental weaknesses.	3
The idea of the paper is interesting and the proposed method seems effective. Nevertheless, more complete experiments in image classification would allow to better evaluate the interest of the proposed method. 	3
Overall this is a very thorough empirical study of NMT scaling that is sure to benefit the NMT research community immensely when read alongside Ghorbani et al. 2021.	3
"The major takeaways for the paper, I think, are:
- Changing in model sizes and architectures have very similar scaling patterns, with a shared component. 
- Different noise reduction filtering methods (e.g., CDS and Bicleaner) can also be fit into the shared component.
- Adding different types of noises can dramatically downgrade the NMT performance, while independent noises can still fit into the share the component, except the dependent noises via back-translation.

Overall, these findings emphasizes again on the importance of data quality for NMT models, which is somehow known to the community. The paper further quantified such importance with the data scaling laws. "	2
The paper shows strong empirical results on fitting scaling laws. The experiments are carried out on large scale dataset and large models. While this provides valuable insight for practictioners on making decision when training a NMT system, using log-perpelxity without human evaluation is problematic.	3
The contributions of this paper are incremental and the conclusions are also not convincing.	2
The idea proposed in this paper is novel, and its effectiveness is backed up by theoretical analysis and experimental evaluations. Although no strong practical results have been shown yet, the idea deserves to be published soon. I would like to recommend acceptance for this paper. 	3
I feel the paper introduces a novel approach for learning prover-verifier systems. Still, I think the theoretical and experimental treatment of the topic in the current version of the paper is not rigorous enough at the moment. I feel the authors need to address the comments above.	2
To summarise, although I personally find the paper close to impossible to follow, I don't really feel in a position to judge it based solely on this fact simply because I am not an expert in the area, and it is unclear to me what presentation is deemed standard here and what level of preliminary descriptions is accepted as adequate.	2
The goal of coming up with a neural network that can justify its decision is an important task. The authors have done a thorough job of technically analyzing the proposed framework, but experiments could be strengthened by using more realistic settings.	3
Overall I am leaning towards a weak accept. The paper has clear novelty regarding stroke-based image editing via perturb + reverse SDEs. Comparisons and ablation study look good to me except for the one on the level of user sketch control. 	3
The paper is well-written and easy to follow. The results are impressive. Although Stroke-based Editing is not complex, especially on the face,  I think this is a good start of using the SDE to do the image editing task. 	2
The paper proposed an interesting technique to edit image using the SDE. The generated image is not only faithful but also relistic. However, considering the technique novelty and the overclaiming, I vote for a weak reject initially and am willing to listen to the authors and other reviewers. 	2
"Although both the input space partition of neural nets with piecewise linear activations (sometimes called linear regions, convex polytopes, etc.) and node/structured pruning have been widely studied and well known in recent years, this paper provides an interesting and novel perspective to relate them, i.e., explaining why a high pruning ratio can still work, early-stopping for pruning, and pruning by removing redundant units. The case studies clearly explain the empirical observations and well motivate some conclusions. However, the main results are a little bit disappointing to me due to the lack of in-depth discussion of the phenomenons. Necessary math formulation of some important concepts (e.g., splines, grouped partition trajectories, etc) is missing. Both the early-bird metric and the pruning strategy lack a strong or insightful connection to the spline partition and decision boundary discussed in the first part of the paper. The experiments have not explored higher pruning ratio regimes, which are necessary since the reported accuracy does not change too much over the evaluated pruning ratios. 

-------------Update---------------

The authors address most of my major concerns in their new reported experiments and updated draft. Therefore, I raise my score to 6."	2
The idea and the observation is interesting, and most of the experiment results are promising. However, it is unclear how the author finds the subdivision spline to remove in experiment implementation. And in the theoretical part, I suggest the author to provide the convergence proof of proposed algorithm’s under Lottery Ticket hypothesis. 	2
Although the paper has proposed a very interesting perspective on network pruning, it lacks supporting evidence on some of their main claims. Therefore, I vote for a borderline paper with a tendency towards reject. 	2
The paper is well-written and the ideas are well-presented. However, some points listed above need to be clarified.	2
"The idea is seemingly novel but it is not enough for publication in this status.
I recommend reject. But I'm willing to raise the score if the authors answer the questions with more details and write them in the paper."	4
"Just as one can learn NeRFs, the authors show that it is possible to also learn (some version) of RIRs.  Representing RIRs is a big deal, and this paper can potentially be the first one published to use this approach.  However, had this been submitted to an audio conference I predict that it would be summarily rejected.  Unless I misunderstood your representation of the RIR, the problem solved is one that is made to be solvable, but not particularly useful for any real problem.  The log spectrogram of an RIR is not a quantity that one would employ in any audio processing, and the degree of approximation shown in figure 3 is way off from a high fidelity result (alas there were no qualitative listening experiments in this paper).  Although I appreciate the initiative, I would have expected a lot more depth in the first neural RIR paper, and this is coming short by a lot.  It's nice to see that a simpler version of this problem is solvable, but I do not see a useful contribution here.

If, on the other hand, I'm mistaken and you are actually estimating the phase as well, then I would like to see some results with more realistic audio experiments and potentially some quantitative listening numbers."	2
In general, this paper studies an interesting problem and presents some preliminary attempts. But overall, the proposed method seems just another way to represent an existing dataset from another work. The paper overclaims its contribution and there are multiple places in the paper that needs more clarifications or analysis or writing improvement. Therefore, I recommend reject of the paper at this stage but happy to discuss more.	2
Overall, the manuscript considers a very interesting problem, but I am not convinced that the proposed method would be sufficient to provide a solution as implied in the Introduction. Specifically, given a large database of room impulse responses, the neural network appears to learn how to store a smooth field based on those values (since log magnitude is the target), but it's not really clear how the geometrical side information could be useful for generalizing beyond a room. I also found the paper to be a light on some of the details, which I think are critical.	2
The overall idea of the paper is good. But it is hard to follow and some analyses are not clear.	3
Overall, this paper raises important questions about the usefulness of current post-hoc explanation approaches for NNs and provides a range of empirical results to support these claims. While the contribution is incremental compared to previous studies, I believe the authors do raise some important points that will help to stimulate more research in improving post-hoc explanation methods. However, the manuscript is quite dense, lacks some important clarifications (e.g., about the performance measures and the empirical study) and more rigorous statistical tests to substantiate the authors claims. For this reason, I cannot support acceptance of the manuscript as is, but I am willing to reconsider, should my concerns be addressed. 	2
"Overall this paper is an analysis paper of 3 self-proposed metrics on posthoc analysis of spurious signals. The metrics turn out not to be accurate enough for the tasks at hand, especially when the spurious signal is implicit or unknown. There is no discussion how to make the metrics better at detecting spurious signals. 



###### POST REBUTTAL #### 
Update the score to 6 "	2
This paper needs better illustration of their algorithm and better justification of the novelty. 	2
"Please see above.

In order for this work to be acceptance quality, the authors should at least provide more details about the iPrune approach and compare it with more state of the art approaches. Additionally, the authors should perform experiments on more challenging datasets in order to substantiate their claims."	2
As mentioned in the paper, this work relies on two existing works. Even though successful adaptation of existing works is challenging and valuable, I believe the contribution of this paper is still very limited since it overlaps with an existing work (i.e., https://openreview.net/pdf?id=r1fYuytex).	1
In general, I think this paper is not ready for publish and still has a big room for improvement in terms of novelty, writing, and technique contributions. 	1
I like the theoretical results. However, some more detailed comparisons with prior works, along with take-aways for practitioners, are necessary to make this paper better.	3
I did not feel the paper made significant empirical or explanatory contributions, and was unable to assess the theoretical contributions. I thus overall would recommend against accepting the paper.	3
"In short, I found the problem interesting and the solution clean and understandable.

However, the technical novelty in the work is relatively low and it mainly involves combining existing works in a smart way. I would urge the authors to either extend the theoretical results eg. by looking into more complex hypothesis classes or to improve the experimental work."	2
"This paper studies a very interesting setting. The approach is meaningful and can be used for future reference. However, I could not understand the extent to which the authors' claim of ""benign overfitting"" is supported. Therefore, I vote weak reject. If this point becomes clear to me and readers, I will raise the score to acceptance."	3
Overall, the reviewer thinks the paper is solid while lacking some novelty. The reviewer appreciates the details mentioned in both the main paper and the appendix.	2
Overall, I vote for accepting. First, the overall logic of the paper is relatively clear, basically answering two questions in Pros of **Main Review**. In addition, I think it is reasonable to consider the supernet partition problem from the perspective of gradient, and this work proves the effectiveness of this method through experiments. Hopefully the authors can address my concerns in the rebuttal period. 	2
"This paper presents a new partition strategy for few-shot NAS, which empirically seems to be a more efficient solution to alleviate the weight-sharing problem in one-shot NAS. The intuition/algorithm details are well demonstrated and the experimental evaluation is clearly stated.
Overall I believe the quality of this paper has met the bar of ICLR community, but my concerns about the justifications for its technical designs and some numerical results make me indecisive on posing an acceptance. 

I would like to raise my rating if authors properly address my concerns.
"	3
Overall, it is a paper of good quality. I lean toward accepting the paper because of the intuitive splitting rule proposed in the paper and the supportive results. I will consider raising the rating if the authors can address my above-mentioned questions regarding the experiments.	3
This paper is novel and the results are interesting. However, the technical part is somewhat simple, and the insight behind the observations needs further exploration.	2
"1. Section 3.2 indicates that the estimator is exact for uniformly distributed data, and datasets applied in experiments are balanced. Is the proposed metrics only work for the balanced dataset?
2. Figure1 is fuzzy and the name of methods overlap badly, which affects the reading. The description to the calculation of CL is unclear and confusing. If I understand correctly, is each sample is classified according to all the samples have seen before?
3. The improvement brought by KNNDeepCluster is tiny compared to the base method, which is hard to be convinced of the effectiveness of the proposed method."	2
This paper proposed Intrinsic Dimension (ID) and Cluster Learnability (CL) to estimate supervised and self-supervised model's prediction performance. However, the proposed frameworks are heavily identical to the existing works. Considering the less-structured writing and confusing notations, I believe this work has not met the publication quality.	2
This paper is generally well-written and the motivation is clear. However, I still have some concerns (see weakness) and I think this paper could be more complete by providing theoretical analysis or developing an SSL algorithm which directly optimizes CL and ID.	3
"This paper re-evaluates the underrated baseline of random pruning in sparse training. The paper is well-written and well-structured. Although the paper does not provide new pruning techniques,  I believe the conclusions from the paper can contribute to the sparse training domain.
"	2
This paper has substantial overlap with a previous paper. Also, they have unfair comparisons in their experiments. 	2
The work questions the common wisdom of sparse training and found properly trained randomly pruned sparse networks from scratch can match dense networks in many ways. This finding is counter-intuitive yet important. The paper could yet be strengthened further by providing more interpretations of its observations.	3
"As mentioned in the paper, the technique isn't novel however, the detailed study can benefit quite a few in the community given the potential ramifications of such observations. I would say it is a great benchmarking paper that would help people like ""The State of Sparsity"" (Gale et al., 2019) did, but again it doesn't make a super-strong case for acceptance. I am on the fence with a 5.  However, I am willing to change my mind after rebuttal and discussion with other reviewers. 

I still think this paper is a good asset and will be a great one if the same scale of experiments is done on ImageNet instead of CIFAR and will prove to be a solid exploratory work in that case.

Overall, I am happy with the rebuttal. I want to see these updates and results in the main paper and not the appendix and you can do this as revision in the next few days. 

I am increasing my score to 6 and will talk to other reviewers but push for an acceptance. "	2
I don't believe that the provided results or the techniques used in this paper are surprising. The convergence results provided here are not tight in all the constants, and some of the assumptions made to provide those are too restrictive and don't morally allow anarchic behavior. As a result, I am not in favor of accepting this paper in its current state. 	2
"As mentioned in the main review, I believe this work has potential and is interesting, but difference to asynchronous FL with random client selection (by the server) and addressing the worst-case behavior that dominates the convergence bounds are the two concerns I have.

========

Post-rebuttal: I have read the authors' responses and some (responses 3 and 4) have clarified confusions I had, which is helpful. The other responses are largely in line with my understanding of the paper. Overall, my review of this paper does not change. "	2
"I think studying asynchronous and device-centric communication in the context of FL is definitely interesting, and I feel the algorithms and the analysis in the paper are correct and solid. My major concerns are the first three points in the 'concerns' part of my comments, especially the differences between the submission and that related work.

====update====

I have read the authors' responses. However, I don't think the differences from the previous work (https://arxiv.org/abs/2106.06639) are significant. My concerns regarding the uniform arrival and bounded delay assumptions in federated learning, the experiments, etc remain. Therefore, I do not think this submission in its current form is ready for publication and I retain my original score."	2
The anarchic federated learning is vulnerable to attacks and can be slower than FedAvg because of unbounded delay, local steps, and heterogeneity of clients.	3
I reject this paper because (1) it is hard to find practical applications of the proposed method; one can solve the problem more easily in many practical applications, (2) it is not clear how each of the inputs affects the predictions, (3) the experiments do not include diverse real-world datasets.	2
While each step of the entire process is fair, the necessity of each step is not sufficiently verified. Despite the hypergraph setting and the weak supervision setting, authors show the experiments on multi-view graphs or partitioned graphs (for real-world datasets), and do not show the benefits of weak supervision. Furthermore, the proposed method should be compared with Guassian-embedding based models.	2
Overall, I think the idea of incorporating coarse-grained graph label to assist graph clustering is intuitive and somewhat interesting, and the proposed model also generally makes sense to me. But I still have some concerns regarding the title, the choice of graph labels, and the performance analysis.	3
The problem is novel and the authors have demonstrated that having coarse-grain labels on the whole graph can assist with clustering on a node level. The comments for improvement are mainly on clarifying some details of applying the proposed algorithm.	3
"1. There are some closely related works in the literature. The authors should discuss the contributions more clearly. 
2. Does this paper ""Semi-Supervised Graph Classification: A Hierarchical Graph Perspective"" study the same problem？
3. Is the proposed method related to multi-view/multi-layer graph clustering?
4. There are some writing errors."	3
"This paper represents an extensive work on both theoretical and experimental sides. 
However, the number of typos and missing notation definitions show that the manuscript has not been re-read enough to check that the fact of having moved parts from the general paper to appendix does not hurt actually the linear reading. 
I advise the authors to do another reading pass to fix the typos and problems of definitions listed above. 
I vote for a weak reject.

======  AFTER REBUTTAL ======
I read carefully the other reviews and author's responses. First of all, thank you very much to the authors for their hard work during the rebuttal period and the significative changes brought to the structure of the paper which is improving its quality a lot. 
However, I tend to share the concerns of some reviewers regarding the complexity of the algorithm which is actually rather similar to the state-of-the-art baselines. Hence if the designed methods bring neither substantive improvements in terms of accuracy nor in terms of complexity, the proposed theoretical framework does not seem finally enough for an acceptance. I keep my grade ""marginally below the acceptance threshold""."	3
Overall, the authors have done extensive works to analyze the SOTA linear recommendation models but some critical analysis like the computational complexity is missing. Also, the proposed methods do not perform better than SOTA ones.	3
Overall, I feel that this paper makes incremental contribution by connecting existing recommendation methods both theoretically and empirically.	2
"
postive points:

- results on unweighted matrix norms are generalized to the *weighted* nuclear norm, connecting the latter to the weighted Frobenius norm.
- various models are categorized based on the used regularization (nuclear norm vs Frobenius norm), and it is shown that this explains the differences in prediction accuracy to a large degree.
- the proposed 2 methods / approximations work well in the experiments (but are not motivated/justified conceptually).

negative points:

- the 2 methods/approximations in section 4 are not motivated / justified theoretically (but they work well in the experiments).
- the writing and the flow of the paper could be improved considerably.
"	3
I found this work to be focused on an important problem as well as technically interesting and correct. I believe the authors could substantially improve the presentation of the work (e.g., condensing intro sections, reworking the order and going deeper into results - both quantitative presentation as well as the interpretation in the discussion). I believe the authors could do this during the camera-ready phase and therefore recommend this work be accepted with this feedback prior to publication. 	3
This paper is impressive. Its technique is novel and clear. Experiment is convincing and solid. A clear accept. 	3
The experiments are rather comprehensive and illustrative. I look forward to get some feedback from the authors.	3
"In general, the paper's contributions wrt the novelty are clear, and the proposed methods are well-defined. In addition, I found that the paper has a well-organized structure so that it is clear to understand the proposed and other practical techniques to improve training. Thus, I'm inclined to accept the paper. However, I found that several aspects of the submission can be improved. I'm inclined to improve my evaluation if the aforementioned weak points are well addressed.  

  
===== POST-REBUTTAL COMMENTS ======== 

The rebuttal had addressed most of my concerns. Consequently, I raised my score from 6 to 8, and I also raised the ""Empirical Novelty And Significance"" score from 2 to 3.

"	4
"This is a timely paper that explores acceleration methods for diffusion-based models. The authors also explored different aspects / benefits of the proposed framework which I think will benefit the community of researchers working on SGDMs, including the smoothness of loss function and a new numerical integration method. 

My recommendation is on the lower band of 8: accept, good paper. "	4
"This paper is exceptionally well put together, in terms of derivations, presentation, and experimentation. The narrative is well-written, well-motivated, and logical throughout. To the best of my abilities, the proofs are entirely correct. The only way I can see to improve the experiments is to offer the authors more compute or data.

As a result, I believe this paper to be of the highest quality and recommend that it is given particular attention at the conference."	4
"I recommend the paper be accepted.  It presents a potentially significant improvement to the training of score-based generative models which is well grounded in theory from physics and demonstrates strong empirical performance for sample generation when compared to baselines of similar complexity and compute budgets. 
"	4
The paper needs organization and the significance needs to be more clear. Also, I am skeptical about how much better it performs over the baseline given the authors only give results on their defined calibration rather than how discriminative the model remains. When optimizing for curves similar to KM, it is expected to obtain them as such. How would the other metrics change?	2
The paper overlooks several prominent works in survival analysis, including a baseline closely related to their proposed Kaplan-Meier regulariser. Additionally, the paper's focus on calibration alone in the experimental results is problematic since calibration and accuracy are equally important but orthogonal metrics.	1
Time-to-event predictions accounting for censored data is a wide-spread problem. Considering the relative simplicity, such as lack of hyperparameters, of the proposed approach, I believe it has the potential to have modest impact in several applied areas. On the other hand, the empirical support for the work is rather limited, so more evidence is likely needed before wider adoption and impact.	3
The proposed KM regularizer looks very promising and is quite intuitive. While the experiments presented so far are instructive, they could be substantially more thorough to help us understand the pros vs cons of using more KM regularization, and also what we can say about how well-calibrated different standard survival models are.	2
The first application of non-autoregressive transformers to the important problem of multilingual MT, with a novel idea (code-switched back-translation) specific to that application. Results are generally impressive, but the experiments are missing crucial model-scaling data points, and they miss the opportunity to evaluate code-switched back-translation independent from the GLAT architecture the authors favour.	3
In short, this paper explores an interesting direction: applying non-autoregressive models to multilingual translation. But the experiments suffer from unclarity, unfair comparison and over claims. The paper could be largely improved with more analysis and comparisons.	2
The paper is well-written and contributes a novel approach for multilingual translation in NAT. The design of the code-switch back-translation is reasonable for boosting the performance of multilingual translation and the empirical analysis is convincing. Experiments show that switch-GLAT is better than other NAT/multilingual baselines in limited benchmarks and has close performance compared to bilingual AT baseline.	3
I think this paper has a lot of potential, but but has technical issues that should be addressed before acceptance. The experimental setup and problem statement are the primary concerns for me. What they measure seems strong, but it is the connection between those measurements and the stated goals of the paper that are the limiting factor. 	3
The paper shows competitive results compared to strong baselines and a promising direction for semantic preserving out-of-distribution detection. However, I think some suggestions on the ablation study and some comments on the scalability on large datasets should be addressed to strengthen the paper.	2
The idea of using temporal structure to perform out-of-distribution detection is an interesting problem with many important applications. While the authors provide some experimental results that suggest their approach outperforms other methods, there was little justification for the different choices made and no analysis of how these decisions impact their model's performance. In addition, the authors did not discuss or compare other approaches for estimating spatiotemporal uncertainty from sequential data and thus it is difficulty to assess the novelty and contributions of the work. 	2
"The strengths of this paper are that is addresses an important problem, it provides thorough experiments and it is well organized and easy to read.

Unfortunately, the paper suffers from a number of major weaknesses, in that the title and intro paragraphs are misleading, there are issues with terminology and there are missing references to the literature on filtering and uncertainty estimation. Moreover, the results do not show if the differences to baselines are significant.

Given this, I recommend to not accept this paper, but I encourage the authors to address the weaknesses and resubmit to a later venue.

EDIT AFTER REBUTTAL: The authors have addressed my comments to a large extent, and the paper now looks much nicer - although, I am still not convinced about the technical novelty compared to the vast literature on filtering, outlier detection, uncertainty estimation and calibration. Hence, I raise my grade to 5."	2
"- Besides the generalist teacher, what is the other contribution?

- Please discuss the generalization ability of the proposed method.

- Discuss more about the experiments. "	2
I lean towards weakly rejecting the paper, based on the limited applicability/experiments mentioned above. If there is a general claim to be made, I would like to see more evidence and other domains tested, beyond image classification. As it stands, I do not find the technical contribution sufficient for acceptance. 	2
The motivation of this paper (life-long meta-learning) is attractive, but the method and experiments fail to well support it. I tend to reject this paper, and recommend the authors refine it and submit it to the next conference. 	2
The studied problem is interesting and important and the whole paper is easy to follow, while the technical contribution is limited and the experiments are not very strong to support the advantage of the proposed method.	2
Overall I find that the proposed method novel and principled. As paper has demonstrated the proposed method could have great impact multiple application domains. As mentioned in the review, some information could be verified through discussion, but I am leaning towards acceptance for my initial rating.	3
I am overall in favor of accepting this paper. The proposed formulation and solution are well presented and the application to different problems is interesting.	3
"Preliminary recommendation: unsure, because of the missing references to existing methods that achieve something comparable.

Question to the authors:
- Can you comment on differences/advantages of their approach compared to existing ones ?
- Can you discuss the necessity/feasibility of learning a ""full classifier"" ?

**Edit after authors' response: I am satisfied with the precisions added by the authors to the manuscript and I strongly recommend it for acceptance.**"	3
The authors achieve some positive results from cotraining of groups image classification models designed to focus on shape but not texture or vice versa. However, their ensembling results are acheived using very primitive ensembling which is not state of the art. They also overstate the odds that spurious correlations are unlikely to exist in unlabelled data.	2
The paper is clearly below par, can be rejected.	1
"This paper has a number of strengths, that combined makes me recommend the paper for acceptance: Of high relevance, well written and correct, proposes a valuable framework, and contains sound and well designed experiments.
One weakness can be pointed out, not however any cause for not accepting this paper in my opinion: The experiments are performed on old datasets.

In summary, I propose acceptance for this paper and believe it will be of interest to a large portion of the ICLR audience.
"	3
"The current contribution is an exploratory work, combining several state-of-the-art methods (for instance, self-training and co-training are used in the experiments). 
There is a lack of technical novelty. 
"	1
"The idea of the proposed method is interesting and might be effective.
However, the idea itself of sharing the parameter is not very innovative and rather incremental.
Experimental settings are ambiguous and seems to use very weak settings.
"	2
Potentially a great paper, but if so it deserves to be much better explained.	3
"The proposed modification to the Transformer architecture is novel and I believe would be interesting for the community but the methodology and motivation could be explained more clearly and provided with more context, including more details on the hyperparameter selection and on how the DictFormer is trained. The experimental results would be even more convincing if confidence intervals are provided.

#### Updates during paper discussion
Based on the author's responses to the reviewers' questions and updates to the manuscript (including clarifying some of their methodology and statements and including confidence intervals in the results section), I've decided to increase my score. "	4
"I recommend acceptance given that the paper clearly describes related work, and proposed model. The authors proposed an efficient transformer model that can be trained with less resources. The authors perform an  evaluation of the proposed model with different language downstream tasks, and the model outperforms related work on machine translation and language modelling.
"	3
I think this paper raises a good question about the interaction between the frequency and quality (i.e., quantization) of model updates in FL. Still, neither the intuitions nor the experiment results convince the reader much about the proposed approach. I'm marginally recommending accepting the paper mainly based on the assumptions that the problem statement is novel and the theoretical methodology is sound to my current understanding of the paper.   	3
"In conclusion, I think that authors of this work made a good job. The idea of adaptation of quantization and frequency of communications is very nice; however the broadcasting procedure seems to be very time-consuming.
For me the Experimental section was impossible to follow, but I think that minor modifications of the Figures can resolve this problem.

All in all, I think that after some changes this paper can be accepted and the presented algorithm can be used in real-world FL applications."	3
Overall, primarily based on the experiments, I recommend the paper be accepted. However, I cannot be as confident of the paper's contribution as I would like to be because of a lack of comparison with a crucial part of the literature (B.1, B.2).	2
"I prefer to weak reject ( marginally below the acceptance threshold) the paper.
- The clarity is an issue that makes it a little bit hard to follow.
- The empirical evaluation is weak so that the transferability looks limited. The scalability and generalizability should be discussed in the paper as well."	2
This paper tackles an important problem of reusable skill (option) extraction from a large multi-task dataset. The proposed method is simple and works well in the discrete task, CRAFT. However, the training process is not intuitive (soft selection during pre-training and hard selection during RL). The experimental results on the continuous domain, DIAL, are noisy and show only marginal improvement over prior work. Moreover, comparisons to continuous latent skill approaches are required to show the technical contribution of the proposed method.	3
Submission proposes a specific architecture for option learning. An aspect that in other work might fall under implementation details. The work has some interesting experiments on combining options from multiple experiments, but the evaluation and connection to previous work is very limited. 	1
"The proposed OCN model is an interesting extension to the recent OMPN. However, my main issue is that the demonstration dataset generation process has drastically reduced the full sequence decomposition task to an overly simplistic case. The writing can be improved significantly as suggested above to improve clarity of the proposed ideas. Further, some closely related prior work [2] on modelling temporal hierarchy in RNNs using similar ideas have not been compared/discussed and many others not referenced [1, 3, 4, 5].
"	2
* Since the baseline for this paper is Deep Inversion lack and there are many quantitative experiments in that paper, the lack of quantitative results in this paper is not justifiable.	2
"Overall, this work has an interesting motivation (alleviating the need for extensive tuning of regularizers for class inversion), but I am not certain if the benefits of Plug-In Inversion, as described by the authors, are fully supported by the experimental evidence provided. As such, given also the fact that the individual components of the method do not seem novel by themselves, I marginally lean towards rejecting this paper, pending extra discussion during the rebuttal process.

**Update after rebuttal**: See response to the authors' comments below."	3
"Clearly state your recommendation with 1-2 reasons

I rate this paper as a weak accept for the following reasons.

The reason to accept is: They provide a class inversion method that produces interpretable visualizations for recent architectures (Vision Transformers + MLPs) that existing methods do not work for.  This could accelerate the development of better classifiers by helping researchers interpret and fix their behavior.

The reason this isn't rated higher is: The baseline comparison was inadequate.  The authors should compare their visualization method on Transformers/MLPs to the best existing approach that exists before this work.  Perhaps that approach is a plain inversion method.  Perhaps there are methods that can be borrowed from the methods of targeted adversarial examples.
"	2
The model aims for a model-agnostic and hyperparameter-robust method to investigate vision models by model inversion. I highly appreciate this goal. Meanwhile, the current manuscript has several weaknesses. Therefore, I recommend this paper be marginally below the threshold.	3
Overall the paper suggests a method that makes an improvement over an existing state-of-the-art, but the efficacy of their enhancements are not fully tested. In principal adding textual data that is semantically less ambiguous than image data helps. 	2
The authors use additional information in the training labels to improve few-shot image classification. The solution is a combination of different existing tricks. Without proper ablation experiments, it is difficult to infer generalisable knowledge from the results. 	2
"Overall, I recommend the rejection of the paper. My major concern is that the main contribution of the paper, i.e., separately learning the visual and semantic features, is not very intuitive and well justified. Furthermore, the proposed modality combination approaches are well-known methods.

----After rebuttal----

I would like to thank the authors for their effort to answer my questions. Even though the technical novelty of the paper is limited, as also supported by the new experiments added, the responses to my questions are satisfying. Thus, I increase my score from 3 to 5."	2
"I have some concerns described in Main Review. Therefore, I recommended ""3: reject, not good enough"".

----After rebuttal----

Thanks for the response. I still have some doubts about the technical novelty of the proposed method, but
given the other reviews and response, and the strength of the experimental results, I increase my score from 3 to 5."	2
"The paper presents a nice idea for an interesting problem, and a
strength of the paper is the level of complxity used to solve and
describe the problem. For complex combinations of tasks (where tasks
depend on each other), the approach may not be sophisticated enough as
the sole solution to the problem. Experiments and results are OK, but
could be more systematic.
"	3
I am not expert in the area of HRL, and hence I am only trying to provide an educated guess. I am recommending weak acceptance given the problem is interesting and the approach seems to make sense, along with an interesting set of experiments.  	3
From the position of a non-expert of the domain, I found the paper well-written and instructive, and was convinced by the idea of matching the transition policy and target policy distributions, but cannot really judge the novelty.	3
"The authors introduced an energy consumption attack on Neural ODE models, which is a novel application of energy consumption attacks. This is a novel application idea, but the idea is not technically novel. Furthermore, the paper is poorly organized, and experimental analysis can be improved. For example, the authors can add the analysis with simple defense strategies such as thresholding the number of ODEs iterations or out-of-distribution detection of adversarial examples generated by universal energy attack.
"	2
The paper makes a clearly identifiable contribution to the literature by demonstrating energy attacks on neural ODEs. Hence, the reviewer leans towards accepting this paper. A more thorough analysis would have made the paper stronger and more influential. 	3
I will update this part after the review phase...	3
"In general, I feel this paper has several limitations on the technique. Some technical details are not correct. The paper presentation is not clear. Some experiment settings are not reasonable. 

I suggest reject this paper."	2
Nice article. But requires a more thorough comparison with true wasserstein.	2
Overall, I found the problems studied in this paper interesting and important, the proposed method reasonable, theoretically sound, and well supported by experiments and implementation details.	3
The proposed formulation is appealing and I like the idea. However, in my opinion, some theoretical and experimental aspects are lacking, and that prevent a publication at the moment (see the different points above). There is a big discussion regarding where the method can applied and it could be replaced by a discussion on theoretical aspects which are not mentionned. Regarding experiments, I encourage the authors to focus on two experiments (Generative modelling and domaine adaptation) and to make extensive experiments with recent baselines. 	3
"The paper is marginally innovative. The method it proposed is intuitive and clear. However, it is unclear if the proposed methods is applicable to any form of learning-to-rank problems or it is tied to certain learning-to-rank models, such as MF. It would be very useful to clarify the how generic the proposed sampling approach is applicable explicitly. For the learn-2-rank models mentioned in the paper,  I would recommend it for a poster paper, the writing can also be improved by directly citing the models used in the experiments instead of listing them in appendix. 
"	2
"Overall, I like the observations made and the proposed sampling idea, but due to issues discussed above I found the contribution incremental and leaning towards rejection.
"	2
As mentioned in the weakness, the limitations on novelty, methodology design, and conducted experiments prohibit its acceptance.	2
This work gives sufficient theoretical analysis about the class-imbalance problem and provides a simple but effective debiasing method to advance the optimization performance for item recommendation tasks. Sufficient experiments are provided. Overall, it's an interesting, solid and well-organized paper studying an important problem.	3
"The author presented CLSVAE to handle systematic outlier. The problem is well motivated and clearly presented, very easy to follow. The formulation is very clear. The performance of outlier removal is satisfactory on simple datasets.

The idea and formulation is a little incremental and similar to RVAE [Eduardo et.al.] with the main difference could be the semi-supervised mixture model.

As mentioned above, I wish the author could answer the two following questions:
1. How correlated is the denoising quality and the downstream task, e.g., classification? could it be formulated together?
2. How is the model related to blind-source separation and ICA? Could the author gave some insight?"	2
The paper presents and interested approach that is well formulated. However, the main motivations and assumptions are not well comprehensible, theoretical and practical implementation aspects are mixed, and the presentation is lacking clarity in some places.	2
I am not an expert in this field. I tend to weak accept this paper, but I will also see if other reviewers have serious concerns.	2
I feel the theoretical results are strong and the authors provide sufficiant empirical results to validate their theoretical findings.	3
Based on my comments above, I cannot recommend its acceptance. 	1
Interesting motivation and direction however the paper is not well written and there is no discussion for the results. Also, the numerical results section can be improved greatly.	2
The author investigates an interesting problem, logical rule learning. It also proposes an interesting idea to decompose the long path into small pieces to recurrently learn rules. Although the idea is interesting, the model and the experiments are less convincing as pointed out in the weaknesses. It will be better for the author to clearly illustrate the reasoning why they choose the  RL model to learn rules and how they handle uncertainty during the deduction. They also need to include more baselines and larger datasets to make the experiments more convincing.	3
An interesting approach with good results for a limited task setup.	3
"This paper presents a novel algorithm to solve program induction formulated as a
relational pathfinding problem. The proposed approach outperforms existing rule
induction methods in several benchmark datasets. It has missed several important
references and needs further clarification, so I would like to recommend to
accept if the authors can fix those problems."	3
This paper proposes a simple defense against backdoor attacks on pre-trained language models. Aside from the large room to improve writing, this paper has many other issues. While the simplicity is fine, this paper failed to show the exact effect of the defense as the rare tokens would be still updated only when it appears in the training data. Also, the evaluation was done for similar tasks where words can be directly grouped into each class. Moreover, the exact poisoning procedure was not explained. Thus, it is not possible to confirm the benefit of the proposed approach.	2
Given the points listed, I give the current rating here. It would be helpful if the authors can address my concerns.	3
This paper targets an under-explored fundamental challenge in pre-training and adaptation trends. This is a brave and valuable step in the security research of pre-trained models. I enjoy reading this paper, and this is the first time I have seen a reasonable solution to the inheritance of the backdoor phenomenon. Although some problems need to be corrected, I believe authors should be able to take them in hand.	3
See above.	2
"1) From the theoretical point of view, multiple analyses are missing. The only results are for GDA and bilinear cases. The authors didn't provide any analysis of their framework for the convex-concave case.

2) Authors didn't provide any result for the Nonconvex-Nonconcave case( convergence to $\epsilon$ stationary point). I think the result in [1] can be used to provide guarantees in this case. This is necessary to show the effectiveness of the method for GAN-related applications(e.g., the one in section 6.2).

3)Author motivates the subject from a theoretical and practical point of view. 

4)In the introduction, the author reviewed the previous works carefully.

5)The authors successfully provide simulation results for their algorithm.

6)The paper is well-organized and well-written.

7)the code is included for the bilinear case but I couldn't find the code for the image generation experiment.


[1]Wei, Fuchao, Chenglong Bao, and Yang Liu. ""Stochastic Anderson Mixing for Nonconvex Stochastic Optimization."" arXiv preprint arXiv:2110.01543 (2021)."	3
"As a summary, I think this submission is well written. I think some more work could be done to compare the obtained rates with the ones already present in the literature but it should be relatively easy to implement.  Also, I have some concerns regarding the theory and the comparison with respect to the baselines.

I will increase or decrease my score depending on how the authors address my comments. (for instance, if everything is clarified I will increase the technical and empirical novelty score as well as the final score) 

For now, I recommend accepting this paper.
"	3
Because the original idea of the paper was briefly introduced in Azizian et al. (2020), but more importantly, because the theoretical results presented in this paper are already covered in Bollapragada (2018), I am not convinced the paper is novel enough to be published. Moreover, I have a concern about a detail in the proof of Theorem 2.	1
Based on my above comments, I currently would not vote for accept because of the major concern above.	2
I think this paper makes a certain contribution in finding out a simple condition for the global convergence analysis in the mean-field regime. However, the quality of the manuscript could be improved. 	3
The paper makes a novel contribution for convergence of NNs in the mean field regime with a clean and simple proof.  	3
The studied setting is not too much different from previous work of non-convex optimization in the lazy regime. Therefore, I cannot recommend accepting this paper.	1
"I believe the contributions in this paper are solid, and the take-home message is of interest to the ICLR community. I will consider adjusting my score if the authors can address some of the aforementioned concerns.  

----------------------Post-rebuttal update----------------------  
Thank you for the detailed reply, which addressed some of my concerns. In my opinion this submission is above the acceptance bar (I think 7 would be more appropriate for my evaluation, but that is not an option...)"	3
It is not clear why the main message of the paper around the three stages of learning is interesting for the community. The paper does study several cases including random features. Nice experiments are included supporting the main message of the paper. 	1
Although this is not a breakthrough paper and the technical contributions is not very high, I believe this is another nice paper showing that a lot of DL phenomena are can be explained by Kernels. The analysis also offers some basic intuition of neural networks' learning/training dynamics. 	3
"I see the contribution of this paper in providing a theoretically sound
treatment of the observations presented by Nakkiran et al. (2020) in a
controlled setting. I think this paper would be a valuable contribution to ICLR,
and recommend acceptance.

*Edit 14.11.* After the first round of responses, and after reading the other reviews, I have increased my confidence score from 4 to 5."	4
"The authors evaluate deep convolutional networks through the lens of hierarchical locality. The paper evaluates the performance of deep neural networks by performing extensive experiments that show the tradeoff between space versus frequency.  The eigenspace restructuring theorem has a well-chosen name.  However, eigenspace restructuring is a well-known concept and the novelty of the theorem is limited. 

Clarity of the writing and its overall organization needs improvement.  The word frequency needs to be better defined.  It is not clear if there are any novel insights.  


----
Update:  My original score was too generous and it was in anticipation that any mathematical inconsistencies or ambiguities would be addressed.  


"	2
The current score is mainly due to the issues related to clarity.	3
Although the paper looks very interesting, I found it very hard to follow. I think there is substantial room for making the paper more accessible (see the detailed comments above). But also, my lack of familiarity with the tools used in the paper (i.e., graph computations) may have made it difficult to read for me. I could not follow how the authors arrived at the LRLF-SRHF- and similar interpretations. Therefore, I can not recommend acceptance. 	3
The paper provides solid foundations to understand the mechanism behind (deep) neural networks.	2
"1. This paper considered the sample complexity of learning multi-player Markov games with relaxed equilibrium notions (CE and CCE) and special structure (Markov potential games). The results in this paper are quite interesting. However, starting from Section 4, the presentation is a bit hard to follow. First, the algorithm for CCE, the first key equilibrium of MG, is relegated to the appendix. I went over the proof quickly and it appears a technique from [Tian et al. 2021] has been adopted to improve an H factor in [Bai et al. 2020]. However, [Bai et al. 2020] considered NE and it remains unclear what difference the properties of CCE make in this paper to allow the authors to achieve the claimed sample complexity.

2. The description of the CE algorithm is also unclear. It seems the FTRL technique is the key to the design of this algorithm. Is this technique brand new or adapted from some earlier work? Also, it might be better to define some jargons (e.g., sub-expert) for readers to fully understand the meaning of the algorithm. Also, it would be better to provide more intuition behind the algorithm design for CE. The authors mentioned that they adopted and modified the policy of [Bai et al. 2020]. It would be better if the authors could be more specific on what modifications are needed here.

3. Although the paper is mostly a theory paper, it would be nice if the authors could provide some experimental results to verify the sample complexity of the proposed algorithms."	3
I feel the paper has quite nice technical contributions. The results are very interesting too. 	4
Overall, the paper is well written and the theorectical results are worth publishing. 	3
"Overall, I think it is interesting paper with solid contribution. I vote for acceptance.


------------------------------------------------------
Post rebuttal:

The author revised their paper and addressed some minor issues. My positive evaluation for the paper remains and I vote for acceptance."	2
Overall, I feel like the theoretical discussion is not significant enough, and the associated experiments are also weak (in the context of theoretical papers)	2
"Overall, my vote for the paper is a (weak) reject. I think the framework of the paper is original and the ideas are intuitive with several interesting results and implications. On the other hand, the statements of some of the results are very weak (to an extent that they are not meaningful). The writing of the main text, as well as the proofs, are non-rigorous (which was partially an intention of the authors, at least for the main text, but might have affected the paper's mathematical quality).

==== Update after response and revisions

The revision addresses my main concern about the weakness of the results of Theorem 3 and 5. On the other hand, I'm still of the opinions that the heavy uses of asymptotic representations and the choices of present thee results of the main text in non-rigorous manner have affected the paper's mathematical quality. A few other concerns are left unaddressed. I thus raise my score from 5 to 6"	3
"This paper develops a mathematical framework for consistency regularization. Several theoretical results and empirical results are provided. Overall, I like the general idea of the paper, however, there are several major concerns with the way it is executed. In particular, the major issue is that the theory and experiments are disjoint and do not support each other. (1) The theory is too tied to linear models and it is unclear how it could be extended beyond linear models. (2) The theory is only applicable to linear models with $\lambda \to \infty$. This regime is not even viable in the empirical world when we deal with neural models (as also evidenced in the experiments of the paper). (3) While the experiments are interesting, they are not conclusive on their own and lack important details. For example the regularizer function $\rho$ is not specified. Also, baselines for these benchmarks are not compared against. While there are many things that I like about the paper, it does not tell a coherent story, and especially one that would benefit the ICLR audience, and hence I recommend the paper to be rejected in its current form. I hope the authors can clarify some of the explicit comments/questions that I have raised in my review during the rebuttal.


====== update after author response ======

I would like to thank the authors for their extensive responses to my original comments as well as the follow-up comments, and also for their revisions to the paper which has improved the paper significantly. Thus, I am raising my score from 3 to 5. While some of my previous concerns are addressed, there are many remaining concerns that would require another careful/extensive revision and would require another round of review, which is why I still don't think the paper is ready to be accepted. I would like to emphasize that I like the general formulation of the problem, and the general positioning of the paper and I think the paper would be a nice contribution to the literature once the problems (especially with mathematical exposition) are fixed. Here are some explicit pointers for the authors:

- **Imprecise and inscrutable mathematical exposition:** There are lots of imprecise statements (which also **Reviewer s3Ue** complained about) still in the revised paper. For example, what does $\gg$ mean in Assumption 2 (page 15)? At the same time, the math is not followable. I still could not follow some of the proofs.


- **Hard to gain intuition/takeaways from theoretical results:** While the authors present several results, it is hard to understand takeaways from the developed theory. While they have addressed many concerns, several still remain. For example, RHS of (the revised) Thm 5 does not depend on any of the data augmentation parameters, such as $d_{\text{aug}}$ and $\alpha$. What is the takeaway from this theorem given that the same bound also applies to ERM and DA-ERM?

- **gap between theory and practice not discussed:** Although I explicitly gave feedback to address the gap between the theory and practice, it is not well discussed yet. For example, the theory is developed for $\lambda \to \infty,$ while there is a practical sweet spot for $\lambda$ in the experiments based on the optimization challenges. While I think that this gap should not be the reason to not accept the paper, it warrants a discussion around the results and their practical applicability, which is currently missing from the paper.

I hope the authors would find these comments useful in revising their paper for a future submission."	3
The assumptions under which the paper operates are quite restrictive and do not see that they hold in practice, and thus apply to widely used data augmentation schemes along with deep learning models. The crux of the theoretical contributions is restricted to linear models, which further limits their scope. The paper does not also cover well the prior work on empirical risk minimization problems, especially the line of work on vicinal risk minimization. Experiments show rather incremental improvement (1% relative) and it is quite strange that there is no substantially larger effect of augmentation when the replication factor goes from 3 to 15.	2
This paper is well motivated, well written, and the results are impressive. But I don’t fully understand some of the derivations (see main review). So at this point, I can only suggest a weak accept with low confidence. If my concerns were addressed, I’m happy to raise my score.	4
"This paper studies the representation learning in linear MDP and significantly improves the previous sample complexity. However, as mentioned above, I'm concerned with the following aspects:
- Sample complexity of the algorithm and fairness comparing two algorithms in different settings (time-inhomogeneous v.s. time-homogeneous)
- Assumption 2 sounds too restrictive for me.
- Definition 3, though appeared in the literature, I wonder if it can be solved efficiently

Based on the concerns above, I lean towards rejecting this paper but I'm open to further discussions.

-- post rebuttal
I've updated my score since the authors clearly addressed my concerns."	3
"The paper is well written,
Address an important problem.
However, there are pieces of this work that are still missing to complete the work. 
"	2
The authors don’t properly address the difference between the newly proposed method and Flambe, and I feel the proposed method, although delicate, has so many restrictions that the authors don’t properly address. Meanwhile, the proof is not well-polished. I hope the authors can refine the writing during the rebuttal and make the intuition much more clear, compared with the current highlight of analysis in Section 4.2, that implicitly use several convenient property of the infinite horizon setting.	3
The paper introduces a novel framework and provides a detailed analysis, but the readability issues are shading it. 	3
The paper is quite rich and insightful from a theoretical perspective, but currently, it is unclear if and how it can be implemented in practice and if it can work well. The authors should prepare a much stronger experimental section and clearly discuss the limitations of their approach. I am not negative about the paper, but I believe that this part is crucial to recommend acceptance. I would be willing to discuss my score after that.	4
"To summarize, the paper studies an important problem, and proposes an architecture with strong theoretical justifications. In practice, I'm not convinced that the new architecture has any real advantages over existing, simpler architectures.

I (weakly) support the acceptance of the paper."	3
"Pro:
- A simple GNN that is well explained
- Good writing

Con:
- Insufficient experimental evaluation
- Quality of mathematical statements"	3
"I think this is a good paper that provides a novel GNN architecture that on the one hand is more expressive than standard MPNN and on the other doesn’t require k-th order tensors to compute. The theoretical aspect of the paper is very thorough and gives insights both into the expressive power of RGP and on lower bounds for the task of counting subgraphs. On the experimental side, the current experiments presented in the paper provide evidence of the expressive power of RGP.
My two concerns are whether RGP is really more computationally efficient than k-WL, and whether it would outperform other MPNNs on standard benchmarks from the GNN literature. I would be happy to read the author’s response and consider raising my score accordingly.
"	3
"I vote for acceptance of the paper. It provides a new perspective of the ""rich regime"" of neural network training. The claims are backed by analysis on (deep) linear networks and align well with empirical evaluation. "	3
This paper makes some interesting and novel contributions towards understanding the learning dynamics of neural networks through the lens of NTK evolvement. On the other hand, the setting where a complete answer can be provided is restricted, and the analysis in the paper is not rigorous.	4
Overall the paper is interesting but the relation to implicit bias results is not discussed. I will be happy to adjust my score if the authors can address my questions, and also compare the results to known implicit bias results.	3
marginally accept.	2
I appreciate the author's effort in constructing such a complicated model to solve the challenging tasks, but the paper's writing has big flaws. Without adding enough details for a better evaluation, I do not recommend accepting this paper in its current form. I would be happy to increase my score if its writing can be improved.	3
Overall, I thought this paper was well written and presents a significant contribution in a novel area: modeling dynamics for text-based games. And the proposed extension to OO POMDPs will be generally useful for anyone working on text-based games. The given ablation studies and comparison to model-free baselines provides believable evidence for the effectiveness of their model-based learning approach. And while this work focused mainly on games, it's clear that their results could have useful applications in the field of conversational agents, which is growing in importance. I had some questions about the technical details of their method, especially the formulation of their objective. But these could probably be well answered in follow up discussion or in an appendix.	3
The paper is well-written and the results are strong. I have a few comments on the formulation and presentation of the paper. I also suggested an ablation study.	3
This is a good work on a timely subject. The contribution is not groundbreaking but should be significant enough to warrant acceptance.	3
"This paper tackles a new generalization problem for dynamics forecasting and proposes a model supported by experimental results. However, this model can only be applied to problems with relevant weak supervision which may not always be available in practise. Moreover, the definition of relevant parameters is unclear and the robustness of the model to the choice of these parameters is not measured which may restrict its application to other datasets. There are also unclarities on the ability of the model to adapt to changing boundary conditions with AdaPad, some ablation studies are missing and I have concerns on the theoretical analysis which brings limited value to the paper. For this reason, I am giving this paper a weak reject.

--- Post-Rebuttal comments ---
I thank the authors for their response. After studying it, the theoretical results still have some major issues and feel disconnected from the model. In particular, key assumptions are not enforced in the model (e.g. lipschitz continuity) and the generalization error of the model in Th3.3 is uncontrolled as the upper-bound is not minimized by the model (the Wasserstein distance between domains is fixed and is high in all generality). Its use for the model is thus not very convincing. On practical aspects, the capability of handling boundary conditions should be better justified and evaluated. For this reason, I keep my score unchanged and recommend rejecting this paper."	2
This is a good paper that I'd like to see accepted for its combination of theoretical results, empirical results and methodological novelty.	3
Overall, this is very interesting and useful work. The problem is well-motivated, and the approach and experiments are carefully designed and generally convincing. If the concerns about the theory are addressed, I would be happy to increase my score. Adding the additional info and experiments requested could increase it further, and make this a particularly strong paper.	3
Overall I think the paper is technically strong and authors have shown effectiveness of their method. Although the scope may be somehow limited.	3
"Overall, I think this paper makes non-trivial contributions in the search space design for HKGs with extensive experiments, and thus may be interesting to both KGs and NAS community.

===after rebuttal===
I have read the rebuttal and my score remains positive."	3
I feel the paper is not currently clear in the contribution and there are some disjoint between the method section and the experiment section. Also I am not entirely convinced of the necessity of a new NAS framework specifically designed for GNNs on HKGs. It seems like a problem of limited scope, and many new configurations also apply to GNNs in other tasks.	2
"This paper introduces a new variant of meta-reinforcement learning by leveraging transformer, which can capture long sequence and context dependence for fast adaption in new tasks. Empirically, the proposed method shows some improvements over the existing ones. However, a few major concerns are as follows.
(1) The authors claim in the abstract that transformer can capture long sequence and context dependence for fast adaption, but why do we set N=2 for all experiments? Is the transformer a good fit here? The experiment in appendix D.2 show it cannot get better result with long sequence. 
(2) In Fig. 8, its performance is not comparable to PEARL. Is it possible to try different N and compare with PEARL?

Overall, it is a good paper, but not good enough for publication. More convincing experiments needed to show the advantage of transformer here."	3
"The paper is proposing an improvement of the transformer architecture, introducing a mechanism of memory reinstatement for meta-learning in sequential decision making.
The problem addressed in the paper is important and up-to-date.
Unfortunately, several state-of-the-art approaches are missing in the evaluation part.
Moreover, while several preliminary results are encouraging, the results depicted in ML45 remain limited compared to the proposed baseline approaches.
"	2
"My main concerns about this paper are all related to clarity. It would help to have the full algorithm written precisely in one consolidated place (even if it's high-level pseudocode), rather than spread throughout the paper in informal descriptions with undefined terms.

I would be happy to revisit this if the authors would like to clear things up during the discussion period."	2
"While that paper studies an important direction in RL of adopting new network architectures, such as transformers, the paper, unfortunately, brings very little novelty or insight.  The proposed method largely builds off of the existing work (such as RL^2) and essentially proposes to replace RNNs with transformers. The empirical study is also questionable and,  in my opinion, misleading. Moreover, the experimental methodology is a suspect as it doesn't provide an extensive ablation or support all the claims.

Taking this into account I suggest to reject the paper, as it in its current form doesn't meet the acceptance bar. "	1
While this paper has the merit of the originality of the approach, it is hard for me to provide a string feedback due to the unfamiliarity with this Koopman theory.	3
This paper provides an interesting viewpoint in understanding the success of pruning based compression methods. The method is easy to implement and is very practical  (at least for small-scale networks).	3
The paper clearly presents interesting results, the experiments seem sound. However, the novelty/significance is not overwhelming and hence the borderline score.	2
"To summarize, I feel that the paper can be a meaningful contribution to the field. However, the ""operator theoretic perspective"" of this paper -- to my feeling as a non-expert -- is somehow overstated. I think that Koopman pruning is an interesting and useful pruning heuristic on its own, but I am not sure if it can genuinely explain magnitude-based pruning either before or after training convergence in a realistic setting, where network parameters cannot be modeled by a linear dynamical system. "	2
"I felt that whilst this was a reasonably well written paper highlighting the interesting concept of PCs, which I was not previously aware of, the methodological contribution wasn't clear enough and that the experiments were presented in a way which was potentially misleading.

EDIT: Score changed to 6, see comment below."	3
The idea of PC for lossless compression is new and interesting. However, following the main contribution of PC and the main idea of PC is somewhat difficult. Giving simple examples on PC for clarification is recommended.	3
The manuscript provides a class of tractable models for lossless data compression. These models are well defined and algorithms are shown. Parts of the work need clarifying for novelty and significance. However, experiment results with faster and competitive compression rates are very convincing.	2
Overall the work is interesting and timely and I am leaning towards an accept if the authors can convincingly address the points above.	3
I think the paper is lower than the acceptance threshold due to the lack of technical significance and question in empirical evaluation, as detailed above. 	2
Overall, the contributions of the paper are unclear and the novelty seems to be limited. In addition, the comparison and ablation study in the experiment is not totally convincing. I would like to suggest a rejection. 	2
"My concerns are mainly from three aspects:
- The complexity analysis is vague while it is one of the main contributions of this work. 
- No adaptive attacker is evaluated.
- The reproducibility of empirical evaluation is a little weak.
"	3
"Overall, I vote for rejection. The idea is novel and interesting. However, I think this work lack sufficient theoretical and empirical analyses.

-----

The authors have carefully addressed my concern. My score is thus raised to 6."	3
All in all the paper is a nice read and provides interesting insights. The paper is sound and the proofs are good to follow. I consider it a high quality submission that could, however, still be improved a bit in its presentation.	3
I tend towards accept due to the importance of making progress on what I see an underserved area.  However, I have some questions about the practical consequences of the work and whether other definitions of expressivity might have more potential. 	3
"The paper targets a fundamental problem regarding the separability of equivariant representations in a pretty general setup. The theoretical results are interesting and new (as far as I know), but the relevance to current research challenges is not clear. If this issue is addressed in a revision, I would be willing to upgrade my score.

after rebuttal:
I appreciate the authors' thoughtful response. I will keep my score at 6 at this point."	3
Although this paper has a good presentation, for lacking theoretical guarantee and insufficient experiments, I regret to give a weak rejection.	3
The approach to analyzing MHSA taken by the authors is interesting and worth digging deeper, especially due to the clear empirical advantage of the proposed modification. However, the paper contains several issues in claims, theory, and connections between them. On the side of delivery, the paper has good potential, but in its current state, it is not well-polished and requires a major improvement to facilitate better understanding.	3
"The paper address an interesting and relevant problem, namely generalisation of the MHSA mechanism, and it presents interesting, if somewhat terse and preliminary, experimental results. However, while the view as tensor decomposition is promising, the seemingly ad-hoc commitment to Tucker without a clear rationale or systematic study is unsatisfactory and leaves an unfinished impression. The paper does put forward a novel idea and has high potential, but it would require a major overhaul to fulfil that potential and turn it into a possibly really strong and useful contribution, rather than a fleeting quick-shot.

I find it difficult to give a recommendation for this paper: If one follows the conventional criteria, there is novelty and the potential to lead to interesting follow-up, so one might argue for acceptance, even though there isn't a strong case for future impact. On the other hand, it feels a bit premature and a bit more work could make it a lot better and more significant."	3
The paper introduces a general Self-attention, which is interesting and insightful. However, a general model that costs more (time or space) but slightly performs better is not that attractive.	3
The paper presents an optimizer by combining several ideas from previous papers, and shows small improvements on some datasets.	2
Though there is certain novelty in this work, the theoretical contributions seem to be flawed, and the numerical results are not convincing, as explained in the main review. Thus, my current recommendation for this work is rejection.	2
The paper could be a good contribution to the literature (as well as for practice), however I have several concerns currently holding me back, which I hope the authors can address in the rebuttal (they are all related to the parameter-wise update regime enforced by the algorithm; please see my comments in the main review section)	2
"This paper is overall well-written and contains a comprehensive and
significant set of numerical experiments. A few gaps in the presentation
of the theoretical contributions, however, prevent it from receiving
a stronger score."	2
This is an interesting paper with results that do an impressive job of supporting the paper's claims.	3
Overall, this paper proposes an easy but effective method for weakly-supervised contrastive learning with auxiliary information. The authors provide clear mathematical proof and good results with adequate experiments. However, comparisons with related work should be included to make to final results more convincing.	3
"This paper proposes a structure-aware constructive loss with theoretical justification for representation learning. The theoretical analysis is very interesting and insightful. However, there are some issues in terms of novelty, clarity in the technical details, rationality on the terminology as well as the soundness in the experiments.     
"	2
Overall, the experimental section of the paper was clear and included a good variety of image classification datasets. However, given the lack of discussion/comparison to relevant existing works and limited performance of the proposed method, the contribution of the proposed method is not clear to me at this point.	2
The motivation is persuading and the method is novel. The author also demonstrate the effectiveness of their method on large scale datasets with a number of compression techniques. Although the writing can be improved, I think this is a good paper.	3
The paper does a reasonable job of presenting its sparse encoding scheme, and the analyses and evaluation seem reasonable. The paper feels a bit isolated and theoretical, which is odd given its stated goals. The method doesn't seem bad, but the reader is left with a lot of open questions about putting it into practice.	2
"Novel idea of significance and potential practical value.  Presentation could use further clarity (e.g. elaboration of basic concepts such as fixed-to-fixed) since the readership of this conference is probably not experts in communications.  
"	4
"Given the issues I raised in the main review I will not recommend this paper for publication for now. But I would be happy to change my evaluation if the authors can improve the clarity of the paper and address my concerns.

After rebuttal:
I raised my evaluation from 5 to 6, given the improvement included in the new revision. However, there are still two main issues I want to point out. First, the authors built their work upon Kwon20 and Ahn19, but they only cite those two papers in the latter part of their manuscript.  And related works section is only included as an appendix section. This layout is unusual, and it made their manuscript hard to comprehend for an audience that is not familiar with the specific topic. Second, despite my repeated request for clarification on the computing complexity, the authors still only gave a vague description of the issue. I hope the authors can fix those two issues in their camera-ready if the paper is accepted.  "	3
I like the setting and the method, but I still lean towards rejecting the paper, because the experiments are not convincing enough.	3
The author proposes a new setting of semi-supervised long-tailed classification and clearly demonstrated it. However, the new setting seems minor and not so novel. I thus give the score of 5. 	2
There are various obvious weaknesses in this paper including the comprehensiveness of the experiments and the novelty of the problem definition and the proposed method. Therefore, recommend the authors significantly revise and re-submit this paper to the other venue.	2
I think the proposed setting is interesting but the experiments are weak. Since the paper is not a theoretical paper, I think the novelty of methods should also be considered. However, the proposed method lacks sufficient novelty since the only additional component is a pseudo-label generation with a consistency loss. 	2
My overall attitude towards the paper is borderline. I appreciate the simplicity and effectiveness of the method provided in the paper. However, I believe that a more straightforward baseline should be compared in order to justify the design of the alternate sampling strategy.	3
"The paper studies an interesting topic but lacks clarity. I have concerns with the experimental setup, and with how the paper justifies the significance of its proposed approach.


---

**POST-REBUTTAL**: Most of my concerns are addressed, but I remain concerned with the submission's significance (see discussion thread)."	3
"The theoretical analysis of the variance of the norm to impact the bound of the prototypical classifier is interesting. However, the descriptions seem to contain errors, and experimental validation is not satisfactory. 

==
Post rebuttal
==

My concerns have been solved. I have slightly increased the score accordingly.  
"	3
Overall the theoretical analysis is interesting, however the novelty seems to be marginal. The insights provided in the paper are quite intuitive  and proper connection with theory and experiments need to be explained better. I would like to see how the authors response the raised issues.	2
"

Overall the paper has a central point by presenting a framework of multi-agent learning. The model is cooperative, so a potential function exists and learning converges for slow learning rate regime. 
A discussion section is lacking, in particular with regard to the significance of the work and results. 
What insights can be provided by the analysis from the paper?
Currently the conclusion section include mostly a list of future applications. 

The claims in the paper are supported and correct to the best of my knowledge. The significance of the results is debatable but the model is indeed novel. The empirical data shows an example of chaos and two examples of convergence, with and without noise. In my opinion, the model is novel enough for me to recommend a 6, but the technical results are expected and perhaps maybe that can push it to below the acceptance threshold.


"	2
"The extension of the performative predictions framework to multiagent systems is an interesting problem. and even the scenario under study with linear predictors and MSE loss greatly simplifies the analysis, I appreciate this is a first step.
This is mainly a theoretical paper, but some of the mathematical derivations have raised some questions.

Metareview
----------------
The authors have responded to most of my comments clearing my concerns on the derivations. I think that an experiment showing sensitivity to initial conditions is missing. Although the novelty seems incremental with respect to previous works, there are a few small innovations, so I am increasing my score to 6."	3
"Overall, I think this paper makes a reasonable contribution, showing that the behaviours of using exponentiated gradient descent differ with using different learning rates. However, given there already exist observations and results about exponentiated gradient descent could be chaotic in several game settings, and the performative prediction studied in this paper is also a special game setting, I consider the contributions and technical novelty in this work on an incremental level.


----update----

I would like to thank the authors for the feedback. My main concerns are addressed and thus I would increase my score."	2
"A significant improvement of the theoretical runtime bound is given. Some of the technical content overlaps somewhat with previous work. The experiments show good results but are limited. The paper is well written. Therefore, I tend to acceptance.

-----------------------------------------

I thank the authors for the answers, they have answered my questions."	3
"In summary, I think the main result in this extended abstract is a lower-order term improvement over the naive algorithm and I found the analysis unnecessarily complicated in order to achieve this lower-order improvement over the naive algorithm (please correct me if I am wrong!). On the other hand, the lower bound shows a nice separation from the centralized setting and although not entirely surprising, the result is interesting to me from a theoretical perspective.

Post-rebuttal update: I acknowledge that I have read the author response to my review. I also acknowledge that I did indeed misunderstand the significance of the upper bound with respect to the previous work. Specifically, because the JL-trick can only be applied to batches of $d$ rows, then it is not applicable in the dynamic setting in which the solution must be updated after each row. As I already thought the lower bound was of independent interest, I raise my score from 5 (weak reject) to 8 (accept).

Regardless, I hope the authors will consider improving the presentation of the technical statements. In particular, Lemma 3.3 could either decrease the number of closed-form formulas or add intuition to some of the formulas. "	3
"The paper discusses an interesting problem, taking an algorithm optimized for space complexity, and tuning it to also benefit time complexity.

The technical core of the paper certainly exists, but I'm uncertain if it is sufficient. I lean ""marginally above accept"" on this point, since I could see myself referring to results these in the future, but it's just not a lot. I like the lower bound quiet a bit.

The experiments are barebones, but if the theoretical contribution is sufficient, then that's something the authors can fix up for the camera-ready version.

I'm particularly interested in discussing with other reviewers if the technical contributions are sufficient. I could easily be swayed either way."	3
Although there is not significant technical innovation, this paper is a solid technical paper that solves a basic linear algebraic problem in the online setting and deserves to be published.	2
Taking the above into account, I recommend acceptance of the paper. While I feel the empirical evaluation could be improved, I find the motivation and intuitive arguments behind the proposed algorithm convincing enough to be a useful contribution in the literature.	3
The paper develops a novel method for an important problem. I, however, have a few comments above that I believe would improve the paper.	3
The main idea of the paper is interesting and relevant for the field. However, I feel that the setting and the comparison needs to be revisited to have practical application.	3
This paper proposes a simple but seemingly effective method for model-based RL in cases where obtaining transitions for training from the real environment is extremely costly. The method is based on well-founded principles of Bayesian experimental design. The paper itself is clearly written and easy to understand, and technically sound. The empirical results sufficiently clearly show the advantages of the proposed methods. More realistic experimental settings could be considered in future work.	3
"This paper proposes a nice idea: to learn class functions by first applying a feature transform, where features are chosen to be invariant to conjugation (in particular, low-order irreducible characters), and then processing those by a neural network. The authors justify this by noting that irreducible characters form an orthobasis for the space of square-integrable class functions, and that (by their algebraic structure), high-order characters can be generated from a few low-order characters. The paper is well written but at this conference I would expect to see much more technical content, stronger results, more convincing numerical experiments. I think that this can become a strong paper but the current version records a nice idea with solid potential and presents some initial numerics.
"	3
"As the authors mentioned, the paper does not mention the application of the proposed paradigm around machine learning field.
Also, it was not clear how the proposed paradigm would solve the problem of Image Alignment, which was mentioned in the paper as a possible application.
These facts may indicate that this paper is a little different from the scope of this conference."	4
The paper presents an interesting perspective of learning conjugate invariant functions. It provides a simple method for the non-linear approximation of class functions. The attendant discussion for many problems of interest is also valuable and useful. The paper does not propose a new theory -- it crisply teases out the importance of building in such invariance to many problems of interest. While valuable, it needs validation on one problem of interest. I think the paper has the potential to be a strong and valuable contribution. 	2
"By using the theory of characters and class functions, they have created a new method for learning class functions.
However, the scope of application is limited and its usefulness is somewhat questionable, so I judged it not worthy of high evaluation."	2
I didn't follow the main thesis that this paper tries to convey, and I guess this may happen to a broad audience of the general ML community as well. In my opinion, the main goal of the publication is to spread the idea so that more people in the community could appreciate it. From this perspective, I have to vote for rejection for now.  	2
"I want to first thank the authors for writing a clear paper and conducting extensive experiments. 

The experimental results show performance gains, however, I am not convinced that this approach is novel as it has been shown in previous work that self-training can improve performance and as we know that GPT-2, 3 can generate quality examples.

To appreciate the result more, I recommend authors to actually focus on discussing challenges in finetuning generator given training data. If they could improve the quality of generation in a novel way, maybe the paper could become more appealing. At the current state, I feel like GAL is very straightforward self-training with the unlabeled data generation with large-LM generation.
(Additional comments:
In case I missed some contributions of this paper, If we remove the good quality of GPT2,3  what are the benefits that GAL is bringing as a framework?  Could you elaborate on why generating in-domain data is a novel idea?)"	2
The proposed framework is simple, well described and very powerful. Experiments are numerous and relevant. I propose to accept this paper.	3
The authors study how to generate data for semi-supervised learning. The main claim is that the unconditional autoregressive language model is able to provide good quality data. However, this claim is still validated based on comparison with other state-of-the-art data augmentation methods. 	3
This work has extensive literature review and provides enough context for the reader to follow their thought process and the decisions made by the authors. Results are convincing and span multiple NLP tasks and research areas (tabular and CV data). There is in-depth discussions about limitations and benefits, and about the decisions made for the framework. My opinion is that this is a clear accept.	3
I enjoyed reading this paper. Assuming that this idea is as novel as claimed, I think this is a valuable contribution to the community and recommend acceptance.	4
"In this paper, the authors propose a new framework, Bayes Augmented with Memory (BAM), that takes advantage of past experience by allowing the agent to choose which past observations to remember and which to forget and demonstrate that BAM generalizes many popular Bayesian update rules for non-stationary environments. The variety of experiments demonstrate the ability of BAM to continuously adapt in an ever-changing world.

To the best of my knowledge, this is generally a good paper with a clear central idea. I have only two minor concerns:

For simplicity, this paper focused on binary values for the readout weights as it allowed for a simple greedy discrete optimization algorithm to be used. This assumption limits its practical application scenarios.

Although a simple greedy discrete optimization algorithm to be used, it still makes the proposed BAM have a relatively high time complexity. The paper could be improved if the authors can provide the time complexity of the proposed BAM. If it is difficulty, the comparison results in terms of the running time are needed.
"	4
The paper could be an interesting contribution in the area of Bayesian online learning in a non-stationary environment. However, unless solving the scalability problem and demonstrating the benefits of the memory, the paper isn’t persuasive enough.	3
I think the proposed framework can be potentially interesting and useful, but at current presentation lacks many practical considerations to be dealt with, especially how it can be applied to non-trivial models requiring approximate inference for computing posteriors. It is quite disappointing thus to only see experiments on conjugate models. 	2
A solid paper providing the first solution for an important problem in federated adversarial training. 	4
The proposed techniques are not well motivated. The rationality of the proposed method on non-IID FL setting needs more explanation.	2
Empirical results are good, but the rationale of the proposed method is unclear and not well explained. The theoretical results don't justify the method, instead, within the proposed context, justify algorithmic convergence.	3
The paper tackles an interesting direction in federated learning, and is well-organized. Overall, I enjoyed the paper itself and the presentation of ideas, but I do think the contribution is incremental.	2
The paper is clearly written and has novel though a bit incremental results, which builds on previous works and contain results that are similar in nature. I am leaning towards marginally accepting the paper.	3
I am wondering if one could prove the results of this paper by Taylor expanding the CNTK kernel. The benefit of using Taylor expansion is that the assumption that the inputs are on the hypercube would be unnecessary. In particular, a degree-p monomial <x,y>^p for x,y \in R^d can be decomposed as <x^{\otimes p}, y^{\otimes p}> where x^{\otimes p} is self tensor products of x. These vectors are in dimension d^p which is the same dimension as what you would get from Gegenbauer expansion. 	2
Overall, I believe the paper is well written, technically correct as I checked most of the proofs or I’ve already seen/dones some sketches of similar ideas. I feel no classification experiment on CIFAR-10(or other dataset) is really needed as this algorithm doesn’t try to derive any optimization algorithms, yet rather try to describe the space of functions associated to this framework. I think however that given that this paper tries to explain deep neural networks, it'd be nice to validate this approach empirically. My further concerns are rather linked to the motivations of this study and the choice of locality and/or laziness and/or some modules.	3
"Overall I lean slightly negative on the paper. My main criticism is that the novelty of the approach seems to be low. Moreover, the closest related work that I am aware of does not appear to be cited or discussed. Finally, while the method performs well in simulation, there are no results that clearly evaluate generalization and/or performance on more challenging datasets. I would be open to revising my score if the authors can clearly address these concerns.
"	2
"The topic is of general interest and the paper is mostly well written. However, there exist concerns about a lack of novelty and incremental improvements.

The authors are strongly encouraged to address these shortcomings by:

i) provide a better justification of the OpenFWI dataset, especially on how OpenFWI bridges the gap between simulation and real seismic inversion

ii) motivate the idea Neural Network+PDE  better

iii) improve the experimental section by studying if the proposed method is model-agnostic
"	2
I am very positive about the paper and would recommend acceptance, but authors could also considers comments/points mentioned above. 	4
"Overall, I like the paper. However, I have several concerns or confusions. I am looking forward to author responses and engagement such that I can keep my score or increase it. 

(After the rebuttal, I have decided to increase my score)"	3
The idea of using a kernel function to address the conditional sampling problem in contrastive learning is interesting and novel. The experimental results validate its effectiveness in some contrastive learning tasks. However, my major concern is that whether  Fair_{CCLK} can actually remove sensitive information of conditioning variables (See the second point of the Weakness in Main Review). More elaboration regarding this issue is needed. Also, Fair_{CCLK} should also compare to other fair contrastive learning methods to verify its effectiveness. 	3
I believe that the paper as its merits and really like how they have seemingly incorporated CME into representation learning and obtain good performances. However, there are still important references missing which I believe reduce the novelty by quite a margin given that it s a simple derivation from [1] applied to different setting. In addition, there are some clarification on the experimental side which I would like know. **I am more than happy to increase my score if the authors are able to clarify my concerns and will be looking forward to their rebuttal.**	3
This paper proposes an importance weight sampling technique to improve the InfoNCE loss, where the weight is estimated by using kernel embedding operator, but as the evidence of advantages of using the kernel estimation is not sufficiently shown. I am inclined to recommend a reject of this paper.	3
"I think this is a great paper, as it proposes a new practical method for building equivariant networks which is broadly applicable, universal and performs well in practice. I have serious concerns about theorem 4. If the authors convince me why it makes sense, or if they remove it, I will increase my score.

I've updated my score after the response and revision."	4
The paper studies an important problem that could potentially have great impact: How to adapt existing architectures to become invariant/equivariant to a certain group while maintaining the expressive power and computational efficiency of the original backbone model? The paper provides a simple yet effective solution. The technical statements are sound and the empirical results are impressive.	4
"The structure of the proposed model has already been seen in Kicki et al. 2021 except for the use of frames, and even if frames are used, MLP+FA for example does not seem to be able to handle changes in inputs such as changes in the number of nodes in the graph.
When combined with the existing strong methods, it gives good results, but this could be achieved by, for example, concatting two GNNs and then transforming them with FNN, so we decided that it is not worthy of evaluation.

After some discussion, the score was raised because the theoretical uncertainties and doubts were resolved.




"	4
"In addition, I have some comments on the technical parts of the paper:

- The definition of the frame $\mathcal{F}(\mathbf{X})$ for the case of point clouds and $G=E(d)$ in page 5 is not clear. Why ""$\mathcal{F}(\mathbf{X})$ [...] is the collection of E(3) Euclidean transformations defined by [...]"", while in this case, I think, $\mathcal{F}(\mathbf{X})$ must be a subset of $E(d)$?

- I am curious to know why ""generically the frame would consist of $2^d$ elements [...], while for rare inputs $\mathbf{X}$ the frame can be an infinite set""? I think this remark is crucial for the proposed method as it affects the size of the FA framework.

- I think the proof of Proposition 1 (Appendix A.6) has some flaws:

     -- Line 3: What is $\Lambda$? It is not defined yet.

     -- Line 6-7: It seems to me that, you need to prove that $\mathbf{RO}$ consists of eigenvectors of 
$$[(\mathbf{R},\mathbf{t})\mathbf{X}]^T \cdot (I-\frac{1}{n} \mathbf{1} \mathbf{1}^T) \cdot [(\mathbf{R},\mathbf{t})\mathbf{X}]$$
which is 
$$(\mathbf{R}+\mathbf{t} \mathbf{1}^T) \mathbf{X}^T \cdot (I-\frac{1}{n} \mathbf{1} \mathbf{1}^T) \cdot \mathbf{X} (\mathbf{R}+\mathbf{t} \mathbf{1}^T)^T$$
rather than 
$$C = \mathbf{R} \mathbf{X}^T \cdot (I-\frac{1}{n} \mathbf{1} \mathbf{1}^T) \cdot \mathbf{X} \mathbf{R}^T$$
as you claimed. If this is exactly the case, I do not know how it can be true."	4
The method is technically sound, and the paper is easy to follow. However, there are still many details are either hidden or unclear, which hinders its reproducibility. Besides, the method is only evaluated on CDFSL, which is a relatively small dataset. It would be more convincing if the method is applied and compared on Meta-Dataset (another larger cross-domain dataset). Hence, this is a borderline paper to me. 	2
In this paper, the authors propose a framework for cross-domain few-shot learning. The idea is reasonable and the authors also provide a good analysis of the components of the framework. The proposed framework outperforms the baselines in most of the cases. 	3
The authors are encouraged to respond to the comments listed in the paper weaknesses during rebuttals.	3
The paper is ok to me, even if it not so general, since it cope with a very specific configuration (single-source domain, FSL with meta learning) which could be just one case of a more general situation, which may lead to better results (multi source domain, or domain which are kind of closer to the final ones). Experiments seem convincing.	2
While the work shows a promising approach (as measured by outperforming Fast-AT-GA at $\epsilon=16/255$ in terms of standard accuracy while beating it in speed and in robust accuracy), the evaluation is lacking. In particular, the evaluation does not give a speed/accuracy tradeoff for each training routine, and does not properly hyperparameter search for each compared algorithm in the comparison. Finally, the work has some minor issues in its characterization of previous work.	3
"To summarize:
Strengths:
          - the paper studies an important and relevant problem.
          - the experiments are well done. 
          - the proposed adversarial training approach has nice theoretical motivations.
          - the paper is well-written for the most part.

Weakness:
          - several ideas and claims are handwavy and lack mathematical rigor 
          - the assumption behind the main theoretical result is too stringent
          - statistical significance --> only 5 random experiments"	3
"I'd tend to reject the paper because of the highlighted inaccuracies and unclear points, and because the contributions seem straightforward applications of BLO theory to adversarial training. Additionally, the new linearization does not seem to significantly improve state-of-the-art results.

## UPDATE
My final evaluation is based on the current version of the paper, as the authors had the chance to update it.
I increased my score to 5, as the inaccuracies have been corrected, namely:
- the attribution of adversarial training has been rectified
- the additional $\lambda$ norm_2 term in the linearized inner problems has been explained
- the phrasing for the Hessian-free assumption and projection's derivative has been corrected
 
I still tend to reject the paper for the following reasons:
- definition of gradient alignment and autoattacks is not provided
- justification for using autoattacks is not provided. ""everybody is doing it"" is not a sufficient justification per se
- comparison wrt baselines is based on mean values, hence the reader cannot judge the significance of the improvements. Doubts about the significance of the empirical results has been raised by other reviewers as well.

Finally, I still have some doubts about the contributions of the paper to the BLO literature. In the rebuttal the authors claim ""(1) we show that the choice of lower-level linearization (gradient sign-based vs. non-sign case) could be a key to simplifying BLO with lower-level constraints, and (2) we derive the closed-form of the implicit gradient of lower-level constrained BLO assisted by lower-level linearization.""
Hoewever the two linearization schemes are not novel, neither the derivative of the projection onto a convex set."	2
Good perspective for interpreting fast adversarial training, but in the current version, there exists several key points that requires careful justification to make it as a much stronger piece of work.	3
"Paper is well written and a natural extension to the linear formulation of successor features. Re-implementing the model based on the writing itself should be possible and I thank the authors for clear text. 
The latter half of the paper is less well developed coming across as a little undercomplete. I wonder if the paper would benefit more from more significant transfer learning experiments uitlizing the second order model while relegating the auto-correlation matrix / alternative to epsilon exploration to the appendix / future work."	3
In summary, the paper needs more clarifications and convincing experiments to support their main claims, especially around why is second order SF is better than linear SF.	2
"The paper presents an extension to deep successor reinforcement learning [2] which adds an extra quadratic term in the reward model, resulting in a new 2nd-order SF framework. The idea is interesting and the motivation is clear. My major concerns are about (1) the clarity of the paper, and (2) the significance of the proposed method, and I suggest the authors include additional baselines and experiments in the rebuttal period (see concerns above for details). Therefore, I don’t think the paper can be accepted with the current version and vote for weak reject.  


[2] Kulkarni, Tejas D., et al. ""Deep successor reinforcement learning."" arXiv preprint arXiv:1606.02396 (2016).
"	3
"The paper describes an extension to the successor feature framework.
Pros:
- a new formulation with provides a more robust reward component
- the approach can be used for exploration during transfer
- a clear improvement over the linear approach with less parameters
Cons:
- it can be seen as an incremental work
- the experiments do not reflect, under the same conditions, the extra
effort needed with the second-order rewards"	3
In general, the manuscript was well written and organized logically. The GrID-net modeling was also presented clearly. I believe the new model has broad and timely impact on single cell data integration and understanding temproal gene regulatory mechanisms. 	3
"Overall, the paper introduces a model that, while a simple GNN, which appears to be a very reasonable approach for solving the problem of detecting the effects of chromatin accessibility on expression. The problem itself is interesting and important. In addition, the manuscript is generally clear and well written though the authors should be more careful about how claims are interpreted and described. However, the work does not offer many novel technical innovations since the GNN architecture is standard and the domain specific modeling/insights is lacking. Also, aspects of the evaluation of the model are concerning since the authors do not use out of sample data and should consider other baseline.  
"	2
"Overall the method is sound and the authors have put in a lot of work to showcase the merits of their approach. I wouldn't say that the proposed approach ""outperforms existing methods for inferring regulatory locus-gene links"", but it seems to be an incremental step forward. Keeping in mind that the problem tackled is an exceedingly difficult one, I am inclined to give this paper the benefit of a doubt.
"	2
"Overall I find the paper to be very well written, with a new method clearly motivated from a real-world problem the field of genomics. For that reason i am inclined for acceptance of the paper. I would like the authors to mention the strict assumption of causal sufficiency that their work assumes and make it clear that their results may suffer from it. I would also like to ask the authors to answer my questions above.

--------------------- update ---------------------------

Since the authors did not respond to any of my comments and concerns above, I decided to reduce my score. Not clearly stating that causal sufficiency is assumed is a major point, as it leads to wrong ""causal conclusions"". Moreover, the lack of explanation from the authors regarding their intuition in some key points of the paper contributes to lack of clarity. In addition to the above, the lack of comparison with linear Granger or at least discussion about it, make me decrease my score significantly. It is a pity because this could have been a good paper, if the authors have taken the time to answer and fix these points by answering to the reviewers. Without their responses/edits the paper contains strong overstatements.


------------------- update after authors posted their rebuttal -------------------
I am reverting my score to its original value, given that the authors answered all my comments and updated their paper accordingly."	3
Overall, while the goals of the paper are well-founded, and their analysis seems potentially interesting, a combination of lack of clarity and some gaps in their overall argument weakens the paper significantly.	2
This work lacks novelty, is hard to follow, and shouldn't be included in ICLR.	1
An interesting paper, yet the writing needs improvement. It is important to understand the underlying mechanism of models like GRU. However, I am unsure if the presented result is useful enough for designing a better mechanism for RNN/GRUs.	3
"To the best of my knowledge, the paper appears experimentally sound and different from previous work.
I firmly believe that we need more work trying to uncover the inner workings of neural networks.
I am therefore recommending acceptance in the proceedings, assuming that the differences to the analysis from Karpathy et al. (citation in main review) will be discussed briefly."	3
See above.	2
While this paper has some promise, it has some major issues that prevents me from being sure of the significance of the contributions, such as the inaccuracy in comparison of complexities, or per iteration costs. Moreover, the assumptions seem quite strong to guarantee some important conclusions (such as boundedness of the sequence, see the remark in Th 4.1 (b)).	2
Overall, the paper looks good and the results are aligned with randomized coordinate methods, i.e. O(1/N). 	2
The paper is well written and has certain contributions. I would support its publication in ICLR but my support is not strong.  	3
"I think the paper presents a really interesting approach to distributional DRL in context to risk-sensitive RL. Although there are some issues with the clarity, my biggest concern is that advantage of the proposed approach is not clear when compared to the existing literature on risk-sensitive and constrained RL. Although the paper could use more empirical strengthening, nevertheless, I think the will be useful to the community and hence recommend weak acceptance. 

"	3
"STRENGHT:

The authors propose a new policy gradient algorithm based on a risk measure that has not been considered in RL so far. The considered risk-sensitivity measure comes from the cumulative prospect theory and aims both to mimic human decision making, and to generalizes a large class of other risk measures.

I believe that risk-aware RL is important, as intelligent beings rarely optimize for the ""average"" case, but they often act optimistically or pessimistically, usually giving higher weights to rare events.

WEAKNESSES:

The paper lacks a proper background, clarity, and precision. The mathematical notation should be improved, and while some passages could be skipped, since they are widely known by most of the audience, some passages instead remain obscure and need further clarification. I think that some passages are wrong, and I am waiting for the authors' response.

The main objective of the paper remains unclear. What is the benefit of utilizing this method in RL? Why are other risk measures (like VaR or CVaR) not enough? 

The authors did not compare their risk measures with others, which I think was instead necessary. 

UPDATE
---------

As the author improved the clarity of their submission and clarified some of my doubts (for example explaining the lack of comparison with VaR & CVaR), I am raising the score from 3 to 5."	3
The main technical contribution is a novel generalization of the policy gradient expression to consider CDF weightings and a sample based algorithm that weights sorted trajectories to compute the gradient. The variance reduction extensions are fairly trivial. The empirical evaluations are limited to the open AI safety gym and are conducted on a PPO inspired version of the policy gradient result. These look promising, but the role of an arbitrarily chosen reward modification makes it hard to interpret how much the results depend on this  and how robust they are.	3
"While the paper analyses an interesting problem and its experimental analysis is sound, it is not clear how novel its derivation is. Moreover, the author did not provide a clear picture on the policy gradient method they propose, for which further proofs should be included. The main claim of the authors, that some degree of risk-aversion may help risk-neutral optimization, is not novel from an empirical point of view, but it is still largely unexplained from the theoretical one, for which, unfortunately, no contribution is given in this work.
Due to a balance between the aforemetioned weaknesses and strenghts I suggest a weak reject."	2
"An important and relevant paper which addresses the problem of computing bounds for the value function of a policy trained with domain randomization and tested on one instance of the latent MDP distribution sampled during training with domain randomization. The paper doesn't have limiting assumptions besides the absence of analyzing how results extend to the fine-tuning case. 

As a reviewer, checking the correctness of the paper wasn't easy because all relevant proofs are not in the main paper (9 pages) but mostly in the supplementary material (14 pages). The proof overview (section 6) isn't enough to check the paper correctness. "	1
In summary, I believe that this work addresses an interesting problem. Its main shortcomings are the lack of discussion linking the particular contribution and assumptions to particular use-cases as well as the absence of any experimental demonstrations.  Addressing these shortcomings will result in an interesting contribution.	2
"This paper formalizes the common practice of ""domain randomization"" and develops algorithms and bounds for the performance of domain randomization depending on if the set of randomized MDPs is finite (with or without separability) or infinite. The central result is that the performance gap is $\tilde{O}(\sqrt{H})$ in all cases. The formalization of domain randomization is accurate and the analysis of the problem setting is thorough."	4
"I appreciate the whole theoretical framework this paper establishes for studying domain randomization techniques for sim-to-real problems. I think defining the problem formally and properly is a good contribution.

The high-level proof overview makes good sense to me. I also manually checked the proof for finite simulator class (but not very carefully) in the appendix. The whole proof seems legit despite some minor issues, which I tend to think is relatively easy to fix.

The construction of base policies (Algorithms in the appendix) may also be interesting both for sim-to-real theorists and practitioners.

Overall, I recommend strong acceptance assuming that all the equations in the main paper and in the appendix are correct or are relatively easy to fix.
"	3
I have several concerns about the technical part and empirical results.	3
While sharing conceptual similarities to the gradient fields by [Shi 2021], unrolling the structure optimization process in the form of a graph network is an interesting approach and casting the model as a fixed point search has parallels to deep equilibrium networks. Its potential is demonstrated by good performance in different experiments. The proposed theoretical framework, on the other hand, is a restatement of common concepts in equivariant architectures, which have been presented in a similar or more general form elsewhere. In addition, the different sections (primarily related work) suffer from several fundamental problems which further detract from the quality of the work. Due to the two latter points, I tend towards rejecting the work in its present form.	2
This paper shows solid empirical performance with the proposed DNN architecture for molecular conformation generation. However, there are some logic or details that was not very convincing to me, e.g., the proposed method being a neural energy minimization. I would like to see them clarified for me to raise my score.   	3
"Overall, I thought this was an interesting paper and the proposed method achieves impressive empirical results. However, for now, I have gone with an overall score of 6, as I would like to see some of the experimental aspects better resolved (see 2.i-ii & 3.i-ii). Happy to consider raising my score if this gets addressed in the rebuttal!

I went with a lower confidence score as I have not gone through the proofs in the appendix in detail and only have a higher-level knowledge of some of the pieces of related work.

##  Rationale behind the significance scores
I put _""3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.""_ for the technical contributions question below. I did not choose the top score, given that Shi et al. (2021) also consider parameterizing the gradients of the energy function directly. (I realize that there are differences however, such as the training routine and optimal transport loss).

I put _""2: The contributions are only marginally significant or novel.""_ for the empirical novelty and significance. This paper's method is evaluated on existing datasets (Ramakrishnan et al., 2014; Axelrod & Gomez-Bombarelli, 2020) in a way that is similar to existing work. To introduce a new empirical evaluation the authors could consider evaluating how well the models trained on one dataset transfer to another or consider directly comparing the predicted gradients (from the NN) to gradients derived from traditional approaches. (Although in my opinion making more of a technical contribution and less of an empirical one (or vice versa) is fine).
"	3
Overall, I found the proposed method potentially interesting for fields in adversarial examples. However, the paper needs relatively large revision to fully support the claim and elaborate the novelty and significance of the work. The experimental results are ok but not strong enough to be accepted. 	2
"Overall, I believe this paper merits publication. The only drawbacks I can see are the lack of discussion and some minor claims being not fully rigorous. I'd raise my rating if the author can give a more in depth discussion and analysis on the relation between adversarial robustness drop and changes in perturbation distribution.

---
After seeing other reviews, I think clarifying the learning theoretic framework in the current work is necessary, otherwise, I'd lower my score. It is unclear whether the authors explicitly thought through this point. It can be my biased and overly optimistic interpretation.
"	2
The observation results lack reasonable verification. And, the evaluation of adaptive attack is missing.	2
See the main review.	2
Very interesting and solid contribution!	3
The main contribution of exponential family dynamics seems not very significant. Gradient discrepancy is sidelined as a minor contribution but is focussed on a lot more in the empirical section, which makes the paper seems disconnected. 	2
"The paper seems to provide a new family of SGD algorithms. The potential of the model seems to be broad. The theoretical analysis is recognized and deserves their own interest. However, the current state of the empirical study does not show promising performance. 
"	3
I am not confident with my review as this paper is quite technical and I am not familiar with this line of work. I would appreciate having more insights about the application of the new techniques and the relevance of exposing them together in this paper. 	2
This paper tries to understand the over-smoothing problem of deep and wide GCNs using the theoretical framework provided by GNTKs. The results on the exponential decay of trainability, residual connections, and critical connectivity are insightful and should be interesting to the community. The experimental results are well-aligned with the claims and show the soundness of theoretical findings. Given the novelty and significance of the findings, I am happy to recommend this paper for acceptance.	3
"The over-smoothing problem of deep GNNs is a well-known problem in the graph ml community, which impedes the researchers to fully unlock the power of the GNNs due to the limits of the model capacity. The authors try to provide a new perspective via the GNTK view. However, there exists logical gap between the theoretical results and the claims and the derivation of the proposed C-DropEdge algorithms. Detailed experimental results are also necessary to better demonstrate the effect of the proposed method. **Thus, my recommendation for this paper is ""weak rejection"".** If the authors can address the questions in the above ""Weakness"" section well, I am willing to consider raising my scores.

********* After Rebuttal *********

The authors have addressed by concerns and I am willing to raise my scores to ""Weak Accept""."	3
"The experimental results prove the consistency between the theory and reality.
For the weakness of this paper, the proposed solutions to solve this problem are far less attractive and the comparison with the baseline is not fair."	2
The authors present an interesting perspective to justify the decay in performance of deep GCNs. On the downside, the provided solution is not studied within the same theoretical framework and does not show promising empirical results.	3
The paper's insights are good, but the theoretical analysis seems based on a wrong equation, so they may need to recheck the results. Experiments conducted on more types of datasets in different fields and scales are required.	3
"In summary, the proposed algorithm seems interesting. However, there are some major problems in the opening remarks of the paper, the explanation of the algorithm is not clear enough, and the few-shot classification performance of the proposed method is not fairly compared with recent methods. These concerns need to be addressed. 

"	2
As mentioned in the Main Review above, the merits and concerns are pointed for reference and discussion. Generally, the main concern is on the originality as the SWA is an existing method, and the detail analysis on the mechanism how SWA specifically contribute for few-shot learning is not adequate.	2
"The submission applies an existing idea (SWA) to the few-shot learning setting, which is interesting and shows promising results. However, there is a disconnect between the simplicity of the idea and how it's presented by the submission, and overall there aren't a whole lot of new insights to be gained beyond the observation that SWA works well for few-shot learning.

---

**POST-REBUTTAL**: The response addresses most of my concerns. I would like to see those clarifications integrated in the main text."	3
In general, I vote for accepting the current paper if the authors could address all of my concerns above. I think the authors have presented a model for a novel whole-brain calcium imaging which was only recently published. Overall, the modeling strategy makes sense. I like that the authors tested on different variants of the model, which are all relevant, and provided insights on model selection. One limitation of the current work is the model generalization. I think the application of the model is limited to the current training data. I’m not convinced that the current model could generalize to chemosensory tasks, other than the ones in the training sets.  My major concern is about the clarity of some parts of the paper and some additional improvement in the work. I hope the authors could address my concerns in the rebuttal.	3
"While the approach is very interesting and the problem considered is important, the paper lacks comparisons and clearer description and investigation of the results. Furthermore, some justification on specific choices need to be done. See above for more detailed comments.

Post discussion: I'm increasing my score to ""accept"" since most of my comments are incorporated, I thank the authors for their hard work in revising the paper and incorporating additional experiments/visualizations."	2
"The paper highlights that known biological constraints can be included tractably within latent variable models of biological neural networks (in this case, the C. elegans). Moreover, they provide some evidence that these constraints improve model fits.
The model goes beyond existing linear latent variable models of C. elegans. However, the conclusions do not include strong measures comparing against existing methodology and it's difficult to gauge how much advancement this model provides. While the presentation clarity was adequate, organization and model presentation could be improved."	2
Overall, the paper provides a good aspect in modeling neuron activities by using connectome constrains as VAE prior, but it’s not clearly written and lacks novelty or significance in both technical and empirical aspects. 	2
"See above, this paper mis-use the concept of NAS-Bench that is common to NAS researcher. After reading the entire paper, I do not find it interesting as a NAS practitioner. Evaluating some algorithms on different tasks is a good empirical report but cannot be qualified as a top-tier conference paper due to lack of novelty and effort.

 "	1
I really like the contributions and the insights offered by this paper. Even though there is no novelty per se, I think the NAS community will benefit from this curated collection of diverse tasks. I lean towards acceptance and I will increase my score after most of my concerns are addressed.	1
This NAS benchmark lacks architecture datasets for the collected ten datasets and technical novelty.	2
The attempt of creating a new benchmark is very important and this paper shows promising results. However, my major concern is about the design of the proposed benchmark (see concerns above). Hopefully, the authors can address my concern in the rebuttal period.	2
The paper proposes a meaningful direction and benchmark for NAS. But whether this benchmark can measure the generalization of NAS method is not clear, the technical contribution of the paper is limited, and the writing of the paper should be improved. 	4
The authors suggested an innovative solution for the analysis of biomedical time-series. They have performed thorough testing of their approach on several tasks, and obtained convincing results.	4
A good paper is presented founded on a deep learning-based representation strategy that can be used as a data-driven wavelet representation holding attention mechanism. The experiments show the potential impact in different databases devoted to time-series classification.	3
T-WaveNet, a novel tree-structured wavelet neural network is proposed for real world time series signal analysis. If the authors could show robustness of the method against a limited amount of training data by experiment, this paper would be valuable for many participants of ICLR.	3
"This paper presents a solid and reasonable way to combine classic frequent-temporal analysis and the recent advance of deep learning based approaches. 
However, it seems a little incautious to claim the effectiveness of the proposed model as a general purpose approach for time series analysis based on the narrow scope of the benchmark datasets. I personally suggest two options for fixing this:

+ To conduct a more extensive evaluation over diversified datasets, e.g. there are 128 datasets available in the latest UCR time series website (https://www.cs.ucr.edu/~eamonn/time_series_data_2018/), all of them or a representative subset of them would be a good collection of benchmarks. 

+ To narrow-down the scope of the approach as an effective method for health monitoring related time series analysis. In this way, some concrete analysis should be added about some unique domain-specific properties of such signals, and the author would also justify why the proposed approach would be able to learn the representation of these properties more effectively. 

Post rebuttal comments:
My concerns have been mostly resolved given the update and additional experimental results. 
 
"	3
While all contributions are very interesting, the evaluation of the paper is focused on another completely different problem. So, it is impossible to evaluate if the graph generative model is able to replicate the global and local characteristics of real networks.	3
The current version of the draft needs some work. The paper lacks motivation and novelty in terms of its approach. The paper also needs i) more empirical results both with larger datasets, ii)comparison with more recent state-of-the-art approaches and iii) some discussion/analysis. Many parts of the paper are unclear and missing important details.	2
This paper introduces a general model for mixed-membership community detection, along with an approximation algorithm to recover its parameters. This algorithms shows promising results on real-world datasets; however, the model has some interpretability issues that undermine its main selling point, and reduce its novelty compraed to the existing literature.	3
The paper proposes interesting ideas that might be useful for robot learning tasks. Experiment results are positive. There are some questions as raised in the above section.	3
see above.	2
Bringing inductive bias to mostly unstructured policies is useful for RL. The results are promising, but some claims are overblown or not well supported.	2
The paper has an interesting premise about the advantage of encoding symmetry as an inductive bias in the data. It is novel in terms of introducing equivariant SAC but the claim of introducing equivariant DQN is ambiguous as it has already been introduced in previous work (Mondal et al. 2020).  Their experiments show improved performance of equi DQN and equi SAC over other methods for rotationally symmetric problems in robotic manipulation (which is different from previous works).  Although their results look very promising,  their claims may be better supported by additional experimentation  such as evaluating the performance of the proposed models in non-symmetric environments. Additionally, in order to ensure fair comparison, their equi DQN needs to be compared with a baseline DQN model with the same number of parameters. 	3
Please see above.	2
"Given the simplicity of the approach for single image test time adaptation, its improvements on vision transformer and my concerns listed above, I marginally accept this paper. I am willing to reconsider my rating based on other reviews and authors responses.

-------- post rebuttal ---------------

The paper studies an interesting problem setting and also bring improvements in this setting, particularly on vision transformer. However, my concerns on authors claims still persist after the rebuttal and I also see that the novelty of the paper is limited. After considering the authors response and other reviewers comments, I rate this paper as borderline and keep my original score."	2
Test-time model adaptation is an interesting problem, providing a new perspective to improve model robustness. The proposed method builds on intuitive assumptions that are easy to understand. There are mainly two concerns regarding novelty and effectiveness. I think the authors may need to rethink designing experiments to outstand advantages.	2
"I really like the general idea of the paper of trying to find a method where adaptation to a single test sample is possible, with resetting the model after each sample. To my knowledge, this has not been done yet and it would be an interesting contribution to the field and highly relevant for practitioners. This is actually very similar to how robustness is evaluated when model weights are frozen and inference is done on every point separately. Here, one wants to adapt to each test data point.

It is obviously a harder problem since so far, models have been allowed to adapt to the whole test set, e.g. TENT studied this setting. And indeed, single-point BN adaptation is currently the setting closest to this one. So it is fine to not claim state-of-the-art results on ImageNet-C in this hard setting. Of course, methods that are allowed to see the full test set and adapt to it will be better. Therefore, I think the authors should refrain from wanting to claim state-of-the-art results over methods that are allowed to see the full test set, since it is an unreasonable request. Methods that are allowed to see the full test set should always perform better, and if they don’t, there is likely an optimization issue like with the vision transformers.

I would suggest to the authors to pitch their paper in this setting: It is a new problem that no one has really studied so far which we propose a solution for and we ran all the necessary control experiments. That said, if one has access to many samples from the target domain, it should still be better to use all of them for adaptation. I think writing the pitch in this way should make the paper much more clear and easier to understand and put into context.

If one were to pitch the paper in this setting, one would need to test whether other test time adaptation methods work with model resetting, i.e. does TENT work for a batch size of 1 and with model resetting? Does one really need the different augmentations as claimed here?

I think that the paper cannot be accepted in the current state, but I think the general setting is very interesting and it can become a relevant contribution if the pitch is changed and more results are added. I am open to have a discussion with the authors during the rebuttal period whenever they wish to. I am happy to raise my score if my points are addressed.
"	4
"The main contribution of the paper, the method MEME consists of plugging together two existing techniques, namely test-time augmentation and test-time entropy minimization (Wang et al., 2021). This combination is straight-forward and the technical novelty of the paper is quite small. 
On the other hand, the empirical evaluation is relatively extensive and well structured . Consistent albeit small gains of the proposed method are observed. As discussed above, there are some ablations/alternative design choice that could be added to the experiments. Including those would give  a more complete picture.

In summary, I see this paper as marginally below the decision threshold, due to its limited novelty and incomplete ablations."	2
The authors should compare with more related works and provide some theoretical analyses to make this work more convincing. The ideas are totally heuristic and the experimental results are also not satisfying. 	3
Overall, the proposed approach introduces some interesting and practical ideas with some exciting experimental applications, however does not sufficiently explore the most novel elements of the contribution, ablate them sufficiently, or compare them to prior methods appropriately. 	2
Overall I enjoyed reading this paper. The authors’ method appears methodologically sound and it seems to provide considerable benefits when applied to complex datasets. However, there are some weaknesses in the manuscript that somewhat undermine the paper’s story. For now I am recommending a weak accept, and I am willing to revise my score upwards if the authors address my concerns.	3
The method proposed in this paper does not contain any particularly novel ideas and seems to be based on heuristics (maybe the proposed quantities could be derived from more general principles?). Additionally, it appears that weak supervision is not appropriate for the paper's empirical settings as seen by the lack of improvement from the downstream model and the stronger performance of non-weakly-supervised methods (active learning).	2
Overall, I think its a nice paper with a sound and simple approach to improve weak supervision's label quality using active learning. The contributions are novel and useful in practice. I am inclined towards accepting this paper. I need some clarifications in the experiments section to be more confident in this assessment. 	3
interesting model, experiments could be stronger	3
"SemiRetro seems an interesting method for retrosynthesis that combines ideas from both template-based and template-free approaches. It also seems to get impressive empirical performance (with the caveat that some SOTA methods are missing from the comparison -- see W4). However, I have currently gone with a lower overall score as I worry the method is similar to previous work (see W2) and I feel the paper could do a better job at explaining where the performance gains come from (see W1).

The other scores:
- correctness: see Q6 & W3. 
- technical novelty: see W2.
- empirical novelty: the paper mainly makes use of existing benchmarks/evaluation techniques.
"	2
"The proposed method has achieved impressive experimental results on the small dataset USPTO-50K. The proposed semi-template is novel and effective, however, it has been published in previous work. The presented DRGAT is quite incremental. The synthon completion method is the same as the GraphRetro. The overall framework is very similar to previous work G2Gs and GraphRetro. In summary, except for the performance boost, no new ideas or methods are proposed in this paper, and the contribution and novelty of this paper are limited.

*****************************************
After reading the author's response. I am quite confident there is an information leak in both G2Gs and this work. The authors did not canonicalize the test SMILES during model evaluation which is not a correct way to evaluate their method. They can not reproduce the reported results with canonicalized test SMILES. Please the authors canonicalize the test SMILES and update experimental results."	2
"It is great to see the stead improvement of the retrosynthesis. 
The semi-template idea sounds reasonable and good to me. 
The explanations on the DRGAT is clearly not enough, and in general, figures are hard to see on the screen. 
My positive score is with the expectation that the insufficient explanations and low-visibility figures are properly addressed in revisions or the camera-ready. 

=============================================================

After author feedback;
Thank you again authors for the efforts to revise the manuscript and discussions. 
One of the reviewers raise the concern about the leakage problem of the USPTO dataset, and I agree the problem is important. 
Some strong competitors are also introduced by other reviewers. 
As a result, we observe a few but important updates that are not reflected in the revised manuscript. 
I still believe the proposed semi-template approach is new and valuable to the community, but this time I cannot recommend this manuscript for acceptance. I expect the authors to come up with a manuscript including all new updates in near-future conferences. "	3
The paper introduces a simple adaptation of contrastive learning to tabular data by introducing a novel ‘data augmentation’ on tabular data. While the method outperforms several baselines, the experimental setup is not very convincing. Strong baselines like XGBoost are missing, a comparison to the related prior work by Yao et al. is missing, and only relative performance improvements over baselines are reported while absolute performance numbers are missing altogether. 	2
Overall I like the paper, there are few concerns regarding use of negative samples and below par performance on non contrastive methods. It would be nice to have some more analysis on this front.	3
This paper is well-writtern and the motivation of paper is clear. I agree with the contributions of this paper but I still have some concerns (see the weakness). I am happy to raise my score if my concerns are addressed.	3
Simple and intuitive contrastive learning framework for tabular data, backed up by extensive experiments. Considering the sufficient experiments, the proposed SCARF algorithm is expected to perform well on any tabular dataset, and the simplicity of the algorithm is expected to draw attention and will bring subsequent methodologies.	2
The paper is interesting and technically sound. The experiments are comprehensive and inspiring. However, the assumption of pair sentences will probably make it unrealistic in reality.	3
"- This paper clearly points out that bi- and cross-encoders have been evaluated and dealt with separately in the literature. From this, it proposes to combine the strengths of the two different paradigms by relying on cyclic knowledge distillation (or data augmentation).
- The paper is generally well-written and easy to understand. However, it is a little bit doubtful whether (1) the proposed method is novel enough and (2) the reported experiments are comprehensive enough. Furthermore, I feel that this paper does not contain a thorough analysis on ""why"" the proposed method works, which is what I want to know the most.
- Therefore, my suggstion is a weak accept (or on the borderline to be accepted).
"	2
Overall, this paper introduces an unsupervised method that seems to effective in sentence similarity tasks, but it lacks the insights and empirical investigation of why this method works well. The work is not well-supported. So I would consider it marginally below the acceptance threshold.	3
The approach is straightforward and nicely tied the bi-encdoer approach with the cross-encoder approach. The improvements compared to SimCSE is significant. The writing is clear and the reproducibility is good. However, I would expect more theoretical justification on why TEnc-mutual could be much higher than TEnc-cross. Moreover, training stability (of self distillation) and aggregate training time (for mutual distillation) deserve more discussion in my opinion.	3
In summary I think the proposed method is interesting and technically sound. But the benefits over previous methods are not clear both in theory and practice.	3
"Overall, this is an interesting approach and the authors have demonstrated empirically on a variety of models that it appears to perform well. 

Two points should be addressed:

- I believe that it would be good to give much more details about neural network architectures, initialization, gradient computation etc. 

- The limitations of the methodology should be better spelled out. The authors should for example illustrate the failure modes of the algorithm.  

More generally, it remains unclear whether any of the recent proposed Monte Carlo methods relying on training flows/neural networks by minimizing KL/maximizing ELBO are competitive with AIS/SMC when taking into account the training time.  
"	3
"This work ntroduces a novel sampling algorithm with lots of interesting
properties. This paper makes a solid contribution with the strengths and
limitations of the approach very clearly articulated."	3
"The paper is overall good. The method is based on taking the established framework of inference as control via Schrödinger bridge problem and utilizing NNs as a parameterization for the policy. Exposition is clear, convergence is characterized, the properties of the algorithm are highlighted on relatively low-dimensional problems.

====

After rebuttal

I thank the authors for addressing my comments. I trust that the authors will include the promised changes and additions regarding the training time and the comparison to ParVI into the final version of the paper. I maintain my score ""8: accept, good paper""."	3
"Recommendation

- I vote for ""accept"" with the reasons that the proposed method is simple and somewhat novel, and also it shows strongly effective results. I think the results are able to contribute to the style-controlled synthesis communities."	3
"This paper is well motivated, the idea of applying 'style equalization' is interesting. Also, the showed experimental results are impressive.

Upon the weakness I've pointed, I would suggest the authors:
1) demonstrate how M & phi can be trained in the expected way, which avoids the model collapse to shortcuts.
2) add discussions and comparisons with the very related line of work, i.e. content-style disentanglement
3) adding ablation studies and comparisons with more SOTA approaches

Minors:
By looking into the paper, I am not clear about the training and evaluation datasets. For example, on TTS, what is the training dataset and what is the evaluation dataset? Are they trained on VoxCeleb, and then evaluated on VCTK & LibirTTS. Or they trained on VCTK and evaluated on VCTK, and the same for LibriTTS?  If the latter, are the evaluation performed on 'seen' or 'unseen' speakers?

"	2
While this paper proposes an interesting idea for style transferring with nice experimental results, the benefit of the proposed idea over reasonable baselines is not clearly exhibited. The connection to related works is also not well discussed.	3
"Overall the paper introduce an interesting approach to address the problem of synthesised image detection/attribution. Although the proposed method certainly has merits and applications, I feel that the paper has slightly different terminology of image detection/attribution versus the literature. At the current state of the paper, I am at borderline but would like to hear the authors to elaborate on my points raised above.

Update: I have read the authors' response and other reviews. I think the main advantage of this paper is a novel strategy to fingerprint a GAN model, while the main drawback is that it works on multiple instances of a single GAN source, instead of multiple GAN sources like the baseline approaches. I strongly encourage the authors to clarify this in their next revision.

I upgrade my decision to borderline accept."	3
Based on the significance and novelty of this work, the reviewer’s recommendation is “Accept”.            	4
"My main concerns about this work are related to the contribution, which in my opinion is too limited with respect to state-of-the-art, and also to the insufficient experimental validation. I think that more work is needed in terms of experiments to support the claims, in particular about the generalization, uniqueness and robustness of the fingerprints. For these reasons I believe that this submission is not ready to be published.

UPDATES
Authors have better motivated their proposed method with respect to current literature and also added more experiments, that makes the proposal more convincing. I still believe that some more work on the uniqueness and robustness of the fingerprints should be done, however this can be carried out in a future work. 
"	2
This paper addresses an interesting problem about fingerprinting data generated by GANs, in order to be able to trace any misuse of deepfake. The paper improves state of the art by addressing a major shortcoming of previous works. Overall, the presented solution represents a clear impact on industry-deployed GANs. 	3
Overall, I think the strong points( the motivation, novel technique, and important set of metrics for such tasks that follow-up works in this direction would find useful.) are important and overweight weak points (there are more like better to know). So I would recommend a clear acceptance, although the authors are enraged to address may questions/concerns to make the draft more comprehensive.	4
Although I believe this paper provides several important insights, there are several critical issues that prevent me from supporting acceptance of this paper. I am happy to engage in a discussion with the authors, to see whether my concerns can be taken away or whether the paper could be improved.	4
I think this is an excellent paper. It is well-written, the proposed algorithm has strong results, it asks important and critical questions about continual learning research, it validates the proposed model with a large amount of experiments, is well-referenced, and provides source code and 14 pages of appendices to ensure reproducibility of the results.	3
This could be a great paper. It is important to understand what works in practice with current algorithms and architectures. Analysis of worst case performance is not the only important case in many practical settings. Hence, I lean towards acceptance despite the gross lack of nuance and generally imprecise writing. As I said, the authors could turn this paper into a great contribution by providing nuance instead of hyperbole, deference to the audacity of other researchers to face challenges which the authors avoid axiomatically, instead of off hand comments. I will (reluctantly) not oppose publication close to current form, but I cannot confidently recommend acceptance before the writing is improved.	2
This paper provides both theoretical and empirical insights to support the claim that splitting the capacity of a model into several small learners can benefit continual learning, which have the potential to guide future continual learning research. While the empirical results to some extent corroborate the theory, the link between the outperformance of Model Zoo and the reasons predicted by the theory were not entirely obvious and could potentially be clarified with some additional experiments. Additionally, the experiments were independently lacking in some ways detailed below. I would consider increasing my score if these issues are largely addressed with revisions. 	3
Overall I vote for accepting the paper. I like the approach of directly estimating uncertainty. I have some concerns on providing intuition on the estimated uncertainty, which I believe could be addressed by some additional discussion and some more analyses. Hopefully the authors can address my concern in the rebuttal period. 	4
"The main factors influencing the recommendation are the strengths and weaknesses highlighted above.

The paper can be improved significantly by:

a) Adding narrative to help justify why the secondary network generalized to OOD settings where the primary network couldn't. Perhaps present results that shine light on just this sub-aspect.

b) Improving the overall flow and presentation."	3
"This paper provides a novel view on uncertainty estimation and is an interesting read. I believe it is held back by some overly ambitions claims and a lack of depth in discussion at some points. Experimentally the paper is again interesting but fails to drive home its main points. 

I currently can not recommend acceptance. If the authors improve their discussion and claims I will raise my score to a weak accept. If the authors provide some experimental evidence for the existence of bias in a realistic setting and DEUP accurately estimating that bias while variance-based methods to fail, I will raise my score to a strong accept.

**update**

After a discussion with the authors I have raised my score from 5 to 6. "	3
It is possible that there are some interesting ideas, but the writing is extremely unclear. I cannot understand this paper. 	2
In brief, I was impressed by the theoretical results and connections of the new alignment loss and conventional JSD bound. Experiments seem to prove the model is workable on toy data and digits datasets. However, authors still need to explore and demonstrate its feasibility and value in more challenging learning tasks using high-dimensional or structured data.	3
It's an interesting idea of using non-adversarial learning to realize multi-distribution alignment. But lots of concerns remain as given above. Thus, the impact of this paper is weak.	2
The framework advances the improvements in the problem of multi-distribution alignment, although it is an extension of prior works.	3
"The authors proposed a new algorithm for classification problems combining a new quantum data loader for images and
an existing trainable quantum circuit. However, the paper lacks arguments that motivate using their new approach
because the authors did not provide neither empirical evidence that their quantum algorithm outperforms existing
approaches nor improvement of the running time to train their circuit"	2
The novelty is insufficient as the embedding part and parameterized circuit part is not new. the evaluation is insufficient: no comparisons with state-of-the-art; no evaluation on real quantum devices; no ablation studies; no justifications of why the proposed embedding can outperform others.	1
The paper presents an interesting idea, however, since the new contributions appear to be limited to equations seven and eight, I am not convinced the contributions are sufficient for inclusion in the proceedings.	2
"Based on the concerns I mentioned above, I recommend rejection for this paper at ICLR 2022.
"	2
"The authors present a simple approach that is sound, effective in a specific online continual learning scenario, where a replay buffer is kept, and new tasks are not completely unrelated to previous tasks. On the other hand, the authors only provide experiments on MNIST and CIFAR, and do not provide any additional information on how their algorithm would handle more challenging situations. 

Overall, the proposed method is interesting for the research community but the authors should provide more information on how it would behave under more challenging scenarios, discuss its limitations, and include in the text a more complete comparison with other setups that justifies for a non-expert reader why they did not compare with Harrison et al. 2019, He et al. 2019, [A] ..."	3
I am not an expert in this area so I believe that my opinion should be discounted. Based on my understanding, I think this is a solid and well-written paper, with extensive results that demonstrates the improvement of presented approach to its comparing prior work. 	2
"Overall, while the paper is well-written, I have doubts about the motivation behind the problem statement. I am concerned about the missing relevant literature and inadequate experiments.

--- Post rebuttal update

I thank the authors for the detailed response to clarify some of my concerns. I also read the other reviews and the response from the authors. The authors have clarified the existence of the problem statement citing prior work. I agree with the existence of the problem statement. However, I am not entirely convinced with the motivation. Experiments on real-world applications requiring online meta-learning would have significantly strengthened the contribution. I acknowledge that the experiments comparing other online-learning algorithms (such as LwF, iCaRL) and algorithms such as MOCA is a step in the right direction, but requires more analysis (using more datasets). 

"	2
This paper proposes a method named FOML which is a further step of the existing method FTML that tries to remove the task boundary. However, such an idea is not new and there is already a similar work that exists. Experiments are not strong enough to support the claim. 	2
This paper is very well-written, tackles an important problem, and overcomes two shortcomings in existing approaches. 	3
I recommend acceptance because of the diagnostic section of the paper. While there are a few concerns, I think with a bit of revision and better baselines, the paper presents a nice set of results especially with the diagnostics section	2
This work proposes a new model for prediction physics dynamics. This new model uses graph neural networks to compress the physic graphs into lower-dimensional representations and then uses transformer models to predict the dynamics. Through empirical studies on three physical scenarios, the authors show the advantage of this method compared to the previous SOTA. However, more results need to be presented to show the power of this method in complicated physical scenarios beyond oscillations, where the transformer models indeed gain advantage by their nature. 	3
"Overall, I think the paper targets a very interesting direction, and I can encourage the authors to continue their work in this direction. However, in the current form I find it difficult to argue for accepting the paper straight away. The training and simulation setups leave too many open questions to properly evaluate whether the attention approach actually provides benefits or not.

Post rebuttal: the authors have answered my questions, and added two interesting simulation results in the appendix. So I've adjusted my score to a more positive one, and (seeing the other two positive reviews) I'd be fine with acceptance."	2
The paper proposes a clever twist on conditional normalizing flows and shows the versatility of the idea on several different weakly supervised problems. An illustration would help to make the paper more accessible which would increase it's impact.	3
The idea of the paper is interesting and seems to be effective. But more explanations and comparisons are needed.	3
"Conceptually, constrained flow formulation of LLF is interesting and novel. However, its numerical results are not too convincing and there are some questions related to the practicality of this method.  I also do not think that unpaired point cloud completion should be an example of weakly supervised learning.  As it stands right now, I do not think that this paper meets the bar of ""powerful and effective tool for weakly supervised learning problems"". The paper can be much improved (especially for weak supervision practitioners audience) by addressing the questions under Cons and additional points above.
"	3
Overall, the idea of modeling the range of possible labels for instances based on weak supervision instead of simply using a single estimate is a very interesting one.  However, the paper suffers from a number of major issues.  1) The experiments use default hyperparameters for all techniques, thus it is difficult to tell whether the relative performance of methods is indicative of their quality or the quality of the particular default values of their hyperparameters. 2) The normalizing flow component is not well motivated nor is it clearly presented in the context of learning from weak supervision. 3) Some of the proposed used cases lack strong practical motivation.  Many of the major issues I present in the main review seem difficult to remedy in a short period of time, though, if they are the result of a misunderstanding on my end and small edits to the paper could alleviate future readers from similar confusion, I would be willing to change my score.	2
The proposed method is novel and the experiment is sufficient. More experimental analysis can further improve the quality of this paper.  	3
"Strengths:
1. Good, well-scoped inspiration from neuroscience 
2. Thoughtful and interesting experiments 
3. Very informative figures, several of which are also aesthetically appealing and clear 
4. Overall very interesting problem and motivation, plus a well-written paper 

My critiques:
1. Lack of clear claims, especially at the beginning, making it difficult to have a good lens with which to view the results - see examples
2. Claims appear by the end, but they are somewhat speculative and disparate - see examples

Presentation notes: 
- The figures are very rich and useful. Better signposting in them would help the reader really get the value of these experiments 
- Visual examples of activations for certain points would help build intuition for why these might be surprising results "	1
The paper presents several interesting characterizations but I'm not sure what they are for. It could be that the work is not well motivated or just that it's not written to make the motivation and implications clear. Either way, I recommend rejection.	1
This paper does a nice job of comparing representations learned by an identical network architecture, given different training objectives. The results are interesting, but not completely unexpected. The idea of comparing an RL-trained network to other networks doing image classification and regression is a good idea, but the results are about what one would expect. Where they are not what one would expect, they are not explained or analyzed in any depth. Hence I don't believe this paper is ICLR-worthy.	3
This paper has definitely made me think about some questions in the similarity of representations from different objectives, so I am inclined to accept. However, it also has limited applicability due to the simple environment and objectives, and could use some improvements in presenting a more general finding rather than detailed individual models (see main review for suggestions).	3
Overall, the paper studies an important problem (self-training on multilayer networks) and tries to connect some theoretical takeaways to practice. However, both in terms of theory and experiments, I found that the paper does not broaden our conceptual understanding of the method, nor does it introduce interesting theoretical tools for analysis. However, I believe there is a lot of scope to build on this work (relaxing assumptions, choosing the right model family and loss functions) to get a more convincing publication. 	2
This paper is theoretical rigorous and has novel technical contributions.	3
The paper tackles a challenging and important problem. While the experimental results seem to match the theoretical predictions, there are potential significant flaws in the proofs of the main theoretical results. The paper would have been a pretty strong paper, but due to these potential errors, I unfortunatley cannot recommend acceptance.	3
"This is an impressive, technically solid and well-written paper with some intriguing results. There are only very minor issues. 
The paper could perhaps be made even better by adding further reader-friendly discussions of some of the related works to make it appeal to a wider audience, as well as perhaps by discussing the main results and their implications in more details. 
"	4
Without the above concerns addressed, this paper could have major drawbacks. Thus, for the time being, I'm recommending rejection.	3
This paper provides a novel perspective of combining uncertainty and representativeness criteria together in deep active learning. However, the performance gain compared with baseline models is marginal.	4
Based on the main review above, I vote for a weak rejection of the current version.	3
"Active learning is helpful to acquire labelled data for supervised learning. Formulating the batch active learning as sparse approximation problem is interesting.

However, the reviewer has three main concerns: (i) while the maths looks correct, there are a lot of approximations used in finding the solution. These approximations may affect the overall performance and hinder the applicability of the paper. (ii) the empirical performance is minimal comparing to the random baseline. (iii) missing the key comparison to Batch BALD [1].
"	3
The paper is well written and explored. The empirical analysis is reasonable. Overall, I found the paper quite interesting.	3
I tend to vote for rejection because it is unclear to me why truncating the latent distribution could necessarily improve the dequantization quality. Although the authors provide two explanations in the introduction, they are not very well justified. Apart from the truncation technique, the proposed model doesn't seem to differ too much from existing approaches.	2
While this paper proposes a simple idea for dequantization, I am not convinced the proposed method is needed in the first place, and would need to see much stronger empirical evidence to be convinced otherwise.	3
The results in this paper are good, although the method itself is not quite novel.	3
This paper studies two important problems in the computer-aided organic chemistry and proposes a graph-to-sequence architecture called Graph2SMILES. However, the empirical results do not show a superior performance of Graph2SMILES to existing methods, and the technical contribution is incremental.	2
The proposed method is similar to NERF as proposed by Bi et al., so the technical novelty is limited. Also, the experiment study missed important comparison baselines. Furthermore, a more comprehensive ablation study is needed since several engineering techniques are employed, and it is difficult to tell where the performance improvement is really coming from when compared to the Transformer vanilla model.	2
"Overall, I think the authors' proposed model for reaction prediction makes sense. However, as mentioned above, the paper's writing could be improved, the technical contribution is limited, and the experiments also show only limited improvements. Altogether, I therefore suggest to reject the paper at the current moment. I am happy to adjust my score in case I missed critical parts.
"	2
Interesting basic idea, but incomplete analysis to substantiate the paper’s claims about how this might work in practice with animal data.	2
"I recommend acceptance because the novel demonstration that behavioural differences under different curricula can be used as a signal to compare candidate loss functions is valuable. However, the demonstration seems somewhat narrow in scope and it's not clear how useful it would be in practice, so my recommendation is not a strong one.
**********************
EDIT: updated my score from 6 to 8 based on author response and changes to the paper"	3
"The idea of using the curriculum learning time as a criterion to disambiguate learning processes is novel and the authors have done a good job in clearly demonstrating that using a wise choice of tasks and curricula. However, the results are not conveyed to the reader correctly and can mislead the readers if presented in this form. Therefore, I encourage the authors to put some effort into improving the plots and correcting the mistakes. 

Due to the above reasons, I have given it a rating of 6 but I believe the ratings can be increased if the authors address the weaknesses."	3
"Though the focus of this work is mostly on computational neuroscience, the research scope might be broad enough to be of partial interest to the audience at ICLR. However, the research has some weak points, and the conclusions are supported by a single experimental setup, thereby undermining the overall strength of the paper.

------
After revision, I update my score from 5/10 to 6/10."	2
"The ideas of using intra-class mixup to increase angular separability and adding angular margin to the OoD scores are interesting.  

However, the presentation can be improved.



"	3
The paper proposes an interesting idea of using intra-class mixup to improve separability between in- and out-of-distribution data. The paper also shows that cos(\theta) can be a useful measurement for OOD detection, and can be added to other OOD measurements like MSP and ODIN. It would be great if more results can be provided	3
"This work proposes intra-class mixup coupled with an angular mesaure for OOD detection. The idea of an angle-based measure has been explored in the past (not mentioned in the work) and it is interesting to propose it in the context of deep nets.

The work, however, misses some key elements in the presentation of the method and has a weak experimental setup."	3
I recommend to reject this paper. The theoretical contribution is marginal and mostly unmotivated and the empirical results only compare to relatively weak baselines. On top of this, the paper clearly lacks polish. 	2
"The paper presents a convincing analysis of replay sampling in actor-critic algorithms with theoretical results applicable to modern deep RL algorithms. I think it would be a nice addition to the literature.

"	4
I think this paper is interesting and careful studies of existing algorithms that work well in practice is required. However, this work needs more analysis (either experimentally or theoretical justifications) for it to be ready for acceptance. There is only one theory result (Theorem 1) which characterizes the off-policy actor-critic algorithms. However, this result statement is itself not novel and is derived from a vast majority of similar results in RL literature. In practice, I expected a lot more ablation studies characterizing the bias-variance trade-off. The paper perhaps has more claims than what it could demonstrate - and clearly needs more work for it to be ready for acceptance. I would encourage the authors to pick some simpler tasks and demonstrate the bias/variance analyais on some simple mdps too - instead of only showing performance curves for different sampling schemes on some standard mujoco tasks (which often is hard to interpret and not clear how these results are supporting the advertised claim)	2
"Overall, I think the paper is lacking in a few aspects.

1. Theory is not presented in a very clear way. I think the theory results are not very convincing, in that it does not reconcile with certain intuitions. However, I might be missing something here and am curious to hear what the authors say.

2. Theory does not connect with practice. A major point of the paper, to my understanding, is that it entails a potentially theoretically sound framework to explain replay practices. However, I don't think the authors have established a convincing case here -- in understanding the practice, the theory here does not provide much more information than plain intuitions we already have.

"	2
The presentation and motivation of the work is not clear	2
"I do like the direction of research, as it tries to take inspiration from biology to improve ML 
However, the way I see it, what they really have is a nice model for a really specific task but not a lot beyond that.
The paper did not convince me that their design principles would be beneficial for other tasks, 
It is also not clear what exactly the principles are that make this single task work well."	3
I think this paper has significant drawbacks that outweigh its strengths. The proposed method is very difficult to use, and although the authors do a good job explaining the single use case, the paper would greatly benefit from the study of more use cases. Despite the good quality of the writing and the clarity of the explanations, I think this paper should be rejected for the lack of usability of the proposed method, the absence of a theoretical study, and the need for a larger number of use cases.	2
Overall, the reviewer recommends not accepting the paper. Data efficiency and generalizability are interesting and critical problems for deep learning approaches to robotics. Although the work is correctly motivated, the authors failed to prove that the proposed method is general enough to be applied to a broad spectrum of robot tasks. In addition, the proposed approach shares a lot of similarities with works on central pattern generator (CPG), due to which NCAP has very limited novelty. More detailed issues with the paper are listed in the cons in main review.	2
"This paper is interesting but does not proposes a significant improvement to the literature as the gap between the promises made in the motivation and actually delivered work is too wide. 

From a neuroscience point of view, this work does not provide substantial evidence of the importance of the model at either modeling or simulating biological neural systems. 

From a ""theoretical"" point of view, the model does not provide much advancement to the machine learning community either. "	2
"This work presents a novel approach called optimizer amalgamation that distills multiple ""teacher optimizer"" into a learned ""student"" optimizer. They compared three different approaches and proposed to use perturbation to improve stability in L2O. The result is promising compared to previous baselines, but it would be helpful if more analyses (see weakness section) can be done to understand the importance and necessity of different components and more details of the proposed approach can be released to help with reproducibility. 
"	3
I think this paper is marginally above the acceptance threshold. The algorithm proposed is well motivated for a specific task, and the authors conduct experiments with various ablations to their methods. I have some concerns listed in the questions, but overall believe this to be a good first step in tackling the problem.	3
Although the research topic is old, the proposed optimizer amalgamation method looks interesting and the experimental results verified the effectiveness of the proposed method. 	3
Generally, I think the paper is marginally below the acceptance threshold. Although optimzer amalgamation is interesting, it seems expensive to train such an optimizer based on my experience and the authors did not report any time/memory complexity. Besides, the performance is not improved obviously compared with the best analytical optimizer, and the paper lacks the detailed discussion about imitation learning.	2
"- The writing of the paper should be significantly improved.
- The motivation is not well stated in a good tale and the proposed model cannot address the issues stated in the motivation.
- Many statements are too subjective and are not be well justified."	2
This paper has no novelty.	1
"Questions/ Clarification/ Suggestions:
1.  I understand that the time complexity is comparable with GCN, however, I am still curious to know about training time (per epoch) for al the proposed models. 
2. Open Graph benchmark (https://ogb.stanford.edu/docs/leader_nodeprop/) provides a wide variety of tasks and associated datasets that are more interesting than these benchmarks. I would encourage the authors to apply their model to some of them.
3. Why does the performance degrades significantly when a nonlinear layer after each propagation layer is added? The authors have never addressed that in the paper. I think pressing a bit more along that line might lead to a better interpretation of the message passing of the model.

I would be happy to increase my score if some of the above-mentioned concerns are properly addressed."	3
"This paper provides a simple and intuitive way to carry out graph learning based on the tree represnetaion of graph. And the performance of the presented models looks convincing. Source code is give for evluation. But I still have the following concerns:
1. What are the major contributions of the paper? 
2. The time complexxity should include the transfering time from graph to tree structure? Especially for the large scele graph data? "	2
This paper studies an interesting topic. It will be great that the method is also evaluated with other SOTA standard adversarial training methods.	3
Through experiments and theoretical analysis, this paper concludes that hard adversarial samples will lead to overfitting. However, the introduced algorithm is not novel enough, which is very similar to the existing self-adaptive training. Even in the experiments, the corresponding experiments are also missing. Based on these, I recommend ''marginally below the acceptance threshold''.	2
Overall, this paper investigates the impact of hard instance on adversarial training. However, I am not convinced of the novelty and the correctness of the proposed method.	2
Overall I think this is an interesting work, and its quality can be further improved by answering the questions listed above.	3
The proposed idea is concise and interesting. However, it needs stronger justification to show that ABEL has advantage over other learning rate scheduling methods and the authors could provide a deeper study about the behavior of weight norm bouncing from a perspective of SGD dynamics.	2
Linking the weight norm to the learning rate is a novel and interesting take on the topic of automatically tuning learning rate schedules. However, at the moment the work seems almost entirely heuristic driven. The paper needs more work before being ready for publication, both in terms of writing and exposing ideas and in terms of experiments, especially regarding the main assumptions of ABEL.	2
Finding better learning rate schedulers, either in the metric of performance or robustness, is an important aspect of training deep networks. The suggestion by the authors to autotune the learning rate schedule by an internal property of the DNN (namely the bouncing feature of the weight norms) is interesting as well as the conjecture that such oscillations of the weight norm are a necessary condition for complex learning schedules. In addition, the equation governing this bouncing behaviour is simple enough to attract further theoretical interest. The scope of the experiments is also reasonable in my mind, although further comparison with other adaptive algorithms could be useful. 	3
I think this paper presents a strong argument for benchmarks in multi-objective problems. More discussion that ties the benchmark and the evaluation of methods back to various practical considerations (e.g. feasibility of sampling in the objective space, desirability of Pareto density vs. hypervolume) would make this paper even stronger.  	2
"This work aims at finding the Pareto front for a multi-objective optimization problem, which is important for many applications. However, there are many major concerns on both the method (verification rather than optimization, the reason for model building, questionable claims) and experiments (suitable comparison, figures from other works) that make this work unacceptable in the current form.

============================= post rebuttal response ============================= 

I have read the other reviewers' comments with the authors' feedback, but still think the proposed verification method (FJC discriminator) should not be directly compared with other optimization methods (not limited to MTL). Therefore, I keep my initial score (3) and lean toward rejection."	2
Overall, I believe the novelty of the proposed algorithms is marginal and the clarity of the paper is very low at this stage. 	2
Some questions were raised during the review process.	2
Paper proposes a framework and uses spherical coordinates over encoding method to help with geo-aware image classification. The paper propose the title of sphere2vec for novelty but the main improvements to the method is not with the encoding but with the framework itself. For readers, it would be a slightly disappointing results if the reader is reviewing spherical coordinate as an approach. As a paper, the results does show improvements through the framework, in the form of representation learning. Thus, the recommendation is a marginally below acceptance threshold.	1
I don't think this paper is suitable in its current state. The experimental results show a small effect size and it's unclear how stable the results are to changes in the MLP, and other components. There is also a weak connection between some of the stated benefits (preserving spherical distances) and the actual benefits. It's entirely possible that the proposed methods are better in some settings for some other reason.	2
"  This paper explores two important problems: 1) given a geotagged
  image, how to encode location information and incorporate it as
  context in existing image processing pipelines, and 2) how to take
  advantage of a large corpus of images with known geolocation, that
  are not annotated (unlabeled) for the primary task. Specifically,
  this paper explores how to encode geospatial information in the form
  of spherical coordinates.

  For several tasks in geo-aware image classification, the
  proposed multi-scale spherical location encoder outperforms
  baselines. However, the proposed self-supervised method for
  incorporating unlabeled geotagged imagery is limited to a pretraining
  strategy when there is limited labeled data. Additionally, several
  variants of the method are proposed, and it is not clear what variant
  to use when.

  Ultimately, I think this paper tackles an important problem and
  demonstrates the general applicability of the method. While the
  manuscript has minor errors throughout and some missing details, I
  lean towards accept for my initial rating."	3
While I like this work and it proposes an interesting positional encoding method for spherical coordinates, I would like to see the authors' reply for the weaknesses to make further decisions.	2
I vote for rejection since I don't think the technical novelty and empirical usefulness of the results have reached the level of ICLR paper.	2
Although this paper explores an interesting problem, the proposed method is almost infeasible and the contributions are really limited. As a result, I don't think this paper achieves the requirements of ICLR.	2
"In summary, the paper introduces a new type of adversarial attacks, based on NTKs and exploiting the transferability of adversarial samples between models. The authors clearly motivate the relevance of their work and chose a suitable line of argumentation consisting of theoretical as well as empirical evidence. However, this reviewer is missing some rigor in the theoretical underpinning as well as the experimental ablation. The work leaves open a few key questions about the validity of arguments, specifically the connection between the NTK limit and the models far away from the NTK limit,  and as of this point shows anecdotal evidence of the method being generally applicable. This is heightened by the fact that the authors are looking at very special datasets as well, MNIST being small and relatively easy and CIFAR-10 being small and complex. It is unclear how these attacks would fare on larger, production-scale models, trained on, e.g., Imagenet. Specific points to improve upon are:
1. Solidify the theoretical underpinning by providing intuitions and argumentations, possibly proofs, why the second order contributions are negligible
2. Show that their experimental setup indeed can reproduce the analytical kernel by doing ablations with respect to the sample size and show that the kernel tracks towards the limit and at what cost.
3. Addressing points 1 and 2 can then give some understanding of the contributions to the gap in performance between PGD and the empirical NTK-based attack curves
4. Provide some understanding / insights about the different scaling directions of table 3, i.e. why do the directions reverse order as depth increases
5. Give some intuitions / early evidence why this work would translate to test-data with pseudo-labels, as this is the most likely and relevant scenario in practice.

Based on these observations this reviewer cannot recommend the publication of this work, despite its very intriguing line of research.
"	3
"This paper's idea is interesting, but the main concerns are

1. Technical novelty compared to existing works on existing NTK-based generalization attacks. The authors did not discuss the differences at all.
2. Lacking performance comparison to state-of-the-art no-box attacks."	2
"A clear formulation and correct analysis of the threat model is one of the most important things in this kind of work,
and this work is somehow very confusing in this regard. Even though it has other merits, I'd recommend reject."	2
The algorithm is interesting and practical for is extremely small memory footprint, but organizational problems prevent a higher score.	2
Strong results in terms of memory efficiency, which do justify consideration for acceptance in my view; however, the paper could improve in terms of clarity of exposing experiment setup details and choices of hyper-parameters, description of the main methodology, and having stronger application use cases.	3
Without runtime listed in this paper, although this algorithm is memory optimal, it is not good enough. Besides, using linear optimizer to solve the truncated SVD seems not a novel idea.	2
While the paper proposes an interesting algorithm, in the end I did feel much void in the experimental section. Perhaps adding more dimensions in terms of algorithm comparison (one more algorithm in addition to sketch SVD) / adding more datasets could help. In addition, the authors could clarify how trained neural network can be adapted to not just one dataset but a dataset that may be related (time series).	2
"
The proof of the main theorem is incorrect and hence I am rejecting the paper.

--------------
I am updating my scores based on author discussion. Please refer to comments for details."	3
"On the positive side, I think the paper provides a nice and natural extension of MCMC methods in discrete spaces based on locally informed proposals. 

On the negative side, I think that some of the theory could be presented more clearly, some of the claims revisited, and the empirical evaluation could be modified a bit to better show the benefits/drawbacks of each of the method's components.

-----

I updated my score after the authors' response."	3
"**Initial recommendation**

The paper is well written and technically sound. I went through the proofs and confirm their validity.
The paper represents an added value to the existing body of knowledge on MCMC, especially in the discrete domain. However, I have some questions for the authors, which might affect the final result. I initially recommend for a weak accept.

**Final recommendation**

The authors have addressed all concerns during the rebuttal. Therefore, I increase my score."	3
This work presents a simple extension over prior state-of-the-art discrete MCMC methods which the authors demonstrate leads to improved performance on a wide variety of tasks. The method is simple, easy to implement, and appears correct. The authors provide the necessary theory for understanding the method and provide sufficient evidence for the performance of the method. There are some minor issues with the presentation of this evidence that should be addressed, particularly in the EBM training section. I would support acceptance of this work.	3
"The paper shows a closeness between contrastive learning and deep metric learning. They decomposed contrastive loss objectives into two quantities evaluating the geometry of the learned representation space:
1. Alignment, and,
2. Uniformity. However, the paper misses producing few ablations such as considering only positive, only contrastive, etc. How we mine negative samples is not very clear. Overall the paper seems like an extension of [1].


Ref:
-----------------------------------------------------
1. Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere, ICML 2020"	3
This paper studies an interesting problem. Although the proposed method seems naive and incremental, its effectiveness and the significance of the topic really make up for that. Overall, I vote for weak accept, but may increase my rating based on the rebuttal.	2
The paper has marginal contributions and limited novelty.	2
This work extends known properties of contrastive losses to the triplet loss. Namely, the loss can be rewritten as the sum of two terms: a intra-class alignment term and a hypersphere uniformity term. Based on this, a novel regularization term was devised for training deep metric learning models based on adversarial attacks on each one of the two terms. The three proposed variants of the ADML model acheive convincing performance gains across different image retrieval benchmarks and are more robust to subsequent adversarial attacks. However, it is unclear whether gain in performance is due to adversarially perturbing the intra-class alignment and the hypersphere uniformity terms separately or due to InfoNCE having more informative gradients compared to the triplet loss.	3
In general, the paper is well written, and the algorithm presentation is clear. The experimental evaluation is, in my opinion, insufficient if we interpret CLIMB as an imbalanced learning method (see the second paragraph of the review). For instance, more different ways of distributing data among the clients should be investigated. 	3
Although reducing the class imbalance in FL is an interesting topic and the method in this paper is new, I still think their method are not well-supported by theories and experiments according to the above discussion. Therefore, I think it is marginally below the acceptance threshold and needs further improvements.	3
The paper provides good theoretical and empirical justification of a unique problem of addressing extreme local imbalance in a federated data system while maintaining data privacy. However, additional supporting material and stronger empirical support may be required to justify strong conclusions in the paper.	3
This paper proposes a new algorithm on imbalanced setting in federated learning. It enjoys the advantages of not leaking local data and no need of prior knowledge and overall the algorithm based on primal-dual optimization is easy to understand and be utilized. It is a good paper after minor revisions.	3
"The paper shows a simple yet effective way of modifying the existing GPT-3 framework by increasing only a small number of parameters and computation per epoch, which can be a potential addon to many existing Transformer architectures.
"	3
Transformers are notoriously unstable to train. Progress on this problem is of clear value to the field. The baselines could be more challenging and the analyses more convincing, but if these issues are addressed—which I am optimistic about—I think the paper could be a meaningful contribution to the field.	3
"
Overall, this paper proposes NormFormer to improve the PreLN transformer by introducing some modifications. The proposed techniques are empirically successful but not well motivated. I hope that the authors can firstly present the issue inside the PreLN transformer clearly and then provide a detailed discussion about why the proposed modifications can mitigate this issue. 
		"	3
In summary, this paper proposes to add four operations, two LayerNorms and two scaling parameters, in the Pre-LN transformer layers. The operations are simple and can help stabilize pretraining and improve downstream performance in several NLP tasks. However, I think the method novelty is not enough. Moreover, the paper doesn't provide convincing motivation why the four operations help handle the issues.	2
The authors present an interesting idea well. However, the experimentation is very limited (even for a conference paper) and the results given show a very small effect size. More data is needed to convince that the technique is useful.	2
(Copied from above) The problem of learning and optimizing over belief states is an important problem in reinforcement learning, and while the ideas in this paper are not particularly novel, the core idea is implemented and tested well. I thought that the lack of details made certain section rocky, but in general found the paper a pleasure to read. 	2
This paper has interesting ideas that seem to be useful.  Clarity could be improved but interesting enough to publish.  I wanted to give it a 7.	3
"-- After Author Response --

The paper is well-written and considers an important and challenging problem. The empirical results seem to offer a path to progress in a direction that hasn't shown much promise. The paper could be improved by offering more details about RLSearch, a recently introduced approach upon which the experiments heavily rely.

-- Original Summary -- 

The paper is well-written and considers an important and challenging problem. The empirical results seem to offer a path to progress in a direction that hasn't shown much promise. However, the algorithmic results make heavy use of an algorithm that is not sufficiently described in this paper and that has not been peer-reviewed. Though I would really like to have these results in the literature, that makes me uncomfortable with recommending acceptance."	3
I do not recommend this paper for acceptance because there are many essential unanswered questions regarding the proposed method. 	2
Overall I think this paper is interesting and has the potential to contribute theoretically to policy gradient method. There is some additional work (see my main review) that needs to be done to make this paper more competitive.	3
Please see above.	2
The quadratic convergence of the proposed quasi-newton algorithm is interesting, but the conditions under which it holds is pretty unclear. Since this is the paper's main contribution, it's a borderline submission for me at this moment. I will wait for the rebuttal to address my concerns.	2
Although the method and the proof-of-concept implementation is interesting and the analyses including the latent space are informative, the motivation of this work is not clear to me and the paper lacks sufficient evaluation to support the claims. I explain these weaknesses above. This paper looks like an unfinished study. Therefore my decision is rejection of the paper.	2
The paper presents several interesting ideas (DVS+SNN+VAE) in the shape of the proposed architecture. The novelty component I believe is justified since I’m not familiarized with similar approaches in the gesture modeling domain. However, and this is the reason for my recommendation, the evaluation of the proposal somehow only relies on the capacity of the system for modelling the input. It’s not analyzed the contribution of each of the pieces proposed and it’s not measured its performance against other approaches. The experimental setup is what I think diminishes significantly the relevance of the work for the conference audience.	2
"Overall, the proposed approach provides excellent results for gesture recognition on two benchmark datasets and goes the extra step of implementing the neural network on neuromorphic hardware. However, I have some questions regarding the benefits of the individual components of the neural network and concerns regarding the proportion of wasteful computation done by the network, as detailed under ""weaknesses"". I invite the authors to address these comments and believe that the paper would subsequently make a strong contender for acceptance."	2
"My recommendation is that the paper should not be published in its present form. Although the solution is novel and interesting and addresses a problem that is relevant to the community, the lack of comparison to other works in the state of the art, makes the work hard to assess (the comparison to only 1 work in the appendix is not enough either)
Secondly, the experimental section is not complete, and needs to be improved adding evaluation metrics and a proper discussion about them
Finally, the section about the implementation in the neuromorphic chip seems superficial and might need to be left out to focus the paper in the solution/method itself. "	3
"This paper proposes a meta-framework carrying some existing fine-tune methods. The performance looks good but there are still some issues. The technical novelty is limited.

The rebuttal solved most of my concerns. Thanks for the author's efforts!"	2
I like the paper and think that it will add good value to the literature and the conference.	2
This paper presents solid engineering work for adopting DP in NLP tasks using large transformer-based language models pretrained on public datasets. Although the result is interesting, the paper is a combination of several existing techniques and do not have enough novelty to be published as a separate research paper.	2
Overall, I am leaning towards accepting this paper given the strong results and the hope for learning large models with differential privacy. The experiment section could be strengthened with better comparison to baseline methods.	3
"The paper is well-organized and has comprehensive experiments to defense its method. However, some statements and equations are not clear enough. The illustrations and figures are not meticulous. I hope the authors can further polish the paper in detail.
"	2
I appreciate this paper in terms of applying AWP to graph neural networks and showing how it needs to be adapted to work well. Methodological contributions are small, however, and improvements vary across datasets and cases. 	2
This is an interesting line of work, and the pointing out of (and subsequent fixing of) the gradient-vanishing phenomenon in AWP is a valuable contribution. Past this, none of the results are strikingly novel or groundbreaking. I think this is a borderline paper, tending towards rejection, but could be convinced to boost my score slightly if I've misunderstood something. 	2
This is work i) is quite incremental compared to the existing works, ii) does not explain/justify/describe the algorithms well. I do not think this paper is good enough for ICLR.	2
The paper tackles an emerging problem (fairness under distributional shifts) and proposes a new objective and algorithm. While both the metric and the algorithm seem novel, I am not entirely convinced about the methodological contribution overall, given that I am not familiar with the general literature on robustness against distributional shifts. I also have concerns about the value of the experimental evaluation as explained in my comments above.	3
"I do not understand why making the prediction model robust results in fairness robustness.
For this reason, I recommend ""rejection"". However, it the authors explain successively why and how the two concepts - prediction robustness and fairness robustness, are related and how the proposed method utilizes this relation. Instead of intuitive explanations, some equations with rigorous definitions
would be helpful for understanding the value of the proposed method."	2
Limited contribution, inappropriate related work and format violations. 	2
"The ability for fairness properties to generalize is an important problem tackled by this paper. This paper makes a couple novel contributions: 1) a fairness metric that captures robustness to distributional shifts; 2)  learning method to achieve this notion of “out-of-distribution” fairness as well as standard in-distribution fairness. The main technical areas of improvement are 1) include analysis on how well this method works for real-world (as opposed to simulated) distributional shifts and 2) provide error bars to support the empirical claims. I would also encourage the authors to exercise more care/caution around a task like predicting ""attractiveness""."	4
The problem of extracting factual and consistent information from large language models is of high interest to the NLP community, and this work in particular should be of interest to the ICLR community. Overall, this work was well-written throughout (easy to follow in most places except for a few rough parts detailed above). The experimentation work was also of high quality, with interesting results. To highlight a few findings: (1) the use of a relation-classification MoE and its consistently high performance on consistency metric seems promising, (2) the analysis demonstrating the importance of the “subject” is correct fact prediction, and (3) analysis demonstrating the negatives effects of uniformizing objects in train/test sets, which is strong indication that LLMs still do not generalize well to unseen objects.	3
"The obtained results and model itself will benefit the reader and researchers working in this important research area.

"	3
"The paper proposes methods for mapping natural language prompts to continuous prompts, with the goal of improving accuracy and consistency on a factual probing benchmark. The problem is interesting and the method appears to work on this benchmark, but I there are three main changes I would like to see before recommending this paper for acceptance, possibly at a future conference:
1. Drawing a connection to prior work on robust optimization, and providing a clearer formal justification for why this method will improve consistency.
2. Providing more detailed empirical results and discussion (in particular relation classification accuracy). The current results are hard to interpret because models can be consistent but inaccurate, and because models can perform well by over-fitting the entity distribution.
3. Ideally applying the method to a wider range of prompting tasks, to show whether the results will extend beyond the particular setting of factual probing.
"	2
The proposed method is reasonable and analyses of experiments are well described.	3
"I cannot recommend the acceptance of the paper in its present state. There are two main reasons for this. The first reason is the lack of explicitly stated  (unified) research question. Without it, it is hard to tell if the authors have conducted enough experiments to support their research question. I did my best to identify these research questions in the paper and then try to link them to the experiment they conducted. However, I had a hard time doing this. Sometimes motivation given by the authors can be ambiguous. For example, do you use adapters just for efficiency or you are interested in exploring how one can address the inconsistency problem with the adaptors?   The second reason is the lack of experiments with the frozen LLM parameters of MoE. Depending on the outcome of the experiment the conclusions that are made in the paper may change. For example: “While MoE models usually perform better, the small increase comes at the cost of requiring relation annotations at training time and specifying the subject of the entity at inference time.”

However, if those two issues are addressed, in my opinion, the paper has a good chance to be accepted. Reduction of  supervision data that is needed to tackle the inconsistency problem has both academic interest and also can be of the great interest to practitioners. Moreover, adapters are presently has high research interest in the NLP community - demonstrating how they can be used to address the inconsistency problem will also be valuable to the community.  "	3
This paper has interesting ideas but is lacking in how it anchors its contribution to related work and compares against it. More glaringly, this papers suffers from substantial lack of clarity both in terms of its theoretical grounding, and more critically, in terms of how it presents its core framework, how it is designed, and how it is applied. I believe it should be revised substantially before it is suitable for publication at a venue such as ICLR.	3
"My starting recommendation is: weak reject.

I hope the authors can use the rebuttal period to correct any misunderstandings that may contribute to my lower score.

However, assuming I have understood things accurately, I am primarily concerned with the notion that InfoPG is a genuinely decentralized approach. It seems to me that the policy is centralized because of how the communication policy works. I wonder how much the benefit of InfoPG simply comes from the fact that each agent naturally gets more information about the state of the world. More generally, I worry that the work misrepresents itself, using phrases like ""communication"" and ""*k*-level reasoning"" where they may not be appropriate. I am interested in how the authors defend these choices and characterizations.

I also cannot escape the sense that the MI lens is somewhat *post hoc.* It may be possible to show that other MARL algorithms implicitly target MI, if it is indeed a corollary of coordination. Bottom line: a lot of the paper is spent on the theoretical connections between InfoPG and an abstract metric; I think that space would have been better spent on empirical analysis. Again, if I have overlooked the importance of (this form of) MI in these task settings, I hope the authors will correct me."	2
In general the paper is innovative and solid, there are several issues as in the main review section that need to be addressed.	4
Considering the above mentioned points, I believe that this paper is below the bar. 	2
"Combating memorization is very important to improve FL personalization performance, the proposed method is promising. 
However, some experiments on real-world experiments are missing to support the claim. 
Moreover, the linkage between meta learning work (Yin et al) and the proposed work could be better addressed.
Finally, the negative effects of combating memorization is missing, which could further strengthen the paper. 

Overall, I give score 6, and happy to conduct discussion with the authors and other reviewers. "	3
Given the main techniques is borrow from existing literature, I have concerns about original contribution. Besides, the writing is not in good shape and needs major revision. I would suggest the paper for further improvement rather than accepting it at current state.	2
I like the idea of this paper, and the experiments also convinced me.	3
"The paper addresses an important problem and proposes a novel and effective solution that is likely to be used and built upon.  ""Do not exist in prior works"" is a strong statement; there is certainly work this paper builds upon, but, if one draws the line around the ""contributions"" tightly enough, I cannot point to specific prior work in which ""aspects of the contributions exist""."	4
"- I think this is a good paper. There are some issues that need to be addressed but not enough to not recommend it for acceptance.
- The central idea is a good one. I would say this is an incremental improvement to existing work, but has the potential to reduce the action space that agents have to explore and potentially reduce learning times. "	3
Overall, the approach is interesting, and valid, and significant. There are some unclarities about particular aspects (see review), and I think that the positioning can be done better, but my feeling is that they do not hinder acceptance too much.	3
This paper proposed a new idea for reducing communication costs. But the theoretical analysis has some flaws. 	2
This paper proposes an interesting layer-wise model aggregation method to reduce communication cost while trying to keep the same performance with FedAvg. However, the proposed method’s design, theoretical analysis and experiments cannot support its claim.	2
Thanks to the authors for their efforts for their theoretical analysis and experimentally proof of the proposed federated optimization scheme. The experimental results are a little weak to show the effectiveness and the practicality. 	2
"The idea of utilizing layer-wise model freezing for federated learning is an interesting idea, the paper includes reasonable empirical study and convergence analysis to illustrate the effectiveness of this proposed approach. 
Although some part of the cost model in the simulation is a little sloppy, I tend to advocate the acceptance of this paper. "	3
In summary, this paper provides a solution based on sparse tensor cores (specific to NIVIDIA A100) to optimize and accelerate attention operations. Although it yields good performance and nice attention speedup, its practical value on Transformer efficiency is unclear and its generalization to other devices is also unclear (at least, we shouldn’t get slow running on other GPUs or CPUs).	3
This paper presents an efficient self-attention mechanism and achieves speedup. But the analysis and experiments have some issues (see weakness). I will give a borderline due to those concerns and change it accordingly based on rebuttal.	2
The paper showcases speed-ups for a large range of sequences as well as the ability to do both training from scratch and finetuning with no loss in performance. Moreover its compatibility with other efficient attention mechanisms makes it a good choice for a wide range of applications.	3
The paper presents dynamic sparse attention to accelerate the attention module. I give a weak acceptance due to the weakness, I may raise my score according to rebuttal.	3
The paper shows good improvements on non-autoregressive translation without distilling from trained models, however, it is not clear why that is something to strive towards. The approach uses several heuristics and it is again not clear how sensitive this approach is to their values.	3
This paper proposes a new state-of-the-art performance in non-autoregressive translation without the help of an autoregressive teacher for training. However, some concerns about the experiment still need to be addressed to understand the contribution better.	3
The paper studies NAR generation, which is an interesting topic. The proposed method achieves good results, but the flaws on contribution/evaluation prevent me from giving a higher score.	2
The proposed model is interesting, reasonable, and well-motivated. It has promising performance in ablation studies. Nevertheless, the evaluation part has clear flaws and the paper itself is sometimes hard to follow.	4
This paper lacks evidences to show its SOTA performances and lacks important references and ablation study. I suggest authors to improve from the above points.	2
"The paper proposes a new interesting setting for discovering new loss functions based on a predefined task, and extensively evaluates one ""proof-of-concept"" loss function derived with this technique.

My main concern is that the authors occassionally oversell their results, along with some issues related to this, e.g. a potential inconsistency in the hypothesis testing done for Table 2 (see above). Realistically, it seems that the method works for a few of the settings, but not all of them---which I think is totally fine, especially in the light of the results in the DomainBed paper, that showed that a lot of methods proposed over the past years actually fail to meaningfully outperform ERM.

The authors should also critically examine their full results again, adapt the language in the paper to be more realistic about the actual merit of the method and the improvements that can be expected. This should be easy to fix by revising the language in the result and conclusion section a bit, and adding a short discussion of the failure cases.

I assign a score of (5) for now, but would be very happy to increase it --- based on the results already in the paper, I think the paper could become an interesting contribution to ICLR, if presented correctly.

---

**Post-Rebuttal comments:**

The authors sufficiently addressed my concerns, and I decided to increase my score to (6), as I think that this work might be an interesting contribution in the space of domain generalization, first and foremost for putting forward the idea of learning loss functions based on an auxilary domain generalization task itself, which to the best of my knowledge this paper provides the first proof-of-concept for.

For the camera ready, I would however urge the authors to carefully re-check the paper again, and tune down claims that are not sufficiently supported by the presented data. I would also consider to remove the statistical analysis (but keep the average ranks--these are informative), cf. our ongoing discussion, I think in the current form it does not add much information to the paper. If the authors include it, the performed test should be well justified in the supplement, which is currently not the case. I would also recommend to run a test applicable to multiple comparisons, and then perform a suitable post-hoc test, instead of the current practice of testing all models with a Wilcoxon test. It should be highlighted that especially for CORAL/SagNet vs. the proposed method, the gains are rather small in some cases.

If the authors add a bit more critical discussion on their method to the paper, I would hence recommend to accept the paper."	4
This paper adopts the idea of NAS or autoML to search for the proper loss in the domain generalization. Overall the idea sounds novel in the domain generalization while the current versions lack clear justifications (in terms of theory). Based on these, I do not recommend accepting the current version but encourage a major revision for the resubmission.	3
The proposal is novel to my knowledge and interesting. It can also be directly combined with other approaches to further improve performance. The execution is robust and covers a number of well-known datasets; comparisons include recent alternative methods. Results include significance tests.	3
The approach introduces relabelling (which has already shown to be important in multi-task RL) in meta-RL, and shows superior performance on sparse reward environments. The paper would be more compelling if it included evaluations on more challenging environments, to establish the importance of the adaptation component. 	2
"The paper presents an interesting idea about reusing data across tasks in meta-RL. The idea is very intuitive and the paper is well written. However, I’m not sure about whether the approach used to implement the idea of the paper really does what the authors claim it does. Therefore I’d like to see more evidence before I can recommend accepting the paper.
"	3
"The work is well-motivated intuitively, but the mathematical justification for the specific method is difficult to follow (so I cannot quickly verify its soundness).
The empirical study is well done overall, so I lean to accept the paper but would likely increase my score and confidence if the authors can clarify the theoretical motivation for their relabeling strategy."	2
The idea in the paper is well presented and carefully investigated. The proposed method is simple and effective. However, I am not quite convinced about the novelty of the proposed idea and I think the experimental settings can be improved to strengthen the paper’s claim.	3
The paper proposes a simple and sensible method for  multi-domain self-supervised learning that obtains good performance on for both in-domain and out-of-domain problems. Comparison with baselines and experiments with full ImageNet datasets is missing. Overall, I think the paper is marginally above the acceptance threshold. 	2
"
The paper is clean and shows some improvements, but there is not a clear theoretical argument. Furthermore, the paper is limited by using tiny datasets which limit how well we would expect these results to generalize in practice. 
"	2
"Pros: Simple approach and good results for building a unified model for multi-domain data.

Cons: The formulation and motivation may need further clarification. "	2
See weaknesses.	2
While the authors present new state-of-the-art results, there aren't many insights besides the fact that albert-xxlarge-v2 is a good zero-shot model. The authors could design more experiments backing up their main claims (e.g., about compositionality).	1
"The analysis of different language models in a zero-shot Winograd setting is an interesting area and the paper reports some strong results for the Albert model.

The paper appears to be more of an opinion piece and lacks a clear novel contribution. 
An existing pre-trained language model (Albert) is evaluated on existing datasets (Winograd, Winogrande and Timedial) using well-established methods (PLL, NormPLL).
The finding that Albert outperforms more parameter-rich models like XLM or RoBERTa can be considered unintuitive, but the analysis in the paper is not sufficient to demonstrate the cause of this performance difference."	2
While the paper makes inroads in a few different interesting directions (presenting an alternative adversarial WSC-style question format, highlighting advantages of pretrained zero-shot models, achieving SOTA on a recently-released dataset), the connections between the ideas are tenuous, and the individual contributions are not sufficiently impressive to stand on their own. I encourage the authors to pick just one or two of these ideas, and flesh them out in a more thorough, convincing manner. In its present form, I believe this paper should be rejected.	2
The main contribution of this paper is not sufficiently well-motivated or general; the re-implementation of DQN with more parallelism does not pose enough independent interest to be practically useful for most researchers. Suggestions for improvement would be to (1) show that this method could actually extend to settings beyond DQN and improve other RL algorithms; (2) perform a more in-depth hardware study comparing the cost to speedup via more powerful hardware vs. software improvements.	2
"The direction of the manuscript is commendable, and the ideas seem simple and cleverly executed. However there remain doubts wrt. novelty and on the validity of the experimental settings. Furthermore, the manuscript could also be significantly improved in terms of clarity.

---

EDIT: bumped score to 6."	2
The method proposed is not super innovative or creative but it's something that is needed and is well executed. It seems to do what it strived to do (speed-up) and there are no issues with the paper.	2
The paper proposes an acceleration framework for DRL based on synchronized execution of GPU. The performance of the framework is attracting. However, instead of the sole showcase, it is more important to see how the proposed framework can generalize to more DRL tasks.	2
"The paper aims for a reasonable goal and is both well written and well-motivated. However, the introduced methods are either well-known or incremental variations of known methods. Additionally, the empirical evaluation could be improved by using stronger baselines.
"	1
Based on the above comments, I currently recommend rejecting this paper. The novelty of the work lies in incremental changes to existing methods, and I think the modifications are substantial. However, the experiments, in my opinion, do not demonstrate well the impact of these changes.	2
 Overall it is a well-written paper with good motivation, appropriate method, and supportive experiments. The overall framework is novel, however, it relies on many existing methods.	2
In general, I do not see too much theoretical or technical contribution. I would request the authors to carefully position their work with the related work mentioned above. 	2
"My recommendation is weak accept.

I like the idea of a generative model of fragments, and the analysis on its benefit in Section 6. However, I am not very convinced by the RL component, because of three concerns raised above. Therefore, I can't aggressively recommend to accept this paper.

My humble suggestion is to set the generative model of fragments as the main contribution and the RL components as minor ones (or even re-using the existing RL component), because the generative model of fragments itself seems to be novel enough (at least to me) to be presented to the community."	3
"Data-efficiency of RL algorithms is an important research area, and this paper explores how auxiliary self-supervised learning can improve data-efficiency in continuous control domains. The paper is very rich with experimental details, the implementation choices are carefully ablated and the paper is overall well written and explained. 
"	2
The main idea behind the paper is not novel, but the implementation is, and the results are promising. Some claims are not founded and somehow contradictory, and some related works are missing. But overall an interesting contribution!	2
An interesting direct application of BYOL to RL. However, the necessity to include action-conditioned transition models in RL raises additional complications compared to BYOL which have not yet been addressed (or discussed) and I believe these should be included in the paper before publication. 	3
Overall, this paper lacks sufficient novelty for acceptance. It recombines existing techniques in a slightly different way than previously and shows improvements on a very small and narrow set of environments without comparing to the most relevant related work.	2
"Scores:

I suggest marginal rejection (5/10). KSL indeed show state-of-the-art performance on the presented tasks and I like the way the authors assessed the quality of representation learning. However, KSL seems like a combination of existing methods, but lacks comprehensive empirical evaluation in that sense. Moreover, the high similarity with SPR is concerning without further clarification and empirical justification. Some arguments of the improvement learned representation is over-stated.
"	2
The paper proposes a low-rank data model and studies the double-descent phenomenon for denoising tasks. The main theorem presents an asymptotical formula for the risk for rank 1 data, and also approximate formulas for the general rank cases are presented. The theory is presented for the linear case, whereas the experiments cover also neural networks and MNIST. 	3
This paper extends double descent (wrt data size) to a low-rank denoising setting with linear regression. While the result is novel and has some interesting aspects, many aspects of the problem and the results are not clearly explained, and the writing overall needs significant improvement.	3
"**Strengths:**

The paper shows a novel result: the bias-variance decomposition of the rank-1 linear denoising autoencoder. This introduces some interesting new concepts, such as optimal training SNR as compared to test SNR. The paper is clearly written and is easy to understand. In addition, the authors find an approximation to the bias-variance decomposition for bottlenecks of larger size than 1.

**Weaknesses:**

The others equate adding noise and regularizing the model, which is not equivalent if the noise variance is too large. If they want to make statements about regularization, then they should explicitly regularize and see what happens. If the results are similar then even that is convincing, even if it is empirical. Moreover, some of their figures are hard to read and interpret. Finally, the authors leave out most of the details regarding the experiments and completely omit the conclusion.

Overall, I based my score on the fact that the paper is incomplete and rough, not because the work has major problems. This is very interesting work and should be polished before next submission."	4
This paper is a good paper with novel results in the denoising setup, yet some places can be made clearer.	2
The authors show that a multiscale VAE can produce good unsupervised denoising results, provide meaningful analysis, and show that their technique can be adapted to remove structured artefacts. I think this is a meaningful contribution, and therefore I recommend publication.	3
"This work advances the state-of-the-art in terms of use-case novelty and performance improvement. Henceforth, the contributions are significant enough for the acceptance
"	3
The reviewed paper makes claims that appear to be correct, supported by empirical evidence, theoretical insights, and prior literature. The proposed method is based on existing technology but extends in a number of ways, which makes the novelty of the paper significant. The potential impact of this research seems to be substantial, provided that denoising is an important problem in many applications related to computer vision. Overall, this is sound work, which seems to significantly improve the current state-of-the-art.	3
This paper improves  Prakash et al.(2021). The reported facts that the hierarchical representation of the latent variables improve the performance and that the artifacts are encoded at the layers near the bottom would have worth sharing. As for the artifact removal, on the other hand, it is not so clear how effective the proposed method is for variety of images and hence the reviewer downgrades the rating.	3
Se main review	3
This work proposes to add parameters of low-rank matrices into pretrained model, which are tuned for specific downstream task. The method is effective under different types of pretrained models and tasks, and benefits the deployment of large-scale models. My overall recommendation to this paper is accept.	2
"Justification:

I would say that the idea is relevant for achieving efficient pre-trained models that will be useful for many practical applications where latency is crucial. The paper is also well-written, with numerous experiments to support the claim. Therefore, I think the claim is supported with empirical results; however, the paper's novelty is limited. The time efficiency still needs to be supported by training and inference time information. The approach is incremental, very similar to Adapters; adding small trainable parameters to the model.

Typographical errors:
- comtenporary => contemporary

References related to Matrix decomposition by design (please consider to add):
- Compressing Pre-trained Language Models by Matrix Decomposition
- Greenformer: Factorization toolkit for efficient deep neural networks
- Factorization Tricks For LSTM Networks
- Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank Transformer

Questions:
- What is the optimal rank for the factorized matrices? I was wondering to know if you have findings on choosing the optimal rank in the experiment.

I would be happy to increase my score if the authors address my concerns."	2
The authors present an efficient fine-tuning method for LMs that freeze the original model and tune only an additional 0.01% of parameters. This method is efficient both in training and inference, and achieve comparable or better performance than full-model fine-tuning. 	3
This is a well-written paper trying to address an important problem. However, as I've expressed in the main review, some of the claims are unsupported and hence my initial assessment. I am willing to update my score if the authors are able to provide a convincing response!	2
"The paper is a very interesting read. It provides an interesting and insightful analysis of model preferences for various visual cues. The analysis shows the preference of visual models towards low complexity visual cues, such as color and ethnicity. Given the potential impact of the analysis, I recommend the paper to be accepted. I would nevertheless strongly encourage the authors to address my main comments/concerns.
"	3
The paper addresses an important topic, but I feel that the overall impact, measured by depth of presented contributions is too shallow.	3
Although the proposed UDAL can be viewed as a simple combination of the existing methods (progressive distribution alignment and logits adjustment), I believe that the simplicity and empirical effectiveness of the proposed method will be interesting to the reader in ICLR.	3
This paper addresses the topic of semi-supervised learning in cases where the underlying data distribution is severely imbalanced. This is an underexplored problem that is relevant to the AI/ML community and has practical real-world impact. In contrast to other papers addressing this problem, the proposed approach relies only on diistribution alignment without sampling-based strategies. This is done by moving the distribution into the cross-entropy loss computations of recent semi-supervised learning algorithms: FixMatch and MixMatch. The paper is generally well-structured and clearly written. The experimental set up is sound, and experimental results show the promise of the model. The approach is well-grounded in the existing literature, and its motivation is clear. As it stands, I do not see any major flaws with the approach.	3
This paper proposes a new method for class-imbalanced semi-supervised learning. However, in my view, the assumption of the proposal is too strong to satisfy and the novelty of the proposal is limited.	2
My main concern is that the novelty is limited, which makes this paper like an incremental work. Therefore, I recommend rejection.	2
The paper violates the double-blind rule (in the appendix). It lacks novelty, only works on a restricted setting and is a bit hard to read.	2
"
Strong:
1.	Two packing algorithms for bert-large and related pretraining models;
2.	2* speeding up without losing accuracy for bert-large pretraining under datasets such as Wikipedia;
3.	Rich analysis of technical backgrounds, experiments, and code details.
Weak:
1.	Limited to Wikipedia and bert-large is a small scope considering that there are rich pretraining tasks, rich domain and rich datasets of different languages;"	3
marginally below the acceptance threshold	2
Please see above.	3
Overall, the paper proposed a simple and effective method for speeding up BERT training especially on the datasets with a large variance of sequence length. However, due to lack of comparison with other similar baseline methods, I think the current version is slightly below the acceptance threshold. 	3
Overall, I don’t see a significant technological or conceptual contribution.	1
"Overall, I think this project will be a useful tool for the following continual learning researchers. I will recommend acceptance if the authors can address my concerns in the “weaknesses” part. 

&nbsp;

### === Post-rebuttal Comments ===

I thought the authors aimed to establish a unified software framework that makes running continual learning experiments easy.
However, after reading the rebuttal, I think Sequoia has the following major issues and the authors failed to address them in the rebuttal:

- ***Sequoia heavily relies on the previous libraries, such as Avalanche and Continuum.*** I don't think this design is very friendly to the researchers. In my personal view, I prefer a framework that is easy to be understood and includes the most popular baselines. I think your framework should be designed for a researcher instead of a software engineer.

- ***Sequoia hasn't been evaluated on large-scale datasets (e.g., ImageNet-1k).*** If I need to use this framework, I need the framework can reproduce the results of the previous baselines correctly. 

So, I don't think Sequoia is very useful to a researcher like me. According to my personal experience, I tend to reject this paper. "	1
I believe the objective of this paper of merging together continual SL and RL is very important and ambitious. However, the paper in the present form has several weaknesses. It is unclear whether the theoretical framework really achieves the paper’s objective. Sequoia (the software) seems to be still at a very alpha stage in its development cycle. I see very little unification in the methods, which is the main scope of the paper. This strongly limits the methods' reuse between the different settings. It is unclear what advantages sequoia is bringing compared to using its dependencies directly.	2
"While I do appreciate the effort behind this work, I doubt the match between this paper and the scope of ICLR.
"	1
This paper introduces a legit overfitting issue and proposed a solution for it. However, the reviewer is not convinced enough about the effectiveness of the proposed approach due to the concerns raised in the main review plus the fact that the paper needs a major writing revision.	3
"Pros:
+ The method is easy and simple and shows improvement in both experiments that include non-MDP simulations and a real-world dataset
+ The writing is mostly clear
+ The improvement shown in a real-world clinical environment is encouraging.

Cons:
- No quantitative analysis if the overfitting actually happens.
- Simple heuristics like shifting rewards to be positive and avoiding over-maximizing reward is not compared
- The novelty of this work seems limited since similar ideas have been proposed. 
- The distance considered in this paper may not handle missingness or when some input features are irrelevant to the reward.

Although I feel the proposed overfitting phenomenon can be an interesting contribution, the authors should quantify if it happens in the experiments and justify their method by comparing with heuristics like shifting the reward. I think addressing these can further push the paper over the acceptance bar."	2
"My detailed comments are given under the ""Main Review"" section. I have some concerns about the motivation of the method (#2), the theoretical results (#4), the choice of the tuning parameters (#5) and the sensitivity of the estimated values in the empirical studies to the choice of the policy evaluation algorithms (#6). So I give a score of 5. However, I would like to increase my score shall my comments be addressed. "	3
The paper identifies the unexplored aspect of the overfitting issue in off-policy learning. The motivation was easy to follow with some easy examples. I would recommend a weak accept at this moment. Some additional efforts in the experiments (as I described above) would strengthen the contribution more.	3
"I think this work as merit and could potentially make a good contribution to the field  if improved.
I don't think it's top-conference material in its current form, however.
I am sorry to the authors, but I do not have more constructive criticism to provide at the moment."	2
I think the paper made limited contributions to the evaluation of robust continual learning. It doesn't contain much more than what are already known to the research community. This reviewer also doesn't agree with some points. 	1
"Overall my recommendation is to reject the submission in it's current form, however, if the authors can address the comments I am fine with accepting it as a marginally above the threshold submission. The rationale is that:
1- Robustness of experimental settings for continual learning is important and the paper takes steps towards formalizing and unifying it.
2- Reported experimental results and the discussions (although limited) are still useful for the community."	2
The paper is a laudable attempt to improve the methodology of continual learning research. But a lot of the ground it covers has been covered in other papers. And it’s more like a thesis chapter / journal paper section than a standalone contribution to the field.	1
The paper presents a novel and useful idea: distill a complex pretraining model into a simple CMOW model. The initial result is promising while some limitations can be further explored. I think this paper is worth accepting because it can motivate the research on smaller distilled PreLMs.	3
The bidirectional extension of CMOW is straightforward, and the two-stage knowledge distillation method is originally  proposed by TinyBERT. The final performances of the proposed method performs worse than most baselines, e.g., TinyBERT, DistilBERT, MiniLM, BERT-PKD. 	1
"
To distill a better model to a weaker model, one can try different student models, either using shallower/narrower Transformer-based models or other models. If we could tolerate a big performance drop (as in this paper, the authors were satisfied that it exceeds ELMo), there are much more choices, of which CMOW/CBOW-Hybrid might be a good one. My concern is that using CMOW/CBOW-Hybrid is not that interesting, one might also try CBOW with convolutions, for example. It may help if the authors could provide more insights to choose CMOW/CBOW-Hybrid,  or why the investigation of  CMOW/CBOW-Hybrid is important, as we might have so many lightweight models as the student model."	3
It is an interesting paper but lacks insight into the novelty and the application scope of the proposed method	3
My main criticism of this paper is that it presents a fairly specific idea, that completely hinges on very recent prior work by Singh et al. 2021: to the point that the submitted paper is undecipherable unless you read Singh et al. (2021) first (which I had to do). That being said, the paper presents an interesting new perspective and builds on chance-constrained optimization and constrained optimization (but the authors choose not to go deeper in this direction, perhaps because of technical difficulties that are not mentioned in the paper).	3
There are interesting ideas in the paper around learning a safe behavior prior for latent skill-conditioned policy learning. However, as I pointed out above, there are fundamental issue and assumptions on offline data collection that require multiple safety violations in the data collection process, thereby defeating the purpose of the proposed safe RL approach.	2
Overall, a well-done paper with a clear contribution and several good experiments showing the benefits of the approach. The description of the approach can be improved (see issues above, and below, related work, overall story, bibliography, typos, etc).	3
An interesting paper that fills some gaps in existing work and posits some interesting applications of monotonicity.  The first contribution seems a little weak as it is built almost exclusively from existing methods with little deviation while the second contribution has more novelty.  I feel that the second approach needs some better motivation and possibly a better application  -- I would prefer that you drop the first approach to include it.	3
Given the incremental contributions, I consider this paper marginally below the acceptance threshold. It can be made stronger if the authors can provide more insights or make the experiments more illustrative or extend to the case of label's order on categorical features (treated as a partially ordered set).	2
See above.	3
"My primary concern with the paper is that Monotonic Kronecker-Factored Lattice provides a linear space/time complexity model that is monotonic by construction and thus monotonic everywhere in the input space and holds true even under a distribution shift. I believe that this model somewhat undermines the usefulness of the models described in the paper. I think the paper should make more clear why we would prefer to use the methods described over a model such as Monotonic Kronecker-Factored Lattice given that the model does scale well wrt the input space. This is not to say that the methods described are not useful; rather, this is to say that it should be made more clear why the methods described are as useful as claimed.

As for group monotonicity, I think this is a novel contribution that the authors have shown to work well and potentially be quite useful, particularly for the generative and adversarial cases.

For these reasons, I believe the paper is marginally below the acceptance threshold. Should the authors properly address my concerns, I see so reason not to bump up my score."	2
The proposal in the paper is interesting and novel, but the paper has deficiencies in its research methodology. It would be in a much better shape if several more fundamental issues are addressed. 	4
"In summary, I think this is a very good paper that I would personally like to see at the conference.
I have some minor reservations which I hope the authors might address, but unless the other reviewers find something critical that I have missed I am unlikely to change my score.
Nevertheless, I look forward to reading the other reviewers' thoughts and the authors' responses. "	4
"After the reviewer response, I have upgraded my score to 6. My main concern remains that the model analysed is different to what is typically used in practice.

The paper appears to be technically sound, novel and written with the intention of solving an important problem. But it studies a model so far removed from a typical neural network (so it's theory is not widely applicable), and the model does not achieve state of the art performance (so it is empirically not widely applicable)."	3
The theoretical developments presented in the paper are (to my knowledge) novel, and can trigger other ideas for further improvements. The numerical tests presented to support the usefulness of the theoretical framework are not fully convincing, but can be rather easily improved. My score would be a 7, if 7 was available. 	3
Because of the strengths discussed above, I am leaning towards accepting the paper. 	3
"I find the problem setting and proposed method very interesting. The theoretical analyses are valuable, and the experiments are convincing. Though, there is an issue regarding the clarity which make it difficult to fully understand the paper. I rate the paper as above the borderline. 

** Update after authors' rebuttal **
The rebuttal and the revision address my concerns on the paper. I think more positive about the paper and have increased the score. The new result (Figure 7) in the appendix is a good addition and I suggest including it in the main paper as well. "	3
"At the moment, I tend towards a weak reject because I don’t find the connection between theory and empirical evaluation too convincing. The reduction in the BC error shown in theorem 1 and 2 are based on the assumption that $|Z| < |A|$. However, if I am not mistaken, that is not the case in any of the experiments where $|Z|$ is usually much bigger than $|A|$. 
On the other side, the empirical results show a clear improvement over the baselines and vanilla BC, indicating that the learned latent embedding is helpful. 
I am willing to change my score if the authors strengthen the connection between the theory and the empirical results. For instance, they could explain why the transition-based representation might help BC, although the latent embedding is bigger than the original action space.

### Decision due to rebuttal

The authors addressed all of my concerns sufficiently well. Therefore I am happy to raise my score from a 5 to a 6."	3
The paper proposes a new metric for calibration, and propose a post-calibration algorithm for this metric. Overall, the novelty is a little weak and the improvements are not quite clear.	2
"While the concepts introduced and methods proposed are interesting, I do not believe the proposed metric is necessarily robust, and am not convinced of its advantages over an established metric (classwise-ECE, which also has not been reported in this work).
I believe elaborating (theoretically or experimentally) on these points could make a much stronger case for this work."	2
This paper provides an interesting method to improve probabilistic outputs with the proposed definition of local calibration. However, the local calibration concept is not linked with the existing definition of calibrated / Bayes-optimal classifiers, so it might not stand firm as a measure to optimise. Some further links to exciting kernel-based classifiers will also bring better insights to the paper.	2
As discussed above, my primary concerns for this paper is on the motivation of local calibration, as well as some possible improvement over the experimental section (such as comparison with existing approaches). I hope that those concerns can be addressed in the rebuttal process.	2
This is a sensible idea (metric and method) applied to an important problem. The exposition is fairly clear although some design choices are under-discussed. The experiments show that the method works as intended, although they could benefit from some deeper analysis.	4
The paper presents a good starting point for future research in the area and a model for application to several real-world problems such as assisted teaching (when to provide students help and how, for instance). However, the experimental section could be made clearer since the results presented look promising. An additional stronger baseline (though I understand this might be non-trivial to formulate) would have made the claims stand out. At the moment, both the baselines are weak (no assistance/rule-based assistance). Hence. I am inclined to reject the paper at the moment.	3
I am not convinced that this paper is ready for ICLR, it looks like the project is in an early stage. The writing is unclear, results are presented in tables rather than figures -- and are mostly descriptions of model performance rather than model comparisons. I am also not convinced of the strength of the contributions, even if the presentation is improved. The authors do a good job implementing existing algorithms in the context of the given task, however the applicability of this framework is very narrow, to a context of running RL agents with a near-perfect incomplete policy to evaluation tasks with missing annotations. The paper claims that the agent that can request assistance from a human to reduce uncertainty performs better on the given task, than an agent that can not request assistance -- this claim is correct, however it also seems obvious.	2
Paper investigates an interesting and relevant topic. However, there is a major omission of related works. Therefore, my recommendation is to reject the paper.	2
Although I do not have a large amount of research experience in the domain of calibration I do have quite a bit of applied experience in this domain. For that reason, after reading the introduction and abstract I was quite excited about this paper as I was expecting some sort of theoretical breakthrough. After reading the rest of the paper I was just left confused. Unless I am misunderstanding something important, I think there is a large mismatch between the claims this paper makes and the theoretical and empirical results it delivers. 	2
In summary, the paper provides a simple method that empirically generally improves model calibration. However, the theoretical analysis is overly general and does little to identify the specific conditions under which is works. As a result, I am skeptical of the very broad claim that it will be effective on any model and learning problem.	2
This work has previously been rejected from NeurIPS 2021 due to the absence of a discussion on the performance in higher dimensions, the lack of errors bars in the experimental results, and missing baselines. Since none of these points have improved in the resubmission, I will again recommend rejection.	2
"I do not currently recommend acceptance.
My main concern is some conceptual clarity about what different data-sets are doing which is reflected both in the theoretical and experimental sections.
I would like to see more thorough examination of the probabilistic bounds and the degree of slack they afford, which could help better ground the claims that calibration is a free lunch and justify CRM as an approach."	2
"The paper has an interesting and original idea which brings consistent improvement to the established explanations techniques such as gradient-based saliency maps and integrated gradients. However, there are some questions that makes me keep my rating only at borderline accept.

---
**Post-Rebuttal Comments**

*General rationale for the updated score*: after some more thoughts and the discussions during the rebuttal phase, the reviewer remains unconvinced about the claim of the normality of SM to (the extension of) a segment in the decision boundary. In this regard there are two significant concerns: (i) there are explicit statements about this both in the revised paper (see discussion with the authors for some instances) as well as the authors arguments during the discussion, and (ii) the motivation for the proposed method BIG is based on this claim. Furthermore, the paper cites other papers that as far as the reviewer understands do not (explicitly) discuss this claim. Therefore, I do not think I can vouch for accepting the paper claiming (and building on) some formal statements that I cannot personally verify. Consequently, I reduce my rating from 6 to 3. Since there might be a simple point that I am missing here which would prove the claim, I reduce my confidence as well from 4 to 3.

*Summary of the technical discussion*:  The authors, at various points, implicitly suggest or explicitly claim that SM which is the gradient of the network's function w.r.t. the input ($\frac{df}{dx}$) is perpendicular to (the extension of) a segment in the decision boundary. This is then used to motivated a variant of IG, called BIG which integrates SM over a line path from the sample to the nearest adversarial example. For the reviewer it is possible to see: (i) how $\frac{df}{dx}$ for a linear binary classifier will always be orthogonal to the decision boundary since the decision boundary is by definition a hyperplane with the SM as its normal (as described in section 3.1), (ii) how $\frac{d(f_i-f_j)}{dx}$ is orthogonal to the surface $f_i-f_j=0$. However, it is unclear to the reviewer how $\frac{df}{dx}$ can be guaranteed to be prependicular to the decision boundary of a general function $f: \mathbb{R}^d\rightarrow\mathbb{R}^K$ with K being the number of classes. In fact, I believe for the simplest case of linear binary classification, as soon as we (redundantly) model each class with a separate linear model (to become analogue to the multiclass setup), the gradient of each of the linear functions i.e., $\frac{df_1}{dx}$ and $\frac{df_2}{dx}$ will no more be orthogonal to the decision boundary."	4
"While I like that the paper aims to provide a more theoretical justification on attributions, I am not satisfied with the rigor of the theory and the empirical evaluation. I am not convinced that the proof is correct and the alignment with the normals should be checked using ground-truth knowledge. Overall, The paper is well-written, but please fix the grammar. I cannot recommend the paper in its current form for acceptance. If the proof were corrected and the evaluation extend to a ground-truth assessment of the normal, I would reconsider my rating.

**Technical Novelty:** The papers technical contribution is novel. I am less convinced about the significance in its current form. 

**Empirical Novelty:** The paper does not present new empirical evaluations or datasets.

**Confidence:** I am confident about my assessment. I read the proof and investigated the issue of resembles-a-normal-distribution in depth. I still might have missed other issues of the proof. While I did looked at the referenced literature about adversarial examples, I am more familiar with the interpretability side of the related work. "	3
I have a few concerns regarding the quantitative experiments in the paper, which are mentioned in the Weakness section. I will be willing to update my ratings if the authors address all my points. 	2
The conclusion is not novel to me, which is already known to the community.	2
**Recommendation:** reject.  I like the paper.  But I think there are some issues w/ the experimental evaluation that need to be sorted out before it gets accepted.  Happy to change my mind if I missed something.	2
As of writing, I am on the fence about whether to recommend this paper for acceptance or rejection. On the one hand, the underlying ideas do appear to be sound and of merit. However, the issues of clarity are such that I cannot be sure of having evaluated them correctly.	2
"The current paper represents a solid piece of work. The main reason for lowering the score is that several papers have considered post-hoc uncertainty quantification procedures for multi-label classification settings (possibly a more detailed lit review with focus on this is necessary). The authors propose a way of controlling the total number of false discoveries (either in expectation or with high probability) based on conformal inference. The score could be updated once the questions mentioned in the main review are answered.

**Update after rebuttal**

I would like to thank the authors for the detailed responses. Taking into account the general contribution of this work and points mentioned in this and other reviews, I tend to keep the current score."	3
"I believe this paper can be improved by addressing the above comments, e.g., how to pre-specify k (if possible), discussing alternative options to build the nested sets (that may be more complex), and explaining possible failure cases of the greedy approach. Also, discussing (e.g., via simulated data) the advantages and limitations of k-FWER over FDR seem to be important for the users to better understand which error metric is preferred, given the nature of the data at hand (and the existence of prior work).
"	3
"I like the idea and solid contribution of this paper. The main concern I have is the technical contributions. Please refer to the above for details. I would consider raising the score if the concern is properly addressed.

While I am not totally familiar with all the literature, my assessment of the novelty and originality might not be accurate enough."	2
The paper studies an important problem and the motivation is well laid. However, I feel the paper needs some work in terms of improving clarity as well as more extensive evaluation of large language models. They also need failure case analysis elucidating some sample cases.	3
Paper presents a useful dataset complementary to the ARC effort for understanding the limits of DSLs and capabilities of natural language for expressing programs. The dataset appears to be adequately constructed, however a lack of downstream applications and proof of utility make the effort harder to contextualise, and the dataset can be seen as incremental over the original ARC effort.	1
"The paper makes theoretical contributions on the performance of mle compared to empirical risk losses. 
However, the applicability of the derived results in real-world regimes could be improved. "	3
The paper tackles a challenging topic to compare two well-known inferential methods for regression. Some new theoretical results are presented on this topic. However I have concerns about the mathematical correctness of the paper (see Comments (d) and (e)) and the motivation for the study (see Comment (f)).	3
"This paper in about an interesting and important idea, but the notations and presentation need some improvement. 
"	2
Even though there are some unclarities in the text and experimental results can be improved, the paper contains valuable information for the community. Furthermore, these changes should be feasible to implement during the revision period.	3
See the main review.	3
The paper currently suffers clarity in explanations. See the main review.	2
"I don't agree that AAVE can be considered as a third family of SSL methods. There is not enough justification around why one would accept 22%-24% lower performance (even with small batch-size) and use AAVE.
"	1
The use of data augmentation and generative model as another family for self-supervise learning has merit.  The proposed methods share the idea of training VAEs with a denoising criterion, albeit removing the KL term in the proposed method. Experiments lack evaluation in settings beyond classification and performance lags behind self-supervised SOTA.	2
I think the idea proposed is interesting and tackles an important problem, however the it lacks a proper evaluation setup. I do not think the paper is ready for publication and I recommend the authors to retrink the evaluation framework and add relevant baselines and method for comparison.	2
"The paper presents an idea of classifier combination much more than an actual ""network"". The concepts borrowed from multiple-classifier systems and related fields are not novel. Also, the baselines were not adequate, and the choice of datasets to compare was limited. Therefore, I cannot recommend this paper to be accepted."	1
"In my opinion, the experiments presented in the paper fail to demonstrate or give any meaningful evidence to the claims made by the authors regarding the proposed method. While in my opinion the method has inherent drawbacks, the true flaw is the lack of experimental evidence which is a major issue. Also, I believe the claims could have been better supported with a wider discussion demonstrating how the proposed method relates to other previously published work on training deep models in settings with limited training data. 
"	1
"In this paper, the author proposes a novel deep learning architecture that uses ensemble of numerous simple SVM classifiers as network layers. Their result shows that the SVMNET outperforms other deep convolutional neural networks such as ResNet-50 with less training time for cases in which the number of labeled training samples is small. 
According to the experiment result, it supported the statement to some extent. For two of the three datasets, SVMNET has better performance when the training samples are less than 200 compared with ResNet models, but the ResNet catch up to the SVMNET quickly after the sample increases. Also it would be better if they can do comparison with pre-trained ResNet and show the results in their paper."	3
See above.	2
Overall I think this is a good paper. Please refer to the above section for detailed review.	3
While the results do clearly show improvements in both forecasting performance and computational efficiency, many additional details on the architecture need to be included before the paper is ready for publication.	2
While the proposed methods hold great promise, it has several issues to be addressed regarding the fairness of the experiments, a missing experiment dataset, and more detailed explanations to be self-contained. 	2
This is a well-written paper studying a multi-agent distributed optimization under general consensus-type interactions between agents. I personally think that it is not of relevance for ICLR, while it might be on interest to some larger learning conferences such as NeurIPS. 	3
In summary, the paper may contain publishable results, but is not sufficiently clear to understand what those results are, let alone verify them. In addition, the lack of a numerical illustration seriously undercuts the claims of significance. To be publishable, the authors should focus on clearly defining the problem, presenting the solution and its significance, and illustrating on a numerical example. 	3
While the possible technical issues can be discusses further, the score is mainly due to lack of empirical section which is a major weakness of the paper.	3
In summary, this work extends the existing results on the finite-time error bounds for the problem of distributed (decentralized) linear stochastic approximation to a more challenging communication setup. The paper is marginally novel and the contributions are fair. However, the manuscript has two main issues, (1) the comparisons to the previous works are missing, and (ii) the structure of the paper, as well as the presentation of its results, require some work.	2
"Numerical simulations are suggested to compare the proposed algorithm and existing ones as well as validate the proved finite-time error bounds for both constant/time-varying stepsizes.

--------------
After rebuttal: After reading the other reviews and authors response, I have updated my score. 
"	4
This paper does a lot of good things and is fairly comprehensive in terms of trying different model sizes and dataset sizes. My biggest problem with this paper is that it (mostly) only explores the effects of scale on a limited 2 task setting. These results may not hold for longer task sequences that are often used in continual learning, and thus I recommend a borderline reject.	3
Studying the effects of model size and pre-training in continual learning seems pertinent. However, the claims at the moment in the paper seem to be overly general given the evaluation. The paper has experimental setups that make it hard to decouple the effects of various factors including distribution similarity of pre-training to CL sequence data, optimization hyperparemeters, finetuning steps, performance of first task. 	2
I believe the paper lacks novelty and some claims are not well-supported. Moreover, the experimental design has flaws as mentioned in the main review. 	3
"I found the paper to be interesting and timely, but recommend rejection for now due to a need for better experimental setup and more details in the figures. I would be happy to raise my score if the authors justifiably address these apprehensions. 

**Post Rebuttal**: The authors provided detailed response to the raised concerns (though primarily in the appendix) and sufficiently backed up their proposed hypotheses. I am hence increasing my score to recommend acceptance for the paper. "	2
The algorithm proposed in this paper is intuitive and highly relevant in the context of multitask learning, but I find the experiment section didn't show the effectiveness of the proposed method clearly. Many missing important details result in many question marks while I read the paper. I think a clear and convincing experiment section is, to some extent, more important than the theoretical analysis given the gap between those regularity conditions required and the practical use cases. The experiment section of this paper definitely has room for improvement.	3
"The paper is well written and presents a nice general framework, although it is not clear to me that there is a significant performance improvement compared to previous work. Therefore I lean towards acceptance.

---------------------------

After reading Reviewer's 6MZF comments, I agree that there are major similarities to Algorithm 2 in Kamani et al that need to be acknowledged. I disagree with the authors' claim that Kamani et al only makes sense for a specific choice of $F$, since their algorithm can be stated for general $F$. Therefore, I decrease my score by 1 point."	3
The paper presents an interesting setting for multi-objective optimization, but suffers from a lack of discussion of scalarizations, assumptions, why non-convex Pareto frontiers can be discarded, and the novelty/convergence of the optimization.	2
There are some concerns regarding the motivations of this work. Also, similarities with the current work and existing methods not referenced appropriately, which seems to reduce the novelty of this work.	2
"This paper proposes a solution for personalization in federated learning where clients only exchange a small portion of parameters with a server for collaborative learning. The problem and proposal looks interesting, but it would be great to hear from authors on some issues raised above before final evaluation.
"	2
I strongly encourage the authors clarify contributions explicitly acknowledging previous works [Liang et al. 2019, Arivazhagan et al. 2019, Collins et al. 2021, Li et al. 2021 ], as well as  [Singhal et al. 2021 Federated Reconstruction: Partially Local Federated Learning https://arxiv.org/abs/2102.03448]. 	2
Overall, this framework is interesting and theory is new. However, the novelty for me is somewhat limited. If they can come up with theory that can explain their empirical observations (ReLU network), I believe it will be a strong work.	2
The contribution is unclear, which constitutes strong reason for not accepting the paper.	2
"This paper applies local SGD to GNN training. The idea is not very novel. The experimental results are promising, but some important points need clarification.
"	2
Well-written paper that considers an important problem and presents a simple, effective solution. There are some concerns however which I hope the authors will address. 	3
"The paper is fairly well written and the proposed result, albeit simple, is powerful. The theoretical guarantees provided round up the good work. More importantly, the topic is timely and very relevant, and I, therefore, recommend acceptance in the conference, provided they add standard deviation results to Table 1.

My assigned score is 7."	3
I think the optimization technique this paper presents is nice. The empirical results are thorough, showing advantages over previous methods. The theoretical part is a bit unclear. specifically, whether Theorem 1 actually shows a lower bound (i.e. is the second term there irreducible). 	3
This paper is well-structured and contains enough detail overall. The problem setting is well-motivated and the proposed method showed promising results towards addressing the issue. However, the concerns listed above should be addressed to make the submission sound and complete. 	3
"The authors propose a modification to variational autoencoders which improve them in a conditional setting. The show promising results on image completion on a number of datasets and also in the area of medical imaging. However, the paper suffers from a few drawbacks. First, the technical and empirical novelty is limited. Second, the choice of datasets used for evaluation is simplistic. 

In light of the other reviewers concerns and the response from the authors, I upgrade my score. "	2
"Overall this is a strong paper.  I am supportive of seeing this published.  I am missing an explanation of the authors' objective function and why it should find a diverse, but accurate prior.  This may just be my stupidity, but I think a clear explanation of this might be helpful to other readers (apologies, if I just read over this, but it wasn't obvious to me reading the main paper).  I am happy to increase my score if I can be satisfied on this point.

In view of your answer I have increased your score."	4
The paper presents a simple but well-articulated approach for conditional image generation, where one wants to avoid training the entire network from scratch. I believe the idea of using existing publicly available models is good, and this method neatly utilizes such available models to achieve impressive performance (and to avoid complication and challenges of training the full model).	3
Overall it is a nice work, but I’d like to see some deep analysis on experiments, e.g., why the proposed model is better, and in which cases, etc. 	3
The idea and the focused problem of the paper is very interesting and can be impactful. However, technically speaking, the current form seemed quite incremental, and a minor modification of existing ideas to achieve SE(3)-invariance. 	2
This paper is well written and focuses on a simple yet important problem. I tend to accept and am happy to adjust my assessment based on the rebuttal.	3
"This paper proposes ChIRo to improve the expressivity in molecular representation by well tackling chirality. It has a solid theoretical foundation and a clear and relatively well-designed model structure. Although the experiment is not yet sufficient to demonstrate its application value, we believe that ChIRo's effectiveness can be claimed after being evaluated on diverse tasks on molecular representation, such as molecular property prediction datasets in MoleculeNet.

In summary, it's a good piece of work but needs some improvement."	4
Concluding, while the results seem to be interesting the clarity of the proposed approach should be improved. 	2
Overall this paper needs a bit more deep analysis and rewriting to back up the experiment results before it can be accepted.	2
The paper's empirical performance is solid, the technical novelty lies in making the Graph Query Embedding method work with Neural Networks operators. However, the paper lacks clarity and the related work is poor. Happy to increase my score, should my concerns be addressed.	3
"The results obtained are promising and the possibility of solving negative queries is a strong point in favour.
An appropriate discussion of related work is lacking."	3
"Copied from above.

## Strength
1. Clear theoretical justification of the proposed method, although there are some imperfections.
2. Good presentation and visualization (e.g., Fig. 2).
3. Relative comprehensive results, considering various datasets, baselines and metrics.

## Weakness
1. Only relatively small tabular datasets are used, which is not the most widely used case for DNNs.
2. Some details may need to be further clarified."	3
Overall an okay paper with only minor (not too critical) issues as pointed out in the main review.	3
Overall the paper has merit in terms of identifying examples that are robust upon small changes in the training procedure. The claims regarding to counterfactuals, explainability and causality I believe are a stretch. As such I recommend that the paper is changed such that it is in the proper context	2
"
Overall, I like the paper and enjoyed reading it. However, I am currently recommending a 5 as I am not convinced by the correctness of the claims in the paper. Specifically, I have some concerns about the validity of Theorem 2 (elaborated upon in my main review). If the authors are able to clear up this aspect, I'm happy to reconsider my score. 
"	3
"I like the topic and motivation of this paper, and the derivation of the method is interesting. However, the most important part of this paper is the evaluation. This paper needs to explain better how the algorithm is applied in real-data scenarios. Until it can be clarified what precisely is happening in the experimental sections, it is difficult for me to judge effectiveness of this method. I am also concerned that there are no uncertainty estimates in the real-data experiments and only a few run-times are reported. It seems that careful revision is needed for this paper to be ready for publication, and that is the main reason for my score.  

========================================

**Update on review (11/29)**

The algorithm used in the real-data experiments seems to differ in a significant way from the centroid method derived in Section 3 (see my discussion with the authors). Further, these differences can only be ascertained by a careful read of the Appendix, which leads to a lack of clarity on what is being proposed. I think this paper needs a better explanation of how the methodology used in the real-data experiments matches with the initial derivation of the centroid method. I do think that both algorithms are quite interesting and that proper attention to both of them would improve this paper. 

I also think that more care should be taken to report the training times for all of the experiments. I understand that training time is not the main focus of the paper, but carefully reporting run-times/overheads would clarify to what extent the centroid method is practical. 

Overall, I think that improving the quality of bootstrap is a promising idea that should be further explored, and this paper takes an interesting route toward doing so. However, I feel that this work needs to be further developed to be ready for publication, so I have kept my initial scores the same."	3
I think the paper is understandable and has a nice idea though not novel and needs a lot of clarification and revision before being accepted for publication	1
"The bootstrap is an extremely versatile technique, and is applicable in many situations, for example, with dependent data, on extremes, cases where standard frequentist or Bayesian asymptotics may not hold, and so on. While the present papers approach does not cover many of these bases, it does address the most commonly observed scenario: where standard mathematical/calculus/probabilistic properties hold (like interchanges of limits and integrals etc) and the hyperparameter estimator has a limiting Gaussian distribution due to the Central Limit Theorem. This is the most important special case to tackle, and as an early attempt to address the computational aspects of bootstrap, I think this paper is showing quite a bit of promise.


Despite the long list of weaknesses mentioned above,  my initial feeling about this paper is quite positive. Bootstrap is a very strong alternative to Bayesian or ad hoc methods used in ML, and this paper may be a breakthrough to address the computational challenges of bootstrap. I am excited about this, but would appreciate considerable additional details and clarifications. 


Revised comments: I thank the authors for the updates on the paper and their detailed responses. While I am a strong enthusiast of the bootstrap and I like this paper, I still have several concerns. It would take a major revision to address these concerns, so I am leaving my rating unchanged. 

1) The bootstrap essentially is a technique for uncertaintly quantification, so it's main computational burden is during training: I am not convinced about the realism or practicality of the authors' premise that there is unlimited training resouce/time but limited time during inference. In any case, during inference, the bootstrap is just a bunch of forward model runs: for example in deep learning, it would be just evaluating what we get from the architecture for a given set of weights and biases. It certainly helps if the ""set of weights and biases"" is small, but I am struggling to find a realistic example where it is hard to simply run the code for a given architecutre with given weights and biases and test set features. 

2) The roles of m and M need to be studied more carefully. 

3) I am not convinced that the centroid method sufficiently addresses the issue of variability or stochasticity in the hyperparameter space. We need proper studies that the tails of the distribution are adequtely captured using the centroids method, which will take more work than what the paper currently contains. "	3
"It is overall an interesting paper providing an acceleration technique for bootstrap - but I would suggest the following:
1. Provide a clarification in the beginning of this paper, rather than in the appendix, that this paper focuses on ""model compression"", hence inference time improvement and model size management, rather than reducing the training time.

2. Clarify experimental setting parameters (e.g. size of the dataset, size of theta, etc.). See above for details.

3. Comment on comparison vs other previous bootstrap variants."	3
This work provides a slightly stronger bound connecting Membership Inference attacks with differential privacy, however, the presentation could be improved and some parts are a little hand wavy.  I would also like to see more discussion on whether MIA are the right attack to consider with DP, as some recent works have shown the attack to be weak.	2
While the paper presents an important and novel technical contribution, it has some writing issues. In particular, the background section needs to be somewhat expanded and a particular section would benefit from more clear writing. 	3
"1. The paper is well written and the bound is insightful.
2. The paper still needs to elaborate on the bound from the perspective of the defender.
3. More detailed comparison about the bounds of MI are needed.
"	3
Based on the comments above, this paper doesn't strike me as a great paper. The ideas are respectable, but the execution could have been better, and more depth would have been appreciated.	2
"Overall, UVC has made a good attempt at model compression for visual Transformer models. But the novelty is limited. This method needs to provide a more rigorous comparison, and at the same time, it needs to be verified on more visual Transformer architectures to prove its effectiveness and generalization.

"	3
This seems to be an overall well-executed paper, with reasonable amounts of novelty and potential. However, the authors need clarify on a few raised questions, on which my final rating will depend. 	3
While this joint optimization is a reasonably novel effort in the (somehow niche) ViT compression field, I cannot endorse this paper further given several critical experiments are missing. I’m willing to revisit my rating if the authors can provide the above questioned results. 	3
I believe this paper provides good theoretical results on a new second order method for optimizing neural networks in the over-parametrized setting. Content-wise, I'd strongly appreciate empirical validation / implementation. On the formal side, the paper is a bit hard to follow, and contains many typos. It would benefit from careful proof-reading, and correction. I do not recommend acceptation in the current state of the paper, but could be convinced otherwise.	3
This paper is theoretically sound but could be further improved based on my comments.	2
"The main text of the paper is not self-containing and not justifiable (suggest to reorganize the paper). 
Several claims about the literature are not accurate.
No advantage compared to gradient descent methods for practical scenarios."	3
"In general, I appreciate the technical depth of this work as a theory paper. And I think the techniques for reducing per-iteration cost can be useful in future studies. My main concern is the novelty of the convergence analysis and the practicability of the proposed algorithm. Above all, I think this paper is marginally above the acceptance threshold. 

-----------------

Thanks for the response. After reading the response and other reviews, I decided to keep my original evaluation. 
"	3
The paper's proposed method is a straightforward combination of several well-studied ideas, namely tensorization of linear weights, tensor product embeddings, and variational quantum circuits. The biggest strength of the paper is in its experimental results, but these consist of a few experiments on MNIST with a limited number of baseline models.	2
It is not clear that the proposed method is better than the existing QML models. Hence, the main claim of this work is not justified.	2
The paper is an improvement of quantum neural network algorithm.  The idea is simply but interesting for machine learning community.  However, novelty is fair, and the experimental evaluation is less convincing. Some important problems like computational cost, gradient exploding are not well discussed. 	2
"Strengths:
- Quantum Machine Learning is a very important and growing field of research. This paper shows one potential way to use quantum computers for machine learning.

Weaknesses:
- The contribution of the paper is limited (see comment below). 
- Experimental results are also limited (see comments below)"	2
Due to the limited technical contribution/novelty and insufficient experiments, I tend to borderline reject the paper.	2
"The paper proposes a potentially interesting method to interpolate between GD and GN, and is certainly moving in the right direction of a very important problem in scientific machine learning, but the paper as it is right now is a little confusing to read. It would also benefit a lot from some ablation experiments showing the benefit of each individual part of their methods. I vote for marginally below threshold, but am willing to change my score if my concerns are addressed.

Let me know if you have questions about my review."	3
This manuscript fills a blank for optimization in physical deep learning with a nice and clean method. The presentation is relatively clear to me. I would recommend an accept.	4
The paper is well-written and organized. The idea is simple and effective. It is better to discuss more details on the settings of the hyper-parameters in the proposed method.   	3
The authors did a novel exploration in using Swin-Transformer for transform coding and proved that SwinT-ChARM (the authors proposed) demonstrated better compression efficiency as opposed to CNN counterparts. It is a nice exploration and provides new perspective in image codecs.	4
"Despite some reservations about the technical novelty and the runtime comparison, I still think this paper deserves an ""accept, good paper"" recommendation.

This conclusion is based on the strong empirical results and the thorough evaluation. The results are also somewhat surprising to me, which is an indicator of important research. I would have expected transformers to improve RD performance compared to conv-nets, but I expected a smaller difference and for runtime to *increase*, not *decrease*.

I also think that if I were to start working on learned image/video compression, I would use the model in this paper as a baseline.
"	2
I think the paper could be accepted considering its effective design for transformed-based compression as well as the comprehensive analysis. 	3
The contribution of this paper is simple and direct, and the results also show good performance. But since transformer itself is not new, I wish to see deeper analyze over the experiments. So my recommendation is marginally below the borderline.	3
This paper shows that Transformer based transforms can replace Conv based transforms in image and video compression, and simultaneously achieve better RD performance at much faster decode times. The results are strong across datasets for image and video compression and multiple entropy modeling techniques. The abundance of evidence and details, along with the state of the art results in performance make this a very strong paper.	4
This paper is a comprehensive pipeline for continuous object learning and presents some valid designs for this specific problem. But it is a bit unclear how this can be extended to real-world data where a robot agent needs to obtain the instance segmentation mask through interaction for a large diversity of objects. Some of the evaluation metrics are not very clear to illustrate the model performance. 	2
Generally speaking, the paper is well formulated and well written. This paper has a good formulation. Also, the proposed framework is well illustrated and easy to follow. Some small issues remain but they do not affect the overall quality of the paper.	3
Due to insufficient comparison with other related studies, lack of evaluation on real image datasets, and less rigorous in theory, this research still needs a lot of improvement to meet the accepted standard.	2
Overall, I think the basic idea of leveraging network weights for object-centric representation is interesting. However, I am quite doubtful about its practical value because of the need of a mask during testing and the lack of real-world experiments.	3
A good paper that helps explain some of the recent results studying on-policy methods in MARL.	3
In summary, I think this paper has some interesting results but can be further improved on both theoretical and experimental sides.	3
I recommend this paper for acceptance. The work is novel and does not have any glaring faults apart from some of my (possibly to-be-clarified) doubts with the experiments. 	3
"The approximation is inappropriate which makes the contribution of the paper meaningless, unless I left something important.

### After rebuttal

There is an error in the proof of Theorem 2 which is claimed as the major contribution of this paper.  During rebuttal, the authors changed these equations (10 -15) several times, but unfortunately none of them are correct. In the latest revision (from equation 13 to 14), the $new$ definition of $A_{\pi_j}$ is given (the definition below equation 15), but it does not have any actual meaning (i.e., it is *not* an advantage function), just a function by definition. This DOES NOT accord with the learning of IPPO, MAPPO, or any other MARL methods I know. 

Thus, I keep my score unchanged. 
"	1
I think this is a good paper, which makes an interesting methodological contribution to an increasingly relevant subfield. While it would benefit from better contextualisation of the hyperparameters of the method, and from a softer touch in interpreting experiments, I feel confident in recommending it for acceptance.	3
While the work seems technically sound, there are some loose ends like those mentioned above. Empirical comparisons with some baselines also seem to be missing. So I recommend only a marginal accept in it's current state.	3
"Overall, the submission is borderline. The proposed algorithm is mostly based on prior work and has few non-trivial theoretical guarantees, but it does perform well against other baselines in interesting practical settings such as rate shift.


EDIT: I have read the author response and my review remains largely unchanged, though I would raise the score to 7 if this were possible within the ICLR format. Rejection sampling is a well-known strategy within the domain adaptation community for dealing with bounded covariate shift. The modification to approximate importance weights is more novel, but not of great theoretical interest since it is basically an immediate application of monotonicity. Finally, while the authors do provide a theoretical guarantee on the threshold cutoff parameter $\tau$, this is only meaningful with respect to a very narrow set of algorithms (namely those w/ one fixed cutoff parameter that satisfy PAC-guarantees). There is no obvious way to actually connect this to a guarantee on the optimal prediction set sizes, even in the PAC setting (nor is this ever discussed). However, the empirical results do a good job of making up for these weaknesses, so I would give the paper a 7 if I could. Unfortunately ICLR does not give provide this option."	2
The method proposed by the paper is interesting and new. However, I found some claims of the theories in the paper need to be clarified or made more formally. I hope the authors can address my concerns in their feedback.	3
The reviewer doubts the motivation of using neural networks to the confidence found when we have an analytical equation for that. 	2
The idea is interesting and as far as I know it is novel. However, both the theoretical and experimental results should be verified more carefully. See Main Review for details.	3
Overall, I think that the studied neural contextual bandit problem in this paper is well-motivated. The idea behind the proposed algorithm is novel and effective. Tighter theoretical guarantees and extensive experimental results are provided to demonstrate the superiority of the proposed algorithm. Therefore, I recommend “weak accept”.	4
Overall, I liked the paper, very clearly written, organized. I am convinced with the novelty of the technique being proposed, I had a few concerns around some applicability issues in real life due to time complexity. I have detailed my questions in the previous box. 	3
"### After Author Response
1. The paper presents a novel idea -- using adversarially perturbed problem instances to evaluate the generalization performance of neural combinatorial solvers, and shows it exposes the weakness of some state-of-the-art models.
2. The paper shows that adversarial training with the adversarial data may be a useful practical method for improving neural solver's generalization performance.

-----------------------------------
### Initial Summary

While I believe the technical results presented in the paper are valid, I tend to disagree with the authors on the significance / potential limits of their approach, in particular on their response to the theoretical challenges raised by Yehuda et. al. (2020)."	2
Overall, I feel this is a worthy piece of work that is relevant, presented clearly, and accompanied by a good evaluation.	3
As a result, I believe the paper offers a solid contribution that should be interesting enough for the community working in the area of neural solvers for combinatorial problems. Furthermore, I believe the message conveyed by the paper on the lack of robustness in SOTA solvers must be heard as, otherwise, neural solvers will never be able to even catch up with exact and complete solutions for the studied problems and instead will continue to be perceived as no more than a nice exercise for ML practitioners.	2
"In summary, I think the paper proposes an interesting problem along with a simple solution that is clearly explained. As listed above I do however have some major concerns, both conceptually and in terms of the experimental evaluation, and thus cannot recommend acceptance of the submission in its current form.

Post Rebuttal
============
After considering the authors' rebuttal I have increased my score to recommend (weak) acceptance."	3
Adapting skill learning approach to dynamic worlds and learning skills incrementally are good ideas, however the current set of experiments do not sufficiently distinguish the author's approach from existing literature, nor pinpoint what is important about their setup. With the baselines, ablations, and clarifications above included, I think this will be a strong paper that I look forward to rereading.	3
To me, the problem is real and practical. The methods that the authors propose is novel. Authors provide comprehensive experiments to supports the proposed method's superiority compared to existing baselines. Nonetheless, I have several concerns regarding fair comparison of the DISk vs existing methods. Specifically, I'd learn more about its computational efficiency and see additional experiments and ablations.	3
Overall, I lean towards accepting the paper but I have serious concerns about the adequacy of the evaluation protocol. While they clearly prove the advantages of DISk, I'm not convinced that by making advances in these toy environments we are truly getting closer to developing agents that can acquire useful skills autonomously. I acknowledge that the paper mostly follows standard practice in the community, which is why I still recommend acceptance, but the submission would be much stronger if authors could provide results in other settings (e.g. Fetch, Atari).	3
Although this paper presents many empirical results, my main reservation is how much they can afford a deeper theoretical insight that can motivate follow-up studies. In addition, the approximation method to compute the Hessian matrix essentially follows by the existing methods which does not seem very novel. 	2
While study of generalization measure and extensive empirical study is important problem towards understanding generalization behaviour of deep neural networks, the current submission has issues that needs to be addressed before being presented to general audience. 1) The main message / lessons beyond Thomas et al., 2019 is unclear or not existent 2) usefulness of TIC as generalization measure need to explained 3) overall clarity needs to be improved 4) HPO experiments at the currently presented results doesn't demonstrate robustness and requires validation on more diverse settings. The reviewer believes addressing these issues requires major revision and won't be able to  suggest acceptance for ICLR 2022.	2
The authors deserve credit for funning a lot of experiments. However, there is minimal original contribution. Furthermore, the paper is poorly written and organised, and of insufficient mathematical rigour to warrant publication at ICLR. 	2
"Overall, I think this paper focuses on a challenging topic and has the potential to be accepted. I would like to see more explanations on the details and update my score after the rebuttal period. 


==========================
After reading the authors' response, I decided to keep my scores unchanged."	3
The main contribution of the paper is enabling the deployment of radiomic features alongside ViT architecture for prediction of Chest abnormality. The study is somehow limited to only one dataset, the results are not supporting the claims and limited. There is no insight provided about how this new feature helps with current shortcomings of CNN including explainability. 	2
Overall this paper is well written besides several comments I made above. The classification results on AUC are not convincing to me that integrating radiomics features in this proposed way is better than the state of the art. 	2
This paper presents a Transformer-based model for joint classification and localization of diseases from chest X-ray images, leveraging an image branch and a radiomics branch via a feedback loop module of bounding box generation. While the proposed method has novel and interesting components, the experimentation and evaluation in the paper need to be improved. The proposed CheXT should be compared against the more recent CNN baselines such as ChexRadiNet [1]. There are also some concerns with the experimental details and analyses that need to be addressed. 	3
The paper needs a proper evaluation on the fair baseline. The weaknesses are discussed above.	3
I see a balance between pros and cons in this paper, yet I do have some positive tendency as the method itself is clear and intuitive to me and can boost performance in some interesting cases (not always, but still). Therefore, at least until I see some more clarifications from the authors and other reviews, I prefer to rank it slightly above borderline, with an intention to revisit and yet in a positive spirit of leaning towards acceptance :-)	3
Overall, this seems to be a good paper. A well-motivated and intuitive idea is proposed, and it appears to work well across a variety of datasets. However, critical details are omitted (e.g. what is the complete loss function and algorithm?), there is a lack of uniformity in the evaluation (raising suspicions of cherry picking), and related works are not cited satisfactorily (e.g. black-box source-free domain adaptation). As a result, I believe this paper is only marginally above the acceptance threshold in its current form. If the aforementioned concerns can be addressed, I will improve my score.	3
I recommend this ms to be accepted.	3
The paper is meaningful in exploring the new application of GNNs in irregular MTS related tasks. However, the paper is concerned w.r.t. its unclear description in some important concepts, the technical limitations, the lack of discussion and comparison of some related methods, and the non-comprehensive experiments.	3
The presentation of the paper has big space to improve. The source codes are unavailable, and the main results cannot be confidently reproduced.	3
"- Novel modular egocentric instruction following system with multiple technical contributions.
- Generalization to unseen environments is impressive.
- Dependence on task-specific instruction templates needs to be discussed in relation to previous work. Claim that this can be directly compared to other work that does not make use of low-level instructions is questionable."	3
"In summary, the paper proposes a modular approach for Vision Language Navigation tasks. The assumptions are heavily customized for ALFRED, but some general lessons can be used to other domains.

The most interesting contribution was the level of abstraction of the semantic map and policy that allows separating the exploration and exploitation from the natural language instructions and from acting given the current situation and information in the map. Furthermore, given that actions are related to objects, and the agent is located in an environment, it makes sense to have a predictable policy, allowing to adjust independently the issues related to the map.

So, I propose acceptance."	3
"I am leaning in favor of acceptance because the method presented achieves SOTA performance on the ALFRED task, showing how domain knowledge can be leveraged for improved performance. 

However, I do not feel strongly about this stance because the performance boost appears to be achieved by baking in domain knowledge that may keep the method from being generalizable to other domains. 

Despite this concern, I think this work would still be of interest to the community since it may also serve to highlight the need for improvements in high-level planning/control in these types of visual language navigation tasks. Similarly, other components of the model are also hand-crafted and could serve to highlight ripe areas of work for the community (e.g. a low-level deterministic policy is used, which does not open receptacles). 
"	2
I am looking forward to clarifications on missing information from the authors during the discussion phase.  Overall, while I do agree that the proposed approach shows impressive results on the ALFRED benchmark and the paper has useful insights, however, the overall approach doesn't have much novelty to offer. Therefore, I feel that the paper is good/useful, but slightly below the acceptance threshold due to novelty concerns.	3
Investigating the uncertainty/stability during the learning in games with different initial conditions is an interesting problem. But from my understanding, this problem has not been well formalized in the paper, and many additional clarifications are required to make the people unfamiliar with this research problem get the paper's key points.	2
This paper adds some new results extending previous work that established divergence from Nash equilibrium for a class of learning algorithms for games.  I was not convinced of the main argument of the paper, which claims that the advantage of using differential entropy to analyze the learning dynamics.  What seemed more novel was the analysis of the Jacobian of the population game.  Hence, overall I am recommending weak acceptance.  If I were to be convinced that the use of differential entropy is a promising approach for the field, then I would raise my score.	2
I’m not super familiar with the related literature of this work. But as far as I’m aware, the perspective adopted in this paper on studying the dynamics of online learning in games is original to me. The characterization on the evolution of cumulative payoff vectors seems pretty novel to me. I feel the results obtained in this work may have implications on other studies on the behavior dynamics of online learning in games, especially for those who need random initial conditions. 	4
"To summarize, although the paper could improve from additional experiments and the technical contributions are limited, I have appreciated the motivations and model proposed by the authors, as well as the obtained theoretical results.
Hence, my accept score."	3
The idea presented in the paper novel and interesting to me. My major concerns are with the experimental sections - I did not find it convincing enough and I am unsure of its use case given the current results.	3
Nice and clever work. I vote for acceptance due to the “fresh blood” in the so conservative area of validation techniques. This work represents a first empirical study in this new direction, and a lot of future work/studies seem to be followed. So, a lot of things would be nice to check and study to get comprehensive overview of the novel approach, but this first step is enough to be presented at ICLR 2022.	4
"
The paper is clearly written and well structured, with good
motivation, examples/illustrations, and the techniques are
understandable and novel to the best of my knowledge.  The work
is mainly empirical, but the experiments need to be substantially
extended, and the basic approach may need further development, 
to show the benefits of mutation validation.
"	2
The paper proposes a new method for model validation/selection which is simple, makes use of ALL the training data, and appears to be more sensitive to overly complex models; this ultimately leads to simpler models via a more robust selection of hyperparameter values that better fit the true underlying data distribution. My main concerns are how to choose an appropriate fraction of the training examples to mutate, and how this method can be adapted to regression tasks.	3
Though the set-to-hypergraph prediction is an important topic, the technical contributions of this paper are limited. The main statement is not rigorous and well explained. 	2
The paper works on a critical problem and proposes a reasonable solution. However, there are some fatal typos and confusing statements. The experimental section lacks some critical results like absolute running time and only tests on one dataset. The authors should solve my doubts above and modify the paper.	3
Very unaccessible paper which might be understandable by experts in the field but not by a wider audience. Presentation needs to be improved, mathematical notation needs to be completed and experimental analysis needs to be clarified before I would consider this paper eligible for acceptance. 	2
This paper proposes a sufficient method to predict hypergraphs from sets and showed performance increase. Currently, it lacks sufficient detail, and may require further evaluation to justify the claims.	2
Overall, I vote for accepting. I like the idea of integrating specific isolated aspects of biological neurons into otherwise conventional ANNs. Here, the authors show that ANNs can benefit by such an approach and I think that further exploration of such methods may advance both spiking neural network and conventional deep learning research.	3
A potentially interesting idea, but not yet validated	3
In the first practical example it seems that you use this coding in different time windows that correspond to different bins in a sequence and that these sequences are then processed independently. This information processing is far too simplified to qualify as temporal coding and thus to correspond to a spiking neuron model. The other experiments seem to show a clear advantage to the rank coding and seem promising to extend it to the coding of the maximum but also of the successive values. I encourage the authors to apply this kind of method to larger images as it has already been done in the literature.	3
In summary, the paper demonstrates clearly an elegant and novel notation to represent a large portion of operators in deep learning workloads, and developed a meta-framework with solid engineering. The reviewer recommends to consider the acceptance of the paper as a novel good deep learning meta-framework.	3
"**Positive aspects**

- The proposed extension of einsum is useful and I can see it having large adoption in the community of scientific computing.

**Negative aspects**

- I don't believe that ICLR is the correct venue for a paper like this. 

- The novelty and quality of the paper are rather limited. 

- The motivation for the paper is weak and much of the advantages brought by the proposed ""einops"" were already brought by the original ""einsum"" notation. 

- The presentation is, at times, confusing.

- There are many non-rigorous claims that cannot be tolerated in a scientific publication.

- There are some design choices that could lead to unexpected problems when using the library. "	2
"I found the tool very useful for any application involving tensors with Python. The paper is focused on deep learning applications. However, I am sure it is also very useful for dealing with Tensor Networks (TNs), where many core tensors are interconnected. The paper lack of a description of its application to TN contractions, for example. 

It seems that current implementation only consider operations on single tensors. I would suggest to include also operations on two or more tensors, similarly to einsum in numpy and Pytorch, but using a more flexible indices labelling.

A comparison with previously available solutions, via numpy or Pytorch, for example, in terms of computation cost is missing in the paper, making it difficult to evaluate if there is some price to pay.
"	3
"This paper introduces an interesting tensor manipulating library in Python, supporting Einstein-notation style operations over multi-dimensional arrays for deep learning. The library manages to run the convenient interface over different popular deep learning frameworks. 

However, there is a lack of solid empirical study to validate the effectiveness and efficiency of the design and clear discussion about the difference from the existing tool, i.e., einsum. "	2
The paper provides an interesting model to learn tasks on knowledge graphs. However, there are some aspects (limitations of the model and of the extraction procedure, settings of the evaluation) that would need more details and/or discussion. Finally the are some parts that are dense in concepts/theory and are hard to follow (more examples/schemes would be useful to improve readability). 	3
"This paper proposes a GNN-based method, named MGNN, which allows the predictions to be explained symbolically as logical reference to improve their interpretability. However, MGNN does not outperform existing approaches on some benchmarks, and it may be difficult to apply to large-scale knowledge graphs. 
"	2
The idea of the paper is interesting but the technical contributions are not very significant and the advantage of the proposed approach is not obvious.	2
"The main paper is well written and I think it is technically sound (although I found the proofs difficult to follow and did not fully verify them). ~~However, I'm not convinced that the rules they extract really count as ""explanations"": as far as I can tell the extracted rule sets can be extremely complex and may not have any more explanatory power than a gigantic lookup table~~. Empirical results are also not particularly strong; their system achieves comparable performance to other methods but seems to require orders of magnitude more rules and also has fundamental limits to its expressivity.

**Update (Nov 18):** After discussion with the authors, I have raised my score from 5 to 6; some of my concerns in my initial review were based on a misunderstanding of the proposed method, and the authors have addressed many of my comments regarding clarity and experimental details in the revised paper. I still feel that the paper could be improved by describing in more detail how to use the extracted rules to explain predictions, since this seems to be the main way their approach differs from prior work.

**Update (Nov 22):** I have raised my score again from 6 to 8. The authors have added more discussion of how Datalog engines can use the extracted rules and how accurately the truncated rule-extraction algorithm approximates the MGNN. Since the rule-based approximation performs fairly well (and could be improved by running the extraction algorithm for a longer time), it could be used directly for making predictions in domains where explaining every prediction is critical, and then standard Datalog tools could be used to explain all predictions."	3
"To sum up, the paper is too immature to publish in ICLR for several reason:
- First, it lacks a rigorous analysis on complexity compared to vanilla transformer, on various sequence length. Varying the sequence length, the performance of a LMU based method can be changed. 

- The paper is weak in baseline. There are many recently work towards building efficient transformers, using low-rank, approximation, kernel based method, and neural architecture search. All of the work provide stronger performance than vanilla transformer. It is better to compare with at least a few of them.

- Perplexity or cross-entropy on a pretraining task such as LM or MLM can be very limited. Plenty of related work (such as gMLP [4]) have been demonstrated useful in the pretraining task, but they fail to outperform transformer in the down stream tasks with finetuning. Many work failed to provide pairwise information as a self-attention can provide. 

[4] Pay Attention to MLPs"	2
"This work proposes a very interesting application of a novel, parameter-efficient architecture on a large-scale language modeling task. However, the paper presents an incomplete comparison that plays to the strength of the LMU architecture by comparing only parameter-matched models. Given that a significant fraction of the LMU compute is hidden in non-trainable parameters, a fair comparison should also compare models trained with an equivalent amount of training computation; ideally in terms of total training compute used.

Another weakness pertains to the limited number of tasks used for this comparison; also evaluating on tasks other than language modeling (say downstream tasks like the SuperGLUE benchmark, or Machine Translation) would significantly strengthen the claims made in the paper.

Overall, given the above two limitations I would recommend rejecting the paper. While the proposed architecture is interesting and well motivated, the paper needs stronger empirical validation and a fairer comparison against existing architectures.
"	3
Overall, the idea is interesting and the goal of the paper is definitely valuable. The framing and the delivery, however, are greatly lacking. My main concerns are regarding the validity of the evaluation and the lack of experimental details and comparison to alternative efficient variants. For these reasons, I think that the paper is not ready yet because it lacks sufficient evidence for the main claims and requires additional experimental efforts to better demonstrate the effectiveness of the proposed method. 	2
The paper presents a very interesting idea with strong empirical results on language modeling, that could inspire a new generation of transformers. However, there is some uncertainty with respect to the connection between the experiments and the research hypotheses, which makes it challenging to convince oneself of the value of the results. I am looking forward to reading the authors' response.	3
"Overall, I find the algorithm to make sense intuitively and the experiments are quite thorough. The results are superior to the current baselines and therefore I am satisfied with this paper.

~However, I have a few concerns and clarifications. If they are resolved, I would recommend the paper to be accepted.~
[Update]: I am satisfied with the author's response. I am increasing my score."	3
"Question: Both ZO-AE-DS and FO-AE-DS outperforms their counterparts without AE module. Considering the denoising nature of AE, the DS+AE can be viewed as a type of stacked DS. Have the authors tried other combinations of stacked DS? Is it possible to also increase the performance like inserting AE to FO-DS? Overall I think the proposed zero-order optimization for black-box defense is a very promising application scenario. Although the method seems doesn’t outperform FO version with surrogates, I think it has broader applicability owning to its minimum assumptions.

"	4
"Overall, this paper formulates the problem of black-box defense and provides a feasible black-box defense framework. It is clearly written and easy to follow.The updated experiments are comprehensive and convincing.

[Update]: Though the method still can not really handle ImageNet, the authors' rebuttal addresses most of my concerns. Also, the setting of black-box defenses is novel and should be encouraged. Hence, I am increasing my score to 8."	4
I like this work, but I do hope the authors can further highlight the contribution, especially when comparing this work with [1].	3
A strong, well written paper describing a novel approach for incorporating knowledge into a transformer encoder. The main components are not highly novel from a technical point of view, but their combination is.	3
I am leaning towards acceptance. The way TOME infuses textual knowledge into Transformer models is new. This approach actually integrates a passage retriever into a transformer-based LM and potentially covers broad downstream tasks. But, I think the analysis on the experimental results can be improved.	3
The paper presents a large-scale pretrained Transformer that attends to mention encodings (EaE+VKB). It improves over EaE but lags behind state-of-the-art. 	3
The paper is clearly written and easy to follow. The motivation of the proposed method is intuitive. The model design is reasonable. The experiments are solid. In general, this is a good paper. One suggestion is to add more entity-insensitive task, to verify that the proposed model will not decrease its performances on such tasks.	3
This is a good paper with novel idea. More results are needed to make it better.	3
"The paper proposes a novel method, inspired by meta-learning algorithms, for 3D shape reconstruction from images. The proposed method produces results which are comparable or better than the current state of the art approaches. On the other hand, the paper does not provide  a clear explanation why the proposed approach is suitable for the problem of shape reconstruction and why it is expected to perform better than approaches employing a single network. Adding such an explanation, and a more in-depth analysis of the improvements brought by the bi-level optimization will significantly improve the paper. Currently I recommend ""weak accept""."	3
"I am generally positive about this paper since it proposes a novel method to single view reconstruction using few shot learning. But it needs a more detailed illustration about the training process and more evaluations on the design choice. If the authors can resolve those issues I mentioned above, I am willing to give a higher score.
"	3
I believe the idea proposed in this paper is interesting. However, the paper presentation should be improved and further discussion is required to make it a strong paper.	3
Nice contribution. Needs more empirics	2
"Originality and significance: The main theoretical results presented in the paper (conditions on the monotonicity and bounded error) are novel to the best of my knowledge. The conclusions on using well suited sigmoid functions are interesting and relevant to the body of work on differentiable sorting networks.

Quality and clarity: The paper is well written and the presented results are easy to follow.
"	3
"This paper is a strong theoretic paper that proposes to replace the sigmoid function in differentiable sorting networks by one such that the network becomes monotonic and has bounded error, which guarantees the correct gradient direction when training sorting networks. Empirical results show the effectiveness on the task of sorting 4-digit MNIST numbers and SVHN.

While I think some additional experiments could be performed, I think this paper should be accepted given the well motivated idea, strong theoretical results, clear presentation illustrated by figures and the resulting empirical results."	4
This paper presents an important and under-looked infrastructure contribution: a platform to make collecting human data easy for any existing simulators. The platform makes use of good decision decisions. The accompanying Atari dataset could be useful for useful for muli-agent and multi-modal RL research as well. Therefore I recommend a strong acceptance for this paper.	4
"The paper introduces a useful tool that can be impactful for the community (Imitation Learning, Offline RL), however, I believe that it is not ready for publication due to the presentation and lack of dataset analysis. I encourage the authors to focus on the following two points:

- Describing the dataset in more detail providing the insights about the data and their properties.
- Explaining why the dataset is challenging to the existing methods, and how it can drive the further development of the fields of imitation learning and offline RL."	3
Overall, this is a valuable piece of work. The proposed framework CrowdPlay, including the software and the dataset, is interesting and attractive for researchers in the areas of Reinforcement Learning. It would be better if the above issues could be further addressed to make it smoother.	4
"The proposed architecture is potentially interesting, even if it seems to go against Sutton’s “Bitter Lesson” by specializing an architecture for a specific case, rather than relying on learning. It does look brittle due to the hardwired nature of the choices that are being made; in particular the number of policy modules in the agent and the division of states across the policies. I am open to being convinced these aspects don’t make the agent brittle, and would raise my score in that case, but that would require more extensive experimentation and baselines.

There are some misnomers, or uncommon uses of terms: mainly ‘inhibition’, but also ‘episodic memory’. Otherwise the paper is well-presented; changing the terminology to be more in line with the literature would make it mostly easy to read.

My general recommendation at this point is to review the agent architecture and see if this is a reasonable point to present results. It might be interesting to see what this kind of structure does to agent performance on a wider array of tasks, and how those tasks are solved. If it does well generally, this would be a strong paper, but in its current form it looks a bit unfinished.
"	3
Although I appreciate the attempt to bring another neuroscientific concept into RL, I'm mainly worried about the significance of the reward function engineering in the presented results and thus a smaller importance of the proposed method.	2
Currently, the paper is lacking an adequate discussion of related works, as well as appropriate baselines beyond SAC. As the authors are proposing a new method for transfer learning, a comparison with existing works is necessary for proper empirical evaluation. Given that this method shares some similarities with hierarchical RL, these may be useful baselines. Looking to these baselines could also provide additional tasks and environments explored in previous works. Without these improvements, the proposed method is difficult to evaluate, as the larger context is missing. For these reasons, I cannot recommend acceptance at this time.	2
The clarity of the paper is not sufficient for assessing its contributions and how it positions itself with respect to the prior literature. 	2
I recommend weak rejection for now, considering the weaknesses listed above.	3
The paper presents a SHAP-based explainability method for MIL models. The paper is easy to read, but it lacks the sufficient comparison with proper baselines. I mentioned a paper that should be used to build a stronger baseline.	2
"The paper addresses the interpretability of multiple instance learning, which could use more attention in the literature, but it has weaknesses in terms of the novelty of ideas, the clarity of terms and the experimental evaluation.
"	3
"As stated on my review, there are quite some merits regarding the presentation/reproducibility of the manuscript, and the simplicity of the proposed method. 
However, I have concerns regarding the validation of the proposed method. On the one hand, it seems to cover a reduced set of MIL scenarios. Therefore, its generality cannot be guaranteed. In addition, at this point, it is hard to assess how conclusive the reported results really are. The fact that some of the proposed methods perform better in one or another dataset, suggests that the overarching goal of the manuscript -  of proposing an explanation method - has not been achieved. "	2
"This paper proposes a relaxed loss to narrow the loss gap and reduce the distinguishability between the training and testing loss distributions, and further prevent the privacy leakage. However, I have some questions about the theoretical analysis, i.e., why the ""RelaxLoss increases the variance of the training loss distribution"". "	3
The approach is clear, and the extensive experiments performed on variety of datasets against reliable baselines proves the merit of the proposed defense algorithm. However, the intuition behind some assumptions and steps in the algorithm is not clear and authors should certainly discuss the limitations of their method.	3
Overall, I like this paper. The used techniques are well-motivated. Also, the experimental evaluation is very thorough to include several datasets, multiple existing defense baselines, and include adaptive attacks. So, I recommend accepting this paper.	3
Overall, the paper introduces an interesting approach for estimating Shaley values in real run-time. The effectiveness of the method is well demonstrated across different tasks/datasets.	3
"- Overall, I think there should be an explicit comparison of the computational complexities of FastSHAP and other algorithms. 

- Instead of comparing the performance, a more extensive comparison of the runtime is needed. "	3
Overall, the paper makes an important contribution for post-hoc explanation of ML models, in computationally efficient ways. The paper is well written with strong results, but there is further room for improvement. 	3
Please see the Cons in the main review.	3
The approach looks promising, but is limited in terms of the experiments and the comparison with existing approaches that makes it difficult to assess the true potential of this approach	3
"This paper proposes and tackles a very novel problem in continual learning that would have great significance to the field and is important to explore. They are able to demonstrate a somewhat novel effective algorithm that significantly improves performance, and does not degrade over time. Unfortunately, this paper is largely empirical and does not have sufficiently diverse experiments to characterize this problem. Given this, I currently recommend weak rejection for this paper. While it explores a large variety of activation functions and hyperparameters, all the networks are relatively small Feed-forward networks. They compare several methods and demonstrate that regularization is somewhat effective in some cases, but does not fully explore why regularization works or other methods. It currently falls short of fully explaining the root cause of the problem.

=================================================================
Post Rebuttal:
I would again like to thank the authors for including the additional experiments on model size and dataset change speed. I believe this paper is borderline, and am updating my review to marginally above the acceptance threshold. The additional experiments improved understanding and I believe this is a very useful direction to explore. I also agree with reviewer gcKs and Evj3, that this should be compared to existing continual learning methods and papers. The other datasets should also be useful to experiment with since they should still demonstrate at least some loss in Neural plasticity with 20 stationaries according to the other experiments. I also agree with reviewer kzUn on the depth of understanding this paper provides. This paper highlights a very useful novel direction to explore continual learning problems and a reasonable solution, but would drastically be improved if it provided much further depth."	3
The topic of the paper is of potential interest for the community and the empirical analysis sufficiently broad. On the negative end, the depth of technical discussion and of the empirical analysis does not support the claims. Originality of the contributed CBP algorithm is minor.	2
"Overall, I vote for marginally accepting. I like the idea of continual reinitialization and handling it by the proposed generate-and-test method. My major concern is about the limited novelty of the paper -- its build upon previous work by Mahmood and Sutton 2013, and some misleading terms such as ""continual backprop"" and ""feature"".
Hopefully the authors can address my concern in the rebuttal period.

[After rebuttal]:
After reading the other reviewer's comments and authors' feedback, I would like to lower my score due to the same concerns as other reviewers as mentioned."	3
"The proposed kernel classification layer is a promising idea, however
I believe it needs more theoretical and empirical analysis to make it
generally applicable.
"	2
Given the identified weaknesses, I recommend a weak acceptance for this submission. I am glad to increase my score if the authors are able to resolve my concerns.	3
The advantages of the proposed method over the related methods are not discussed and demonstrated well. There is also limitations of the proposed method as described in may main review in the sense that its capacity for creating highly nonlinear decision boundaries is limited. Therefore, I believe the contributions are not enough for acceptance.	2
Overall, this paper is well written and the idea is clear. Great amount of experiments are conducted and empirical novelty is shown. Though the originality of the idea is not very strong, I would conclude my current review for this work as: merits over flaws, and marginally above the acceptance threshold.	2
Paper provides useful insights about the role of the generator and discriminator in GAN transfer learning, as well as the behaviour of transfer learning itself (i.e., improved coverage but not fidelity).	3
"In general, this paper has conducted a series of experiments with the stylegan2, and got some conclusions, which are helpful for subsequent papers and the community. The analysis and experiments are not very comprehensive. In many ways, [1] is more comprehensive than this one, although [1] is from a few years ago.

[1]. Transferring GANs: generating images from limited data
"	2
This paper answers important questions in GAN transfer learning, and therefore makes a solid contribution.	3
consider a important problem, but heavily rely on the the results in the previous work.	2
The paper proves the convergence of the optimizing nonlinear implicit networks. The proof techniques follow the standard approach for DNN, and I think it is a good starting point for the theoretical analysis of implicit networks.	3
The paper sets the foundation for the training theories for implicit models. Though some common techniques are employed to in the derivations, the authors successfully tackle the key issue of well-posedness to make the convergence result possible. The reviewer believes this result is significant for implicit models which have become increasingly popular in the community.	3
"
Overall,  I think this paper makes a clear contribution to proving the convergence of gradient descent for an implicit neural network with ReLU activation with infinite layers.  So I recommend acceptance. "	3
Not quite ready for top conferences like ICLR	2
"The problem setting is good, but the main conceptual weakness is a lack of explanation for how the model is supervised to be equivariant to different data augmentations. Furthermore, empirical results seem to suggest that the proposed data augmentation does not have much of an effect on performance, which seems to undermine the motivation for this paper.

EDIT (AFTER REBUTTAL)
Thank you to the authors for your response. Based on the authors' response, it appears that the majority concerns I had raised will be addressed in the next iteration of the paper. Therefore, I maintain my original score and look forward to the new and improved version."	2
The paper well demonstrates the benefits of counterfactual data augmentation for model-based RL. However, the technical contribution seems limited and involves very strong assumptions. I, therefore, consider it below the acceptance threshold.	2
"- The paper claims to use structural causal models but this is the case only in a trivial sense at best.

- The causality perspective provides no novel insight.

- The experimental setup pales in comparison to other uses of domain randomization."	2
Paper presents an interesting construction for controlled learnability, but the setting of the paper seems to be a bit misleading. Paper argues that learnability lock provides privacy through their encryption scheme, yet in the same paper authors break their own scheme. Paper also provides no cryptographic underpinning as to why any privacy is provided at all.	3
"- Overall, the paper tackles a reasonably well-motivated idea of introducing perturbations to prevent unauthorized training over a labeled dataset. The authors show that the approach achieves this objective and is additionally more robust when compared to previous works.
- My biggest concern is the significance of improvements over prior work, which appears limited to a highly specific failure mode. I would be glad to increasing my score if the authors could address this."	2
Overall, I believe that the idea of the paper is interesting and novel, the results are appealing. The writing is very clear and structured, the method is compared to other attacks and defenses and is shown to outperform previous methods. I vote for accepting the paper. 	3
Important research on data protection/authorization, simple yet very effective method, some level of technical improvements over prior work but addresses two key limitations, rich experiment, good writing. A quite solid paper overall. 	3
"This paper is an extension of the previous work ReQueSt, with the focus on learning safe policy from human demonstrations only, without simulators or specifications. There are some concerns about the approach. It has a very nice demonstration in challenging pixel-based 3D tasks.
"	2
Overall it is an interesting paper with some good results shown. I am not sure the results are sufficient enough. I would also like to see more detailed comparisons with the state-of-the-art.	2
I generally like the idea of the original paper (and hence also this paper) but in my opinion this paper is not enough for publication in ICLR. The contribution is very limited over the original ReQueST.	1
The paper makes an important promise-- but the formulation and experiments are not coherent with this promise.	1
I'm voting for borderline acceptance for this paper but I'm not very familiar with this area.	2
Proposing a novel approach to solve the problem of inferring the network structure of games with unknown utility function, and in the experiment section authors show the superior performance of their approach compared to other alternative approach.	3
Applying transformer-like architecture to network inference for network games is an interesting direction, but it is unclear how much is the technical contribution of the paper, and the experimental result is not very strong, as discussed above in the weakness part. Therefore, I think the paper is marginally below the acceptance threshold.	1
"The paper successfully applies Deep graph inference model to the problem of network discovery in network games. The proposed architecture is not very novel and the problem framework, while more challenging, doesn't lend too much to methodological novelty.

With regard to empirical results, I am not convinced of the generalizability and significance of the results. Therefore, I currently recommend rejecting the paper. Providing more extensive evaluation with better baselines and models as well as showing extent of generalizability would strongly help improve the contributions of the paper."	2
The idea to use joint differential privacy instead of differential privacy in MTL is interesting. The privacy analysis and the convergence rates presented are fairly standard. The theory doesn’t seem to stand on it’s own and more experiments can be done to rigorously test the introduction of this idea. Hence, I believe in it’s current form the paper is not ready for publication.	2
The work is complement, although only one algorithm for one problem, the analysis and experiments are sufficient.	3
The paper considers/formulates an interesting problem and is generally well-written. Unfortunately, the convergence results do not appear to be strong (or clear). Additionally, there is confusion about the notion of privacy that their algorithm provides and the role that JDP is playing in the paper. For these reasons I cannot recommend acceptance for the current form of the paper. 	2
"As this is a preprocessing step that essentially is similar to matrix completion with graph side information, then I believe that comparison with this literature is missing. Otherwise this is a very good paper. The approach they have for the problem, which could be seen as matrix completion with graph side information, is I believe novel in that domain, as well as in this framework for graph neural networks. Therefore, comparison with this domain is important here, as other methods in this domain could also be well justified as pre-processing steps. One such method that both scalable (using similar final equation, but dervied from a different approach, conjugate gradients), must be compared both in the paper and experiments in my opinion. Otherwise, it feels like the work is trying to avoid comparison with similar methods by claiming this work is related to GNN, while really it feels like this is a matrix completion with graph side information method, that is then used for GNNs.

After feedback:

I still feel that the motivation for using this particular method of estimating the missing values is not clearly better than other approaches. Also, the link to GNN seems weak as essentially this is a method for completing missing data and with this seems better suited to the domain for estimating missing values, where a stronger comparison with other missing values estimating methods can be compared - for example in comparison to recommender system approaches like collaborative filtering with graph side information. The link to GNN could be a small subsection in that style of paper. I don't see why running GNN is such a large part of this work when it's a stand alone preprocessing step. Therefore I've lowered my score from 6 to 5. "	3
In general I like the idea of using FP as a preprocessing step. However, I am not convinced that the idea is novel enough to be accepted in ICLR, details are referred to my main review.	2
Under this perspective, I regard that the novelty in this paper is limited.	2
"
Initial assessment : accept

The paper is well written, and easy to follow. The problem the authors address is interesting, and the solution provided is simple, yet elegant. The experimental results are thorough, both in terms of comparing performance and speed of the method. Missing value imputation is a problem that is often ill-addressed when it comes to ML research. In this paper, the authors specifically look at imputation in the context of training GNNs, and use the graph to perform the imputation. 

I feel the authors need to address the difference between the proposed method and label propagation a bit more. They claim it's different because of labels being discrete and the diffusion operator being different, but I'm not clear why that's a deal breaker. Won't the algorithm be the same but with a new operator? 

Another point to address is the extension to vector valued features. They claim the extension is straightforward, but a note to this effect will be nice. 

other comments:
P1 last paragraph : I’m not sure if being unaware to the task is a bad thing necessarily.  isn't it better to do imputation in a task agnostic fashion?

Fig 3: Please make the text larger. The figures are hard to read. 

Fig 3: Also any intuition why Random is so much worse than Zero in all these datasets?
"	3
While automated curriculum learning is an important topic, the purported problem they are attacking seems a bit too obvious. I am not sure how novel this is, and their solution appears needlessly confusing and demonstrated only on toy examples.	2
"I'm suggesting marginal below acceptance threshold because I find certain parts of the paper unclear. The experiments support the argument of the paper which is great however I'd like to understand and see the paper improved in a few aspects:
- Understand if the notion of aleatoric parameters is the correct one here. See C4
- Understand C6 and C7 better in order to follow the complete argument and the proofs."	3
Although the paper tackles an important problem, the proposed technical solutions do not seem to be entirely convincing and the experiments are insufficient. I believe this paper needs another iteration before being ready for publication. 	2
I think the paper falls short because of strong assumptions made by the proposed method on simulators, uninformative theory, and poor presentation of the problem being addressed.	2
I have not read the proofs in detail. This optimization angle on NCE seems rather new and the overall expose seems convincing and beneficial to the community.	4
Based on the above comment, I believe this paper gives interesting theoretical results on NCE and would vote for accept.	3
"
I think this is a good paper and I am in favour of its acceptance. I do have a few points of concern / thoughts on how the paper can be improved, but I hope the authors will take these into account.

As such I have set my recommendation to ""6: marginally above the acceptance threshold"", but will increase my rating if the authors are able to satisfactorily address my comments.

[Edit: updated my score to ""8: accept, good paper"" after author response] "	4
This paper is generally solid, although there is some confusion.	2
This is an overall well-executed paper. Besides the above comments, the authors need clarify on a few raised questions.	3
Overall, this paper is novel and interesting. However, some points should be improved in the revision.	4
The paper has provided some novel contribution in terms of the theoretical properties for unsupervised learning. I felt that the paper could have been better written and more experiments could have been conducted to make their results more convincing. But nevertheless, I do think that the results that the authors have presented are novel enough for a publication at ICLR.	3
"Unsupervised FedLearn

comments:
- not so much to mention but only some 'concerns'
-  Unsupervised learning is really different to supervised learning.
   In your case you consider local cluster assignments as labels and use
   subsequently a supervised learning scheme to generate predictions.
   This is not really unsupervised because in general SL methods focus 
   on margin maximization of the class boundaries ... which is not the same
   as approximating a cluster distribution (so I would say your title is a bit
   wrong). Maybe better: Federated learning with surrogate labeling?
-  Further what happens if you have no uni-modal data as shown in Fig 1?
   In fact your # of labelings may change during the optimization or you
   artificially colaps different distribution into single once if the #of cluster
   is not appropriate
- 'Without any labels, FL becomes significantly harder than before, since it is unclear how to compute
   the local gradients at each client and how to aggregate them for updating the global model  '
   --> well there are unsupervised learning approaches which are gradient based
   ... you only need a differentiable cost function!
-  if the labeling in the experiment (for labeled data) follows the data distribution
   I do not expect a substantial problem ... but if the data are non-unimodal and the
   #of clusters does not fit it may become a challenge to get reliable models
- theory is likely correct and code is provided so reproducibility should be given
"	3
Reject	2
The paper studies an important problem, but the experimental protocols adopted do not allow any conclusive results. The main claim on the benefit of quasi-character level vocabulary in low-data regime (over a well-selected subword vocabulary size) isn't justified by the presented experiments, due to the issues pointed out above.	2
This paper adds to the ongoing discussion of character-level versus subword-level MT, providing more data points in more architectures indicating the character-level advantage with small training set sizes. It also adds an interesting point about domain flexibility. Overall, the experimental contributions do not seem to be sufficient to warrant a main-conference paper; the results on low-resource scenarios were expected, and the results examining domain flexibility and forgetting are not enough to carry a paper. More experimental emphasis on the utility of quasi-character-level versus character-level, or more experiments on preventing forgetting as suggested in the future work could strengthen the paper substantially.	2
This paper is only comparing 2 approaches: standard subword tokenization vs the proposed quasi-character. The paper is missing several critical comparisons, for example, character-based NMT, and quasi-character on different vocabulary sizes. There is also no report about the performance, despite the author criticizing the performance issue about character-based NMT. Lastly, the paper is also missing related works that have the same claim of low-vocabulary size is better for low resource NMT.	2
Overall, I believe this paper tackles an important and interesting problem in the differentiable physics community. While I believe the proposed approach has merit, I would like to see some of the aforementioned issues (particularly discussion of failure modes and choice of baselines) before I can recommend acceptance.	2
This paper proposes an intuitive and effective way to utilize differentiable physics in long-horizon motion planning for soft materials. Compared to previous papers like DiffTaichi and PlasticineLab, this method can better handle multistage tasks. The demo videos show its capacity to plan very long tasks (up to 140s).	3
I think the paper presents interesting work that achieves non-trivial results in deformable object manipulation. Some of the details are missing from the text as discussed above, which would help better gauge the impact of the work and help reproduce the work by other researchers. Overall, I think the work makes a good contribution to the conference.	3
"This is an experimental paper that argues for using non-uniform (exponential) and non-linear (max and median vs avg) aggregation of long time-series inputs when used with LSTMs.  The ideas are simple, but potentially could be quite effective for some applications. Unfortunately, experimental results are flawed, and only partially convincing. 
"	1
Overall there is value in studying the problem. However, as evident from the Cons above the paper falls short of doing a thorough empirical study. There are multiple problems like not studying longer history length problems, not comparing on well-known multi-variate forecasting benchmark tasks, and not comparing with a simple baseline that feeds all the granular data (this can be done for history length 48). 	1
Okay but its a way from being accepted to ICLR or something else top tier. 	2
Even though the motivation of this work is clear, I have multiple concerns about the scientific rigour of the experimental setup (see the review section). Based on the current status of the manuscript, I am recommending to reject this paper.	2
"TL;LD: it's an important emerging area. It's a reasonable (although not formal) way to perform transpilation. Using self-supervision is likely one of the most important aspects of the system because 99% of the code that we have today is currently unlabeled. To my knowledge, only a handful of labeled datasets exist (e.g., POJ-104, Google Code Jam, IBM/MIT's Project CodeNet, Microsoft's CodeXGLUE, etc.)

Weak accept."	2
"The proposed method achieve significant improvements over the baseline TransCoder and the DOBF models. However, I found the following weaknesses, resulting in my recommendation.

1. Bottleneck on the availability of test generation tools thus limiting the number of languages it can be applied to.
2. Quality of generated tests dips on more complicated code segments, and we do not know what effect that has on the performance. The method is trained on Java functions that can be executed in isolation.
3. Method to generate test cases for translated Python and C++ codes seems to be dependent on expected input/output pairs, and it's not clear how the proposed method would work for more complicated code segments, or for languages with no direct mapping of data types and constructs."	2
"The idea of the paper is specific to translation between programming languages and may be of interest to a subset of the ICLR community. There are interesting ideas and overall good evaluation of the paper.
"	4
A well written paper that introduces a practical method to generate a supervised parallel corpus for code-to-code translation. The findings are  generally well supported by evaluation, but some additional tests could strengthen the paper.	3
I would recommend the paper for acceptance. While there are some issues, the paper is generally well-written, explores new techniques, achieves good results.	3
Overall, I found the first four pages of this paper quite enjoyable, but found the remainder large impenetrable as someone unfamiliar with the area. Given the fact that obvious ablation experiments are missing, I do not think that this paper is ready for publication, lacking both in clarity of writing and strength of evaluation.	2
My main criticism of this paper is the lack of discussion and comparison with existing works, as described in point 1 and 2 above. The proposed modifications to the NDTT framework [(Mei et al., 2020)](https://arxiv.org/abs/2006.16723) don't lead to significant improvements over the original methods.	2
I recommend the authors to improve the writing and clarify whether their approach aims to contribute to general-purpose continuous-time modeling or on incorporating continuous-time logic specification via the NDTT framework. In the current form, neither of these two objectives is fulfilled adequately (see comments above).	2
The paper provides an efficient approach for correcting inaccurate outputs of large nlp models while leaving the model otherwise intact. They show the advantage of their proposed approach over the existing methods through a number of numerical experiments. 	2
In summary, the paper studied an interesting problem but the practical significance is doubtful. The empirically results are not strong. A concern of fair comparison is raised. 	2
"I deeply appreciate the practical contributions of this paper and believe that it can have significant impact on the applicability of the large pre-trained models. That said, I do have several minor issues with the experiment design. I summarize those issues in the ""Weaknesses"" subsection of the main review."	3
The paper proposes an effective, novel, and general technique for manually editing pre-trained models. While the method is not universally better than other editing techniques (especially for smaller language models), I can see immediate interest and impact of this work in large AI models. I vote to accept the paper.	3
"I thought this paper was generally well written and I appreciate the overall goal of it, which is to make camera-based measurement of certain vital signs more easily accessible and therefore useful. I also appreciate the effort to apply and adapt a new technique (vision transformers) to the problem, as well as the benefits of adjustments to TS-CAN to improve its performance. While the results on the cross-dataset experiment look strong, I find the technical contributions and analysis to be a little light for the level I would expect of ICLR and I would have preferred to have seen more careful study of a single method to understand its benefits. For example, if focused on EfficientPhys-C, what are the effects of the changes to the model vs TS-CAN? Or if focused on EfficientPhys-T, what other architectural alterations could be explored to improve latency? I think that the approach introduced in the paper is good and that with some additional care towards framing, explanation and analysis, this can be a nice paper and contribution to the physiological monitoring literature.

"	3
"At face value, this paper produce superior HR detection result, and more efficient algorithm. However, at closer inspection, a more direct comparison, and evaluation on a more challenging dataset are still needed to validate the effectiveness of the algorithm. In my opinion, the contribution is still not enough for a top-tier venue like ICLR.
"	2
In summary, I feel the work lacks sufficient technical contribution in terms of novelty and empirical motivation. The experimental results are good but somehow expected. The writing of the paper needs improvements.	2
"The paper is an interesting contribution in the task of camera-based physiological measurement.
Two neural models are presented with two different approaches: convolution, and vision transformer.
They are alternative version of the same network called EfficientPhys, and represent valuable examples of efficient deep learning: they show a lightweight pipeline having removed the data preprocessing, and a spatio-temporal attention build on the parameterless tensor shift (for the convolution version). Nonetheless, they obtain good performance in terms of accuracy (MAE, RMSE, ro) and latency, with the convolutional model in the Pareto frontier.
The inherent lightness of the architecture makes the models portable to multiple devices platforms, facilitating the access to healthcare in low resources scenarios.
For these reasons the paper is to be accepted, with minor changes mainly to improve the readability."	3
"(Questions)
1) Why do you believe that embeddings are optimized to preserve the natural clusters of the real-world knowledge in its metric space? At least the three embeddings used in the paper never explicitly optimize any clustering objectives. They want the words closely in the raw texts locate closely in the Euclidean space. 

2) Geometry often means the characteristics of the manifold or topological essence defined by a collection of open sets (that will indeed define suitable metric for metric spaces). Some expressions about geometry or invariance would be too subjectively used.

3) It is interesting to see how clustering velocity is defined as area under the curve in the graph of increasing number of connected components. Different types of graphs (maybe different scale-freeness and degree distributions) naturally differs in this clustering velocity, but it is a bit doubtful that this metric can be drastically different even for two graphs with similar degree distributions by adversarially tweaking couple of local connections. As both models and new metrics are proposed, there must be a natural question about the stability and sensitivity of this metric.

4) More active form of contributions that can impact modern representation learning and transfer-based language modeling will benefit the audiences in the community.


(Minor Comments)
A2) left figure  attach the figure reference as the actual figure is located in the previous page.

A3) figure 5 not in the main draft.
"	2
"
In sum, I think the authors look at an important problem. But there are some limitations of this study, e.g., not looking at contextual embeddings."	3
This paper studies an important and interesting problem. However, it does not provide a good enough solution to the problem to meet the bar of ICLR.	3
This paper studies an interesting problem and proposes two novel measures. This paper lacks some key elements, such as problem formulation, rationale discussion, and empirical validation. As its too many weaknesses, I have to recommend a reject.	3
"
This is an interesting submission with a very novel (at least, to my knowledge) approach to choosing the step-size for SGD.
Effective step-size selection for stochastic optimizers is a challenging problem with potential for great impact on current machine learning practice. 
As such, I think this work is a good fit for ICLR.
Theoretically, the submission is sound and I enjoyed reviewing the theoretical arguments (see ""Theory"" below).
However, empirically, the work is weak and, in my opinion, essentially unfinished. 
I found the decision to focus the experiments on heuristic extensions of Eigencurve to non-convex optimization to be strange, given the theory does not even apply to general convex functions.
The majority of figures are completely unreadable and all results are reported without distribution information; it is to judge the significance of the experiments as a result. 
See ""Experiments"" for more details.

Although interesting, I recommend that the submission be rejected given the unfinished state of the empirical evaluation."	4
"The paper proposed an interesting learning rate schedule that achieves an optimal asymptotic rate on strongly-convex quadratics. However, the schedule relies on first computing the Hessian spectrum and the empirical improvement is not significant. 

Overall, this is a borderline paper and I'm inclined to reject the paper (though I'm willing to increase my score if the authors can address my comments)."	2
I think the paper is clearly below the bar of acceptance. From the theory side, many points are not well supported (e.g., comparing upper bound to upper bound, comparing the minimax rate, etc.) and little technical novelty can be found. From the practice side, the improvement is too small to justify the cost of computing second order information. 	3
Overall this paper offers an interesting perspective and approach to learning rate scheduling on an existing quadratic setting that shows practical implications in image classification. The experiments are thorough (including various ablations in the appendix) and validate the improved performance from the approach, especially when the number of epochs is limited. 	3
By proposing an efficient (sub-linear-time under certain assumptions) exact sampling algorithm for NDPPs, this work makes substantial progress in scaling up NDPPs to real-world applications. This paper is overall well-written and easy to follow, and it also demonstrates a decent amount theoretical novelties.	3
"Overall, I like the paper and recommend acceptance. This is the first paper to give efficient algorithms for exact sampling from NDPPs (which can be used in a variety of real-world applications like recommender systems etc) and thus this paper can have a good impact.
"	3
Overall I think this is a well written paper. My only concern is that the more scalable sampling algorithm is achieved by imposing more restrictions on the kernel, which might limit its usage. However, this might not be a big issue because it is shown in the experiments that its predictive performance is not degraded.	3
All in all, the small lack of true originality is compensated by two useful theorems that can prove useful to the community.	3
The paper has improvements compared to previous work based on the system design and new algorithms, however, some critical parts seem incremental.	3
This is a well written paper, with a novel contribution that is likely to leave an impact in the emerging field of generative models for molecular structures. The main weakness, in my opinion, lies in the complexity of the generated molecules, which at this point is not clear whether it can lead to relevant practical applications or not. But overall, I am still leaning positive in the light of the gap in accuracy with respect to previous methods.	3
"
The paper extends autoregressive flow models for 3D molecular generation to also generative 3D geometry. This helps with some challenging molecular discovery tasks, and a set of ablations and comparisons demonsrtate the effectiveness of the method.  The main concern is that it is somewhat incremental over the most similar prior work which does not generate 3D molecular geometry."	3
Overall, the paper builds largely based on the setting of G-SchNet. There are a few designs to improve the performance. However, the benefits of the designs are not thoroughly/fairly evaluated.	2
Overall I find the work to propose an appealing convolutional operation, in that it attempts to satisfy optimal design for a given model size and complexity. I believe the paper was difficult to read and would benefit from improved explanations of concept and flow.	3
I believe this paper provides an interesting analysis suggesting how to configure the hyperparameters in efficient convolutional operators. The novelty from my reading of the paper was mostly confined to the analysis and conclusions around group number. I thought there were some key areas of related works missing, and the evaluation section was promising but a bit underwhelming relying mostly on CIFAR and custom ResNet models. Overall, I would argue that this paper should be a borderline reject.	2
Overall, the idea and content of this paper are quite clear, the main concerns from the reviewer are the novelty of the proposed method and practical implementation for the CNN community. 	2
Overall, I think the authors have done a great job in explaining solid calculation for their optimum. However, their results are not convinced in terms of real elapsed time (slower GPU times) and marginal improvements in gain on number of parameters. The structure and representation of their work is good but empirical evidence is not sufficient.	2
"The proposed model is clearly of interest to the ICLR community. Both the discussion of the issues with modifying the prior and the simple and effective solution of introducing a latent transformation should provoke some interesting discussions and further work. The paper is exceptionally well-written and presented, and while it would be a stretch to call it a groundbreaking paper, it is a solid ""accept"".
"	3
While I think this is solid work, the current presentation is not necessarily kind for readers to understand the technical contribution specific to this paper.	3
"The paper shows that modifying a decoder in a VAE could be helpful and allows to still use a simple marginal distribution over z’s (e.g., a standard Gaussian distribution). However, the presentation of the paper is somehow misleading in many places that making it difficult to follow the flow of thoughts. Therefore, I believe the paper is ready to be accepted.

=== UPDATE ===
I would like to thank the authors for their rebuttal. Indeed, some of my concerns were fully addressed and the paper looks better now. However, I am still not fully convinced how to properly choose the transformation $g$ and what is the recipe for that. Anyway, I decided to increase my score to 6."	2
The paper is well written and convincing. The new framework is not novel from a technical point of view but the details of the layers introduced to model complex topologies are. There is a major need of methods for embedding human knowledge into deep learning architectures and this paper is on a good direction. 	3
The paper proposes a novel approach to solve a challenging problem: long-horizon multi-tool deformable object manipulators tasks. The idea can be beneficial for the robotics community and seems to be extendable to quite different tasks in the same setting.	3
"UPDATED (2021-11-21): The authors answered my questions and updated a few sections of the paper. I think the work is now in a great state and should be highlighted.

OLD: I'm pretty happy with the paper. It's very novel, well-written, and would make for a great contribution to the robotic manipulation literature."	3
This paper formulates a pipeline of applying differentiable physics to long-horizon tasks. The experiment setting is very challenging. It deals with sensory observation, deformable body, variable tools, and complex tasks. The proposed method solves those problems reasonably well as shown in the video. I think their method has the potential of generalizing to other similar tasks.	3
The paper addresses the challenging problem of long-horizon RL by using differentiable physics and imitation learning. It is a inspirational method in terms of the way differentiable physics is coupled in large-scale complex problems and the decoupling of different sub-steps. However, there are some issues described above that are not clearly demonstrated in the main text nor in the supplementary document. So, I decided to give a score of 5 as the initial rating. I am happy to raise my score if my concerns (together with those from other reviewers) are properly addressed.	3
"For the reasons listed above, I think the current paper needs some further improvement. Therefore I would like to recommend rejection.
"	2
The paper provides some novel results and intuitive ideas to show the existence of universal lottery tickets, and shows that depth can be used to reduce the width required for strong lottery ticket hypothesis. However, I have some concerns regarding the required width of the network, which might limit the overall practicality of the results.	3
The authors may need to carefully address my concerns and questions, or I am not sure whether this paper is of good quality or sufficient importance.	3
Overall, I think this paper studies an interesting direction. However, due to some concerns on the significance of the result, I am not sure whether the result is above the acceptance bar of ICLR. I currently lean to give a weak rejection but I am happy to adjust the score based on the authors' response.	2
I like the proposed idea. However, some experiments on reconstitution and generation would better illustrative the improvements using the proposed method. 	3
The paper is well written and easy to follow. It provides interesting and effective pretraining methods for GNNs. However, it is still not clear how to handle the combinatorial multiplicity of 3D molecular information in a principled way even though we can observe empirical improvement in prediction accuracies by this pretraining in a limited set of datasets.	2
This paper proposes integrating 3D geometric information to improve molecule graph representation learning. The idea is interesting and the experiments are comprehensive.	3
The current version of this paper is good and above the acceptance threshold. More convincing experiments could improve the quality a lot. Hence, I recommend a weak accept to the current version and will consider improving the score if the experiments could be improved significantly.	3
The paper could be more persuasive on their main contributions for providing more empirical evidence, such as the 'robustness to noise' and 'scalable solution'.	3
In this paper, the author introduced a DGT for dynamic graph representation learning to leverage the graph topology and capture implicit edge connections. For the weaknesses, it is suggested to provide more concrete experimental results to better validate the effectiveness of the proposed method.	3
This work proposes a new DGT for dynamic graph representation. All the claims of existing challenges and their corresponding solutions are correct. The novelty of this work is moderate and the experiments on two datasets justify the superiority of the model. 	3
"While the topic of the paper: time series modeling of graphs is certainly important and needs to be highlighted, i find the paper doesn't really touch upon the core research question of what models are good for time series modeling of graphs, i.e., how the time domain and cross-section domain should be best combined. Instead, the paper mostly focused on a few engineering tricks for applying the transformer architecture and some pretraining tasks. This makes the paper quite disappointing and not so important for the research community, hence, I recommend rejection. This paper is maybe more suited for engineering or Kaggle style conferences, but not ICLR, which should highlight eye-opening scientific research.
"	2
"Overall I am still positive on this paper. The analysis is very interesting and insightful in characterizing the trade-off between variance and bias. Currently my score is 6 but I am willing to increase my score if all my comments are addressed. 


---------------------------------------------------
I think the authors' response to my comments are very reasonable. I have increased my score to 8.
"	4
The paper is well written and easy to follow. The analysis make the improvements of PER-ETD over ETD evident and convincing, and the empirical results, while limited, illustrate well the theoretical contributions when dealing with a significant mismatch.	4
"Overall, I think the paper is solid and sound, it is marginally above the acceptance threshold. However, I would consider changing the scores if my concerns are addressed.

------------
After Rebuttal:
After reading the author's rebuttal and the comments of other reviewers, I decided to increase the score to 8. I appreciate the author's thorough and conscientious rebuttal to address all my concerns."	3
The paper addresses a fundamental problem in RL is a simple way and proves that such a method achieves an optimal tradeoff between bias and variance in emphatic algorithms, leading to polynomial sample complexity. The claims of the paper are also illustrated in a toy domain, showing great potential. The paper could benefit from some more explanations and clarity.	3
Overall, I think it an interesting and lightweight approach that is general (amenable to many problem) and shows good performance w.r.t generalization to larger problems. However, it has limited technical novelty and the experimental results shows it is not as effective as tailored classical or neural solutions.	3
Whilst I believe the core idea of Neural SA is interesting and novel, I do not feel that it is sufficiently explored or demonstrates strong enough empirical performance to recommend acceptance at this time.  This is primarily because the algorithm neither achieves SOTA performance on considered problems, or demonstrates unique benefits not found in other systems (I believe the ability to generalise to larger problems was intended to address the second point, however, for the reasons discussed above, I am not convinced by this argument).  In my comments above, I have tried to present a few suggestions for how the paper could be extended to address these concerns - and would of course be open to pushback from the authors - however in it’s current form, my opinion is that this is just below the acceptance threshold.	3
"I like the idea of viewing simulated annealing as the Markov decision process, and using reinforcement learning to enhance simulated annealing. My major concern is that the proposed neural simulated annealing is clearly not competitive with other baselines. 
 "	3
"I think that the paper presents a detailed and thorough framework for improving simulated annealing with a good set of experimental evidence to support the claims. I hope that the feedback provided will help the authors further improve the current draft during the discussion phase, and am open to adjusting my score based on the discussion. 

I am also happy about the reproducibility statement provided by the authors, which outlines that the code will be published upon formal publication of the paper. 

----- Update During Discussion Period -----

The authors addressed many of my questions and reservations in a satisfying manner. Assuming the authors make the changes they outline in their response, I think the paper will be significantly improved and merit acceptance. I am increasing my score to reflect this and will follow up on the technical discussion. "	3
The authors have presented a simple and potentially interesting approach for generating privacy-preserving synthetic data from a real-world dataset, but I am concerned by the fact that selection bias was introduced when the authors applied inclusion and exclusion criteria on the patients from the original dataset. Furthermore, the method is only applied to one real-world dataset, so it remains unclear to me how well it can be applied to datasets with different characteristics (e.g. size, amount of missing data, data types).	2
"Neither of the two components of the paper (i.e., privacy preservation and counterfactual prediction) are novel (they are adopted from previous works). The claim that the proposed counterfactual prediction component generates true counterfactuals is not supported (and my understanding is that it cannot be supported either). For these reasons, I think the proposed algorithm is not practical and therefore, I recommend rejecting the paper.
"	2
An interesting paper, but it seems that the proposed problem is not well-addressed by the method.	2
Basically, it is an OK paper for me. The novelty and experiments are good to show effectiveness.	3
"Experiments devote significantly more compute to THOR than to Switch. Accordingly, it is hard to interpret the results.

------

I've raised my score given the authors rebuttal and updated results (3 --> 5)."	2
The paper presented a simple and novel MoE method. The experiments and ablation study are extensively conducted. The results look quite promising and the method should be easy to be adopted. I vote for acceptance. 	4
The paper is well written and the proposed ideal seems novel. But my inclination to recommend accept (8) is tainted by some doubts about the primary comparison given with Switch Transformer.  I've outlined three key questions/additions which could assuage these doubts.	3
"- **Limited novelty**.
It seems to me the two major contributions in-place distillation and upper-attentive sampling are already proposed by the existing works. It is a little unclear what further modifications the authors did other than stitching these two techniques together. Would be great if the authors can give some stress here considering the improvements in the results.

- **Lacking the deeper explanation of the improved results.** 
In the results part, it seems to me that the proposed methods just magically improve as compared to on the baselines. Not clear the underlying reasons and insights for them. Would be nice for the authors to point out as I might miss them."	2
This paper presents several simple methods to reduce the training cost of once-for-all networks, showing clear and significant training cost saving without losing accuracy. But, I find the novelty of the proposed method is a bit weak, and the generalizability of the method on other design spaces and transfer learning is unclear. 	2
"The paper is overall well-written, but the novelty for this paper is kind of limited. The author should prepare more experiments to justify the performance and address the technical contribution for this paper.

I will raise my vote if the concerns are addressed."	2
"Overall, I think the paper has limited technical contribution, and the training speedup is not surprising (less than 2x). If it is a combination of techniques but significantly improved the training speed (say 5x-10x), which makes such network training easily-doable for everyone, then I would say it is a large contribution. Currently, I think the author need to further improve their method.

======== Post Rebuttal ========

Thank the author for the response. My concern about the novelty and significance of the method and results still remain. I agree that combining existing techniques may need more tuning. But this is hard to be considered as enough contribution, especially given that the performance is not significantly improved. Therefore, I will keep my score."	2
I think the paper is well written, clear, and has technical novelty. However, there aren't enough new results, to justify acceptance. 	2
This paper provides a concrete framework to analyze the convergence rate of SGD over dependent data. By addressing the phi-mixing process to model the dependent data, the convergence rate of variants of SGD are compared and the benefits of mini-batch SGD over SGD for dependent data are clearly demonstrated from the analysis. This paper includes interesting insights and the addressed framework will be helpful to analyze SGD for other stochastic optimization problems using dependent data. 	3
Overall, there is almost no theoretical novelty. So I am voting for reject. 	1
Due to existing works in this area, I do not think the current version of this paper is beyond the publication bar.	2
"The overall idea is interesting but there are concerns regarding the experiments as well as the introduction of external memory. 


=== Post-rebuttal Comments===
The authors addressed most of my concerns in the feedback.  I kept my score and learned towards acceptance. 
"	3
"While this work appears to be a promising extension of the variational semantic memory of [Zhen et al., 2020] to models with multi-level prototypes, it omits proper comparison to [Zhen et al., 2020] on a central experiment, which, when included shows the proposed approach produces worse performance than the baseline.  The author response should address this discrepancy.
"	2
This work provides a logical extension to the existing work in (Zhen et al. 2020) by introducing a hierarchical variational memory framework. Through the experiment results it is evident that the proposed method provides considerable improvements over existing approaches. I have some concerns regarding the actual importance of dependencies within the latent variables. I'm still inclined to accept this paper, and would be willing to increase my rating if the authors address my concerns.	3
The writing clarity requires a significant improvement. It's currently a major drawback hindering the understanding of the proposed model (I understood at the later part of the paper but it was hard until reaching there). The evaluation is quite simple and uses toy tasks. I'm doubtful about the potential of the proposed model to extend to more realistic and complex settings such as image inputs. 	2
"
Unfortunately, I recommend the rejection of this work. While I agree with the premise of the submission that model-based lifelong RL is a relevant area of research, with potential implications on real-world applications of lifelong RL, the submission as it stands appears to not be ready for publication. On the technical side, the approach seems to add just a few incremental changes to multi-task HiP-MDPs to adapt them to the lifelong setting. This on its own is perhaps relatively minor, since the novelty comes from adapting it to a new problem setting. However, such technically incremental contributions should generally be accompanied by strong empirical evaluations, which is not the case in this work. In particular, the low overall performance of all agents on MuJoCo domains suggests that none of the agents are learning to achieve meaningful behaviors, which raises questions about the conclusions reached by the authors. Moreover, the authors should have compared (at least qualitatively, but ideally also empirically) to existing work in lifelong model-based RL. On the flip side, the submission does include an interesting insight of replacing the task-specific model with the shared model whenever the task model is uncertain."	2
"The paper is phrased within a sequential approach to meta-learning that has been widely studied within the supervised learning community (e.g., Baxter 2000, Pentina and Lambert, ""A PAC-Bayesian Bound for Lifelong Learning"" 2014 and much later work), with explicit performance bounds. It would be nice to acknowledge these roots. The present approach is plausible, and combines previous work, such as HiP-MDPs, BOSS, variational Bayes, in a sensible manner. However, I do not find that the level on innovation in this combination of approaches suffices for publication at ICLR, nor did I find the theoretical or experimental results of sufficient  interest (see comments above). 

Following rebuttal: Following the authors' response and my response to their rebuttal I have lowered my assigned grade due to my dissatisfaction with their replies, which served to enhance my existing concerns about the paper.  "	2
Overall this paper introduces techniques that the field of audio-visual speech recognition has rarely leveraged into this field, and makes task-specific modifications. The idea is not thoroughly new but the application is good. As I am no expert in this field, it is possible that I have missed something.	3
Very well written paper, presenting an effective approach for learning embeddings for multi-modal data.	3
The authors propose an effective adaptation of HuBERT for lip reading, by adding cross-modal iterative training. The contributions are logical and the results are strong.	3
Overall, authors approach is promising for reducing the need for a large labeled visual dataset for training VSR models, and the result shown in the paper is significant.	4
The paper studies an interesting problem proposes a simple solution and has sufficient empirical support for main claims. However, the technique parts and over clarity needs significant work before publishing.	4
"I think the paper addresses an important problem and proposes a seemingly reasonable method that might be useful in practice. However, in the current state I'm not sure if there is enough evidence to demonstrate the value of the paper (see detailed review). I would be happy to increase my rating if such evidence is presented during the discussion phase. 
"	3
In summary, although the proposed partial equivariance is novel, it's not well motivated and the current experiments/analysis do not prove the benefits of partial equivariance.	3
Overall the paper is of high production quality but misses the mark on claimed novelty. The biggest potential weakness is the omission of G-Steerable CNNs as baselines which are quite relevant here. There are a few minor presentation issues but these can be fixed with a revision.	2
This paper proposes a practical algorithm for FL based on recent advances on the NTK, and FL-NTK. It provides interesting preliminary and promising results. However, there are several aspects related to clarifying intuitions and differentiating this paper from previous work that still need to be addressed. 	2
"Overall, the proposed NTK approach for FL is interesting and has novelty. However, there are a few key limitations/weaknesses: (1) FL-NTK has a higher risk of data leakage and communication cost compared to FedAvg. The proposed CP-FL-NTK may address these issues but at the cost of requiring additional resources (i.e., the trusted key server and the shuffling server), which could limit the practical usage of the method. (2) As a general approach, a more comprehensive and rigorous experimental evaluation is expected to fully demonstrate the effectiveness of the proposed method.


"	2
The paper is well-written and the methodology is very clear. However, it seems that the paper needs to address the usage of NTK more concretely, e.g., clarifying the width limit. In addition, the second algorithm with communication efficiency and privacy protection also needs more rigorous analyses. Overall, this paper is under the bar of acceptance.	3
In summary, I appreciate the novel framework introduced in this paper for FL problem. Some of the results are however not completely clear to me. I also suggest a clear discussion on privacy issue that is central to the FL problem. 	2
"The question (S1) is highly relevant, and I think the small-scale reproduction (S2) could be meaningful. However, the implications of what is shown in the paper for LLMs is not sufficiently clear (W5), so the significance of the work in answering the question (S1) is not clear. The paper also has a few other weaknesses that prevent me from recommending acceptance in its current form (W1-4).

### Update after discussion

Thanks to the authors for their detailed response. My concerns have been largely addressed and I appreciate the greater empirical connection to LLMs via the additional experiments; I have updated my score to reflect this. I do have recommendations to the authors to increase the clarity of the paper, which I do think should be improved for the next revision:

- I agree with Reviewer BHn6 that the title overclaims and does not reflect the contributions of the paper. The current version is suggestive of something like ""An Explanation of In-context Learning [in Large Language Models like GPT-3] as Implicit Bayesian Inference,"" which is *not* the contribution of the paper, because the paper explains in-context learning in a simplified theoretical model, and provides some  empirical evidence that this captures important aspects of the same in LLMs, but does not strictly ""explain"" the same in LLMs.
- Related to the above, everywhere in the text where the term ""language model"" is used should have greater precision on the distinction between the idealized setting and the practical setting, because this term is used interchangeably for both the theoretical model and a large language model. 
- I find the terms ""in-context predictor"" (theoretical notion) and ""in-context learning"" (behavioral notion) confusable. It would be worth writing ""*ideal* in-context predictor"" and ""*behavior of* in-context learning"" to make this less so.
-  OOD = low probability is an imprecise notion and one that does not seem to be connected to how the structure of the prompts is used in the analysis. I think the authors can improve on this point by clarifying what they mean by ""we can no longer factorize the examples under the pretraining distribution."""	4
I think the theory behind the paper is really interesting to understand in what conditions few-shot prompting might work. I also think that there needs to be more experiments with real data to understand the gap between their simulated experiments and real language.	4
"This paper contributes a framework for theoretical exposition of a complex empirical behavior observed in language models. This framework and its results offer useful insights, and these are corroborated by a clear set of experiments. There remain some questions as to the motivation for ""in-context learning"" as a specific phenomenon of interest, the potential limitations of some assumptions that seem critical to the proofs, and the gap between the toy setting and ""in-context learning' as it arises in large language models. In my view the strengths outweigh the weaknesses, and my score could potentially be improved with some further discussion of the points above.
"	3
"The paper raises interesting connections between Bayesian learning and certain aspects of in-context learning, based on a simplified formal model exploiting mixtures of HMMs. While the formal developments are extensive, the take-home message is somewhat obscured in the description, making it difficult to assess the true impact of the paper.


*After reading authors' responses*: Thank you for your detailed answers and for the clarifications in the paper. I am not fully convinced by the large significance you put on the OOD nature of the prompts relative to the pretrained distribution --- technically true, but in a narrow and not very illuminating sense IMO ---, and still think your title is an exaggeration. However, I feel that your paper deserves to be discussed by the community and therefore I am raising my score, leaning towards acceptance."	3
"I feel quite conflicted on this paper: on the one hand, the DST is to my knowledge novel and fairly interesting. On the other hand, the lack of ""good"" baselines makes it hard for me to judge the performance of this method, and I am concerned that a lot of the complexity of the DST is not actually necessary. Tentatively I think I will recommend rejection due to these concerns, but I would be happy to raise my score if they are addressed."	4
The method seems novel and interesting and results are promising. The only drawback is the weak section describing the experimental settings, which, however, can be easily corrected.	3
This paper created a molecular graph locally differentiable by using a differential scaffolding tree (DST). To the best of our knowledge, it does appear as a novel approach to make the molecular optimization problem differiable at the structure level rather than utilizing latent spaces or employing RL/evolutionary algorithms. The authors developed a generic molecular optimization approach based on DST, which was validated through comparison when previous methods and benchmark datasets. The paper appears well written and the results are promising.	4
Overall, it is a good paper with some interesting designs like DST. The proposed method shows a new direction to do lead optimization. I prefer to accept this paper although it contains some cons that hard to avoid.	4
The authors present interesting analyses and results regarding a modified version of SGD in optimization. Their characterization of the recent literature, and the experimental design based thereupon seems to need more attention.	3
This paper reveals that heavy-tailed noise+gradient clipping can help SGD eliminate the sharp minima. Beautiful theoretical analysis and insightful numerical experiments are provided. 	4
"The following two reasons are the main weaknesses, based on which I recommend rejection:
1. The theory seems irrelevant for machine learning / deep learning
2. The experimental evaluation is weak because it uses outdated architecture and because the result only improves on badly performing vanilla training strategies

Therefore, taken as a theoretical paper, I find the theory limited and irrelevant. Taken as an experimental/method paper, I find the improvement and methodology unconvincing. "	2
"The contributions in the paper are limited and not well-focused. Overall, I do not think the paper is at a level for acceptance.
"	2
"The studies an important problem in the context of federated learning, namely designing communication efficient learning algorithms in the presence of Byzantine clients. The paper presents an interesting weight quantization-based algorithm and analyzes its convergence in an i.i.d. setting. The authors also demonstrate the superiority of the proposed algorithm on existing communication efficient federated learning algorithms in the literature on two standard image classification benchmarks. 

That said, there are some questions that remain about the claims made in the paper (see weaknesses above)."	3
"This is an interesting paper, but the idea is quite similar to SignSGD paper, and has no thorough comparison with existing works. Better comparison & ablation are needed to better understand why FedVote works better than existing schemes. 
"	2
I have some questions about the theoretical analysis of this paper. I hope that the author(s) could kindly elaborate.	3
In my opinion, the contribution of the proposed method is novel and interesting. In addition, the writing of this paper is clear and easy to understand. The experiments are sufficient to prove the effectiveness of the proposed methods. However, more experiments should be designed to prove the proposed method further.	4
I think this work is good in general. The writing is great, the evaluation seems thorough, and comparisons look fair. The method is easy to understand and the empirical results are convincing to me. The authors discuss recent and related works and their method seems original but I lack the expertise to judge the novelty of the approach. It builds significantly on previous work (DER) so I'd argue that some aspects of the publication exist in previous work. 	2
I am not an expert in this domain and have not much experience with related works. However, the idea is very simple and reasonable and the results outperform previous existing methods. However, more analysis is needed for the claim.	2
In this paper, a novel idea of continual learning is proposed. The authors clearly describe the proposed approach and also give a detailed analysis. Overall, it is a well-written paper with an interesting idea. The results also show the benefits of the proposed approach. 	3
"Updated after Author response:

I think the paper should now be accepted at ICLR. It is a significant step forward for the field, with a modeling approach interesting from the ML side as well as non-trivial, domain-relevant empirical validation (which many other papers on the topic at ML conferences do not have).



nit: with regards to the baseline experiments side: I would suggest to move the QED task to the appendix."	3
"
~This is a refreshingly original paper on molecular synthesis planning
and synthesizable molecular design.  It is a strong, foundational
paper that is well written. The authors could do some a little
work to improve the reproducibility and enable future benchmarks
against the same data.~

[Edit: this work closely relates to Bradshaw et al, but the relationship
was not made clear in the exposition.  Until the authors 
address this concern and clarify their additional important contributions,
I do not recommend publication of this paper.]

Edit2: This is a strong paper that is well written and deserves publication at ICLR22.
"	3
Overall, the paper is well-written, and the method appears to be promising.	3
"I suggest to reject this paper and encourage the authors to revise it for next opportunities.

While I consider the method proposed in this paper is reasonable, the relationship to the existing work is not discussed enough, and therefore, I consider the paper is not very mature to be published as an academic paper. I would suggest the authors to revise the paper so as to clarify the difference from the existing work and how much the difference contributes to improve the performance. To do so, additional experimental results would be necessary."	3
The theory is new, but the theory does not seem to support the authors' claim that BPPA finds a better data-dependent solution. The experiment also do not seem to fully support the author's claim.	2
The paper addresses the use of BPPA algorithm and the mirror descent algorithm to find the max-margin linear classifier in case of separable data.  My only concern is if there is some other work if this has already been addressed and relevance of the work to present day machine learning.	2
In my opinion, linking this to implicit regularisation is an over-sell of the paper. The authors derive some interesting results on margin convergence, but similar results also exist in the literature, and proving convergence results in terms of the norm that you are strongly convex with respect to is not new.	2
I recommend accepting the paper. Even though the setting is restricted to binary classification with linearly separable data, it adds some new understandings of the properties of BPPA and gives some suggestions on how to choose the Bregman divergence.	3
Based on the above comments, I believe the paper lacks in the areas of contribution, applicability, significance, practicality, experiments, and writing. As a result, I believe the current version of the paper is slightly below the acceptance criteria. I would like the authors to address the comments above before I can increase the score of the paper.	2
"In summary, I think this paper needs to be revised because of the following reasons:
1. The algorithm and the analysis seem a little incremental.
2. Missing important citation and comparison."	2
The paper provides a clear intuition on why multistage algorithms are useful in practice. However, I made several critical comments in my main review regarding the correctness of the results, the technical novelty of the proofs, and the connection between experiments and theory. As of now, the paper seems weak (at least the way it is presented now). However, I am looking forward to reading the authors' rebuttal.	2
"The theoretical result is new in FL.
The proposed method has strong empirical performance in typical datasets.
Their match the lower bound and  fill the blank. 
Though it has some limitations, I think it is worth to be published.   
"	3
The paper studies an important problem. However, there are some challenges with respect to the writing, motivation of the solution and potentially several important directions that can be addressed. 	2
The paper has a weak motivation for the new setting. The proposed method seems too heuristic and the evaluation for the new setting is not appropriate.	2
"The presented method of re-training a model to forget a specific class is very interesting. However, the part for identifying the most relevant model parameter is missing some essential details. For example, how the parameters are selected (manually or automatic) or the number of selected parameters. This information is essential to understand the method. Moreover, the influence of the parameter selection method is not studied in the evaluation part.
"	3
"The setting proposed by this paper is novel and practical. However, there exists some technical flaws that need to bu further solved.

Please see the ""Main Review"" for details."	2
"
Overall the paper addresses important topic and has conclusions that could benefit the community. The experiments seem too be well done and rigorous. 
"	3
The issue of the different performance of participating and non-participating clients in federated learning - in particular for heterogeneous datasets - is interesting. The proposed contributions of this paper are a straight-forward data split strategy, a proposed strategy to split data into heterogeneous local datasets, and an empirical evaluation of the performance of participating and non-participating clients that confirms the intuitive expectations on the effect of participating clients. While the issue the paper addresses is interesting, the proposed contributions are fairly straight-forward and provide little novel insights. Thus, I vote for rejection. 	2
A very nice paper. It proposes to measure both out-of-sample generalization and participation gaps and give several recommendations for training federated learning models following a new protocol. It also proposes a new way to create synthetic federated datasets that reflect heterogeneity in a more natural way than label-based split.  The experimental analysis is very concrete and the results look solid. I recommend acceptance.	4
I feel the idea and the experimental results are trivial if we view it from a commonly-used federated data splitting perspective. If the authors can propose some remedy to this participation gap, this will be an interesting piece of paper.	2
"The proposed method is novel and interesting and shows promising results in various settings.  My main problem is quality of writing and presentation. if the authors can address my comments and questions and improve the general presentation quality, I'll happily increase the recommendation to accept.

**Post-rebuttal/revision**
The authors have done good job addressing my concerns (as well as other reviewers'). The experimental results look interesting and while presentation (in particular writing) can be improved, I think the paper should be accepted."	3
"**Pre-rebuttal:** S3 achieves very good performance but the technical novelty is limited. In the current version, it seems that S3 is more like a combination work. Thus I give the initial score of 5. I will consider increasing my score if authors can well clear my concerns.


**Post-rebuttal & Final recommendation:**

I thank the authors for their further reply and respect to other works. 

It is relatively a hard decision for me. Experimentally, this paper does achieve promising performance. The authors did a good literature review in the paper 
and elaborate the difference among S3 and other methods in the rebuttal. The designed S3 framework only has few hyper-parameters which are shown robust. To this, I appreciate the efforts and contribution.  However, I still think the combination of components of S3 lacks enough novelty since each component is not new in the literature. 
For the DivideMix that the authors mention, I admit that DivideMix is also a combination paper. However, the performance of DivideMix 
consistently outperforms all the previous methods by a large margin especially when the noise rate is high (at that time). While it seems S3 only has little improvement compared to C2D, which is also a work that uses self-supervised learning. In some cases, C2D is even better. Further, considering the supervised, self-supervised training framework is also explored in other works (Co-learning) and S3 currently does not have theoretical justifications such as why KNN voting-based relabelling, or other component, works on both close-set and open-set label noise, I remain my final score as 5.  




"	2
"This paper focuses on learning with noisy labels problems, which constructs an iterative learning framework to refine labeling set and train the model parameters.  It demonstrates its advantages by comparing with current baselines on a range of datasets. However, the novelty of this paper is ad-hoc, which stacks the benefits of sample selection and contrastive learning. Considering several previous works have explored them [1, 2, 3] and the proposed method in this paper has minor difference with previous works, it is below the bar of ICLR for the area of learning with noisy labels. Another problem is k-NN search for neighborhood voting is computational-expensive especially for large-scale datasets.

[1] Label Distribution for Learning with Noisy Labels. IJCAI 2020
[2] Contrastive Learning Improves Model Robustness Under Label Noise. CVPRW 2021
[3] Learning from Noisy Data with Robust Representation Learning. ICCV 2021"	1
The consideration of function identification is an appealing tool for analysis since it only requests the outputs to be similar with high probability. The derivation then follows the analysis of quadratic neural networks. I think there are some discussion and comparisons needed (see bullet points above), and therefore I decided to hold my scores.	4
The paper investigates a very relevant and interesting topic of uncertainty estimation in overparameterized neural networks. The interesting and novel part of the paper is in attempt to give uncertainty estimation for predictors rather than parameters. However, uncertainty estimation depends on the norm of a fitted network which is generally unclear in the interpolation setting. It is likely that this issue can be mitigated by analyzing the norm (e.g. given by GD solution -- right now no algorithm is considered for fitting the network), and for instance using some arguments as in Arora et al. 2019, however I feel that right now the entire picture is not very clear and the paper could benefit a lot from clarifying this.	3
While I think the paper works on an interesting topic with several interesting applications, I feel technical quality of the current version of the paper lacking strength; however, I would not strongly argue for rejection if the authors can provide convincing rebuttal or are willing to improve on the technical quality of the paper.	2
The paper nicely introduces frequency information into SGD, with various interesting theoretical analysis and empirical results.	3
This paper is well-written and made solid contributions for adaptive learning-rate in long-tail distribution. 	3
The paper addresses the problem of adaptive learning rates in recommender systems. The authors propose a novel and simple method to incorporate token frequency information into the learning rate schedule. The authors also show provable benefits over SGD. I think the paper can be further improved by changing the pitch to focus more on recommender systems and enriching the experiments as said in the main reviews.	3
This paper proposes to integrate the frequency information of tokens into the optimization algorithms for a fast convergence. The idea is novel and the proposed algorithms are easy-to-implement. I recommend to accept it. However, the experiments are not comprehensive and I want to see a thorough evaluation which considers the offline training and the matching stage. Besides, more loss functions should also be investigated. 	3
"The approach seems interesting and promising but:
* the technical part is too hard to follow and requires more explanation on the reason for the use of each module 
* the experimental part is incomplete missing at least an important experiment to show how well the proposed model answer to the problem of the accurate detection of the time series segments; and details on results to understand the ablation study. "	3
The SegTime method presented in the paper might be effective as the model takes the whole input sequence at once, rather than a sliding window approach. However, this paper needs further experiments and evidence to properly support the author’s claims. Moreover, technical novelty is a bit limited. Therefore, my evaluation of the paper is “marginally below the acceptance threshold”. If all the issues mentioned are fully addressed, I may reconsider my assessment of the paper.	2
While the authors clearly put a lot of time an energy into this work, I think it would benefit from workshopping with others in the field. As noted above, there are many missing pieces from different parts of the literature which could be used to improve this work and better situate it with other progress in this area. 	2
The paper presents a rather complex model for stepwise time series classification. The proposed approach relies on ideas borrowed from semantic image segmentation models like DeepLab.  There several issues to be resolved related to motivation, novelty, clarity, hyperparameter specification and experimental validation.  	2
This paper provides an interesting study of antibody loop generation with both novel methodology and extensive empirical evaluation. This combination of strengths makes it an excellent paper that should be of wide interest.	3
Interesting approach to an important problem. Convincing empirical results and baselines. The approach might be limited to specific use cases in practice depending on the availability of a predictive model for the properties of interest and the knowledge of the antibody frame.	4
The proposed method is novel and the paper is well written, validated by thorough empirical studies. 	3
I enjoyed reading the paper. However, I'm uncertain about how much value this paper adds in efforts towards understanding the nature of self-supervised representations and the results are not very interesting. As a result, I think the paper falls slightly below the acceptance threshold.	3
"I think that the present paper deals with a very important problem, namely the ""understanding"" of learned neural representations. The proposed approach to do this by synthesizing ""natural"" images generated by a generative model conditioned on these learned representations (here a diffusion model) is intuitive and reasonable. The generated samples are of high quality and seem to be in agreement with the representations. 

Unfortunately, this approach is not novel (only the use of a diffusion model instead of {GANS, ARMs, flows} is new here) and the present work would, in my opinion, benefit greatly from being placed in the context of this existing work. In addition, there are some specific questions (see above) that I think are not yet answered satisfactorily. 

++++++++++++++++++++++++++

Score raised to 6 after the rebuttal
"	2
"
The reasons I am concerned about these two main objections are: 
- quantitative results are almost a must in order to properly evaluate methodological progress, and  check that the method is behaving as it should, and to properly compare models/methods etc.,
- The methodological contribution of the representation conditioned diffusion model is relatively small (it sufficed to simply describe the changes one needs to make to ADM in words). This is not a problem in itself. But I do think it is therefore important to demonstrate the method's usefulness in analyzing and understanding pretrained models. 

Consequently, I am currently not in favor of acceptance. However, to recognize the generally high quality of the work thus far I am opting for a weak reject. I am unlikely to raise the score to an accept without significant updates to the work, or strong arguments in favor from other reviewers. "	2
"The method in this paper is unsurprising but gives good results.
I find the experiments thorough and insightful, and would recommend an accept."	3
"The generative model for sequential data with relational constraints proposed in this work is interesting to me. Still, I have several concerns as mentioned in the main review.
"	3
I think the major contribution of this paper is the framework. While some evaluation metrics are questionable, but I can see the coverage is better than the last year's submission, and there are improvements over different metrics. Besides, evaluating generated sequence is a challenging task and is out of the scope of this paper.	3
"Based on the strengths and weaknesses explained above, I believe that this paper is at the borderline; I slightly tend to reject it since the clarity issues that I raise are important. However, if the authors could well address those concerns and we have high confidence in its camera-ready version, then I would consider elevating my rating. 
"	3
"1. Technical innovation is not enough to meet the bar of the conference. 
2. Empirical studies do not fully convince me the effectiveness of the approach. 
3. There are some issues on organization of the content. 
4. I cannot think about broader applications of the proposed approach in sequence generation."	2
It would be interesting to show an equivalence between model and data poisoning attacks in FL. However, the paper is written in somewhat confusing manner and lacks in details, which makes it difficult to understand and assess the claims.	2
"This paper reveals an equivalence between Byzantine gradient and data poisoning attacks in the context of personalized federated learning, which is an important and timely topic. This insight suggests that the claims in the existing literature that Byzantine gradient attacks are unrealistic is misleading. To my knowledge, this result is new and its contributions to the field are significant. However, this paper also has several issues, which are listed as follows:

1. The attack model considered in this paper is somewhat simplistic in that the authors only considered the single-strategic-user case. Although it is understandable that this renders the problem more tractable for theoretical analysis, the results may not be very useful because Byzantine gradient attacks are not necessarily from a single malicious user. In fact, most works on Byzantine gradient attacks allow multiple Byzantine workers. It's unclear whether the results in this paper could be extended to multi-attacker scenarios.

2. The authors proposed a PAC framework as a foundation to evaluate the performance of various Byzantine and data poisoning attacks. To my knowledge, this is also a new and interesting contribution. The authors demonstrated the relevance of this PAC framework by showing that linear regression and classifications are PAC learnable under this framework. But I wonder whether this PAC framework continues to be meaningful for more complex learning models than linear models. Having further discussions on this aspect would be very interesting.

3. Most of the experiments in this paper are conducted on MNIST and Fashion-MNIST datasets, which are relatively simple. It would be more interesting to demonstrate the claimed equivalence between attacks on more sophisticated datasets. Also related to the previous comment, most of the experiments are based on linear models (a simple two-layer neural network is also used). I think this paper could benefit from more experiments with more sophisticated learning models."	3
"I find the topic of this paper extremely interesting. Understanding the relation between gradient attacks and data poisoning attacks are very important. This paper takes an initial step in understanding the relation between these two attacks for simple models such as linear regression. However, as stated in my comments, I have serious concerns about the presentation of this work. The main ideas presented are not easy to comprehend. The exact threat models are not also clearly specified. I also have some concerns about the proofs (In particular about equation 120, page 33). I will be happy to increase my score if authors can provide sufficient response to my concerns.

"	3
I would recommend that the paper is marginally below the acceptance threshold. Mainly because I don’t quite follow the intuitions of some assumptions. Refined analysis with weaker assumptions and clearer presentation would be appreciated.	2
While the studied topic and the proposed notions and results are interesting, I believe that the paper will benefit from substantial further comparison to existing work and also from further discussion of the results from Theorems 4,5,6.	3
While the empirical results basically show that CIC can outperform multiple baselines on URLB with an appropriately tuned value of $\alpha$, I am mainly concerned about the correctness of the claims and the novelty of the work.	2
The paper proposes a new algorithm for unsupervised behavior learning that is rigorously shown to be more effective and clearly argues for its design choices through supplementary experiments. 	3
"Although the proposed algorithm CIC is a variation on an existing algorithm [2], it shows impressive performance gains over several existing algorithms on a large suite of continuous control tasks from the URLB benchmark. These large-scale empirical evaluations and analysis of several algorithms for unsupervised skill discovery methods on a standard benchmark such as URLB would benefit the community. 
"	2
"To my understanding, the main selling points of this paper are:
- It tackles the very relevant problem of unsupervised pre-training for reinforcement learning;
- The methodology is clear, and a quite natural extension of previous works in unsupervised skills discovery literature;
- Strong empirical results, CIC seems to advance significantly the state-of-the-art performance of unsupervised pre-training in continuous control domains.

Instead, potential shortcomings are:
- The novelty seems limited, as CIC is essentially similar to APS (Liu and Abbeel, 2021) with a different discriminator loss (which has been employed for unsupervised skills discovery before);
- It is not completely clear from the paper what are the specific factors that lead to such a performance improvement over previous works.

Whereas the reported empirical progress might be a sufficient reason for acceptance, my current evaluation is just slightly positive in consideration of the mentioned concerns. I do not think the limited novelty is a crucial problem here, if the authors could better clarify in their response how the CIC methodology is so successful, I will consider raising my score to a clear accept."	2
Since this is largely a theoretical paper, it is unclear if these results are novel since it seems like just plugging the power decay assumption into the log likelihood formula.	2
"Overall, the result seems interesting. I have only minor concerns about the correctness, but I think these can be resolved (or it is possible I have misunderstood). I think the exposition could be improved to provide better context for the results, and tie them more concretely to real problems. Overall, I think the paper is above the threshold for acceptance. 
"	3
"The results in this work are interesting, but in my opinion the discussion is largely misplaced with respect to existing literature. In particular the abstract and introduction. I am willing to raise my score in case the authors address these issues in a revision. 

------------------------------------------------------------------------
**Update after the discussion period**

During the discussion period the authors have addressed all my questions and have welcomed my suggestions. In particular, they have extended their GP treatment to include the ""strongly regularised"" regime, finding results which are consistent with the literature. Although the rates derived in this work are known, this work derives them under a different framework, and broadens the scope under which they can be rigorously established.

For these reasons, I am updating my score towards an accept."	3
I feel that some parts of the paper need more explanation and exposition. For example, discussion about the rates' optimality and modeling choices such as knowing $\sigma$ would significantly improve this paper.	2
The paper is an interesting read. I agree with the authors that this approach require few gradient evaluations, however, the paper lacks rigorous justification on (i) why the method is effective, (ii) whether it minimizes some neighborhood criteria, (iii) how to tune its parameters.	2
"Good paper on a novel and important topic: a XAI algorithm to return counterfactual examples achieved by a generative approach based on probabilistic circuits. Results are in line with the SOTA and considerably faster in terms of execution times.
"	3
"The paper presents a novel counterfactual explanation method based on sum product networks. While the method has some attractive properties, the paper needs to compare against recent counterfactual explanation methods and the evaluation section needs to be more compelling.
"	3
As indicated above, the idea of using SPNs for generating counterfactual explanations in an effective way is relevant, and experimental results look promising. So, I encourage the authors in pursuing this avenue of research. However, for the moment, the paper has clarity issues, related to both the technical part and the experimental part. I think that the technical issues are most important.	2
The quality is okay but falls below the ICLR threshold because of the lack of originality and in-depth discussions. 	2
The paper needs some rewriting of the technical sections, but otherwise is a good paper.	3
The authors do not mention nor compare to other important estimators of differential entropy and mutual information. Furthermore, there are important unanswered questions about the theoretical results.	2
"+ Simple method with consistent improvement in empirical performance.

- Uncertainties around the experiment results.

I will change my score if the two questions about the experiments are clarified."	2
The technical aspect of this paper is not very significant in my opinion. However, empirical evidence seems sufficient and the improvements look consistent. My rating for this paper lies on the borderline for the moment. It is likely that I'll make changes after the rebuttal.	3
Overall I think that the technique proposed in the article is really promising, but that experimental results should be developed a little more before getting accepted.	2
"Overall, this is a very interesting paper with a very relevant and
timely topic (potentially high impact). The theory is strong and
well-explained, even though the current write-up suffers from minor
formulation issues, which, however, do not detract from the overall
clarity of the message. I lean towards accepting the paper with the
proviso that clarity is improved and, most importantly, my comments
concerning the experiments are being considered. At present, it is
slightly unclear for non-expert readers to judge the overall utility of
the algorithm since the choice of comparison partners and their training
have yet to be clarified.

**Update after rebutall**: I thank the authors for their updates to the paper;
my concerns were addressed, and I am happy to raise my score."	3
"The proposed method to cluster graphs using persistence diagrams is computationally efficient, but at the price of losing some topological information in the process. This can definitely be a good approach in some problems, but I would like to have some empirical evidence that ""we do not lose too much information"" in the process, that is that this procedure competes with the (slower) one where we take into account the complete topological information."	2
The use of distillation for churn prediction is intriguing, but it does not appear to make a major difference, similar to the many flavors of domain adaptation.	2
This is a very interesting paper that adds both empirical and theoretical findings to the body of literature. The theoretical underpinnings of distillation as a constrained optimization applicable for churn reduction sheds new light on a very popular method and can lead to new applications. The presented results also support the efficacy of the methods. Overall this paper is a good example of an ICLR paper. I would encourage the authors to address the presentation points mentioned in the main review for a more impactful submission. 	3
This paper provides strong theoretical and empirical results regarding the effectiveness of the proposed distillation based algorithm for the churn reduction problem. 	4
The authors present a novel method for instrumental variable regression that promises to be useful in various applications. However the motivation for the method and its empirical testing should be improved.	3
The paper seems to provide a novel, interesting and applicable method that improves the accuracy of an important causal inference tool (instrumental variables). 	3
"In summary, I am rather convinced that the contribution of this paper is important and its theoretical as well as empirical findings are well described and motivated. However the authors should try to provide relevant applications or settings for their method that would indeed benefit from this novelty (compared to existing works). The experiments do not support the authors' claim that their method outperforms other competitors. Also, they should maybe clarify the paper's content in the title by specifying the context of IVs instead of ``learning causal relationships''. Indeed, from the title one expects to find references to the classical causal inference frameworks (either Imbens \& Rubin (2015) or Pearl (2009)) and specification of causal estimands. 
I will read the rebuttal carefully and am willing to increase the score if the authors address the raised concerns."	3
The proposed algorithm is interesting and has solid contribution. The experiment section is not satisfactory enough.	3
Overall, I think that the studied mean-covariance problem in this paper is very interesting. The idea behind the proposed approach looks non-trivial. Extensive experimental results are provided to show the superior empirical performance. However, this paper lacks theoretical analysis, and thus I could not judge whether the proposed approach achieves good theoretical results as well.	2
The paper was nicely written and provided a simple alternative (the expected quadratic utility function) to the mean-variance objective, which has nice interpretations and avoids the double sampling issues.	3
The paper considers an important problem and its proposed algorithm is justified by extensive experiment results.	3
My recommendation is to reject because the results require more justification on the performance of the method relative to state-of-the-art. The work does not provide any novel perspective over what is already known to the financial mathematics or reinforcement learning community. Authors need to provide more clarity in writing, In particular, in formulating the optimization problem and how the method is implemented.	2
"Overall, I am not convinced that the method is truly useful in real settings beyond the deterministic + spatial problems that the paper focuses on. The formalization of equivalent action sequences satisfied my curiosity but is perhaps slightly overkill and unnecessary. 
"	3
I recommend 5) marginally below the acceptance threshold. There is existing literature that tackles the problem of exploration in a more general way without handcrafting prior knowledge into the problem.	2
The paper is relatively well-written, and proposes a sound algorithm that leads to good empirical results (even if the amount of empirical results is quite low). The algorithm is general enough to be applicable to an Atari game, with minimal engineering effort. The approach is quite novel, and multi-disciplinary (RL, information theory, convex optimization). As such, I think that the paper will be very interesting to people attending ICLR, and recommend accepting it.	4
The manuscript was a nice and interesting read. The formalism is solid, the benchmark is going to be useful to many researchers and the insights are interesting and although somewhat expected from intuition, it is good to see them result from experiments and written down nicely.	2
"I totally agree with the authors' argument about the importance of continual reinforcement learning. I also agree that the RL culture that makes agents' life ""episodic"" tends to prevent us from tackling the problem. I also think the ""non-episodic"" feature is one of the important ones, although the real-world embodied learning, such as that performed by humans and animals, has many other essential features, e.g., the reward function is not unique and environmental dynamics are changing over time even in an episode, 
Therefore, I like the argument of the authors, especially in the Introduction. 

However, it's questionable if the paper has a sufficient paper as an ICLR conference paper though it has great importance in its aspect of ""position paper."" 

1) Theoretical contribution around ARL. 
To my understanding, the theoretical notion of ARL is not new. The general and classical framework of RL does not necessarily have the assumption of episodic segmentation. Therefore, the novelty of the authors' proposal about ARL is limited in a theoretical viewpoint though it is quite meaningful in the ""cultural"" viewpoint.

2) Benchmarks.  
The authors described some benchmarks. However, to my understanding, even though they showed the pre-existing RL algorithms underperformed in the ARL settings, they are not providing a suitable task for developing ARL algorithms. This is because they only showed the pre-existing methods tend to underperform in the ARL settings. 
Some more constructive proposal is expected. 

The paper is very good from the viewpoint of position/short journal paper. However, I am not sure if the paper is suitable for the ICLR conference paper. 
  
"	2
This paper gives two convincing definitions of autonomous reinforcement learning and introduces clear evaluation methodologies accessible for the community. Relevant baseline results are provided, such that the proposed benchmarks are good starting points for future research. While some conceptual challenges linger, this could be resolved by followup works.	3
The contributions are rather limited in my opinion since the theoretical framework essentially essentially proposes re-definitions of existing concepts. The experimental framework is composed of existing benchmarks so it is difficult to see a contributions there.	1
The contribution of the paper is unclear. Provide more examples and convincing experiments about scalability which is one of your main motivation.	1
The authors propose a practical framework mainly aimed to reduce GPU memory usage via the use of representation theory. However, for me, the main weakness of this paper is in its exposition and clarity - including lack of proofs for statements made, lack of comparison with appropriate baselines, incomplete analysis, etc, 	2
While the proposed method is practical, the novelty of methods is limited when compared with existing works that also employ locally permutation equivariant update functions. More importantly, the paper didn't provide a clear comparison to make the proposed method stand out from the previous related works. Thus, I currently vote for weakly reject. However, I still would like to hear more from the authors if I have any misunderstandings. 	3
I think this paper introduces an interesting framework but fails to justify and motivate its constructions. I further feel that the theoretical analysis of the framework is lacking. 	2
"I think the paper tackles an interesting problem and provides evidence that, in some extreme cases such as incredibly low amount of labels per class, it can outperform other approaches.

However, of its two main claims (avoiding over-smoothing and working well with few labels), I think the former is a bit harder to defend (GRAND++ is always better than GRAND when depth is very large but not always otherwise, in addition it is does not always exhibit the same behavior and it not clear why that happens). For the low-label case, I think there are more chances to convince about the improvement brought by the model but I think it would be important to better understand the reasons for it. Intuitively, a strong bias is introduced by the insertion of the source term that definitely helps by providing useful signal for the classification. This should be, in my opinion, discussed more in detail and verified with more examples.

For the above reasons, I consider the paper still not mature enough for publication but I think a more in-depth analysis could bring valuable results.

----

The authors' replies addressed my concerns and after reading their answers to other reviewers and the updates to their manuscript I decided to increase my rating. "	2
Please look at the comments provided above.	2
I like very much the idea of modifying the Laplacian diffusion equation div-grad used in GRAND adding a source term. The theoretical analysis proposed in the paper deals mostly with the simplest case of a linear diffusion operator, which is certainly useful to gain intuition. However, no attempt to transfer the results to more general nonlinear settings are discussed. Moreover, the mathematics contains many typos and a number of statements that are vague or not clear.	3
The overall quality of the paper is good, with a clear narrative. The problems addressed are also much needed. The methodology is straightforward and clear, and there is also theoretical guarantee. So I recommend it for acceptance.	3
In general, this is a solid paper which proposes a novel idea to solve the over-smoothing problem the GNN models using a special form of graph diffusion equation. Solid theoretical and experimental results are provided in the paper.	3
I think this paper is exploring a novel direction, has good empirical results and would be a valuable addition to the literature. Addressing the points mentioned above would further strengthen the paper and I’m happy to raise my score if the authors can provide convincing responses.	3
Fairly well-thought out scalable approach to adding CBR to a tough RL setting which has got some attention recently. Strong results and good empirical analysis. It would be beneficial to get RL practitioners to seriously consider adding CBR type approaches as a new type of strategy for boosting Deep RL.	3
"The paper is well written and presents a useful paradigm for thinking about text game agents and the results are interesting. The main thing holding the paper back is the lack of analysis/ablations on certain portions of the proposed algorithm that make it unclear where the gain are coming from.


====Rebuttal Update====
I've read the rebuttal across the reviewers and am satisfied enough to increase my score."	3
The paper proposes the use of Case-Based Reasoning combined with Knowledge Graphs for improving RL agents for text-based environments. However, the approach has limited novelty as similar approaches have been proposed in the past. On the evaluation side, authors should also compare with other approaches (e.g., based on Transformers) that make use of external knowledge to generalize better. 	2
The paper presents an interesting direction for using a declarative framework (i.e. mixed integer linear programming) to encode combinatorial structures in black-box optimization. The proposed method lacks a theoretical motivation, and the empirical evaluation does not demonstrate significant advantages over existing methods. 	2
The ideas are novel and significant, especially given the modeling expressivity provided by MILPs. However, the paper lacks some justification concerning the scalability of the approach and the fact that neural networks had quite a limited size. Moreover, in my view, the numerical experiments do not explore well the benefits of their approach.	3
It solves a very interesting topic which is defined on a discrete search space with constraints. However, the choice of surrogate model is not convincing and the baseline is missing. Thus, I would like to recommend rejection.	2
"Strengths: The presented framework is flexible and easier to implement than other methods in the presence of combinatorially constrained discrete domains. The experiments show that the performance is also competitive with other approaches.

Weaknesses: The MILP formulation limits the hypothesis class of models to neural networks with linear activation functions, runtime limitations restrict the number of neurons in the network. Mixed-integer domains are currently not supported by the framework, even though the name suggests otherwise."	3
Papers like this one tend to be rejected from highly selective conferences, but they can be very useful and impactful for researchers and practitioners alike in popular areas, such as saliency algorithms. This paper is thoughtful, well positioned with respect to previous attempts to characterize saliency algorithms, and should be an excellent resource for the saliency research community.	3
"
In summary, this article provides 1) a review of so-called saliency approaches for neural networks and 2) a new taxonomy for rating the properties of those approaches. 
- The article would benefit from a more in-depth discussion of the fundamental differences between the approaches considered and what it means for the term ""saliency"". 
- The article would benefit from some sort of evidence that the produced taxonomy is of value to the end users. "	2
This paper proposes a useful framework for evaluating and comparing saliency-based explanations for neural network results. However, it is entirely theoretical/speculative -- the framework has not actually been implemented or tested.	3
I believe this is a very strong paper that significantly pushes the state of the art of DEQs, and should get accepted without reservation (barring something substantial I may have missed).	4
An empirically motivated neural network replacing the role of a traditional root finder (e.g. Anderson acceleration) in DEQ models appears to improve performance and inference time. However, this root finding network introduces a new set of hyperparameters and all the baggage usually associated with deep learning (no theory of convergence, optimisation, generalisation, hyperparameter choice) (weakness 1), and it is not clear whether the choices made by the authors are universally applicable (weakness 1,2). All in all, a good paper that is likely to be adopted by the community and spark future research.	2
This paper presents a method to significantly improve the inference of implicit models. Although the paper does not focus on theoretical contribution, it demonstrates empirical merits very well and the results are impressive. The paper is written clearly and is a solid work overall.	3
Though the paper has some merit and the approach of iteratively embedding audio in the encoder-decoder is interesting, the paper needs some more work, which if presented will make the paper stronger.	3
The paper contributed a method that seems to handle a common case in practice. But the technique does not seem novel enough, the discussion on the method does not seem necessarily comprehensive, and the performance does not seem convincing.	2
The paper can be greatly improved based on the points mentioned in the weaknesses section in the main review for the authors. Please refer to the points before the paper could be considered for presentation at ICLR.	2
"Pros : 
- the authors circumvent the variable message length difficulty by recursively embedding message chunks using a neural network. 
- the weight regulating quality/detectability trade-off is tunable at test time thanks to loss-conditional training 

Cons : 
- the authors evaluate message undetectability through SNR measures instead of relying on detectability by a SOTA opponent. The proposed insertion is likely to be easily detectable by such an opponent because it is not constrained to preserve natural image statistics. 
- the problem addressed is steganography without a shared secret key and thus relies on security through obscurity which was rejected as a sufficiently secure framework long ago. I believe the authors should look for other use cases. The approach might be seen as a form of uncanny compression : instead of sending an image + an audio, one can send an image containing the audio."	3
I initially vote for a score of 3. While this paper finds interesting empirical observations to related works, I am unsure how it improves over the state-of-the-art approach in the literature, such as SOS. After reading the authors' responses to my questions, I am open to raising my score.	2
This work attempts to overcome and investigate weaknesses of LOLA to tackle the consistency problem. Even with explicit consistency loss the authors still find that we still find that the arrogant behavior remains, so the insights from this investigation seem relevant for future work and open questions in this area.	3
I think my decision mainly depends on whether the claim that the paper makes a correction to the existing literature is true or not. And if the paper can it more clear. 	3
"The paper is on an interesting issue yet it is at best a starting point and far from prime time. 

----------------

After the author response:

Thanks to the authors for the response. It unfortunately does not change my assessment. "	2
"The proposed method in this paper has the potential to be a very useful method for solving the continuous optimal transport problem using DNNs.
In addition, the manuscript is clear and well constructed.
On the other hand, the effectiveness of the method for application problems in machine learning cannot be confirmed because there are few implications about real applications and few experiments.
In order for the paper to be accepted at this conference, it might be necessary to apply the method to a real problem such as the image generation problem of natural images, and then conduct experiments to compare the method with existing methods which solved the application problem."	4
"Justification: The paper makes a solid contribution in terms of technical content. The main high level idea is that solving optimal transport maps under some conditions can be posed as a PDE. While this is not something new, for instance, sinkhorn algorithm does the same -- turn a nonsmooth optimization problem, a linear program, into a smooth version, as given in equations (4) and (5). However, many technical details regarding the algorithm are missing -- (i) what is the overall procedure?; (ii) near equation (7), there is some discussion regarding representing u again using a neural network, is the overall architecture different from what is shown in Figure (1)? It seems like some of these questions regarding how and what PDE is exactly being solved can be answered by reading various other papers cited in the experiments, but it is not clear from the submission. In this sense, I feel like the paper spends too much time on background material (which the reader can pick up from any standard book on Applied PDE, for example, Introduction in Optimal
Transport for Applied Mathematicians by Filippo Santambrogio, 2015) rather than focusing on the key insights of the paper. 

The network details are provided in abundant details for verification of the main idea. However, the main task density estimation which the paper mentions quite in detail about in the beginning of the paper is only minimally experimented. For example, the paper claims that finite elements solvers are slow in high dimensions. However, the paper contains no comparisons with these methods on the problems considered in the paper, which are also low dimensional.

"	2
This paper presents interesting and novel ideas for density estimation, and to the best of my knowledge this is the first paper that uses PINNs for density estimation. The paper is clearly written and provides a good overview of related work and background to put the proposed ideas into context. However, since this paper provides only empirical results and no theory, I only think that the paper is slightly above the acceptance threshold. Additional results would help to make the story more compelling, and given additional results I am happy to reconsider my rating. 	4
The paper provides an interesting approach to incorporate ICNN and Brenier's Theorem to approximate optimal transport between distributions. Experiment results are encouraging. But there is no proof to support the claim on the uniqueness of the mapping. Therefore, its application on density estimation is also not fully supported.	2
Presumably the value of this paper is conceptual; I don't see any benefits to ML algorithms (e.g., the loss function proposed is not robust, i.e., some errors in the data labels will cause it fail). While the definition is simple and natural, I am afraid I don't see the conceptual advantages. There are already existing, more powerful definitions that provide some insight into computation, e.g., Statistical-Query Learnable, where every query is answered empirically up to some tolerance. 	3
This work seems to only establish a generalization theory for using neural networks to learn boolean functions and Turing machines. The main contribution is showing that proper neural network structures can represent the target functions and the networks have large margins However, the estimator (and even the surrogate loss function) are not easy to compute, which limits the practicality of this work. In addition, treating Turing machine as a classifier seems restrictive. 	2
Interesting proposal but not given enough motivation or set in enough context to be convincing.	2
The theory of the paper is novel and interesting. 	3
Overall, this paper provides a heuristic algorithm without any theoretical justification. Then, the experiment section should be very strong to be published. However, the experiment section of this work is not good enough. 	2
Given the concerns I have listed above, I suggest “5: marginally below the acceptance threshold”.	2
"As this paper did not conduct the theoretical analysis, and the proposed method is some combination of some existing methods, I would expect more extensive empirical analysis in the experiments.
1. Add more complex datasets, such as CIFAR10, CIFAR100. Note that MNIST and Fashion-MNIST are more like a toy dataset.
2. As Sec3.1 focuses on classification experiments, it is better to add some baselines related to corrupted data classification.
3. An ablation study is necessary since the proposed method is a combination of some existing methods.

Some other issues:
1. The motivation about the M-distance is not clear, why it can be used as model uncertainty to detect OOD.







"	2
The paper presents an interesting application of VAE to the task of image classification with corrupted data. The ability to quantify the uncertainty is important, however, in its current form, the paper only presents a proof-of-concept rather than a technology that can be deployed in real-world applications.	2
I'm recommending weak reject, because although I believe the paper is well-motivated and makes some inroads into an important question, I have concerns about some of the theoretical results, the sufficiency of the experimental results to justify the theory, and also some of the intuitive explanations for the theoretical setup.	3
A novel theoretical analysis that connects neural model's learning behavior to its ability in adapting to input structure. The analysis is done under a simple data-generating model and was relatively accessible. Author empirically verified their claims in simulation experiment. which slightly weakens the quality of this contribution. 	3
Overall I like the paper, and I think there are certain interesting results that are demonstrated, however, I believe the authors should elaborate more on the nature of the feature correlations considered: currently their model assumes two types of features, i.e., ones that are correlated with labels, and the ones that aren't. It perhaps would be interesting to examine whether this assumption can be weakened to something that holds stochastically over the data distribution, as such precise models for data generating processes are hardly available in practice (and are usually substituted by generative models).	3
The paper studies a very central problem in developing an understanding of deep learning today. It builds on a line of literature which theoretically capture settings where neural networks trained with gradient descent are provably more powerful than data-independent kernel methods. Moreover, they authors clearly outline the mechanism by which neural nets derive their additional power. Albeit a limited novelty (due to similarity to prior work), I believe the paper’s contributions are quite significant thereby I believe it clears the bar for acceptance at ICLR.	3
The contributions of this paper, in my view, are a simple model that seems to require learning input-specific features, and a careful detailed analysis of the first two steps of GD applied to a 2-layer network with truncated ReLU activations. I think these are interesting, but might not be sufficient to justify acceptance.	2
"The presented work is interesting, barring a few points commented on above. The motivation for this work needs further clarification from the authors. The experimental evidence of the efficacy of the method is strong. However, the paper in its current form misses some important details, and ablations. If the authors can address these points adequately in their rebuttal, I’d be quite happy to raise my score. 
"	3
Overall, the paper is easy to follow and well-motivated. While some ablated models and baselines are missing, the experimental results are comprehensive and seem to be solid. The main concern of this work is the lack of technical novelty compared with existing works.	2
The algorithm presented in this paper is an appreciable improvement to multi-fidelity variants of Bayesian Optimization, especially because it accounts for the correlation between the learning curves and avoid relying to strongly on low fidelity to make hard decisions on trials to stop or continue training. The experiments are convincing, they are broad and compare good baselines. The paper lacks important details in my opinion with respect to the optimization of the expected improvement and how it ensures a good fraction of the trials continue training. It also lacks analysis of the execution time of the algorithm and more explicit experiments showing it can avoid stopping good trials that progress slowly in the first epochs. I consider the work good enough for publication, but it could benefit from some clarifications and additional analysis. 	3
I cannot support the acceptance of this paper due to insufficient evaluation of the novelty and the effectiveness of the proposed method.	1
"The paper may have some merits in suggesting a ""deep kernel"" surrogate model which, although quite related to previous work, is stated to work even if just fitted on the small amount of data from a single experiment, in an online fashion. However, this has been tried several times before with little success, and details explaining why the current approach should work are missing.

The proposed method uses asynchronous scheduling (much like Freeze-Thaw), but is compared against synchronous scheduling baselines, which have a major disadvantage. Comparisons to SotA methods like ASHA (or PBT) are missing, these are not cited. There is also quite a range of prior work on learning curve modeling for HPO, which is not compared against. Open source code for doing a better comparison is publicly available (for example, Ray Tune).

Experiments are smallish scale, mostly on tabulated benchmarks, and again are not close to what is possible today with parallel computation. Compared to missing alternatives like ASHA, the proposed method is fairly complex and quite likely rather non-robust to handle. For example, it requires retraining a neural network model each time a bit of new data is obtained, which is very difficult to do.
"	2
This is an interesting paper in machine learning and healthcare. The approach introduces novelty in ML and has a potential to broader healthcare applications. However, the paper requires more work to improve writing quality. 	2
Although the paper has some great (but minor) contributions in terms of the novel augmentation for EEG data, there are still many aspects in which the paper can be improved. As per the current state of the paper, my recommendation would be a score of 5 (marginally below the acceptance threshold). If the authors address the above-mentioned concerns and strengthen their claims with additional experiments, I will be happy to update my score.	2
The paper is clear and well presented. The idea of the proposed method seems interesting. The experimental results show the effectiveness of the proposed method. A supplementary material was given to show more detail and results.	3
They propose a new differentiable relaxation of the problem. Some details of the evaluation and comparison of experiments need to be explained or clarified.	2
"Understanding the label leakage problem and protection against it under a split learning setup has the potential to start an intriguing line of work. The main drawback of this work is the restricted setup on which the attack/defense models are built on.
"	3
"1. The paper makes solid contributions in privacy-preserving machine learning.
2. Time efficiency of the proposed methods should be discussed and more ablation study can be done.
"	3
"## Recommendation

I think formalizing attack vectors and proposing mitigations for label privacy is an important contribution that may have a large impact. On the other hand, the current formulation/attack/mitigation is tailored towards the specific split learning setting which (as stated above) might require more motivation. Taken both into account, I'm giving a weak accept for now."	3
It is a strong paper based on the results, but the current version is only going to make sense to experts in the area. It is not a bad paper to accept, but it seems like it would benefit from a substantial rewrite. I would label it borderline.	3
The paper proves some good-to-know lower bounds on private ERM, but a significant portion of the results have been published before. At the very least these prior results need to be discussed. I am not convinced there is enough novel material here to warrant publication. 	2
"## Recommendation

Overall, although the techniques are somewhat similar to previous work, I still think that the problems considered in this paper are important enough that the contributions of the paper (i.e. closing the gaps in excess empirical loss) are above the bar for ICLR. However, due to the potential correctness issue with the pure-DP lower bound, I have to give a weak reject for now. (I'm opening to changing this score depending on whether this issue can be fix and how easy the fix is.)"	3
"--The work appears technically strong and non-trivial.

--The contributions may be viewed as incremental, but the techniques developed could be useful for other problems as well.

--This is a lower bounds paper, so my score for its empirical contribution is not relevant."	3
In short, the proposed GW-based divergence looks interesting in terms of computational and applicable aspects. However, srGW may be considered as a special case of GW barycenter (for one measure) except that the authors provide further insights.	3
The paper presents a good idea, in the form of a relaxation of GW, and its equivalent formulation, and promising results in real datasets. The presentation of the paper can be improved.	3
"Positive: I find that the idea of applying the srGW divergence to the graph clustering is interesting.

Negative: There is a little understanding of the method (srGW),  other parts of the paper are entirely based on utilising  other existing works. "	2
The authors take the well-studied GW problem--which involves optimization over probability matrices satisfying row and column constraints--and consider a simple relaxation wherein the column constraints are removed. They also propose variations of the underlying optimization with respect to various regularizers. The usefulness of the core method and its variations are demonstrated on benchmark datasets and compared to relevant baselines, where the authors obtain SOTA performance. Additionally, the authors propose a novel application to graph completion and provide experiments in this regard. While it could be argued that the novelty scores should be between 2 and 3, I would lean toward 3 because I believe the authors have done their due diligence for each of their claims in the paper. Overall, I think this is a good paper, and I would favor acceptance.  	3
"The paper presents a novel way of automatically extracting valuable information from programs before using standard ML tools to process them. While the core idea is exciting, the writing is in some places imprecise (see above) and the experimental evaluation is missing many details and leaves many question marks on the empirical claims. I like this paper, but I think at this time, it's shortcomings are too significant to allow for acceptance at ICLR.


Update after rebuttal
----------------------------
The authors did a great job of responding to my concerns and answering my questions, alleviating most of my concerns.

I would note that the current draft does not integrate the further explanations and results responding to my question (Q1a).

The draft also has not been updated to include the additional experimental results and ablations that the authors provided during the discussion period, and I would encourage them to include all of these in the next revision of the paper, either embedded into the main text or in an appendix.

Overall, I'm now satisfied that the paper is presenting an empirically meaningful improvement over earlier works. As the reviewing discussion highlighted, constructing program graphs as proposed by Allamanis et al. (and following works based on the extraction of semantic program information) is highly non-trivial and not an off-the-shelf product. I view the core contribution of the current submission as the empirically validated insight that this can be solved by the Semmle static analysis tool. While many open questions about design choices here remain, this is a substantial step forward that deserves publication at ICLR, as I expect it to be of significant interest to the ML4Code subcommunity at the conference."	3
"This paper proposes a general framework for applying deep learning (specifically DeepSet + Transformers) to code. The authors suggest the following steps for handling code-related tasks:
Building a relational database based on the code by applying pre-defined queries and user/task-specific queries.
Converting the database into a graph, defining some user/task-specific random walk bias and length, and user/task-specific anchor nodes.
Applying this random walk, encoding these using Transformers, and feeding the result to some task-specific neural network.

Overall I think it’s a good paper with a minor ML contribution. It shows that the use of the relational graph outperforms the common use of AST and data flow graphs. It has its drawbacks  - heavily dependent on expert knowledge per language and per task. Further, even experts can’t tell which are the best queries/random walk for the given task.
"	2
This paper proposes a new code representation technique that is based on the relational database to capture the program semantics for different tasks, however, the novelty in the model architecture is not enough and the proposed code representation approach is more like a feature engineering.	2
Overall, the paper shows strong empirical results. However, I believe that this is an application with little novelty, and few lessons to be learned for the ICLR audience. Hence, I would tend to reject this in favor of more ML-heavy papers.	2
Overall the paper is not written well. The studied problem is important, but the proposed method is not well evaluated and cannot be claimed as a good solution to the research problem. In the current review stage, I would like to recommend rejecting this paper, waiting for author feedback to check whether my concerns can be addressed or not.	2
The paper in its current form is already a strong contribution, however I would appreciate it if the authors focused on exposition more in the final version of their paper.	4
Although the paper has some caveats and the presentation of the work could use additional improvements, I still find the work interesting and enables a new perspective for the recommender system community. Notable contributions deserve to be called out are 1) it attempts to address the warfare of all parties of multi-sided marketplaces, and 2) it connects user and item utilities in a time-sensitive space. As a researcher working in industry, I'd like to emphasize that addressing the producer/marketer utility is crucial not only for the business success, but also for the market fairness as well as the long-term health of the ecosystem. In this regard, I personally appreciate the contributions of this work.	3
Overall, I believe that the experiments conducted as part of this paper are very well structured and provide multiple useful insights to the community. However, the analysis still falls short in several places (see above). The latent factorization models needs to be better motivated, and the generality of the framework needs to be clarified. Still, this is a very useful work to the community, and if the authors could clarify the questions raised, I would be happy to update the score. I haven't gone through the supplementary material in detail. If any of the questions above have a direct answer in the suppl. material, the authors can directly point to that and I would be happy to update my comments. 	4
"I find the paper well written and easy to follow. The studied distribution shifts are thoughtfully and carefully defined. The authors perform a ton of experiments for the distribution shifts they want to study. I think the proposed benchmark / framework is a great addition to the research community as it will standardize model training and evaluation.


Points I would especially like to see addressed during the rebuttal (partially copied the most important points to me from the main review):

1.	Figure 3: I find it weird that using more samples from the true distribution seems to hurt performance in many cases. Additionally, checking Figure 11 in the Appendix, it looks like higher N is correlated with higher accuracy though it is hard to judge only looking at the image. The authors should comment on this.

2.	Figure 4: Does this Figure show the accuracy on the biased or non-biased datasets? Similar to Figure 11, in Figure 12, it again looks like a higher N results in higher accuracy. In Figure 12, some bars are missing.

3.	Takeaway 3. I do not see how the presented evidence leads to the drawn conclusion that “Heuristic augmentation improves generalization if the augmentation describes an attribute.” In B.2, the authors merely show that “No augmentation always leads to a strong boost in performance.” But it is not discussed in what way the successful augmentations approximate the true generative model. This type of analysis has been done on ImageNet-C and CIFAR10-C in ref. [2] which the authors should cite here. In ref. [2], the authors define a minimal sample distance between the expected distribution shift and the used data augmentation and find strong correlation between the two. Here, a similar analysis would need to be performed in order to be able to do such a claim. In this light, tip 1 is also not grounded on evidence (although it is pretty obvious).

4. Contribution 1: ""We propose a framework to define when and why we expect methods to generalise."" I think the authors addressed the ""when"" question with their benchmark, but not really the ""why"" question. Can the authors comment on which results lead them to conclude why a certain method should generalize?


-> I am also happy to discuss any other points I mentioned but these would be the most important ones to me.




"	3
"===================
Justification:
I recognize the value of the proposed comprehensive framework and all systematic studies made by this paper. I would be more than willing to accept the paper once my concerns are properly addressed.
"	2
Overall, I think this paper is well-written, has a clear and practical motivation, and provides an elegant solution to the quadratic bottleneck issue in transformers. Most of my concerns are about the lack of correctness in some parts of the paper, such as choosing hyperparameters or evaluating LSH attention. However, the authors can easily address these concerns in the rebuttal period. Therefore, I am learning towards acceptance. 	3
This paper propose to learn separate hash functions for keys and queries with the guidance of attention utilization, which is the metric proposed in this paper. However, the implementation is inconsistent with the analyze and important method details are missing. At this point, I would tend to vote this paper slightly below the bar. If the authors can address my concerns, I would be happy to increase my score.	2
The authors try to improve some limitaions on the content-based sparse attention when using LSH-produced sparse attention patterns. A Learning-to-Hash Attention is formulated to enhance its model expressiveness. Experiments show its usefulness of the proposed methods. However, there are questionable points that should be clarified.	2
While the analytic efforts in this paper are both deep and comprehensive, the problem the authors try to solve has been studied with a quite decisive and general conclusion. Considering the overlaps, I cannot recommend accepting this paper.	2
I think the current version of the paper is not well-ready to ICLR quality, there are multiple missing parts in related work and experiments sections. 	3
While technically very sound, this work faces serious problems regarding motivation and provides several negative-seeming results regarding smoothing using uniform distributions. I do not recommend for acceptance, but could raise my score to borderline if a convincing argument of motivation is provided.	3
"The paper is motivated on an important problem - MTR. The approach is also on a promising intuition that a planner should be able to chain the logical operators based on the needs to a task. However, the approach is not well explained with respect to related work and planning capability is not well utilized. 

Beyond improving explaining, I will suggest the authors to attempt more mainstream reasoning tasks and not on toy settings.  "	2
In general, PRIMA is a neat approach for multi-task logical reasoning. However, while the method and optimization are sound, I’m slightly concerned with the results. The two big wins for PRIMA over prior methods are seemingly a reduction in FLOPs (which is hard to contextualize), and possibly evidence that indicates that PRIMA uses shared structure (which has no comparison to baselines). I think this could be a strong paper, and I welcome responses from the authors during rebuttal; I’d be happy to change my score if the above questions are meaningfully addressed.	3
"I think the paper looks at an often overlooked problem and introduces an interesting mechanism to drive the reasoning process. Though I do have concerns about some experimental results that are missing and could provide more information about trade-offs introduced by including a planner-reasoner in the architecture.
"	4
The system is able to achieve really good results in the tests conducted. On the other hand, the article is not structured very well and not entirely consistent with the title.	3
"Overall, the problem being studied is interesting and challenging. The approach proposed in this paper make sense. The experiments questions and design are good. Many major concern is intermittent question 4 in Main Review. Please clarify more for the authors. There are some areas for improvement like baselines, literature review, lack of discussion about . 
"	3
"In this paper, the authors identify a new and interesting problem called zero-shot recommendation, where there is no overlap of user or items between a source domain and a target domain. The main idea is to bridge two domains via the attributes of the item/users, which is called continuous universal item/user ID.

The authors conduct experiments and show that the proposed zero-shot recommendation framework work well. 

Overall, the paper is well presented. The studied problem (zero-shot recommendation) is interesting.

However, my main concern is that the idea to bridge two non-overlap domains via attributes is too straightforward, and the technical contribution is limited.
"	2
"Though the paper is well written, the reviewer finds it difficult to believe the ""zero shot recommendation"" is a novel setting and the proposed methods have differ from content traditional content-based recommender systems in meaningful ways. "	1
"- Optimizing the projection of $\prod_{i=1}^k || U_i||_{\infty}$ is not good enough. 
- I think the authors should also compare with some works that are for adversarial defense/attack tasks. "	3
The paper provides first generalization bounds for polynomial networks, which have been found to be empirically effective in prior work. Also an upper bound is provided for the Lipschitz constant of polynomial networks. Formal guarantees of low error and high robustness for empirically used techniques are an important contribution of the paper. The algorithmic insights from the bounds can potentially guide regularization and robustness techniques used with these networks. My main complaint is that very little space is allocated to providing proof insights, and practical insights of regularizing network weights and combining with adversarial training have limited novelty.	3
The theoretical results in this paper are strong, and they serve as a significant first step in understanding the generalization and robustness properties of polynomial networks. The practical regularization technique motivated by the theoretical results is effective in making polynomial networks more robust evaluated by common gradient-based adversarial attacks on image and audio recognition tasks. The only weakness of the paper is that the proposed regularization technique can feel a bit disconnected from the main theoretical results (due to relaxation for tractability consideration), making it a bit less clear whether the practical regularization is actually controlling the Rademacher complexity effectively. Despite this weakness, I believe the paper still deserves acceptance for its great theoretical and practical contributions in polynomial networks. 	4
Although the technical contribution of this paper is sound, the paper is not ready for publication due to the problems mentioned above. 	3
"This paper focuses on an important problem of open-set detection and demonstrate that a good closed-set representation is a very strong baseline and competitive with more complex methods for open-set detection, which is valuable for many practical settings.
Though empirical evaluation is comprehensive and illustrate performance on large scale benchmarks, given existing prior works novelty of contributions seems rather limited"	2
"This paper proposed interesting finding about the correlation between closed set classifier and open set classifier, and leveraging this finding, a new SOTA open set classifier is generated from a baseline classifier, which has similar performance as  other SOTA close set classifiers. Also a new benchmark dataset is generated.
Overall it is a valuable paper, which is clearly written, and with good experimentation results to support. The weakness of this paper is lacking of more in depth analysis, and providing more insights to the behind rationale, which lower the value of this paper.  
So overall I will recommend marginal acceptance of this paper. 
 "	2
The paper is well written and introduces interesting results that can change the understanding of the open-set recognition. Experimental results prove the main conclusion of the paper, but some additional experiments are needed (see above) to further understanding the problem. Also, in my understanding the distinction between open-set recognition and out-of-distribution is ficticial and the proposed cross-entropy baseline should evaluated also on out-of-distribution settings.	3
The paper feel like it's a small incremental change to existing literature and since I can't really imagine how this is going to generalize to non-gridworld domains, I can't recommend acceptance at the moment. If the authors could maybe include additional discussion of how their method applies to other environments or better, include experiments in non-gridworld settings, I would change my rating.	2
This paper presents a novel UED approach that presents performance gains on several baselines. My chief concern is that the benchmarks are limited in scope to situations that seem most likely for an editing process to succeed, whereas prior work demonstrates efficacy in domains that seem less suited for editing approaches. I recognize that this is an intuitive judgement about the benchmarks on my part; if the authors can put forth a compelling argument that the experiments are not so limited in scope, I would be inclined to raise my score.	2
The incremental nature of the method in combination with the unclarity w.r.t. details and theory behind the method are not allowing me to recommend acceptance. I may improve my score if my concerns are adequately addressed.	2
there is not much to say except that this is an intuitive solution to a good problem. the results over existing tasks look promising, but it would really benefit from more complex environments and edits, where the agents and environments interact in more ways than simply pathing.	2
Overall, I think that this is an interesting paper, showing that generative models contain a lot of information about the structure of images. Being able to build a segmentation model that really works on real data is quite impressive. As stated by the authors, this indeed is an interesting application of generative models that is useful on actual CV tasks. Because of the surprising observation and decent experimental execution, I think this paper deserves to be presented at ICLR. 	4
Overall I think that the paper is well written. My major concerns when it comes to the decision are related to the comparison with non generative methods and the lack of insights and information over the data used for training. Please see above for more details.	3
Although the paper is well-written and its implementation seems to be technically sound, my main concern is the limited novelty of the proposed approach. There are already many existing approaches that leverage GANs in order to segment images, and while this approach presents some useful ideas, I think they are somewhat incremental. Overall, the paper seems to be below the bar, but I would also like to hear the author's opinion and the other reviewers.	2
Overall, the reasearch topic is interesting and the proposed method seems to be noval. The experimental evaluation is not very convicing. I may change the rating score If above issues are handled well. 	3
As Above	2
"This paper studies a practical yet well-studied problem by applying existing techniques. The reviewer finds the paper well-organized. However, this paper has limited contribution, insufficient experiments, and missing details. These major drawbacks prevent the reviewer from championing accepting the paper.  In summary, I suggest rejecting the paper at ICLR and that the authors make major revisions until their next submission.
"	1
The motivation of this work is not clear. This paper lacks sufficient technical contribution and the evaluation is also not thorough. The coverage of related work is quite limited. The writing also needs improvement. Thus, my recommendation is a strong rejection.	2
In terms of technical novelty and experimental setup, the quality of this paper is not enough to be accepted to ICLR.	2
In summary, this work presents a new token downsampling method for efficient vision transformers. The paper is well written and properly structured, but the experiment is only performed on DeiT and ImageNet-1k benchmark, and no time analysis is provided, which is not up to par.	3
Based on my main review above, I think this paper proposes a solid token pooling method for vision transformers that can be potentially applied to other vision transformers based on tokens. I recommend an acceptance to this paper. 	4
The paper proposes a new token pooling method to achieve efficient vision transformers, which is interesting and well-motivated. But the method heavily relies on manual tuning and the generalization is limited. I will increase the rate if the concerns are well addressed.	3
The work provides an interesting insight into KD by investigating the sharpness gap between the teacher and student model. The ATKD method built based on this insight is simple but can significantly improve the KD results. However, this work still leaves several important questions around the justification of sharpness discussed above. The mixed pros and cons lead to my current rating, subject to be updated.	3
"The idea is novelty and interesting. The authors should provide more analysis to demonstrate the effectiveness of this method comprehensively. 

I would recommend this paper for ICLR2022."	3
"In summary, the paper proposes an interesting perspective to investigate the performance degradation in knowledge distillation using a large teacher model. However, some detail and assumption are not well-presented in the paper. 

-- post-rebuttal

The response addressed some of my concerns. I will keep my score. "	3
"The intuition of this paper is clear but the method proposed is not well presented, making understanding this work is hard. 
"	2
"I like the idea of semi-supervised lifelong machine learning and I agree with the authors that previous works on LML only consider the fully-supervised lifelong learning, which require the use of all labeled data and is quite expensive. However, the technical contribution of this paper may be limited. Considering that the performance of the proposed method is not that satisfying either, my current rate of this paper is: ""3: reject, not good enough""."	2
This paper implemented a semi-supervised continual learning framework that can be mounted on top of any existing LML tool. It integrates some methods to solve the scenario where labeled training data is expensive. The problems and application scenarios proposed in this paper are of practical significance. However, in my opinion, the engineering significance of the paper is greater than its academic significance, it lacks of novelty in the view of research.	2
In conclusion, despite the successful improvement of the LML task, I still have lots of concerns about the labeling results, which is the key to improvements. I would be more convinced if more analysis of labeling results could be done in this paper. For the current version, I recommend a negative score as my initial rating. 	3
The paper is clearly written, but the technical novelty is quite limited. The analysis and experiments also need to be significantly improved. 	1
Although the paper has some limitations, it presents good technical contributions. Therefore, I recommend the weak acceptance for this paper. 	3
The paper is well written, with a clean idea and strong empirical results. There are some issues with the presenttaion. I will give a weak accept.	3
Overall I think the manuscript is marginally below the acceptance threshold due to unnecessary limitations of the approach and missing theoretical justification of the proposed discretization scheme of the reverse SDE despite the outstanding numerical results. 	2
"This paper overall does a good job in presenting the technical contribution with deductions, pseudocode and experiments. Ambiguities do exist; I would encourage the authors to address them, so as to make the paper more readable. 

********* Updates after the discussion period *********

This paper is a good paper. I have increased my score."	3
"In conclusion I vote for week rejection because the theoretical part is too hand wavy and IMO the paper cannot be published right now. In particular, IMO, the name of the paper should be changed.
However, except this part, the paper is well written, and experiments are very interesting. If authors take into account the comments, I will raise my score.

"	2
"This work provides a simple extension of earlier work (Lorraine, 2019), which ends up being rather important, since the former simply does not work very well at all on standard HPO problems. However, the work does not do much more than that, It stays methodologically very close to this earlier work, and does not really address any of the shortcomings of gradient-based HPO.

The experiments are well designed to show superiority over (Lorraine, 2019) and other gradient-based HPO, but lack comparison to relevant other HPO baselines (ASHA, PBT), and results are not presented in terms of time spent, so they are hard to interpret.
"	3
"I am inclined towards acceptance, only weakly so at this stage as guidance on setting the introduced meta-hyperparameters could be improved and the unexpected results obtained with WD+HDLR+M are lacking follow-up experiments. That said, this is overall solid work which is well-evaluated both theoretically and empirically.
"	3
"I am not an expert in OOD so my comments needs to be largely discounted. With that being said, I like this paper overall.  I believe the paper is very clearly written, well justified, and provides strong theoretical result and sufficient empirical result. Given this, I am leaning towards accepting this paper. 
"	3
While the submission is certainly intriguing, promoting further the decoupling of semantic and non-semantic distributional shift detection, and also exploring semantic shift along a spectrum of increasing distance, questions remain in my mind about some of the terminology, motivations, claims of novelty, and inconsistencies of reported results with existing numbers that need to be resolved.	2
Overall the paper is well written, with comprehensive evaluations. However, the reviewer is concerned about several conceptual confusion that the paper may introduce (as raised in my detailed comments above). The reviewer hopes to see the satisfactory changes made, and increase the score accordingly. 	3
Overall, I like the technical contributions, which have been proved effective and provide new insights to the community. But I am concerned with the weakness part mentioned above, which should be addressed. I give borderline accept for now and will reassess the paper after the discussion period.	3
I literally found no weaknesses and strongly recommend acceptance.	3
"Overall, this paper lacks sufficient novelty for acceptance. It recombines a set of existing techniques in a slightly different way than has been done before and shows that this seems to improve performance on the small set of fairly similar domains presented. However, for such a minor change from what has come before, it lacks sufficiently strong theoretical or empirical evidence for acceptance.


----------

After rebuttal, the majority of my concerns have been addressed by a comprehensive update to the paper. To reflect this, I have updated my scores to accept."	3
I believe that this is a good paper, after rebuttal the issues I mentioned have been resolved. I like the idea, it is an important problem in RL, it seems to be correctly evaluated, and it will be a good contribution to the RL community as aleatoric uncertainty is generally not modelled in RL formulations.	3
Fantastic framing and decomposition of the sources of uncertainty present in learning an RL policy. The paper introduces inverse variance weighting to RL and does so in an interesting and well motivated manner. Despite some weaknesses in firmly anchoring the foundational pieces of the proposed IV-RL approach, the empirical results are shown to improve over comparative algorithms across a variety of domains. As expressed in my main review, greater clarity about the contributions of each component of the IV-RL loss, framed in a more specific terms, as well as more consistent focus on the proposed contributions throughout the paper will help me raise my assessment from borderline acceptance to full acceptance.	4
Overall I found the problem important, and the paper interesting (both technical and experimental parts) and easy to read. I do have some questions about the framework, and I would be more certain of my recommendation if the authors can help me clarify my confusion.	3
"The paper seems to be onto something that would be of great interest to the community, and the good results indicate that the authors know what they are doing, but the paper is currently almost incomprehensible to this reviewer. Although the authors are welcome to refute or explain the above criticisms, the fact that the reviewer was not able to understand the core idea of the paper during the first reading makes it unlikely that he/she will recommend to publish it in the current form. The reviewer would like to encourage to the authors to rewrite and resubmit the paper, though, as the content seems to be truly significant! 


**POST-REBUTTAL**

Thanks to the authors for their clarifications. However, it was not enough to clarify the paper's main message to the reviewer. The reviewer will therefore not change the score."	4
The authors proposed a new additional condition for value decomposition in MARL, True-Global-Max (TGM) condition, which is reasonable in some respects, but the reviewer believe that in the proof of the author's claim, there are lots of explanation to understand. Thus, if the authors can solve the above mentioned questions, the reviewer will raise the score.	2
The paper is well motivated so as to ensure both IGM and TGM conditions, and therefore stable convergence in MARL. The ideas in the paper are novel, and ITS is based on a proof and can guarantee the stability of greedy actions.  However, I am not convinced that the proposed superior experience replay can destabilize the non-optimal greedy actions.  I have asked a clarification question about this and will improve my score based on a satisfactory response. Other than this, although the empirical results are good, the authors have missing baselines from the experimental evaluations, and missing experiments with respect to proving the scalability of GVR with more agents. Finally, I have concerns regarding evaluations reported on 6h_vs_8z. Therefore, I don’t think the paper is ready for publication as is.	3
"I recommend (weak) acceptance of this paper because 1) the conclusions drawn in this paper, while somewhat incremental, are still very useful and would be a great addition to this line of work; and 2) the experiments are thorough and realistic. I don't identify any obvious flaw in the methodology or analysis.

---------------------

Post-rebuttal: See my comment below. I'm updating the score to 7 (which doesn't exist in the ICLR review form, but just to show that I'm advocating more for acceptance)."	2
I think that the paper places itself nicely within the existing literature by casting the focus on OOD generalization specifically. While the conclusions (that larger models and more diverse datasets are better) are not unexpected, such detailed empirical studies are worthwhile for practitioners and researchers alike. However I think the paper falls short by not going one step further and providing additional insight to try and explain some of its (many) experimental findings. This makes it harder to tell whether and how these results would transfer to other architectures, modalities or datasets considered here, which ultimately limits the impact of the paper. I think the paper is in a good shape, but it would need additional ablations and analysis to warrant acceptance at ICLR.	1
Although the experimental study of the paper is well executed, it provides few novel insights.	2
"1. The topic is important and interesting.

2. Two main contributions about learning rate and ID tricks may be incorrect or not well-supported.

3. The idea of using pre-training model is not new and the novelty of this paper is limited.

4. Unnecessary ID tricks and missed related work.
"	1
This paper studies pre-training of Siamese Transformer encoders for the zero-shot dense retrieval problem. While the empirical results seem promising, there are very little techincal novelty in this work. Since this paper concerns pre-training tasks for retrieval, the author should also show the results of ICT and REALM for the Figure 1 setup. At the moment, I think this paper is marginally below the acceptance threshold. If the author can address my concern about proper ablation comparisons, I am very happly to update the recommendation scores. 	2
"The proposed method is interesting and the experiments show it effectiveness in the scenario of zero-shot learning and few-shot learning.
However, the question is about the necessity to build a complexe neural model to compete with much more efficient BM25 model in the zero-shot learning situation."	2
"The paper proposes the approach of independent cropping of a paragraph into two segments for unsupervised training of the dense retriever. The authors argue that independent cropping enables the model to possess lexical matching ability, a property which improves retrieval performance.  

First, I feel that the writing in the Introduction and Related Work sections needs major improvements, specially that of crediting and citing the previous work in the field of unsupervised contrastive training. I have also provided my suggestions for the same under Weakness section. Second, I feel that the authors should correctly implement a ICT following the original paper and perform training using the same settings as done for their model. Third, instead of comparing with BM25, the emphasis should be on comparing results with ICT and providing an in-depth understanding of why or why not their proposed model works better. 

Due to these shortcomings, in its current form, the paper falls substantially short of the ICLR publication bar. 
However, I would request the authors to provide their responses to the points raised in the Weakness section and under the Other Questions."	2
Duo to the novelty and insufficient experiment, I think the paper should be further refined and improved.	2
This paper proposes an interesting alternative to gradient descent that can be used to train models with a weight distribution suitable for one-shot pruning. I find the method section well-written and convincing however I was expecting to see many other relevant work here and the experiments section fails to compare to these relevant works, so it's difficult for me to evaluate the effectiveness of the method compared to simpler/other alternatives.	2
The idea is interesting and the method is technically sound. The choice of the constraint set should be motivated and clarified to improve the quality of the paper.	4
"The paper proposes an interesting algorithm for training large DNNs. Particularly, the proposed method could handle different pruning ratios. 
There still exist concerns regarding the scalability of the algorithm. The authors are encouraged to address it with more experiments."	3
In summary, I think this paper is below the acceptance threshold since the main claim lacks enough support. And current results may be due to sparsity incurred by the large learning rate $\alpha$. In addition, experimental results are not enough.	3
I am inclined to accept this paper, because it contains novel ideas and the experiments are solid. Writing would profit from simplification though.	3
Overall, the paper presents a new idea for learning the inference noise schedule in generative diffusion models and the experimental results are promising. However, closely related methods exist [1] and a more detailed comparison isn't presented (it's not one of the baselines). My main concern is that I found the presentation and writing of the paper lacking in clarity and difficult to follow, as indicated by my questions above. This also makes it difficult to check the derivations and develop a solid intuition for the method. Related to this, the motivation for the specific approach and many steps in the derivations are a bit unclear. In conclusion, I think the paper is not ready for publication in its current form. However, the method does seem promising nonetheless. Hence, I would be willing to raise my score if the weaknesses and questions can be addressed.	3
"I think the proposal of the paper is sound and the developments are correct. However, the choice of the task for which to provide empirical evidence is in a single domain (speech) and very specific (extremely strong conditioning). I personally find the paper hard to follow and lacking motivation/intuition.

[Update: After authors' response and improved manuscript I decided to raise my score from 6 to 8]"	3
"Although I believe the motivating idea is very compelling, I don't believe this paper is ready for publication. In summary, I believe the paper currently lacks: 

- More thorough empirical evaluations comparing against other methods.
- Experiments showing the method's potential for generalizing to more naturalistic data, as well as its usefulness for downstream tasks.
- A more clearly focused narrative motivating why it's appropriate for a venue like ICLR as opposed to a cognitive science publication, as well as more thorough contextualization among related work (particularly comparing against recent alternative methods for this problem).

I thank the authors in advance for their response, and am also interested in seeing other reviewers' thoughts. "	2
A well-written description of a method for a chunk-learning algorithm, with learning guarantees and qualitative demonstrations of sensible-looking chunks across a variety of domains. Quantification of results was a bit lacking.	3
I like this algorithm and think it has potential. I can see how it can be applied both to standard ML tasks, but also how it could unlock a more symbiotic human-ML collaboration through its interpretability. The motivation and build-up from cognitive science is clear, and all else aside, because of its writing, I felt this paper gave me a lot more valuable insights than most. That said, I’m just not convinced by the current set of experiments. I can’t glean how well HCM will *actually* perform (vs. baselines on standard datasets), particularly the online variant, and I suspect it’s not computationally that practical either. With some of these comparisons added, I think I could accept, but for now it’s a reject from me.	3
This paper is missing significant background on classic hierarchical structured prediction. Because it is presenting a classical parsing algorithm without a single citation to pre-neural structured prediction as a field, I believe that it is extremely similar to existing algorithms that are rarely in use today.	1
"The paper studies an important problem. The theoretical analysis is interesting though it has its own limitations. It will help if the authors comment about the novelty of the proof techniques compared to the previous work. 

Many statements of the paper are misleading and should be addressed. The empirical study and the theoretical study do not necessarily support each other and this discrepancy has not been discussed in the paper."	3
"- However, the paper has many serious flaws. First of all, the paper is studying squared loss for its analysis and does not include a discussion about the choice of loss function. Loss function used in the previous works cited in this paper was cross entropy loss. So, there is this discrepancy between the works/experiments that this work is replicating and the theoretical results that are included.
 - This work is missing important citations for example [1] which initiated this study of importance reweighing methods in deep learning to the best of my knowledge.  Moreover, this has already been studied theoretically for cross entropy and other general losses by [2] which has not been cited in this work. So, it is not even clear if studying this paper for the squared loss brings out any new idea or insight on top of what was already known before.

I would like to make recommendation for rejecting this paper. "	2
"
I think this is an interesting contribution on the theoretical and practical study of worst-case subgroup performance and therefore I would like to see this work accepted. I also think the paper could be improved a bit further by adding experiments that more closely validate the theoretical results (point 5), and by clarifying limitations/shortcomings to this work.
"	3
The actual main contribution of this paper is proving that for linear overparameterized models, the current reweighting algorithms almost always overfit. It provides a possible way of analyzing overfitting problems by reducing a family of algorithms to the commonly used ERM. However, the analysis of neural networks contributes less, and some key steps are still empirical results. Besides, there is a gap between theoretical results and experiments. In summary, the conclusion of the paper is somewhat overclaimed, and there is still a lot of room for improvement. 	3
"This paper shows MBRL methods that utilized the decomposition of the action space and showed experiment results on the control domains.
In some domains, we see improvements in the model error or higher values. But the overall improvement is marginal, and the method can only be applicable to the problems having readily decomposable action space."	2
"The problem of learning decomposed world models is interesting and the authors do show a small improvement on most of the presented benchmarks.

Unfortunately, the clustering method, which is the main contribution of the paper, makes a number of strong and unexpected assumptions that make me question how well this approach will generalize. The paper also suffers from poor clarity in the method section on the theoretical motivation for these design decisions. The empirical results are not very strong to begin with, and were only presented for a smaller subset of the benchmarks in the papers they compare against. 

"	2
Although the paper is well written, the current contribution is not enough for me to recommend acceptance.	2
Some of the observations are interesting from extensive experiments, but there are a number of concerns on the metric to be used (e.g., selective model for uncertainty). There is no new method presented in this work, which makes this contribution somewhat weak for top conferences like ICLR (but could be an interesting workshop paper). The conclusions are not really surprising or are not easy to believe since there are other factors that were not considered in this work.	1
Overall I recommend a rejection of the paper. I think the definitions of sources of uncertainty are important in terms of what we are able to quantify and those definitions need to align with the evaluations, and I believe there are some mismatches in this paper. I think this distinction will immensely help the empirical evaluations performed in this paper.	2
In general, the document presents an interesting review about uncertainty estimating performance related to the models, and training strategies, however, perhaps due to space limitations, the main paper can present a general description of the work and rely on the appendix to give details. This can make the paper complicated to follow since in some cases, it might be necessary to jump into the appendix before continuing with the lecture of the main document.	3
This is a great paper. I have some quibbles about structure, but the basic results are extensive, interesting, and important. I would like to see this paper highlighted at the conference and would be disappointed if it were rejected (unless another reviewer points out to me prior work that I am not aware of).	3
Overall the paper provides a thorough comparison of different networks on their uncertainty quantification. The paper largely did not consider the vast literature on improving uncertainty quantification of any given architecture. 	2
"It is an interesting paper, which improves the best known bound in the field. However, I have some concerns regarding the novelty, comparison with the previous work, and technical significance. If the authors can address my concerns, I am happy to increase my score and recommend acceptance.   

After the discussion period, my concerns are addressed so I changed my score accordingly. "	3
Overall, this paper is well written. The techniques of extending from a deterministic PE algorithm to a stochastic PE algorithm through Markovian sampling are similar as in [Wei et al, 2021], with novel design on the estimators and necessary adaptation to the PE framework. I am leaning towards accepting this paper but have some concerns about the novelty.	2
"Overall I find the paper well-written and provides good contribution to the current line of works for solving two-player zero-sum Markov games. In comparison to the standard policy extra-gradient algorithm, the proposed stochastic policy extra-gradient algorithm is model-free, and is proven to obtain a better sample complexity than the state-of-art. The paper also presented clearly the intuition of the algorithm and the new technical techniques that were involved. 



-----
Post rebuttal update: I have read the authors' response and will keep my original score."	3
The impact of entropy regularization on the exploration is elusive. The assumption that the minimum steady-state probability under each policy pair during the policy optimization steps is strong. It would be good if further insights could be provided on these.	3
Overall, I think this paper makes a solid theoretical contribution in the problem of gradient-based algorithms for zero-sum MGs. However currently the contributions seem slightly incremental over the two recent prior works, and there is a lack of high-level discussions on where (population vs. samples) the exact improvements come from. 	2
"In summary, the paper presents a novel approach with three hierarchical levels for skill learning. The approach is novel, the results are good, the text is well-written, and the problem is relevant. The experimental evaluation is rather limited and several questions arise with respect to the benefits over previous approaches, the robustness and generality.
"	3
"I think this work has some very interesting experiments and results, particularly around the question of how we should trade-off leveraging existing skills against learning new ones, but these are currently not highlighted very prominently, due to a lack of clarity in problem formulation and experimental results.

There is a wealth of research around hierarchical mixture latent variable policies and skill discovery which is missing from this work, and reduces the strength of the architecture as a contribution. Refocusing on the fact that the architecture makes it easier to develop methods and strategies for transferring to new settings would help to distinguish this paper from this related work.

====== Post rebuttal comments ======

Thank you for your response and for the hard work in addressing the feedback provided. I feel the paper is much stronger for this, so have updated my score.
"	2
The proposed three-level hierarchy for learning motor skills might not be groundbreaking in itself, but it brings some interesting advantages and flexibility that is illustrated in the paper. It is interesting to see how far you can go in complicated cases by reusing an already learned skill set. At the same time, you might be able to achieve higher rewards in the end if skills are also refined. For that reason, this paper is definitely worth reading. It is possible to imagine training schemes in which you gradually reduce the regularization of the mid-level skills to make them more specific at the end of training, and thus possibly get the best of both worlds.	2
"STRENGTH

1. The paper considers a very important problem (skills extraction and reuse).
2. The proposed method is sound. Furthermore, the employment of variational inference with deep NN allows the method to be scalable and usable on high-dimensional tasks.
3. The experiment section is well structured and supports nicely the statements made.

WEAKNESSES

1. I think the paper is not very clear. In particular, some passages are missing, and some equations seem not to be correct.
2. I think that a large part of related work is missing, especially work prior to 2017.

I would give 8 (Accept, Good Paper), but I really think that this paper should be organized better, and the equations are not clear. I have an issue with every single equation in the paper. May I ask the authors to clarify my doubts?



================ SUMMARY OF CHANGES ======================

My main concerns were mainly about the clarity of the paper and the lack of a large body of related work. 
I think that the authors addressed both the issues in their new version of the paper.

The new paper is much clearer, and the proper discussion of related work helps in defining more rigorously what was the contribution of this paper. Therefore, I increased my score from 6 to 8."	3
"For now, I have an important concern on the main algorithm and I will recommend rejection unless the authors address my concern.


--------------------------

I have read the authors' response, and I have increased the score accodingly."	2
This paper developed a very interesting zeroth-order actor-critic algorithm that nicely integrates gradient-based critic training with gradient-free actor training. While this paper introduced some interesting ideas, the motivation for the new algorithm design may need to be clarified more. The theoretical strength of the new algorithm should be analyzed in further depth. There are rooms for improvement regarding the experimental evaluation too.	3
The paper presents a well-motivated and elegant idea to train an actor-critic algorithm, and the empirical evaluation is sufficient. I'm borderling recommending acceptance (I would be happy to see the paper accepted, but could live with it being rejected in case I missed anything). I think that more recent baselines would make the paper stronger: strong theory and motivation, then a strong comparison against state-of-the-art baselines.	3
"I find the question approached by the authors to be an interesting (although not novel) one and requires studying from various angles in order to shed more light on how recurrence and depth are related. The authors have made a nice effort to perform various analyses (both quantitative and qualitative) to study their hypothesis that recurrence mimics depth of processing. However, I find part of this study (Section 3) to be quite similar to prior work in this area although scaled up with more datasets. In Section 4, I find contradictory claims such as the authors claiming that performance rise not fully explained by receptive field size, and contradicting this statement with models containing dilated convolutions (exponential receptive field growth with depth) that produce large performance gains. The recurrent models studied are quite simplistic and lack several interesting components that have contributed to the expressivity of recurrent processing in the past (such as gating, bypassing, nonlinearities, temporal decay etc.). 

At the current stage, I feel that the readers do not gain much new information about the relationship between depth and recurrence beyond what has been established by prior work. I suggest the authors to experiment with more sophisticated recurrent networks and explore more novel analyses (such as the maze challenge, which I find to be really interesting but needs to be scaled up to explain further similarities beyond receptive field size growth) to understand the relationship between depth and recurrence. This is what I feel is required to truly expand our understanding of (whether, and under what conditions) recurrence mimics depth, and under what conditions / tasks the former is more expressive than the latter and vice versa.

======= Update after author response =======

I have read the authors' response and updated manuscript, after which I am increasing my score to 6. I believe that the updated manuscript's contributions are clear and builds on the interesting work from Liao and Poggio, 2016, adding further information with novel analyses and a new dataset (Maze challenge). I believe this work will spark further discussion and exploration of the similarity between recurrence and depth."	2
"The paper has interesting results on how hierarchical representations emerge in recurrent networks akin to stacked networks, despite the former sharing parameters across its ""depth"". However, the similarity in task accuracy between recurrent and stacked networks with similar number of non-linearities is not terribly surprising. It also doesn't shed light on whether these models approximate similar functions on whether this finding can be generalized to other task paradigms. I believe this is a promising research direction and encourage the authors to analyze the similarities and differences in the information learned by the two paradigms in more depth."	2
"It is nice to see this work take a closer look at what exactly differs or not across layers with distinct or shared parameters. This is an important topic to scrutinize since deep models are now so common while sharing weights across depth is still rather rare. The intuitive or ""folkloric"" understanding of deep networks as strictly hierarchical is not beyond empirical analysis, and so in general I welcome this effort. Nevertheless there is more to be done for this work to be sufficiently thorough and informative. Putting aside the missing related work, and considering only the content in the paper, the analysis done here is quite narrow. The only modality considered is vision, the only mainstream task is classification, and standard/unshared models are only studied where they most closely align to recurrent models. Inference is studied in several aspects, like accuracy, feature visualization, and layer-wise prediction, but optimization is not addressed at all. That is, does a recurrent model train differently than its paired feedforward model, even if they ultimately converge to similar accuracy? In summary, I encourage the authors to continue in this direction but with a wider lens, and re-submit with more coverage of different data and tasks, and most of all broaden the bounds of which shared vs. unshared architectures they consider (for instance, deeper nets like ResNet-50 due to its popularity, and perhaps hybrid architectures that still have pooling but share all weights within a ""stage"" at a given resolution, and so on.).

**Final Review** The author response addressed the points requested for rebuttal: inclusion of missing related work, an experiment at greater depth with greater resemblance to a standard ImageNet classifier, and more justification of the layer-wise activation analysis. While I still find the technical and empirical novelty of this work to be lacking, as much or all of it can be reconstructed from the related work, I can see value in the unified experimental setup and the juxtaposition of different results. As this paper could bring more attention to recurrence and depth, as earlier work seems not to have done, it could inform the community. I have therefore raised my score to 6."	1
I recommend accepting the paper because it highlights the importance of computation depth instead of the number of parameters (which is commonly assumed to be a good indicator of the predictive power). This might motivate further research in models with increased computation depth. Despite the weaknesses, the authors presented enough evidence for supporting the main messages of the paper.	3
Compared with the previously published methods, however, the new method did not show a significant superiority. However, it was a good application of VAE for an important clinical question. 	2
The authors introduced sVAE, a prediction-based, interpretable deep latent model designed for clinical prediction tasks. However, such tasks based on high-dimensional and complex data exist not only in surgery but also in a wide range of fields in the real world, thus generalization is important considering the backgrounds of ICML participants. If the authors aim to keep the scope within the clinical or surgical field, a more specific venue would be appropriate.	3
Generally, there are no major concerns with regards to the empirical setup (nothing that was done appears incorrect or suggests incorrect results), except that more should have been done to validate the results. The technical contribution over the vast amount of other work on disentangling latent representations is clear but perhaps slight and therefore the applied angle seems to be emphasized (indeed, ‘surgery’ is even in the name of the model) but, apart from using some relevant datasets and showing some qualitative results, there is almost nothing about the applied context that seems to influence the design, nor the analysis of the results beyond the standard ML metrics.	2
"*Implicit Bias of Adversarial Training for Deep Neural Networks*
contributes a milestone result to the theory of adversarial
robustness. The majority of the paper is clearly written and
correct. However, the Adversarial Training for Linear Neural Networks
portion of the paper has several flaws: potential errors, vague
language and ill-defined statements (such as ""approximately the same""
and improper use of limits in Equation 52), and use of undefined
notation and definitions (such as alignment phenomenon). Nevertheless,
the contributions are significant and novel, and the paper would
receive an accept if Adversarial Training for Linear Neural Networks
is amended or removed. 
"	4
This paper makes a solid theoretical contribution. It could be improved with more empirical verification of the results.	4
While the paper shows some insightful results on the convergence of adversarial training for deep linear networks, the assumptions for the analysis of deep nonlinear networks seem too restrictive to me. Also, replacing the gradient steps with the gradient flow seems incompatible with standard adversarial training experiments. The paper would become much stronger after relaxing some of the assumptions and performing the analysis for the actual gradient descent algorithm rather than considering the gradient flow.	3
The theoretical contribution of this paper is good to me (unless other reviewers find technical errors in the proof). I think it is a good supplement to the current theory of adversarial DNN training. 	3
My recommendation is based on the introduction of the novel end-to-end learning approach for AUC maximization and the presented empirical results on 8 different datasets that show general improvement compared to other AUC maximization / CE approaches. However, my enthusiasm was dampened by the unclear experimental setup for parameter tuning/use of validation sets and lack of comparison to highly related work.	3
Overall, this paper is well written and explained. I have listed several questions and concerns in the Main Review.	3
This is an interesting paper that proposed a number of technical advances (formulation of the compositional loss function and its optimization) with reasonable results, albeit there are weaknesses in the experimental evaluation.	3
"The results provided in the paper is impressive.
But the authors argue multiple times that the method proposed in this work is better than end-to-end generative model without enough justifiation.
Thus I cannot say that this paper is above accept threshold at current stage."	3
"While I do not see a significant amount of novel technical contributions, the proposed paradigm of breaking down scene generative models into multiple stages of learning that allows encoding prior structure and human-interpretable elements is valuable. It can help generate theoretical insights, and the ablative analysis provided here is a promising first step. It also has the potential to be practically useful, by allowing users more fine-grained controls on the parameters of the generated scenes.

There are some claims with minor issues, and some lack of clarity in some sections (see weaknesses in the review section above). All in all, I think the paper is above the acceptance threshold, and addressing these weaknesses would make me lean towards a clearer accept
"	2
Because of the above two major weaknesses, I'm skeptical of accepting this paper. I am open to change my rating depends on the rebuttal. For now, my initial rating is reject.	1
Overall, I think the studied problem is important and the proposed method is novel and reasonable. Considering that some experimental comparisons are missing, I believe this is a borderline paper and recommend a weak accept. 	3
"The idea and presentation of this work are, in my opinion, good despite existing problems. 
Furthermore, I think it would be better to move the analysis of COPDGene data into the appendix since all experimental results in this section are missing from the main paper. The illustrative samples in the appendix should also be included as part of the main paper.
"	3
The paper proposed an interesting and innovative solution to an important problem. Also the evaluation is very convincing.	3
The paper proposes a method for computing asymmetrical feature interactions for local explanations of model predictions. The proposed approach is better able to identify mutual and directional redundancies compared to existing methods on a wide range of datasets. Overall, the paper is well-written (aside from some minor clarity issues, see review above) and provides a method that can extend the use of any univariate explainer to generate directional bivariate feature-interaction explanations. One of my main concerns is how this bivariate explainer performs better than univariate explainers at a task seemingly designed for univariate explainers.	3
"The general idea, insight from empirical analysis and results appear promising. However, important aspects of this work are not properly addressed to support the main claims (limited evaluation, comparison to relevant work, general writing quality). The authors are encouredged to consider revising the paper accordingly and re-submit.
"	2
The paper proposes an interesting approach in uncertainty estimation for deep neural networks. The paper has some weaknesses that I hope the author can fix it soon. For now, I recommend a weak accept.	3
Overall, the main advantage is the simpleness but my concern on the significance of the result making me feel this paper is slightly under the acceptance bar.	2
The proposed method is interesting, but the novelty is regular as it is similar to Mixup and the performance is not too impressive when we also consider ResNet50 results. Moreover, the paper completely ignores the IsoMax loss and the IsoMax+ loss that are significantly related solutions. Hence, considering that the IsoMax loss variants already significantly increase the OOD detection performance without requiring costly hyperparameter tuning, this proposed regularization term would be more valuable if the authors show it also increases the ECE, Domain-Shift and OOD detection performance of the IsoMax loss and IsoMax+ loss when combined with them. Finally, it is possible that combining IsoMaxPlus with Min-MaxEnt may produce state-of-the-art results and also make the proposed approach work better (i.e., producing more stable results) when dealing with the ResNet50 model. 	3
The paper offers an interesting approach to improve robustness to domain-shift and better detect OOD data. The major concern I have is about the novelty of the proposal and its similarity to mixup, especially because the latter was already studied in this context (calibration, OOD, and adversarial robustness). Here, I believe that the authors should do a better job in distinguishing between the two methods (and other existing extensions of mixup).	2
I will weakly recommend rejection due to the limited qualitative evaluation and otherwise inconclusive empirical results.  I would be willing to reconsider my score if the approach could be show to promote the expected qualitative effects of minimizing surprise in a simple setting.	3
"I confess that I miss the point of the paper. I am not confident that I could recreate the method and this potentially of by-product of key bits of information being excluded from explanations. I am fairly confident that the authors have a strong technical understanding and justification for what they have done, but this is not expressed clearly in the paper, at least not in a way that I can decode.

## Update after rebuttal

My concerns have been partly addressed by clarification on what measures of surprise can be used and how these would be estimated. The publication of the code also goes some way to addressing my concerns. The other updates also improve the paper and the explanations and so I am changing my recommendation to weak accept."	2
"
This paper is attempting to extend the surprise minimisation to the multi-agent learning regime. My major concern is that Eq.(4) seems to be solely a marginally new way of representing the Q-function. It is not different from the typical Q-Mix or Q learning. 

The core message of how Eq(4) or Eq(7) can minimise surprise is not clear to me either.

"	3
In summary, this work proposes an interesting problem and give a thourough analysis on the proposed method. However, due to the multiple concerns on the theoretical and experimental results, at the moment I can only recommend reject (but marginally below the threshold).	2
The paper is overall a good paper but I think it's marginally below the acceptance threshold because of its limited technical novelty and significance. It is worth of a publication as a technical report but it has a marginal value for the audience of an international conference. 	2
This paper proposed a new model-free RL method for visual continuous control problems. This method is efficient and straightforward. But the evaluation of the proposed method is limited. If the author can conduct more experiments on Atari games and with on-policy algorithms (such as PPO) can further improve the value of this work.	2
"Overall I am towards acceptance of the paper. I think it's straightforward to read and the claims are well documented and backed by empirical results. There are no real technical contributions but the empirical novelties of changing to DDPG/TD3 and the performances are good. Half of the contributions indicated are just related to hyperparameter tuning and optimized implementations of the same algorithm and for this reason it's not a higher score. Hopefully the authors can address my concern in the rebuttal period.
"	1
"This paper presents various improvements for visual RL with continuous control. Various ablations support most of the claims in the paper and notably the approach presented can solve the humanoid task that is not possible with prior methods. 
One interesting aspect of the paper is that it provides different perspectives on RL benchmark. e.g., more emphasis on wall clock time, DDPG is actually better than SAC in the visual domain.
If my concern in the main review is addressed appropriately, I believe this will be valuable to the community."	1
Good and simple idea. I have some issues with lacking implementation details of the method that the authors use in practice. One of the sections seems completely redundant to the narrative, unless the authors provide better explanation for their final statements there. Some of the experiments seem to have lower numbers than publicly known results and in general there is a lack of exploring hyper parameters for other regualrizers. The last experiment is probably the only one that is done really well.	3
The idea is interesting, the approximation is nice, but the experimental results are too weak/incomplete to make this a convincing paper.	3
"Overall the paper is well-written.  It proposes a new regularization scheme that seems to improve deep neural networks performance on CIFAR-10, CIFAR-100, and WIKI-TEXT2 datasets.  The proposed scheme improves performance for both convolutional and recurrent neural networks.  The paper uses linear dynamical systems theory to argue that minimizing the trace of the Hessian improves the generalizability of the network.  Lastly, the mathematical description for ""efficiently"" computing the trace of the Hessian is easy to follow.

My only issue with the paper is that it doesn't include any results to support the assertion that the algorithm for estimating trace of the Hessian is efficient.
"	4
"I do not recommend to accept the paper in its current form since the connection between the theoretical justification and the empirical evidence is weak and the experiments in the paper are not sufficient for an empirical understanding of how the suggested regularizer really works.
"	2
The paper proposes a simple but elegant method to incorporate prior information about optimum in the BO workflow. It's a long-standing research gap and the proposed solution fills the gap to some degree. The best part is that the simple way of integrating prior makes its analysis straightforward using the current techniques. However, it has got some limitations: 1) lack of extension to GP-UCB acquisition function, 2) lack of discussion around the extreme looseness in the bound that can happen when the prior is chosen improperly, and 3) lack of discussion around the relative scaling between the acquisition function and the prior function and its impact on the influence of the prior function in the process. I would go for weak accept. But I am ready to move up depending on the rebuttal.	3
While the proposed method for this problem makes sense, it may be hard for experts to provide such information in practice. Further, the convergence analysis seems limited to Expected improvement (EI) acquisition function only. Last but very important, different methods seem to be starting from different initialization points, which may be problematic.	2
The method is simple and sensible, and experiments convincingly demonstrate that it works with a well-specified prior, and more importantly, demonstrate that a poor prior could lead to problems. It’s a rigorous scientific work that could have a significant impact on the way BO is performed in the HPO community today. 	2
If not particularly original, the proposition is well motivated and linked to the state of the art, plus it comes with extensive empirical results supporting the approach.	2
"
 This solid theoretical work is a substantial extension of Xie et al. (2020) by considering weighting in both parameter space and data space. The main contribution compared to the related work is well illustrated.  The paper is also well written. I have no time to read the proofs step by step, but it would be helpful if the authors can highlight the novelty of the proof techniques.   I support its acceptance of this solid theoretical work after the authors can address some of my comments mentioned above. 
"	3
Overall, the paper is well written and technically sound. The results should be of interest to the community.	3
In sum, the problem setting is quite simple, specific, and appears far away from practice settings. The derived results bring in new message and findings but could be not enough to overlook the drawback of problem setting. 	3
Strong technical part, perhaps somewhat off topic for ICLR22 but nevertheless still a very good paper. 	3
Better if the author(s) could provide more motivations and guidelines on how the weight could be selected, and why the same sequence of weights is used for both the sample space and the parameter space.	4
"Overall I think this paper presents a novel idea to learn stride in the downsampling layer. However, the current results are not good enough to showcase its effectiveness by making the stride learnable. The authors may want to find more ways, as discussed in Sec.4, to show its value.

-- Post rebuttal
After reading the authors' response, I raise my rating to 8."	3
Overall, I recommend the paper for acceptance. The contribution is novel and clearly motivated, and experimental results are encouraging across different datasets. I think this paper can be of interest for many within the ICLR community. There are some improvement points as discussed above, but in my opinion the strengths clearly outnumber the weaknesses.	3
Overall, as can be seen from my review I'm satisfied that this paper makes a good contribution.	4
Overall, I think this is a good paper with an interesting approach on differential stride learning. There is no theoretical discussion and I would like to see some additional experiments, however I feel that this work is slightly above acceptance threshold, mainly given the novelty of the application.	3
"NFQI has many real-world applications, including the use in healthcare. However, the novelty of the method is rather limited. Also, comparisons with additional baselines would be required to further validate the approachg.

Minor comments:
- background loss L_b is not defined in Algorithm 1
- Table 2 is duplicated, so which list of features has been used in the experiments shown in the paper ?
- In figure 5-a superimposed bars are not clear as mixed color is showing up. Separate bars would be better for clarity.

"	2
This paper proposes an approach for intelligent weight sharing in offline RL when the dataset available consists of two groups with slightly different MDPs, and where group identity is known/observable. This problem can be trivially solved by simply including group identity in the state representation. The experimental design is good and the research questions are interesting. However, the problem is simple enough and the solution trivial enough that there's not a whole lot of novelty or significance in this paper. I recommend rejection.	2
There are several concerns about significance of the problem setting and proposed algorithmic approach. Additionally, there are severe gaps in clear exposition outlining how the proposed algorithm is set-up and run. 	2
The presented setting is interesting to look at and potentially useful in clinical decision making, and the simulation results justified the benefit of the proposed approach to a limited extent. However, experiments on real EHR data need to be improved, and there seems to be a lack of theoretical understanding for when and why this approach works well; this will also help generalize the results to more than two groups. There needs to be better framing regarding whether the conclusions are limited to offline settings. Lastly, it's important to include a discussion that relates to past work on factored state spaces and factored value functions. 	2
An impressive empirical paper that investigates many findings regarding the lottery ticket effect found in other domains to RL tasks and algorithms. The paper presents results for a large number of tasks (with continuous and discrete observation- and action-spaces), and loads of relevant controls, baselines and ablations. The main finding is that iterative magnitude-based pruning can effectively find winning tickets in RL, and that there are some qualitative differences between RL and supervised learning (masks seem to be more relevant than initial-weight values of winning tickets). I only have some small criticism regarding one hypothesis, and whether it can be stated as empirically verified given the current set of experiments. Currently I think the paper is interesting and relevant to a large part of the ICLR community and vote in favor of accepting the paper.	3
This an important study and analysis that will motivate further research. However the current version has some major concerns that the authors need to address. 	2
Overall, I think the careful analysis and clear presentation of the findings of this paper make it a valuable contribution to both the lottery ticket hypothesis literature as well as to our understanding of non-stationarity in deep RL. I therefore recommend that it be accepted to ICLR.	2
Interesting idea, but lacks novelty.	2
"Overall, I'm recommending a borderline reject. 
I think that the general concept of creating adversarial attacks for concept-based methods is interesting and relatively novel. However, the implementation of the idea requires assuming that the interpretability model will be re-trained, which limits the applicability of the method. Further, as a paper that introduces a new method, I think that more experimentation is required to demonstrate its effectiveness. "	3
"IMHO, I am afraid this paper has not reached the bar of ICLR due to the following limitations. Detailed comments and suggestions can be found above.

1. The literature review of this paper is incomplete regarding both explanation methods and attacks on explanation methods. 

2. The main claim of this paper is not that accurate. As discussed above, this is not the first work that studies the robustness issue of concept-based explanation methods. It has been studied in previous works. 

3. The technical contribution of this paper is thin.

4. Regarding evaluation, this work lacks some potential comparison baselines.

5. Finally, I would suggest the authors discuss and even evaluate potential countermeasures of the proposed attack. "	2
In the current form, I recommend rejection due to the above-mentioned reasons. In particular, the paper needs to clearly justify the motivation of the work with a stronger experimental evaluation to demonstrate the effectiveness of the proposed method.	3
"Overall, the paper solves an important problem in the literature of contextual batched bandits. The proposed approach of imputation of unobserved actions is sound and both theoretical results in terms of regrets bounds and superior practical performance on real-world datasets are shown in the paper. 
Still unsure about the imputation approach taken and if we can further improve the sketching approach considered in the paper. "	3
"Recommendation: weak reject.
The paper addresses an interesting question, and is generally well written, with a promising algorithmic direction. However,
I believe the central imputation idea gets lost in the many variants considered. I believe having sketching variants
is important and I liked that facet of the paper. However, I believe that there isn’t sufficient clarity on how the actual imputation is accomplished, and what is the impact of such imputation. Further, the paper does not provide intuition on what modeling assumptions and statistical data features are behind the performance improvements when imputation is used."	2
"There is a substantial amount of work which has gone into this paper, and the authors have combined several different ideas – contextual bandits, batched bandits, sketching approximations for efficiency – together in a mostly careful manner. This is non-trivial work. My concerns are that there several areas where the explanations feel limited and not user friendly, where the interesting features of the method are not fully explained, and where there are slip-ups in the theory and reproducibility. The latter two of these points mean that I’m not fully convinced of the effectiveness and utility of the method.

I also question whether ICLR is the right venue for this work. It seems that it has been difficult to adequately explain the method, its theoretical guarantees and its place within the literature within 9 pages, and a more accessible discussion may be achievable in a journal paper. 
"	3
Generally, in the current stage, I recommend rejecting this submission. Justifications are in the main review.	2
This paper aims to solve a domain adaptation problem on the edge device which is a common problem today. The proposed method can help to improve the performance of a various of edge devices which I think is feasible and useful.	3
This submission targets to a practical research problem: how to train a model with high generalization ability via decentralized data. However, the technical novelty is limited and the experimental results are not convincing enough. 	2
Paper solving a problem of interest with a well-explained methodology but questions on results remain	2
"The proposed DriPP model is broadly applicable to neuroscience applications to isolate underlying stimulus-driven neural patterns and unrelated temporal patterns (background neural activity, artifacts, etc.).  

Authors need to provide justifications to support model parameter choices. A comparison with other related work (e.g., epoch averaging) is needed to support authors claims that their work address limitations of these prior work. There appears to be some similarities with ICA, which is given limited consideration in the text. 

-------------------
Update post-rebuttal: Reviewer read the author responses and the revised manuscript, as well as other reviewer comments and respective responses. Appreciated the author responses and revisions to the issues raised by this reviewer and other reviewers, and most of the main critiques are addressed. Potential to improve/extend DriPP method in future work. There is an applications track (neuroscience) in this conference, and this work is of relevance to the broader learning representations community. Revising score upwards. Authors should proof-read the manuscript for typos (also see other reviewer comments). 

"	3
The paper introduces an interesting technique for the characterization of neuronal events, but I do feel that the technique should be evaluated more thoroughly and compared with other approaches.	3
To summarize, the authors propose an interesting DriPP technique to decompose complex M/EEG signals with a bit disappointing twist. The resulting components are 95% similar to what ICA could do, and the ICA is scrutinized at the beginning of the submission, which seems confusing and disappointing.	4
"The paper discusses a modeling approach relevant for understanding how waveforms in neural potential time series relate to known stimuli or background processes. The modeling approach seems consistent and performs well on the given data sets, but wider implications and insights as well as formal analysis are limited. Due to some minor concerns and large questions, my current opinion is borderline and I hope the authors can help clarify any issues raised. 

++++++++++++++++++++++=
Update after revision:
I think my minor concerns and questions were answered. I encourage the authors to carefully proofread the revision. 

My concern with not raising the score more are the limited scope/impact of the work in the learning representation community. Nonetheless, I think that meaningful practical utility may be gained in neuroscience analysis from the proposed approach."	3
Overall, this paper proposed a novel algorithmic idea in offline RL. I think the algorithmic contribution in this paper can be helpful to the community. Though some theoretical claim/results in the paper needs to be better explained.	2
"I am leaning towards a recommendation to reject this paper since the proposed algorithm does not properly regularize the state-action visitation, and the theoretical and empirical results do not support the main points of this paper as mentioned in the Main Review section. 

---
After the author response, I think the paper provides a good empirical study on state-action joint regularization methods, so I increased my score for the empirical novelty and significance. However, my main concern remains so I am keeping my overall score.  "	2
This paper developed a framework that supports learning a flexible while well-regularized policy in offline RL. The technical novelty is sufficient and significant. The empirical results show the method reach satisfied performance on D4RL while I'm looking forward to seeing potentially better results on datasets with more modalities. Overall, I recommend to accept this paper.	3
"This paper has proposed a straightforward but effective solution that can infer Hamiltonian dynamics even when the governing ODEs exhibit numerical stiffness. The problem is very well-motivated, and the paper explains the core ideas in a very precise way. However, I have mentioned in the *main review*, some aspects (especially the assumption that $M$ is a diagonal matrix with entries that are position-independent) should be properly addressed or explained to increase the concreteness and overall clarity of the paper.

********** Post Rebuttal Response **********
I would like to thank the authors for addressing the prior concerns. I have updated my score."	3
This paper addresses the interesting problem and is well-motivated. Also, the proposed approach is novel, and the experimental results are insightful. Although I tend to accept this paper, I have some concerns, which I detail in the main review.	3
There are certain contributions of this paper on proposing characterizing the stiffness of the time series data, which shows its advantage of improving the performance of the Hamiltonian network.	3
"This study is based on an insightful suggestion, and the proposed method is simple but effective. However, the strategy is heuristic, and the generality is unclear.
"	2
Overall I think the paper tackles an important problem and provides a sensible solution to it. However, I am underwhelmed by the empirical results of the paper and not yet convinced that the proposed approach actually works reliably in practice.	3
See above. This paper seems interesting but is not convincing enough for an acceptance. 	2
The idea of regularization in the embedding is interesting but I feel is underdeveloped and I do not find that the paper provides sufficient evidence for its claims that the method is necessary or useful. As such I do not think it is ready for publication at this time.	2
The paper focuses on an issue in latent space based Bayesian optimization and proposed a solution for it. It also appears to provide a useful theoretical tool in regret analysis. However, the added complexity in the pair-wise loss and how new hyperparameters are handled deserves more through theoretical or more extensive empirical support. Therefore, I think an improvement is required for the paper’s acceptance. I will be expecting that some of my concerns are answered in the discussion period.	2
This paper tackles a very important problem with rigor and soundness. However, some key points need to be addressed (see the main review section).	2
While the paper is formally correct and well written, there are other published papers that have shown the main claims of the paper, and have even gone some steps further by analyzing possible defenses against adversarial attacks. Therefore the novelty is too limited, and the key findings are not surprising. There are some new elements, e.g. the analysis of attacks on real neuromorphic hardware, but overall this is too little for acceptance at ICLR.	2
This paper verified the attack algorithms can be adapted to the dvs data. The Sparsefool algorithm achieves high success rates through adding and removing a few events. The authors proposed a next step, which building real-world patches that can fool networks. 	3
The paper is nicely written and organized from an empirical contribution perspective. However the paper is rather weak in depicting its methodological contributions in my opinion, and lacks comparisons/references to relevant SOTA works. I listed my major concerns in the main review, and would be willing to re-address my rating based on the authors’ responses and revisions.	2
The authors propose well-designed experiments to show SSL is more robust than SL in feature learning from imbalance data and SSL learns label-irrelevant features that could be useful for the rare classes.   The presentation can be improved as indicated above.	2
The paper considers the imbalanced dataset problem that is relevant to the machine learning and computer vision communities. The proposed analysis also makes sense and is interesting to the ML and CV communities.	3
This paper focuses on class-imbalanced learning and shows that self-supervised pre-training yields representations that are more robust to dataset imbalance. The problem is interesting. But the results are mainly observed from empirical results, the generalization of the conclusions is not clear. The novelty and contribution of the paper are limited.	2
I think this is an important paper with experiments and limited theory to show and explain why SSL methods are more robust to class imbalance problem which also occur frequently in practice.	3
"The overall idea is novel and shows strong empirical results. 
The paper performs ablation studies justifying different design choices.
The paper shows the ability of the proposed method to disentangle trends and seasonal features through synthetic datasets."	3
Overall I think this paper is well-written with clear motivation and technical path. It is acceptable but can be further improved in several aspects.	3
This paper still has space to improve. I will preserve my score before reading the authors' responses.	3
Overall, I'd like to recommend the paper for acceptance given the clear merits of its representation learning approach which I applaud the authors for. However, I have some open questions which I would like to clarify as part of the review process so right now, my evaluation is cautious. 	3
This paper presents a technique of general interest to the subfield of implicit neural representations, has solid empirical results, and is in my opinion quite surprising. The paper is clearly in accept territory.	4
Novelty is limited. Practical engineering efforts with wide applicability and good results.	2
"Overall, the paper proposes an architecture modification which improves the evaluation time of coordinate-based MLPs without significantly hurting the quality of the signal fit. This contribution is demonstrated ablated clearly for signal memorization, and I believe that the proposed architecture has value for direct signal fitting tasks in coordinate-based networks. However, the effectiveness of the method is not immediately clear for applications in neural rendering, where coordinate-based networks are mainly used. A novel formulation of volume rendering which is compatible with the architecture change is proposed, and evaluated on one scene, which provides promising results that this can speed up the training of NeRF-like models in neural rendering, but this section of the appendix could be expanded upon significantly to create a significantly stronger paper. Based on the current state of the paper, I believe that the architecture shows promise and is ablated well for signal memorization, but may not be as significant as it could otherwise be. Thus, I believe it is marginally above the acceptance threshold.

**Post Rebuttal Update**

The authors have done a good job of addressing my concerns in my original review. I think that while the paper does propose a small algorithmic change and relies on more complex sampling methods to work for general coordinate-based network applications (as noted by other reviewers), the results and contributions are ablated well and the simplicity of the idea could likely have a larger impact upon practical usage of coordinate-based networks. Thus, I have updated my score, and think that the paper in its current state should be accepted."	3
"This paper introduces a new architecture for implicit neural representations. While there are some interesting ideas in the paper, overall I still believe more work needs to be done and better comparisons to baselines need to be made before this paper is ready for publication.

--- Post rebuttal update ---

I appreciate the large efforts the authors have put in to address the issues put forward in this review and by the other reviewers. With these changes I believe the paper is stronger and have updated my score accordingly. In particular I appreciate the experiments showing that CoordX can also help approaches such as ACORN (even though these results still seem preliminary, particularly for Tokyo). Including these results in the updated paper will definitely help give a clearer picture of the contributions. The extended discussion and experiments around NeRF scenes are also helpful and appreciated. 

While I still believe there are some drawbacks with the proposed approach (in particular that using the method for non grid-based signals requires slightly hacky workarounds and that the gains by the proposed method are quite small compared to other methods aiming to accelerate implict reps e.g. ACORN), I am happy for this paper to be accepted in its current updated state."	3
Overall the paper is well-structured and clear, and presents unique and interesting ideas for training unconditional EBMs for different applications. There are some places where clarity could be improved, and some additional experiments which I think might improve some of the points in the paper.	3
The paper spans various different problems, but makes relatively little contribution to each of them. The claims over empirical performances are inflated. The EBM-based methods do not have an advantage over score-based / diffusion generative models in terms of generation and likelihood evaluation (which also avoids estimating partition functions).	2
This paper presents a new method for training EBMs which combines features of two popular training approaches: CoopNets and PCD. While the proposed method makes sense, and appears to work, there are considerable issues with the method’s evaluation, experimental details, and theoretical justification. Thus, in its current form, I do not advocate for its acceptance. 	2
Overall, the paper provide some interesting empirical findings for EBMs with different MCMC lengths. However, current experiment results and lack of novelty make it a bit under the bar of ICLR.	1
"Overall, I do like the analysis for input dependent randomized smoothing, but I think the overall impact of this work is rather limited. I rate the paper at borderline because I feel the theoretical part of this paper has certain limitations and the proposed procedure is also not very effective in practice and has quite small improvements. To further improve the contribution of this paper, as I mentioned in the ""weaknesses"" above, the authors can study the more useful setting of applying non-isotropic input dependent noise, and develop better $\sigma(x)$ function to improve smoothing performance."	2
"This paper presents theoretical and experimental negative results on input-dependent randomized smoothing. 

I think it's likely to be of *some* interest to those working on randomized smoothing.

But the wider community will likely be uninterested by this negative result. 

(Additionally the paper could use more work on the experiments and writing in particular)."	3
The theoretical analysis presented in this paper is both novel and of interest. However, there are several weaknesses of this paper that need to be addressed including the experimental setup, the effectiveness of the proposed method, and the presentation.	3
"The authors tackle important problems of RS. 
They use their original method to solve the problem in a rigorous way and the method seems to be effective. 
I thereby recommend the paper."	3
Despite my concerns, I think this is a VERY valuable work for the society. The paper is well written and organized.	4
I lean towards an acceptance for this work since it provides a convincing demonstration of IBP for latent embeddings, which means more interesting properties beyond adversarial robustness on the input space can be verified.	3
"Overall, the novelty of the paper is a bit limited and I am not sure whether the perturbation on the latent space of generative models indeed captures all the possible semantic specifications. 
"	2
"## Points in Favor
+ Relevant problem and in scope of the conference.
+ Presentation of a framework for auditing deep learning models in respect to
   given specification.
+ Formally sound foundation of the framework and its mathematical workings.
+ Generally the introduction of the framework is given in stages with
   relaxing assumptions, so the understandability is good.
+ Experiments show the improvement with the framework.

## Points Against
- Generative Network used to create the variations: while this strengthens the
   confidence in the model it does move the reliability of the framework to
   the GAN as well.
- The framework is dependent on the encoding function e() and it is not clear
   whether this is always specifiable.
- Partly, the formal foundation is not precisely specified"	3
"Overall, this work I believe would be a useful contribution to the community.  I think however, further streamlining of the manuscript to improve clarity would strengthen the paper.  First, citation of Zhou et al https://arxiv.org/abs/1905.01067 would strengthen the related work, as  Ramanujan et al. note the influence of the paper on their work.  Second, re-plotting the figures so that ""confidence"" is both clearly defined (percentile value for the confidence is not defined) and visible within the plots, would allow the reader to better interpret the plots on their own.  Third, information from Appendix A2, A3, and B1 are useful experimental details and would help the reader if they were presented in the main section of the paper instead of the appendix.  In fact, I found section A3 and B1 more useful to understand the experiments than sections 2.2 and 2.3. Whitespace used in the main paper could be reorganized, and figures 1 and 2, I believe were less helpful in understanding the paper and could therefore be moved to the appendix."	3
Interesting idea but there's a mismatch between the proposed theorem and the experiments.  	2
Given the concerns about the practical usage of the proposed framework, I tend to deem this paper marginally below the acceptance threshold. I'm willing to adjust my scores if the concerns are addressed.	2
"This is overall a very well written paper (strangely with the exception of the abstract), with a clear motivation and experimental method that I believe is novel in the sparse NN realm. The paper asks fundamental questions on the ability of contemporary methods for finding sparse subnetworks both before training, with an interesting experimental setup, and finds results (in the toy problem setting) that suggest these methods are lacking. Layer collapse specifically is identified as one of the problems leading to this issue, and the paper is somewhat convincing that the problem of finding very sparse solutions is algorithmic rather than a fundamental limitation. 

Unfortunately the paper loses some relevance in that it lacks any analysis of dynamic sparse training methods in addressing this problem, also it's not clear that the author's implicit assumption that a dense DNN could find these hand-designed solutions is true — perhaps this is not a restriction of sparse DNN methods but NNs in general. Nevertheless, overall it remains interesting and relevant overall, and I recommend its acceptance - although I encourage the authors to revise the abstract."	3
The motivation and the correctness of the lottery ticket planting are not clear to me. And the empirical findings in this paper are already presented in the previous paper Frankle2021.	2
This paper, as far as this reviewer knows, is truly addressing time series active learning for the first time, and does so in a novel and coherent way algorithmically and experimentally.	4
This paper proposes a technique to improve accuracy in an important field of application where data volumes are high and efficient use of limited user feedback is of urgent need. The plateau model makes intuitive sense in the context employed here and looks like something we would want to add to the toolbox.	3
I vote for a weak ac of the current version since the idea of using plateau model for pseudo labeling in time-series AL is interesting. Although there is no rigorous theory to tell us why TCLP improves performance (at least in this paper), that is, why plateau model could provide a better pseudo labeling, and what's the upper limit of its promotion. The experimental results are fairly good, they demonstrate TCLP's effectiveness in their experiments.	3
"The experiments speak for themselves.
Overall while the idea is simple, and the math are not really satisfying, the experimental part erases most of my concerns."	2
Interesting new algorithms, well-motivated, delivered with good theoretical analysis. The experimental section is a bit lacking, but that's not sufficiently concerning not to recommend the paper for publication.	3
In summary, I think this is a relatively strong paper due to the technical depth and the empirical studies, though some parts could be improved.	3
I recommend acceptance due to the high quality of the paper. My main two concerns are (a) readability wrt applications and (b) experiments across a broader range of regimes.	4
As I said, this is a hard problem and the authors obtained some interesting results. Yet, I am not sure ICLR is the best venue for this paper. Maybe a more theoretic-focused venue is more relevant. 	3
"Here are some (relatively minor) comments:

1. Terminology. I think usually in RL we refer to the ""expected (discounted) state-action frequency"" here as ""(discounted) visitation/occupancy measure"". Calling it ""frequency"" sounds a bit non-standard to me.
2. Proposition 22, should the mapping p being from t to $det(A+t B)$ (instead of $det(A+\lambda B)$)?
3. I was wondering how the results in the current paper reconcile with the results in Azizzadenesheli et al., '18? It seems that in that paper, some ""Gradient dominance"" property can be established, which should lead to some global convergence results. Since it is a closely related paper, a more detailed comparison seems necessary.
4. I was wondering whether the results in Sec. 5 can be extended to the case with $\gamma=1$ (average reward case)? If not, what would be the technical hurdles? Some discussion might be needed."	3
The paper appears to be novel and well written. I am somewhat concerned on how interesting it might be to the community at large, due to its focus on theoretical properties with no clear direct application. In this regard, it it not entirely clear to this reviewer that ICLR is really the correct venue for this work.	3
The paper considers an important and challenging problem, with a set of new interesting theoretical discoveries. I think this is a right step to move forward understanding POMDPs, and could be a nice addition to literature, and thus I recommend acceptance. 	3
Although this paper is interesting in the ability to infer 3D properties of objects, the fundamental flaws in experiments are the main reasons that I recommend revision and resubmission.	3
I recommend reject, because the main claim, learning 3D representations, is not well supported by the experiments, and the benefit of the learned representations over prior work is not well demonstrated.	3
All in all a nice paper but I think it suffers from some drawback as discussed above.	2
Look forward to the response from authors. 	2
Overall, this is a very nice paper. My only concern is about the results relative to the original SAM paper. If the authors address this concern in the rebuttal, I am happy to recommend this paper for acceptance.	4
Modifications are simple and efficient in practice, and may improve the practical usage of SAM across workloads.	3
"This work raises an important question of SAM where the improved training algorithm requires doubled training cost. This paper addresses and improves the efficiency drawback of SAM. The proposed approach is well-motivated and has been verified to improve the efficiency and accuracy of SAM. The authors also provide codes for reproducibility. However, the degradation of SWP's effectiveness in large-scale neural networks, as well as the justifications for the improved accuracy contributed by SWP and SDS, have not been clearly addressed in this paper. I expect the author can answer the points above, and then I can adjust my final score.
"	3
This paper is the pioneer in exploring a more efficient SAM method. This technique can be useful since SAM has been proven to be useful for many networks. Therefore, I would recommend this paper as weak accept. I would be more convincing the practicability of this paper if the authors can show the proposed ESAM works on ViT and MLP mixer, where the SAM has been proven to be very effective.	4
Overall I think this is a good paper, but also that the focus on the navigation tasks is rather narrow and that it would be nice to a few non-navigation downstream tasks with the standard MuJoCo agents.	3
The paper proposed a new practically implementable objective for unsupervised learning that gives qualitatively and quantitatively better performance on classic AI Gym benchmarks and challenging robotic pick and place.	4
"I think the paper can be accepted with some modifications. The method is well-motivated, tackling an important issue with mutual-information based skill discovery methods. It demonstrates impressive qualitative and quantitative results compared to prior works. The paper reads very well and is easy to follow. 
"	3
"The paper proposes a novel method with significant performance improvement. However, the paper suffers from several issues with accuracy of the claims. Further, a version of the method dubbed ""discrete Lipschitz-constrained skill discovery"" is proposed, which is clearly a contradiction (discrete variables cannot be Lipschitz constrained). The paper needs to provide a clear explanation of what the discrete version of the method actually is. Without fixing these issues, specifically points 2-4 above, I believe the paper cannot be accepted."	3
Overall, this work offers a principled and empirically strong approach. There are some weaknesses, but they seem relatively easy to address, and the contributions of the work outweigh the cons. There were also some points of confusion for me in the paper that seem potentially serious, which I included above. I initially recommend a score of 6, but am willing to raise my score if the points of confusion are clarified.	3
See main concerns above.	3
"The paper studies an important problem and has some solid results. However in their current form the major components of the method are not clearly motivated and there are some questions I have about the correctness/justification of the approach. 

"	2
Overall this paper’s contributions are generally reasonable and empirically backed, however its novelty and experimental evaluation leave much to be desired. To this end, I am leaning towards acceptance but would not be upset if this paper is rejected.	2
Empirical results and conclusions that are presented in this paper are interesting. Even if some of the theoretical analyses were made, the experiments are mostly empirical. Therefore, I believe the paper is useful for researchers in this area to read, but this approach doesn't make a breakthrough. Therefore, I rate it 6.  	2
I recommend borderline reject due to the missing of important baselines and I will consider changing my rating after seeing strong rebuttal.	3
This study provides one new training method of sparse model by scheduled grow-and-prune (GaP) methodology. The authors provides  a series of experiments. However, this study requires additional comparions, particularly some moden pruning methods under same experiment settings, such as SCOP, HRank, etc. Alternatively, the authors should include more information about the gains of this method.	3
"I have reviewed this paper for NeurIPS 2021 and gave it a score of 6. But the paper had a bunch of issues making it not ready for publication. 

This time, the authors tried to put things in perspective. The empirical performance of the method is strong but I would like to see the limitations of the method along with a discussion of trade-offs. 

I am giving it a score of 6 but will be willing to increase it to 8 if the authors respond back with valid explanations for weaknesses. 

---------------------------------------------
After rebuttal:

I am convinced about the contributions of the paper and I would be arguing for the acceptance of the same during the internal discussions. Increasing score to an 8."	3
The paper as it stands has major drawbacks in novelty and experimental validation, unfortunately in current form it will probably not be interesting to the general audience. Please see my notes above.	2
This  lacks the introduction and comparison of the methods in the last two years, and experiments on more datasets are needed to illustrate the effectiveness of the methods. In addition, the applicable conditions of the two losses should be further analyzed.	3
Since the novelty of this paper is incremental and the experimental results are insufficient. The reviewer recommends rejecting this paper.	2
The paper is well organized and clearly written. And it clearly described the design of Proto-Triplet Loss and ICNN Loss. Though there seems a lot of overlap between Proto-Triplet Loss and classic triplet loss I still recommend this paper be accepted. 	2
The main drawback of this paper is that the proposed model is not that novel in terms of methodology. But this paper provides an angle to look at the token-wise interaction in contrastive Language-Image pre-training. Moreover, the performance is quite impressive and the experiments are conducted on different pre-training datasets and downstream tasks to prove the model's efficacy. Overall, I think it is marginally above the threshold. I may change my mind based on the authors' rebuttal and other reviewers' feedback.	3
The paper is well-written with strong results. However, the proposed approach has limited novelty and some critical claims are made without strong support. 	2
The performance is very good, which mainly comes from  “big data + big model + data cleaning / augmentation + supercomputers”. The whole method is lack of novelty. 	2
The authors do a good job on putting together ideas from different papers and create a novel pre-training dataset which leads to new state of the art results on vision-language pre-training tasks. However, the contributions seem to be marginally significant, and I'd like to hear more from the authors about the questions I raised above. Moreover, the computational complexity claim needs to be revised and discussed more in depth, since the high price for storing the index might not compensate the small improvement in accuracy performance. If any of those two points are covered in the discussion phase, I am willing to reevaluate my score recommendation. 	2
Basically, this paper is theoretically well done. The assumption of linearity is a strong one that is far from reality, but it is a reasonable assumption for the current theoretical analysis, and it is confirmed by experiments in the nonlinear case. Overall, I think this is a worthwhile paper.	3
"I recommend weak rejection due to the incremental nature of the technical results in light of previous work, as well as the limitations of the empirical section. 

---- 
Update after author response: I appreciate the additional experiments on the MNIST dataset, and believe that this improves the empirical analysis provided in this paper. However, given the results in previous work (e.g. Gunasekar et al. '18, etc) for the trivial group, I still think that the results in this work are fairly restrictive since they only hold for single-channel linear networks with full-dimensional filters. Although previous work has shown that simple closed-form solutions may not exist in general, it would still be useful to provide insight into how these important parameters affect the implicit bias of gradient descent. "	2
"Although I am comfortable with group theory and basic harmonic analysis on groups through irreps, I find it hard to follow the derivations and make sense out of the statements. My impression is that the paper is too mathematical for ICLR audience, though I can really only speak for myself. The paper could improve a lot from more layman's explanations throughout the paper. For example, it already starts early in the paper with talk about Schatten norms, where to me it is not immediately obvious why these are considered.

I understand that it is hard to please every reader, but I believe it is possible to convey the main messages better to a broader audience. Currently it seems overly tuned towards experts in analysis (of implicit biases).

Overall, I do see value in publishing such work as the paper seems otherwise sound."	3
"While the authors use novel methods to derive an interesting implicit bias of the model class under consideration, this class, non-equivariant predictor based on equivariant neural network layers, is not what is generally used in practice. For the paper to be recommendable for acceptance, the authors should make a stronger argument why their results also apply to models actually used. If they do so convincingly, I'd increase my score.



Updated score to 6."	3
"The paper is well written and motivated. I find the results compelling
and useful. There is not much to complain, except it would be good to
explore differences in variance of the different methods. Minor issues
are related to potential reproducibility / experimental setup, and a
reference to prior work.
"	3
"I am leaning toward reject, due to potentially unfair comparisons and incorrect results.

---

My concerns have been adequately addressed during rebuttal, so I now recommend accept."	3
"UPDATE (2021-11-21): The authors have incorporated all of the suggested changes and addressed my concerns reasonably, which is why I'm now recommending acceptance.

OLD: The paper is overall okay. I don't think it's remotely in its best possible shape (in terms of structure, writing, Figure 4, etc.) but this is nothing that can't be solved with a couple hours editing and rearranging things. The main idea is good, nothing in the paper appears to me a major flaw and so I'm mildly recommending acceptance and strongly recommend polishing!

"	3
In general, the proposed method is promising, but the current presentation of the paper has major issues that need to be addressed and clarified.	3
Interesting method with promising results.	3
"The paper provides an interesting contribution to learn to search for high-quality solutions at test time, nicely completing end-to-end learning pipelines for solving CO problems. The proposed approach could be applied to any model that has an encoder-decoder type of architecture, and is experimentally validated on 2 models and 3 problems. A limitation is that it is not clear if it could help a model generalize to instances that are much larger than the training ones, or with significantly different characteristics. I vote for accepting the paper.


### Update after rebuttal

I thank the authors for precisely answering all my questions and concerns.
I am happy to confirm my initial recommendation of accepting the paper.

"	3
The paper proposes a simple extension to an existing RL-based method for combinatorial optimization. Its effectiveness is demonstrated empirically. However, I feel that the results of the experiments should be reported in greater detail, i.e., to compare with the original active search in different performance metrics such as memory and CPU time usage.	2
Overall, I’m leaning toward acceptance because the proposed approach seems to provide a notable improvement over prior methods (in particular, active search by Bello et al. ‘16]) in terms of runtime.	3
This paper is timely and addressing a very important area of work for the protein-ml field. The model is thoughtfully implemented and presented, but the evaluations do not enable strong conclusions to be reached yet. More careful ablations and comparisons are needed to understand implications of this work for scalability, representation learning, and evolutionary modeling.	4
Overall, the paper tries to address an important and interesting problem in learning generative models of homologous sequences informed by their phylogenetic structure. However, the results in particular are not convincing enough to grant acceptance. The paper can be improved by adding evidences of some biological insights gleaned from the model re the evolutionary process behind the sequence. Alternative the paper can focus on the computation aspect and expand on the reasons behind scaling and the extend it can tolerate larger data sets.	3
The authors incorporate biologically-motivated priors into deep models for biological sequences. Their approach is statistically well grounded, and they apply their model to a relevant, difficult task in biology and improve upon it.	3
"Please check my review details. I would rate the paper as: score 4 weakly rejected

I will consider to improve the score if the authors can address my concerns.
"	2
"While the premise of this paper is interesting, I believe that both the motivation for the SRAT method, and its empirical validation could be substantially improved.


## Post-rebuttal Update

I thank the authors for their response to my comments. I have decided to keep my original score as some of my concerns still hold. In particular:
1. With the new results that the authors have added which show the baseline losses (CE, LDAM, focal) with deferred reweighting, the difference between the proposed approaches and prior work is even less significant and lies entirely within confidence intervals. 
2. Figure 15 does not address my concern about the over/under accuracies in Table 1 and 2. In particular, if SRAT indeed led to better separation, one would expect a comparable or lower drop in over accuracy compared to prior work. However, this seems not to be the case as per Table 1 and 2. For example, DRCB-focal and SRAT-focal, have similar overall accuracy, but the latter has higher under (and thus lower over) accuracy. More broadly, this links to my other concern that the loss proposed is not sufficiently motivated, and it is unclear that it leads to better clustering of features."	2
Currently, the paper includes several flaws, but many of them (e.g. the proofreading, or improvement of the legends) can be fixed. If the experimental section is strengthened and the questions clarified, I will consider my rating again. However, I am not convinced yet of the novelty of the proposed model, other than demonstrating that the problem with adversarial training on imbalanced datasets exists. 	2
Overall, this paper firstly investigates adversarial training in a new scenario under imbalanced datasets and propose to improve feature separability for adversarial training that should enhance the performance on imbalanced datasets.	3
Overall, I recommend a rejection to this paper. The problem it studied is a common challenge (amount of training data vs. variance) in semi-supervised learning, while experiments are not conducted properly to support the effectiveness of it against other similar approaches. Moreover, the technical writing in the paper contains a lot of hand-waving example instead of sound mathematical justifications.	1
The novelty of this work is limited. It lacks important related work (graph few-shot learning) discussion and comparison. In my opinion, this work is not suitable for ICLR. 	1
The technical novelty and originality of the paper are limited. The proposed method is an adaptation of the existing self-training and negative sampling strategies. This paper lacks comparisons with more recent self-training baselines to prove the effectiveness of the proposed method.	2
I think the main contribution is: the proposed problem is challenging and interesting, this paper considers the semi-supervised node classification on graph data with only a few node labels available. The main weakness is: the technical contribution is limited. 	2
The present paper presents an effective framework for semi-supervised node classification on few-labeled graph data. The paper is well organized and the motivation of this paper is clear. Although this paper has some innovation, but the model of GCN is not changed. Besides, some comparison experiments with pseudo labeling methods are lack. 	3
This paper presents a novel ab initio method for solving the Schrodingers equation using meta-learning by combining existing neural wave function models with GNNs. The resulting method can solve the Schrodingers equation for multiple geometries while training significantly faster.	3
"The paper presents sufficiently strong results and an interesting demonstration of how a single network can be used to simultaneously approximate a family of ground state wave functions. Results are compared to relevant baselines and with simple additional checks suggested in the main review should be a good contribution to variational methods in quantum chemistry.

**Update**
After the discussion period many of my concerns have been cleared. Changing my score from 6 to 8. "	3
The manuscript is well written and the proposed approach is sufficiently novel. Its advantages are further clearly demonstrated in a series of experiments. As such, I recommend to accept this contribution.	3
Overall the ideas presented in the paper are interesting and potentially useful. However, limited experiments and results fail to support the main claims of the paper. The results could include tasks with label scarcity and some of the choices for experimental settings are unclear. 	2
The contribution is novel, timely and useful to the community. Its assessment is thorough. The clarity of the manuscript could be improved.	4
While the paper has its own merits, it has several issues to be addressed regarding (1) the connection between motivation and the proposed method, (2) theoretical or empirical analyses concerning the RC-equivariance, and (3) some questionable experiment setups. 	2
The proposed method is a simple and effective contrastive learning method and shows great representation power. The proposed method showed the best performance in many different settings such as linear evaluation and semi-supervised learning. The reverse-complement based data augmentation seems simple and effective. However, the proposed framework is overall a simple combination of existing methods and beyond genome datasets, the impact of this proposed method is questionable.	2
Although this paper has excellent motivation for improving the efficiency of current GNN based RL for CO problems, I have some concerns about the approach and experiments. 	2
I think this paper is based on a solid idea and shows good empirical performance. However, at the current state, it is hard to access the significance of this paper since baseline algorithms used in real-world is missing. Furthermore, I would like to see description on the proposed algorithm's difference to existing work of Manchanda et al., (2019) to further support novelty of the proposed method.	3
Overall, it remains unclear what combinational problems ECORD can address. The improvement over ECO-DQN or heuristics is not significant. It is unclear how ECORD improves ECO-DQN in terms of efficiency, which is the major contribution of this work.	2
"Overall, I like the paper, which is quite nice to read and presents a sensible and effective approach for solving the maximum cut problem using RL. Especially, the authors specifically addressed scaling to larger instances by making the architecture more efficient, which is an important and challenging topic.

Unfortunately, I still think the paper should be rejected given that
1) it is too much focused on max-cut and unclear how this framework can solve general combinatorial problems, as the title suggests
2) it is quite incremental to Barrett et al. (2020) AND not clear about the differences and
3) some important baselines are missing. "	2
Overall, I think this is a strong paper due to the compelling results and clarity of communication.  I currently recommend weak accept as I would like to see the experiment I mentioned above (using lower quality, but quick-to-generate 3D conformers) to better understand the tradeoffs involved.	3
For molecules the 3D information is essential and this paper shows an affordable method to include this for improved property prediction. There are two main advantages, firstly it can be used for a dataset where 3D information is not available and secondly it is cheaper than implementing 3D information in the representation. 	3
This paper is an empirical work, so the empirical results are the most important. However, as listed above, there are some key issues with the empirical performance (point 1). In addition, there are also some other concerns: the writing issues (point 2&3), visualization issues (point 4), and equation issues (point 5-6).	3
"The method proposed in this paper is sound. However, the evaluation has some flaws as discussed in the main review section.

I will consider raising my scores if these questions are properly solved during the discussion period."	3
By adding momentum and proximal operation associated with the regularization to the modernized dual averaging algorithm, the authors developed RDMA. Using tools from non-linear optimization and manifold identification, they theoretically showed that if RDMA converges, then it will find the active manifold after a finite number of iterations. Experimentally, they showed that RDMA outperforms proxSGD and ProxSSI in terms of both accuracy (sometimes) and group sparsity (always).	3
I am leaning towards acceptance given that authors address the problems listed out. Overall the idea is good and the paper is well written to be considered at par with ICLR.	3
"Overall, my vote for the paper is a (weak) reject. I think the paper has some strong points and the general results are meaningful. On the other hand, I think the technical writings of Section (Applications to deep learning) are less rigorous and lack important information. Finally, I feel the paper overstates the significance and implications of its results in its presentations.

Update after author's responses and revision: Most of my concerns have been addressed in the revision, with (1) a more rigorous reframing if the significance and importance of the work, and (2) experiments to validate the efficiency of the approach. I think this is a good paper in its current state. "	3
"The paper is well organized and covers the background knowledge required to follow the discussions. It motivates the goal, provides justifications for the choices made in proposed methods, follows a thorough empirical approach, and achieves near perfect results in the considered benchmarks. There are a few points of concerns that need to be cleared before I can fully support the submission. Regardless, I see many of the properties of a good research project and so I lean towards accepting the paper at this stage. I look forward to authors' feedback on my concerns before I finalize my decision.

Edit: I thank the authors for responding to my comments in details. After reviewing their response and the changes made in the paper, many of my concerns are resolved. Therefore, I change my recommendation to accept this paper."	3
Overall, I think based on the results I recommend that the paper be accepted into the main conference. The problem is well motivated, the comparisons are fair and the results compelling though there is some scope for improvement that i highlight in the weakness section of the main review	4
"
TCF appears to be a promising architecture, and the experimental results seem good. However, some design and experimental choices make them difficult to compare with the original Universal Transformers. In particular, the choice of a fixed depth at train and test time might be an unnecessary constraint, the proposed gated-copy operation should be compared with the ACT-based copy described in the UT paper, and the use of the last element in the output  sequence as the basis for decoding might unduly increase the necessity of relative positions and short-range attention (ie the geometric attention and directional encoding recommended by the authors). 

Hence my note of 6, which I would gladly increase if additional experimental results are provided. 

Edit: Thank you very much for the detailed response. I have raised my note to 8."	3
"Briefly speaking, I think the paper is well-written and easy to follow, and he results seems to be technically solid based on my check. The ideas and contents in the paper seems to be interesting and novel, and the experimental results are also providing some reasonable justification of the values of the proposed new approach.

There are some minor areas that might be improved about the empirical experiment evaluations part, as described in the main review section, but generally speaking, I think the paper is valuable to the research community of  of Multi-agent Cooperative Learning, and I don't have too much concern about accepting the paper for publication on ICLR.
"	3
The paper contains interesting contributions, however its theoretical and experimental results could be improved: formal claims should be more rigorously stated, while experimental results could be expanded. The presentation of the paper could be considerably improved. 	3
I vote for rejection since I cannot capture the main ideas of the proposed method from the current version. Most presented arguments do not make sense for me.	1
This paper purports to optimize credit assignment for the MARL objective but the method does not do so.	3
Overall, the method is easy to understand and the paper is clearly presented.  Some organizational and presentation changes could help make the results easier to interpret.	3
Overall, the idea of the paper is novel and interesting, and the experiment results look promising. However, the paper fails to compare LEAN with the most advanced related works on structured pruning. The datasets used in the experiments are too small, and the experimented tasks are not commonly used in CNN pruning works. So it is hard to justify the advances of the proposed pruning method. Also, the presentation of the paper needs to be improved.	2
Due to the aforementioned weakness, I temporarily think this paper is marginally below the acceptance threshold. 	3
Overall, I think there are several nontrivial weaknesses in the manuscript as listed above. I currently give a negative score but I will consider changing it if the authors' feedback resolves the concerns.	2
The size of the databases used for the evaluation seems rather small. Hence, the improvement may be obtained from the adaptation of models to the target domain. It would be better to evaluate this method by using datasets with a larger size to clarify this point. Also the comparison with the existing pruning methods are not sufficient.	3
See the main review.	3
This paper solves an important practical problem in channel pruning with interesting solutions and results. However, in terms of novelty, clarity, and evaluations, this paper needs to be polished and completed. 	2
This paper proposes a new neural network pruning and fine-tuning framework for model compression. Each part of the proposed method is effective and can solve some challenges. However, the novelty and the technical details are limited, and the experimental results are not sufficient (see weakness). Therefore, my rating is marginally below the acceptance threshold. 	3
Fundamental limitations of multi-modal VAEs discussed and experimentally validated in the paper would be of interest of a large audience and can potentially impact many applications of these methods (e.g. in natural sciences, bioinformatics, etc). Intuition and theoretical justification of the limitations appear sound, and is supported by extensive experiments.	3
The authors' contribution on formulating and quantifying the limitation of the existing VAEs for multi-modal data analysis is interesting and valuable. However, I think the authors need to at least address a few of these limitations to show how the current methods can be improved. 	2
Although some concerns remain about the terminology and the scope of the issue the authors are advocating, the paper makes some novel points and contributions to the study of multimodal VAEs. Therefore, I judge this paper to be above the acceptance.	3
"Theory is concise but informative, and experiments are extensive. However, there are a few key things missed from the paper, as discussed in the main review. 

--------------------
Update after rebuttal

I have read the response and I will keep my score. It is a very interesting paper. I encourage the authors to add some of the discussions to either the main text or the appendix. "	4
"My main concern is the discussion of related hierarchical models missing from related work and the emphasis on this being the only work to apply some disentanglement pressure outside the main z latents. This should be an easy fix for this paper.

The compositional objective is interesting and novel and the implementation method is clean. The experiments were well conducted and the well analysed. Overall, I am confident that the authors will be able to address the main issue above and that this paper will award acceptance in this venue."	4
"In summary, the idea is interesting and the results look promising. I hope the readers could clarify these questions and I will adjust the score accordingly.
"	3
"The paper appears to simultaneously give a clear slightly novel generalization to disentanglement losses of prior VAE variants, and to provide an architectural approach to implement their approach.

I have concerns about the scalability, and I am suspicious about whether such a heavy-handed disentanglement can be maintained in larger models.

It also was not clear to me whether the results originate from the loss or the architecture advances (which build on existing switching architectures).

However, either way, I think this is a novel approach and potentially a significant addition to the VAE disentanglement research, and I lean towards acceptance in this case, though I urge the authors to address my questions."	3
I think the authors have addressed most of my concerns. I will increase the final rate.	3
Hierarchical graphical models of the sorts considered here are in wide usage by scientific practitioners, and automatic approximate inference methods that are efficient and accurate are of longstanding interest -- thus the motivation of the work is clear. The nature of the contribution is likewise clear. My primary concern is that the evaluation doesn't fully demonstrate the benefits of the approach (e.g. using predictive out-of-sample log-likelihoods rather than training ELBOs, including a quantitative evaluation of the real-world problem, providing consistent timings, pushing the scale until ADAVI dominates). Secondarily, I think the overall framing, clarity, and writing could be improved. I think the work is below the bar now and am rating accordingly, but I think my concerns should be sufficiently addressable in rebuttal for the work to be appropriate for the conference. 	3
"This paper addresses a common problem of simulation-based inference and proposes a sound and efficient solution to enable inference in high-dimensional parameter spaces. Experiments show convincing results.

My main issue is the lack of quality checks of the approximate posteriors produced by ADAVI. This should matter the most, in my opinion, well above efficiency, wall-clock times, and the number of parameters. If the approximate posteriors are wrong, none of those matter. For now, I do not recommend this paper for acceptance, but I will be pleased to change and increase my evaluation if the authors can present diagnostics of the resulting approximate posteriors. "	2
"Despite a nice motivation, the empirical results show a minor improvement over the already existing methods. The results are also weakly presented for the target problem in the neuroimaging context (small data, no quantitative comparison, lack of discussion). The method is not tested on very high dimensional settings as they are in the neuroimaging context.
 "	3
Please refer to the Main Review section.	4
"I have concerns about novelty, scalability, and practical use case of the proposed method.  

------ after reading the rebuttal

Thanks for the replies from the authors. I have read the reviews carefully. Some of my concerns are indeed solved, but I am still not convinced by the novelty, scalability and usefulness of the proposed method. For example, the use of this method is improving the inference time. The inference speed ups as shown in the comments are not significant, while lots of state-of-the-art inference speed up methods can do at least 10x speedup. Also according to the comments, the scalability is indeed a problem due to the lottery tickets problem, which is the main idea of the paper. So I think this paper's use scope is limited. "	2
"The authors introduce a novel way to prune graphs graph and node classification tasks which can generalize to unseen graphs. They evaluate the method on an extensive number of tasks. The method shows good performance.

Since there is no equivalent technique to compare against, the authors compare against random pruning which is probably a very weak baseline."	3
Overall, I feel that the paper identifies an important problem, but the solution proposed and experiments are not thorough. The algorithm can be refined by incorporating neighborhood information in the mask generation process. The empirical performance can be vetted by comparing with existing pruning baselines.	2
"I couldn't find an issue to reject this paper but it is difficult to evaluate novelty enough.
To be honest, it is quite simple so there is not much to comment on but at the same time, the contribution is clear.
I want to make a decision after discussing it with other reviewers or reading other points that I might miss."	2
This paper proposes a simple and effective algorithm for training models with DRO. Experimental results are convincing. My main issue concerns the training objective, which leads to weaker results when solved more accurately. The preliminary observations discussed in Section 5.2 are a first step in this direction, but the contribution could be much stronger if the authors had investigated this more deeply. Nevertheless, I believe the overall contribution to be worthy of acceptance.	2
The paper addresses an important, modern issue (subpopulation bias in large datasets) and does a good job explaining, developing, and exercising the idea on appropriate datasets with relevant baselines. I recommend accepting the paper.	3
Overall good paper with potential impact. Some things should have been evaluated better.	3
Overall I enjoyed reading this paper. I think the main strength of this paper is in the experiments sections, demonstrating the advantages of their proposed DRO method with respect to previously proposed methods. I am being conservative with my evaluation mainly because I am not familiar with the whole literature in this area (notice my confidence of 3) and I look forward to see other reviewers' reactions to this work. I am willing to reconsider my evaluation if my feedback is addressed adequately and provided that no other flaws are found.	3
"In this paper, the authors study a traditional collaborative filtering problem with users' ratings, where the goal is to predict ratings that are unobserved. Specifically, the authors propose an enhanced autoencoder-based method called NE-AECF. The main idea of NE-AECF as shown in Eq(10) and Figure 1 is that it contains an additional module for each predicted rating by an autoencoder, i.e., h_\theta(g_w) in Eq(10) and the most right part near the output layer in Figure 1.

The authors conduct experiments on some public datasets and show that the newly introduced neural network is helpful in improving the prediction accuracy.

Some comments:

1 Collaborative rating prediction is a very well-studied problem, for which there are lots of existing works. Moreover, in real recommender systems, item ranking is more consistent with a real setting.

2 The time complexity seems rather high. First, the authors use an item-oriented autoencoder, in which there may be lots of users associated with a typical item. Second, the elementwise function is expensive. Third, the number of hidden units is much larger than a typical matrix factorization-based method.

3 The authors do not provide sufficient details or justification on using a large number of hidden units and an additional elementwise function. Moreover, treating unobserved ratings as zeros may introduce bias, which is also not justified.
"	3
This work proposes imputing the missingness through two NNs, addressing the non-linear transformation between hidden and output. The authors provided theoretical justification for why the proposed method is better, but the empirical experiments can be improved. 	3
This paper proposes to use an additional element-wise neural network to enhance the autoencoder for recommendation, and provides detailed theoretical analysis. However, the authors do not comprehensively evaluate their model and only very few experiments are conducted. The fundamental assumption of the proposed method is problematic and the experimental results only show slight improvements compared with existing method.	2
Evaluation based on RMSE on the OBSERVED ratings in experiments on recommender systems triggers an immediate reject of the paper in my view. 	2
The idea of this work is to enforce sparsity in CSC domain in the latent space of GANs and this work showed the effectiveness of it. However, this idea is very close to the work of Mahdizadehaghdam and there are other works to promote sparsity in latent space such as [Zhou]. Moreover, it was tested with very small images, so it is not easy to see this work working well and the experiment with DIP seems to contain some potential issues in terms of iteration number and fairness.	2
There are shortcomings in the overall concept as well as its evaluation. The findings suggest that this might be a promising avenue of research, but it would need to be taken further. At present, the paper boils down too much into simply adding a simple regularizer at the end and observing that it somewhat improves some metrics in a limited number of scenarios. Due to the limitations of the evaluation, it remains unclear whether the proposed improvement carries over to state of the art models and datasets. Similarly, the promised elucidation of the purpose of the feature values never really materializes.	2
This paper generally proposes to improve the existing multi-layer Convolutional sparse coding frameworks by splitting the tasks into independent and consecutive components. The idea is novel and is supported by proper experiments. However, the authors are suggested to address the concerns in the weaknesses. 	3
"The paper is exploring a simple but elegant idea of formulating the typically black-box process of CNN image generation as a process of sparse dictionary decoding. I like this concept and I like that it very simply improves performance of established GAN methods. The paper is also easy to read which should make this trick accessible to the community. On the downside, the technical contribution does seem boil out to a nicely packaged sparse regularization scheme. The authors could have spent more effort analyzing the learned coding. What dictionary atoms were extracted and how meaningful they are? The experimental section covers the main check-boxes including relevant baselines and ablation of the technique but it falls short in presenting the results. The judgment of the performance is limited to a very narrow scopes of quantitative metrics and it is hard to make any conclusion about the actual magnitude of the improvement that is introduced. Furthermore, the authors also do not discuss any limitations. 

Overall, after weighing all the cons and pros I am very slightly leaning towards positive outcome. It is mainly because the idea is clear, elegant and at least to the presented extent working. The evaluation is limited in scope but it appears technically valid. I would also expect the authors to address some of the concerns in the final revision."	3
Given the results and novelty are marginal, I think this paper is between boardline. Please the authors address my questions. I am happy to change my score.	2
Overall, I like the proposed method because it is model-agnostic, simple, and effective. However, as stated in the previous section, since experiments are only performed on low-resolution images, it is not clear how the proposed method performs on images with ordinary resolutions and quality. Novelty is another potential issue, although I think that the paper has made sufficient contribution given its nice characteristics listed above.	3
The paper explores an interesting idea of using robust classifiers to refine samples from generative models; however, the technical contribution is limited. Several directly relevant methods of sample refinement have neither been discussed nor compared against. Overall, the contribution is not significant enough to warrant acceptance. 	2
Novel and interesting algorithm that studies a important problem in multi-task learning. Current empirical evaluation of the paper can be furthered improved. Several design decisions in the final version of the algorithm should be better justified. 	3
Overall, the paper addresses an important problem. It proposes an interesting approach to solve it as well. The synthetic experiment is promising. However, there are quite a few weaknesses -- the target that exact lookahead DRO approach seems to be no different than regular DRO (as I explained above), missing natural baselines (without such baselines its hard to even understand if such a lookahead is needed or not),  problems with real data experiments. If the authors can effectively address these concerns, I would be willing to change my score.	2
I think the general idea of this paper is interesting. However, there are still several important points that need to be clarified. I am happy to have more discussion and adjust my score accordingly if things are properly addrssed.	3
Interesting idea, good writing, moderate empirical results	3
"The paper proposes an interesting method to train hybrid models for both identification and prediction quality. 
Connected two ideas from the literature in a single framework is also a welcome contribution. The proposed method is supported by experiments on different domains. Theoretical aspects are promising but still limited."	3
"Authors propose a unifying framework for hybrid models, generalizing recent methods. They derive theoretical guarantees very compelling theoretical guarantees for the framework, as well as a convergence proof for a simple affine model. However, I believe that the paper is quite hard to read, and propose several models without giving intuition for which model to use. The models also seem to need complex hyper-parameter choice, which again is not very intuitive and not discussed extensively in the supplementary material. 

Thus, I do not recommend acceptance of this paper, but I am willing to increase my score if authors answer my concerns, as the theoretical guarantees derived here are very strong and surely beneficial for a large class of hybrid models."	3
This is a good paper tackling a relevant problem. The algorithm introduced is easy to implement and has promising performance. Some theoretical justification on convergence is also presented. I believe that this is a meaningful contribution and should be accepted. There are some clarity issues that the authors can address in a revision.	3
The experiment results of this paper is very strong and comprehensive. However, the novelty of this paper is not very good. The motivation, theoretical analysis and algorithms are not very novel.	2
"The paper is written in a very clean manner, and is easy to follow. Sufficient background is provided in the introduction which gives the reader a good context to understand the problem setting and contributions. The preconditioning step is a modification of an existing method AdaHessian, and the adaptive LR part builds on techniques used for deriving adaptive LR rules for first order methods (Mishchenko and Malitsky 2020). So the novelty aspect is a bit limited in that respect. The theoretical results are outlined rigorously, although it is not clear what is the novelty of the theoretical results compared to those for other second order methods. The empirical evaluation is quite extensive and satisfactory in my view. I am giving it a 6 at the moment since I have other comments (in ''Further remarks'') which I hope can be addressed during the rebuttal phase.

------------- Post rebuttal ---------

As mentioned in the comments, I am satisfied with the author's response to my concerns and I am happy to increase my score to 8."	2
I like the paper, the algorithm, and its versatility. Especially that one does not need to tune a learning rate can be very beneficial. I did not fully read the convergence proofs though they seem sound. According to theory and experiments, one should always use this algorithm. It would be nice to justify this claim by a more comprehensive study, e.g., more problems, datasets, and other algorithms in the deterministic case and more nets and data sets in the stochastic setting. Only then one can tell if it is superior to state-of-the-art approaches. If such experiments were provided in the paper I would have given a higher score.	4
"I appreciate the authors' efforts on the comprehensive analysis and empirical evaluations of the proposed OASIS. The paper is also very well written. However, both the theoretical and practical results seem incremental to me. The construction in OASIS also seems a bit straightforward. Moreover, OASIS still requires parameter tuning in some of the experiments, and thus is not ""fully adaptive"". "	2
The paper explores an interesting research question to train agents that can communicate using a continuous communication channel and be able to solve the given downstream task thereby exhibiting compositional generalization. Although the underlying hypothesis is interesting, the idea is not completely novel and not evaluated extensively using well-established metrics in the literature. This work is a good preliminary work in investigating this research direction and I would encourage the authors to refine the evaluation and make explicit comparisons with prior work in the future submissions of this work.	2
"The paper presents a well-designed, novel referential game where the communication channel is continuous and audio-based.
The paper fails to demonstrate with its experiments the actual effects of using this continuous channel.
As a result, my decision is ""reject""; I think a new set of experiments highlighting the novel setup will yield a strong paper."	3
"I am not convinced by the motivation of this paper, the presentation is poor and the experimental results are unconvincing.  The experimental design appears to involve some unnecessary approximations.  The paper claims to be an advance on Gao, but Gao addresses the problem of segmenting a speech stream into words when the listener does not have a pronouncing dictionary.  Here the use of the continuous channel appears to me to be an unnecessarily complicated way of adding channel noise to what is otherwise a discrete system. 
"	1
On the whole, I like that the work pushes towards moving emergent communication into a new direction of oral communication. I feel that this work as it stands is an excellent start in this direction. However, I feel that the experiments and analysis could be extended somewhat, into scenarios where it feels somewhat less predictable what the outcomes will be. For example, what happens if the meaning space is larger? Does that work? If it doesn't work, how does it fail and why?	2
The proposed method is a novel combination of existing successful components for LNL. Although the proposed method is somewhat novel, the empirical gain does not assert the benefit of the proposed approach. In addition, some of the presentation (e.g., Fig 1) is not clear to understand the proposed method as it omits the order of the procedure.	3
This paper unifies several well-performed methods for LNL task to build a better framework, which achieves certain improvements especially under high-noise scenarios. However, some in-depth analyses about the effectiveness of the pseudo-labeling in PARS and the inferior performances under some settings still lack in this paper.	2
The paper provides a unified algorithm to learn from noisy label and shows convincing results on various datasets. The results on semi-supervised setting are also appreciated and can be useful for future research in this direction.	3
While this study has some preliminary results that indicate that late-stage interaction between options in a multiple-choice task setting is useful, I found that the work has a few flaws and is slightly premature for publication. Firstly, it relies heavily on specific behavior of the ART dataset, specifically, multiple gold positives and multiple gold negative examples in the training set. Also, the experiments need to be improved a bit to better disentangle the impact of late-stage interaction and focal losses on the performance of this task. Finally, besides the very low resource setting, it's not clear that the proposed changes make a significant impact in the performance of task; the reported gains seem quite modest.  My recommendation to the authors is to potentially improve this work by focusing on the low-resource setting, where the proposed techniques show some potential improvements in sample efficiency (see comments above for specific recommendations on how one could explore such a study).	2
I didn't feel like this paper is strong enough in its current form -- while it obtains strong results on the $\alpha$NLI dataset, I felt like there's unfortunately not much here that other work would be able to build on. I had trouble understanding why the proposed method does well, or whether it would be likely to work on other tasks (or even other datasets for the same task if they lacked the multi-view property this one has). The paper would be a lot stronger with clearer motivation for the technique, some ablations and more thorough comparisons to baselines, and improved writing.	3
"I think the paper proposes a useful method for the task of abductive natural language inference based on good intuition.
However, I'm afraid the impact of this work might be limited (disregarding the fact that larger models like DeBERTa outperform the proposed approach) and the presentation/analyses needs quite a bit of work. "	2
"Please provide a short summary justifying your recommendation of the paper.
The paper has marginal novelty on two extensions of a pretrained language model, but one extension (the BiLSTM-based information interaction layer) is not completely evaluated, while the other (the joint softmax focal loss) is even inapplicable to the experimental dataset. This makes the contributions of the paper unconvincing at the current stage.
"	2
Lack of novelty, not a significant improvement, and no qualitative analysis on the authors' hypothesis (i.e., it's useful to model hypotheses together).	1
The paper presents some potentially interesting ideas. However, I believe that the unnecessary usage of an esoteric formalism and the constraining assumptions do not warrant publication at this time.	3
"The paper proposes a theoretical framework to study compression of NNs employing the representation theory of quivers. However, the proposed theory and its experimental analyses are incomplete.

To improve theoretical results, I recommend first explicating the theoretical results in comparison with the related work. Second, experimental analyses should be extended by applying the theoretical framework for the target model compression tasks in comparison with the related state-of-the-art."	2
"# Final comments

There are two items affecting my score at this point:

1) How applicable is this result? In other words, how often would we consider using radial neural networks in practice, or how much of this paper could be applied in a broader sense?

2) It is not easy to follow this paper when it gets more technical. I know that many reviewers would not be willing to reassess their score if the authors put the effort of rewriting the paper considerably, but you have my word that I would reread the paper and reconsider my assessment in case you do that."	3
This paper is novel on the interpretability by diving into the deep generative models. 	4
Interesting and potentially useful method in a relevant topic area.  The evaluation approach could be improved.	3
The task is not well motivated. There are several technical issues mentioned above. I think this paper is not ready to publish. 	1
"The paper presents interesting ideas to investigate the relationships between variables learned by a deep generative model, but to make the claim that these relationships are causal, more theoretical justification, intuition, and experiments would be needed.

POST-REBUTTAL UPDATE:

I thank the authors for their attempt to improve the paper. Nonetheless, I don't find my original concerns resolved, nor answers to my suggestions. Especially the framing of the approach using causal concepts remains obscure and unconvincing.

As a reply to the authors' specific question on permutation: if two variables are strongly associated but not causal, and the method incorrectly detects a causal signal between the variables (based on the presented experiments it is not inconceivable that this may happen in some cases), then the permutation test will highlight this as highly significant (because the association is removed in the permuted dataset).
"	3
I think that this paper should be accepted. As far as I can see, the presented results are relevant and correct. However, I did not check the proofs in the last detail.	3
This nicely written paper theoretically studies the dynamics of dictionary learning via unrolling + backpropagation style training. However, the authors could consider contrasting the results with the existing literature and clearly articulating how/where unrolling-style training is better or worse than the current of the art.	2
"The paper contains some new results compared to prior works (stability analysis of the Jacobian and limited-depth backprop). However, the papers lacks novelty given the works from Ablin et al., 2020, Tolooshams et al., 2020, and [1] (see above for details). Given the current formulation, results, and version of the paper, the paper's contributions do not distinguish itself from prior works. Hence, I do not recommend an acceptance of this paper. I hope that authors find my comments helpful and can address my concerns detailed in the review.

------------
after discussion

The authors explained how their work with focus on instability of the Jacobian and analysis prior to support identification distinguishes itself from prior studies. Given this and the additional citations, I have increased my score."	3
"Numerical scheme for solving the non-convex problem related to dictionary learning has been extensively studied. For example, proximal alternating method and proximal alternating linearized method. The hard part indeed lies in efficient and effective estimation of dictionary with non-convex constraints. The proposed unrolling scheme does not address the true bottleneck of the problem. The asymptotic analysis presented indeed are rather the application of existing available techniques. Numerical experiments on MEG signals are also too limited to show its practical advantage

"	2
In summary, it seems like a good paper, with a simple and clever idea to improve a fundamental model in structured prediction. The arguments are made very clearly while presenting the idea but the paper lacks enough empirical evidence to back up the efficacy of the proposed idea.  	3
"I like this paper and just think it needs some improved motivation in the introduction. An experimental comparison against neural-weighted finite transducers would strengthen the paper a lot by justifying the claims in Section 4.3.

"	3
This paper formalizes and systematizes how to incorporate regular language constraints into CRF training and inference. This simplifies the incorporation of constraints, and makes it clear when they will become computationally infeasible. It also provides exciting hooks into formal language theory for future contributions. The experiments are not super-exciting, nor are the proofs, but the framework is a nice addition to CRFs overall.	3
Although the discussion is very interesting, the current version of the experiment did not meet the requirements for publication, and many parts of the method were not properly studied.	2
In the field of human pose estimation, the heatmap detection-based method and integral regression-based method are widely used. Previous works showed the performance and behavior differences between these two approaches. However, there are rarely works to theoretically study where the differences come from. This work proposed a unified model to analyze these two methods and is well supported by comprehensive experiments. It's great to see such a theoretical study on that. I'd like this paper, and I believe this work can help the research community in the field of human pose estimation to better understand the problem. Though the novelty of the paper is limited, I think we should encourage such efforts on how to theoretically analyze our methods.	3
Overall, I am positive about this paper. Obviously, the novelty is limited, but I believe that its value is high, particularly for practitioners. Since the analysis done here is more basic, I will give it a Weak Accept rating, but I think it is important to accept it to encourage similar efforts.	2
Because the main claims are not well supported and the proposed heatmap prior loss lacks novelty, I lean towards reject.	3
This paper provides insightful method and strong results. However I have questions about the implementation for now, which makes the proposed method not reliable enough.	3
I think the illustration of the problem discussed in the paper is not clear, and part of the evaluation is not clear enough (please see above for the full review). 	2
My assessment is based on the three aspects: motivation, novelty, and results. The motivations behind proposed modules in this paper do not make sense to me and the novelty of this paper is limited. Most importantly, the comparison to previous methods is not fair. So I recommend rejection of this paper. 	2
The contributions are increamental, and the writing could be improved.	2
"The proposed method is an intuitive improvement on the existing adversarial example weighting method.
However, the experimental results are not convincing enough to reveal the true improvements."	3
"1, why instance reweighting is necesary for adversarial training? can you provide some theory insight for this? From the experiments, we can see that the instance reweighting obains some marginal improvements over SAT, but what is the time cost? why not report the time in experiments? If you cost much more time than SAT, but only obtains very little improve on the accuracy, then it is not a smart method.
2, why ""the safeness should be attack-dependent""?   can you provide some theory guarantee for this?  what kind of attack make the safeness change? Is there any possibility that different attackes do not change the safeness? how different attack change the safeness? what kind of attack change the safeness towars more safe or unsafe? The main claim of this paper is not rigious at all.
3, Can you prove the proposed algorithm converge? and what is the time cost?
4,Why only consider L_inifity attack? why not try L_1 and L_2 attack? why only ResNet (ResNet-18)? why not try ResNet-34 and ResNet-50 and some other networks? why only use CIFAR-10? why not try imagenet and some other datasets?"	1
The analysis of GAIRAT is thorough and insightful and the motivation of using different attacks is clear. However, the evaluation of LRAT is incomplete and I cannot see the benefit of locally reweighting. Therefore, I tend to reject the paper at current phase.	3
"Overall, I recommend reject to this paper. I think the contribution is not sufficient for ICLR.

The improvement is limited compared to SAT and LSAT which is train the model with previous attacks such as CW or APGD-DLR. Moreover, there are limited comparisons as I elaborated in the weak points.

However, if all my concerns resolve properly, I will increase my score."	2
My recommendation is based on the proposed new type of conditional VAE, which could be applied to any other type of dataset, and the fairly convincing results on both synthetic and real datasets. However, I am unsure exactly of the clinical relevance of the paper's stated purpose (of finding nosological relationships between disorders), and I feel a few relevant comparisons are missing.	3
The paper is generally very well written, clearly presented, and a pleasure to read. The technique look sound and the contributions look solid.	3
While the model  and problem being studied is interesting and offers some degree of novelty both in the technical and clinical space,  there are still several concerns with the evaluation and validation being performed. Additionally, the clinical scope of the proposed framework is also not as well justified. For these reasons, I choose to rate the paper as marginally below the acceptance threshold.	2
"Extensive evidence showed that these neuropsychiatric disorders share symptoms and neuroimaging features. For example, several cross-disorder studies showed that major depressive disorder (MDD), bipolar disorder (BD) and schizophrenia (SZ) share biological, neuropsychological and clinical features, despite the criteria for their respective diagnoses being different. (Chang, Miao, et al. ""Identifying and validating subtypes within major psychiatric disorders based on frontal–posterior functional imbalance via deep learning."" Molecular psychiatry 26.7 (2021): 2991-3002)( Zhang, Luheng, et al. ""Three major psychiatric disorders share specific dynamic alterations of intrinsic brain activity."" Schizophrenia research (2021).) Therefore, the utilization of diagnostic information will not help to separate different disorders. Furthermore, these mental illnesses are often misdiagnosed. For example, the diagnosis of 50.7% of mental disorder participants was changed in a 10-year follow-up study, including schizophrenia spectrum disorders, bipolar disorder, major depression disorder. (Bromet, Evelyn J., et al. ""Diagnostic shifts during the decade following first admission for psychosis."" American journal of psychiatry 168.11 (2011): 1186-1194.) 
Therefore, I believe the clinical motivation of this paper is wrong. 
"	1
Overall, I believe this to be a solid paper. There are some remaining issues that need addressing, before I can raise my score, for example pertaining to reporting and ensuring that rigorous statistics and quantifications are used to substantiate the authors' claims. If these comments are adequately addressed, I am sure, that this work would be a valuable contribution that hopefully will also be of interest to other research areas.	3
"The paper provides an interesting idea on connecting discrete action space and continuous action space for reinforcement learning and imitation learning. The writing is clear. But some important discussion seems to be insufficient and hence its novelty and significance is not clear enough.
"	3
The paper presents an interesting framework for learning the discretization of action spaces. Related work needs to be extended to include exploration, the claims of benefits of discrete over continuous action spaces have to be substantiated, the evaluation has to be done on fair baselines and commonly used environments to allow the comparability. I am happy to adjust my rating should these issues be addressed.	3
"I generally like this paper: the proposed method is sound and useful, experiments are insightful and nicely designed, and good discussions around limitations (e.g. pruning optimal actions) are present.

However, the paper makes assertions here and there that are not entirely true (e.g. while related work covers a group of methods for addressing the curse of dimensionality, on various occasions this line of work is dismissed and a limited set of baselines are used). Baselines that use uniform discretization and can scale to high dimensions (regarding computation and generalization capabilities) by exploiting compositionality in multi-dimensional spaces are simple to try and would be very informative. 

Also, discussion and comparison with an important class of methods (*action representation learning* methods) are missing. I referred to one important work in this direction (Ref. [1]), but I'm sure there is more on this topic under different names (e.g. latent actions, action embedding learning).  "	3
 I think the technical side of the story is sound, the method taken is reasonable and the empirical results are valid. However, I think the novelty of this paper is limited. The idea is rather simple, and learning discretization through a gaussian mixture model type of representation is trivial. I am not quite convinced at the current stage that it reaches an ICLR level of work yet. 	2
"The paper proposes an interesting addition to supervised contrastive learning, which removes the lack for data augmentations during training and yields improved results. The results look compelling to me, although I generally have the impression that they were executed under a lack of computational ressources (seen, e.g. by relatively small batch sizes although large batch sizes are known to improve performance, lack of multiple seeds for some experiments, etc.), and hence I am worried about the significance of them (how well baselines were tuned, etc.).

That being said, I am willing to improve the assigned score the paper. To consider an accept, the authors need to minimally show a proof of concept result on ImageNet (for example in a very controlled setup, reproducing the SubContrast result, adding their method and showing an improvement with no special additional tuning, to make the comparison as fair as possible.

As I see it, the method already works on full-scale images. What is the blocker for running a full scale experiment? Are you confident that the improvements hold on full scale ImageNet training? From the data in the paper right now, it is not clear that the improvement will hold in the regime simCLR-like loss functions are typically evaluated on (large batch sizes, ResNet50 models, full ImageNet).

Without full ImageNet results, the paper in my opinion does not meet the bar for ICLR, as the one crucial experiment that would contribute to adoption of the method is missing."	2
In summary, I think this paper propose an interesting idea with some potential of improving contrastive learning by removing the requirement of data augmentation. Nevertheless, after reading the paper, I also have a number of concerns/questions on some statements, method derivation, theoretical analysis and empirical evaluation. Overall, I think the presentation can be improved to make it easier to follow.	3
The authors propose a new method which does not require multiple views in contrastive learning. It has the potential to motivate follow-up works. However, the theoretical motivation seems to be incorrect. The author does not consider the gap in the lower bound. So I vote for rejection. 	2
While I believe the paper is clearly above the acceptance threshold, I have a few comments to clarify or supplement some points in terms of the supervised settings, the rationale for increasing the label-conditional MI, and the proposed loss equations. 	3
The idea of personalized NAS in FL is interesting. The proposed algorithm to learn architecture with a base component and a personalized component is novel. However, the definition of task-personalization is confusing, and the experiment used for evaluation is not sufficient to fully demonstrate the effectiveness of FEDPNAS. Therefore, I think this paper is marginally below the acceptance threshold.	3
This paper proposes a novel NAS algorithm in a federated setting and backs their claims with theoretical rigor. However, the lack of ablation studies diminishes the merits of the proposed technique.	2
Based on the above concern described above, I currently tend to weakly reject the paper but would like to hear more feedback from the author and discussion from other reviewers towards the final decision.	3
The experiments are very lacking, theory and writing is unclear.  	2
I think the results are strong and interesting. There are many typos in equations. While typos are not serious but needs revision. I found some parts of the proofs that I don't quite follow (lacking $B$) and undefined notations.	3
This paper improves the statistical sample complexity (in terms of $H$) for reward-free exploration with linear function approximation. Thus, this paper deserves to be accepted and I would raise my score if the mentioned concerns are addressed.	3
"This works looks like a solid contribution to the reward-free RL literature. To the best of my knowledge, it provides the best known sample complexity rate for reward-free exploration under linear mixture assumptions (especially, it improves over W. Zhang et al. 2021 by a factor of H), and the best rate for a reward-free method that can work with any planning solver (e.g., Jin et al. 2020a, W. Zhang et al. 2021). 

Since I believe these improvements are valuable, I am providing a positive evaluation for this work. A careful inspection of the novelty of the analysis (especially w.r.t. Zhou et al. 2020a), as well as additional reassurances from the authors on some questions over the computational tractability of the proposed algorithms and the sample complexity lower bound, could make me consider raising my score towards an even stronger accept.

I believe that extending the proposed analysis to improve known sample complexity rates under the looser linear MDP assumption, as the authors suggest in Section 4.2, would potentially increase the value of this work from solid to great."	3
"Overall, this is an interesting work. I lean towards acceptance, especially if the comments that were raised are addressed.
"	3
"I find the paper very well written. The proposed method appears to be theoretically well-motivated and, empirically, leads to improved robustness. 

I want to stress however that I have only a very limited overview of the relevant field of research and I can thus not confidently state a definite opinion, especially concerning the novelty of the manuscript."	3
I really appreciate the top-down approach of the paper. Unfortunately, I am neither really convinced by the experimental validation which considers too small models, nor by the theoretical presentation that sometimes feel a bit decorative. Hence my weak reject recommendation, with confidence rated at 3 because I am not familiar with evidential Bayesian models. I am looking forward to discussion with the authors to clarify any misunderstanding. 	2
Given the points above, I think that the paper lacks clarity. As I mentioned earlier, I cannot form a strong opinion before I understand the motivation for the model and the experiments.	3
It is difficult to rate this paper. Theorem 1 has theoretical value. The proposed method is relatively simple (both pros and cons). But I would vote positively for theorem 1.	3
This is a technically solid and empirically strong paper. The theoretical results follow from recent works for mixup (Zhang et al. (ICLR'21)). The experimental results are conducted on three popular image datasets, with three input perturbation models, and do support the validity of the proposed approach. The main weakness of this paper is the accessibility of the technical contribution and the lack of ablation studies. I think several revisions could help mitigate these weakness.	2
This paper is well-written and provides thorough theoretical properties. However, the experiment settings are a bit limited and require more baselines to be compared. 	3
The problem this paper focuses on is important, i.e., improve the generalization performance of deep models. To this end, this paper proposes a new method that exploits the benefits of mixup and noise injection, and is empirically demonstrated to be more effective at improving the accuracy and robustness of trained models. The proposed method is simple and easy to implement. Notably, it has sound theoretical analyses to illustrate the underlying mechanisms. Although for this paper there are still some points which are unclear, currently I think it is overall a good paper.	3
I'm leaning accept, but there are many points of clarification that would need to be sorted out before acceptance.	3
This paper is well written and its claims are well supported by the experiments, analysis, discussions. I believe this paper will bring values to the broader researcher community.	3
"The paper has an ambitious goal and some potentially novel ideas but there seem to be serious issues with
1. Precision: while certainly not every paper in the field needs to be theoretically driven, the work makes claims that would likely benefit from a more rigorous discussion - there is no formal discussion of the intended meaning of disentanglement and no precise argument for why this method would benefit that definition of disentanglement. However, while some additional precision would be helpful, I am mostly evaluating this as an empirical work.
2. Evaluation:  within the disentanglement (non-FID) metrics and the datasets where a quantitive evaluation of disentanglement is provided, the paper does not consistently show that the proposed approach provides disentanglement advantages over VLAEs. The main text primarily focuses on FID and reconstruction FID which is inappropriate for the datasets used and the way in which the paper presents its contribution. Finally, it would have been helpful to have also seen other standard models like FactorVAE or TCVAE.
3. Novelty: the main proposed advantage over VLAEs (described as an inspiration) appears to be the lack of variational regularization, but the authors don’t appear to argue that this is a disentanglement loss. It’s unclear to me exactly what argument is being made for the advantage of this approach.
4. Scaling: it seems that the independence properties should enforce a gradually weaker constraint with deeper models and larger input dimensionalities. Some analysis of the performance of this method as a function of larger input dimensionalities could be beneficial.
"	3
This paper has both strengths and weaknesses, as outline above. While aspects of the model presentation, model formulation, and conclusions could be improved, the strength of the empirical evaluation somewhat makes up for these shortcomings. I’m currently borderline, with a tendency toward acceptance. With revisions, I could be convinced to accept this paper.	3
An interesting approach that is hampered by a poor presentation, somewhat limited experimental results, and only minor justification for why it should be seriously considered as an alternative approach to more well-studied approaches for training neural networks.	2
Using message-passing to train NNs is intriguing, but, like the various flavors of SGD, it does not appear to make a significant difference.	2
This paper applies the well-known belief propagation (BP) algorithm and several variants to train deep neural networks with binary weights. Overall I like the topic of this manuscript and it is a good start to explore the potentials of BP-based methods for deep learning. While this is indeed a very interesting try, there are several aspects to be improved for the current manuscript, such as a systematic evaluation of the potential advantages of the proposed BP-based methods, a comparison with previous closely related algorithms especially the EBP algorithm and other Bayesian deep learning methods, as well as clarifications of some related technical points, as detailed above. In addition, given the comparable (or slightly worse performance with higher complexity) but not competitive results of the proposed BP-based algorithms, it is worth a discussion of the intuitive reason behind it. This is not saying that it is not useful to study an algorithm without SOTA performance, but rather on the opposite, it is very useful to explain the (intuitive) underlying reason why it works this way even it is not SOTA, as well as its limitations. 	3
Although the paper is a close extension of (Baldassi et al. 2016), the idea to use belief propagation to estimate the marginals of the weights of a neural network (NN) in order to make probabilistic predictions is a novel paradigm and the empirical results on continual learning seem promising. I recommend acceptance subject to the clarification of the concerns discussed in the main review.	3
"Although the topic and findings of this paper are interesting, I think the contributions of the current version are limited, especially for the performance of nSGDA and the experiments. 

******************************
Post Rebuttal Comments:

I thank the authors for their efforts in addressing my concerns. However, I still have the major concern about the performance of the proposed method over commonly used GANs such as BigGAN and StyleGAN. The authors respond that simpler with similar performances is better. However, there is no result to show similar performance over BigGAN or StyleGAN. Thus I keep my original rating. 


"	4
"The importance of balanced updates of the generator and discriminator in GAN training is already well known. Using normalized gradient update steps for convergence is plausible, however not enough as shown in [2,3,4,5].  
"	2
I enjoy reading this paper. The findings are insightful with comprehensive empirical and theoretical analysis.	4
"The paper is well written, and does a good job covering all the related material.
The definition of disguised subnetworks is novel and will be useful for future researchers.
The PaB algorithm clearly yields good results on larger NN, something other methods lack.
More exploration of theoretical understanding as well as other weight transformations would be 
useful."	3
"**Pros**
- The paper has a very clear overview of the relevant research, and in general, the paper is written very well
- The numerical experiments are very thorough
- The idea of generalizing the notion of hidden subnetworks is relevant
**Cons**
- The contribution is limited -- the definition of disguised networks is straightforward and the solution method comprises a subsequent application of existing algorithms (not parallelized or nested, but applied sequentially)
- The optimal disguised networks problem is restricted iteratively so that the end result is not very interesting anymore"	2
The method is novel and achieves better efficiency-performance tradeoff than the various baselines. It tackles the question of rethinking optimization strategies for sparse NN training, which I think is an important research direction. I think the contributions of this paper are significant and would support its acceptance. 	4
"1. The setting is new, but the practical value to the community seems unclear.
2. The algorithm is straightforward, joint optimization is not considered.
3. Experiments are promising but not exciting, ImageNet results are absent.

Overall, a bit below the acceptance threshold."	3
The current version is a borderline paper because some important parts seem unclear to me. The presentation should be polished, and the connections among sections should be strengthened. I expect a better version after the discussion period.	4
"A useful formalization of the conditions under which L2DNC works.

An intuitively sensible multi-view based meta discovery algorithm
shown to outperform existing alternatives in the low-data regime.
"	3
Overall, this is an interesting paper, which reformates the problem of NCD to a more realistic setting. In addition, this paper builds a connection between meta-learning and NCD and presents an effective approach to address the introduced problem. Extensive experiments are provided to verify the effectiveness of the proposed method. Some statements and implementation details are not very clear, I would like to see the response of the authors.	4
The theoretical framing of the L2DNC problem is valuable, as it can help us understand what is necessary to solve this problem effectively. 	3
The technical novelty of the proposed method can be improved, and the experiments lack comparisons with SOTA techniques for defect classification. This work is not ready for publication yet.	2
I think this paper tackles an interesting applications. The results are thorough and solid. My main concern of this paper is that the technical novelty is limited. I am thus leaning negative about this paper.	2
Due to the limited presentation, experiments and technical novelty, I am on the borderline for this draft yet. However, I could go towards the accept, if authors could made effective rebuttal to my comments.	2
To be honest, I do not think this work is suitable for ICLR as most audiences may not get interested. I strongly recommend the authors submit this work to a conference/journal in the industry field.	2
"
The work presented in the paper can be regarded as an application of StarGAN v2, while the paper is well writen and easy to follow, the novelty and contribution of methodology is limited, particularly for ICLR 2022.

As only scratch and spots are involved, the number of categories for defects are too limited and it's not convincing to support the claim. Much more defects and products need to be tested and evaluated."	2
I would like to recommend the paper for ‘marginally above the acceptance threshold’ rating. The authors have addressed some of my concerns. Some of my concerns still exist such as how concretely the new analysis using sharpness of loss minima justifies SSL-ST to be compared to SSL-JT. I also have some concerns on the soundness of the loss curve and whether the rise in the loss is due to distribution drift or just recalibration of the SSL queue. Although there is certainly areas to improve, the paper does provide an interesting perspective to compare SSL and SL and also support it with different kinds of analysis that led me to revise the rating.	2
Th authors focus on an interesting and practical task. Prior works have largely overlooked this task and the proposed work is the first to explore along this line. The motivation of conducting such an analysis is strong, since the incremental learning setup is indeed practical in many real-world scenarios. Extensive experiments are also conducted to support the proposed method, which are very solid.	4
This paper empirically studied on sequential self-supervised learning problem, which is never done before, and show that self-supervised learning is more robust to catastrophic forgetting problem compared to supervised learning. Although exploring the problem of sequential self-supervised learning is meaningful, it is hard to find insights beyond empirical results to make improvement on the top of the discovery of this work in this paper. As a result, I prone to reject this paper. However, I am open to any objection and further discussion, if the authors make their point and insight of this paper clearer.	2
The paper provides some interesting observations through pretty thorough experiments on an interesting/important setting for large pre-trained models. They show that continual learning methods can improve the issues they found. There isn't much in the way of explanations of the observations, and the methods aren't novel, but the observations are valuable.	1
I like the set of methods proposed by the authors, although motivation is lacking for why the particular sub-components have been chosen. This seems like a believable/feasible approach to Bayesian RL, and it inspires me to try things out along those lines. The criticism in my main review is about performance assessment of the SWAG component as well as the lack of motivation for the specific  (non-standard, but interesting!) optimization method. 	3
Interesting paper focusing on model-based RL that does not address where the model comes from or the consequences of misspecification. 	2
I believe that this is a good paper making interesting and important contributions to the area of constrained RL and that it is ready to be shared with the community.	3
I think this paper is well-written and the key ideas would be useful for the research community. Though I have a minor concern regarding how much contribution exists for extending unconstrained MDP to constrained MDP settings, the overall contribution of this paper is sufficient; hence, I vote for acceptance.	3
The paper shows a way of interpolating graphs for graph classification and achieves excellent empirical results while showing that the interpolated graphs are invertible functions of the original graphs, avoiding the underfitting issues that may occur with other techniques for applying input mixup to graphs.  	3
"The paper proposes a mixing strategy for the graph structured datasets, but the main claim of the paper that the method is ""Intrusion Free"" is not correct in my opinion."	2
Overall, the proposed method is simple. Its main limitation is their theory on invertibility and their experiments. In their experiments, as a general data augmentation method for graphs, node-classification tasks are also needed. The baseline of MixupGrpah is a bit weak. Stronger baseline - GraphMix is encouraged to be used for comparison. 	2
This work is very interesting, but I have slight concerns about part of the model design.	2
An interesting, somewhat novel approach, but experiments not thorough enough to show that method is truly robust; theoretical contribution seems completely wrong.	3
"The paper presents an interesting idea of using reachability traces that is implemented with a lot of moving parts and evaluations on simplistic domains. I have some concerns regarding the empirical evaluations and the usefulness of the entire approach and look forward to engaging with the authors in the discussion period.
"	3
"Both exploration in sparse-reward goal-based RL and subgoal discovery are important topics in RL. Although this paper contains an interesting idea for subgoal discovery and curriculum design, the proposed algorithm requires several strong assumptions does not technically sound, and the empirical results are not convincing enough and lacks comparison to important baselines. 

---
**Update after the rebuttal**

I'm not entirely convinced by the author's rebuttal for the following reasons. Thus, I will keep my original score. 
1. If the reachability learning algorithm is biased and not consistent with Proposition 1, I do not see why Proposition 1 is needed from the first place.
2. The author's justification for not comparing against HER is not convincing. Single-goal RL is just a special case of multi-goal RL. In fact, HER has been shown to be effective in a single-goal RL setting (see Section 4.3 of the HER paper). So, I believe that it is important to compare against this baseline. 
3. The authors should have clearly showed how much data each algorithm (1, 2, 3) consumes in the plots and how the switching points are determined. The fact that the total amount of data is reflected in the x-axis doesn't address my concern.
4. I'm still not sure why Section 5.1 is needed. My initial comment about ""This result shows nothing more than saying that the reachability reward function (i.e., 0 for all non-goal states and a positive reward for goal state) is a reasonable reward design for goal-based RL."" is not addressed by the author's response. The author's response seems to repeat exactly what I wrote above. This paper seems to propose a proper reward design (reachability) as a solution, but my comment was that a proper reward design seems to be on the problem side rather than on the solution side."	3
I believe the idea behind the paper is not very novel and the performance is not very significant. The theory is poorly written. I recommend rejection.	1
As a simple idea, more discussions and empirical supports are expected to make it more convincing. I'll raise my score if all the concerns are effectively addressed. 	3
The paper proposes a method for using regularization on the final layer of a classifier to improve the performance on novel unseen classes with very few examples. Then delves into the proposed idea and studies it with very good and justifiable experiments and visualizations. As a result, I vote for acceptance.	4
"In conclusion, my main concern is that this work may not have the same starting checkpoint when comparing to prior works and therefore it is hard to evaluate its empirical success in the few-shot learning phase. Another main concern is that the relation between subspace and incremental learning is unclear, since subspace regularization could also be applied to standard few-shot learning, and therefore comparisons to subspace methods on standard few-shot learning seems required. Other aspects of this work look solid to me. My initial evaluation of this paper is “weak reject”.

Authors' response on joint training addressed my concern. However, my other concern about comparison to Tao 2020 and Chen & Lee 2021 was not fully addressed. That being said, I understand there might be difficulty reproducing the numbers. Overall, I increased my rating from 5 to 6 to acknowledge the effort in responding my review comments."	3
The proposed method is simple and effective and the paper is well-written. On the other hand, the main contribution is small and I have some questions about the experimental setup and relationship with previous related work (see weaknesses). Overall, I think that the simplicity and effectiveness outweigh the negative aspects and **I would be inclined towards accept if the authors resolve my concerns and those of the other reviewers.**	3
This paper has a very practical clinical context and if generalizable has good opportunity to for application.It is unclear why the performance is superior to other multimodal methods which should be investigated more with details of performance variability.Overall good work.	3
The idea of using schema-based GNN is interesting and have good performance on DAIC dataset. The paper is well written, but many related works are missing. In addition, the authors should add the interpretability analysis.	3
The paper is presented clearly. But the method is not novel. The comparisons with recent works are also not involved.	2
Overall I find the experiments against the top Mahjong player to be very compelling and am excited by this research direction. However, I think this paper skips some crucial steps in analyzing the tabular method that this is based on. If the authors are able to address this issue and explain why we would want to use this method vs. deep CFR/DREAM or compare to those methods then I would consider raising my score.   	3
"This is a promising paper: ACH seems like it might be a useful addition to the CFR family of game solving algorithms. My main concerns are in the evaluation. The games used are either trivial domains or, when nontrivial, are not shared by related work, so it’s difficult to compare ACH’s performance. The theoretical results prove the convergence of the ACH average policy (within NN approximation error, etc), but the empirical results only demonstrate the ACH current policy, which has no theoretical convergence guarantees, and does not converge in the experiments shown here. And unless I’ve missed it, the paper does not describe how the average strategy should be approximated by a NN, which has been a challenging topic in related techniques, so the explanation is incomplete. If, like Srinivasan 2018, the authors have decided that computing the average strategy is difficult and so they will only hope that the current strategy (approximately) converges, then they should be more forthright about that shortcoming in this paper.

Overall, I’m landing on ‘marginally below the threshold’, but I’m open to clarification from the authors if I’ve misunderstood elements of the paper. If I have not misunderstood it, then I believe a future draft of this paper that included an empirical analysis of the average strategy, and addressed the smaller issues I noted, could be a strong paper. Including a nontrivial poker domain alongside the Mahjong results would be excellent.

EDIT: Updated review based on Nov 22nd draft
Thank you to the authors for the discussion and for addressing the issues I noted in my review. Specifically:
 - The addition of FHP results, and with exact exploitability figures, is a great addition. The exploitability of roughly 9 chips/game, or 0.09 big blinds/game, is closer to Nash than I had expected.
 - The description of ABR as a lower bound instead of an approximation is clear and consistent.
 - The clarification that 'poker' refers to 'HUNLH' in the page 2 footnote helps, but statements throughout like 'only two private cards are invisible in poker' remain, and could probably be clarified as '...in Texas Hold'em poker' without risking the page limit. Still, it's better than it was.
 - The new section 5, that describes difficulty in using a neural net to represent the average policy and confirms that the presented ACH results use the current policy, is a good clarification.

I've updated several of my scores (correctness 3->4, empirical 3->4, recommendation 5->8). If 7 had been an option I likely would have landed there. The missing piece of the average strategy (a weakness shared by the related work) is still an issue, and it feels like a stretch to describe ACH as 'theoretically justified' (for example, in the abstract) when only the average strategy has theoretical guarantees. But the issue is now acknowledged, and the empirical results in Mahjong and FHP are, to me, compelling enough to recommend acceptance."	3
"
Summary & Suggestions

To summarize, I think the paper lacks proper comparison to prior methods and makes unsubstantiated claims about  “outperforms related state-of-the-art methods”. Not only are relevant related methods missing, but the ones included do not have the performance previously reported. The authors need to rework the experiments, and also properly compare prior work. 
"	2
"Taking the core of this paper to be a general policy-gradient update rule for competitive imperfect information environments, with a demonstration of performance in a large game of interest to human players, there is the base of a strong submission. However, the additional theoretical analysis and empirical analysis of small games have a number of issues that do need to be corrected.


------
Post-discussion edit.
The authors have made numerous changes, and I have no remaining concerns with after the Nov 21 revision."	3
The work is technically sound (to my knowledge), but of limited significance.  This thread of work _could_ be of great significance if the mechanism proposed by the authors can be shown to be behind the performance of more realistic models, so I don't want to discourage the author's current line of investigation---it's more that I'm unsure if this is a good match for ICLR.	2
"This paper provides a solid theoretical analysis, which, however, applies only to a specific problem statement and thus cannot be considered a satisfactory argument for why momentum is beneficial in deep learning (unless the authors provide more evidence that their setting is relevant to real-world datasets). The empirical justification does not correctly reflect the considered full-batch gradient descent training. I believe the paper could benefit from another round of revision and is not ready for publication yet.
"	2
"1. Solid theoretical results under a special case

2. Big gap in the paper

3. The reason of considering GD+M

4. Lack of experiment (for verifying the theoretical results)"	3
I do not think this paper is below the accept bar due to its interesting topic, novel analysisis, interesting result, complicated techniques, and good presentation.	4
"The analysis is not very clear, a lot of approximations are made throughout the proof, and some places in the proof are concerning. I don't recommend an acceptance at the current point.
"	3
This seems to be a solid paper with an interesting and well-executed novel theoretical contribution tackling a difficult problem of relevance to many in the RL and planning-under-uncertainty communities. The experiments are also convincing, and compare against popular RL strategies. I believe that the clarity of the paper could be further enhanced, yet point out that the paper is already well-written and easy to follow.	3
"Here I review the key constructive points:
- Beyond the online ant setting in the appendix, I don't see much evidence that this method improves performance outside of partially observable environments in the online setting. The intuition behind why it would help in fully observed environments is also not clear from my reading of the paper. Given this, I think that the paper should focus the motivation more heavily on the POMDP and offline setting. This was the main reason for my score. I'm happy to discuss if I've misunderstood something about the utility of the method in the fully observed setting. 
- From my description above I think there are a few ways in which the experiments could be more convincing. 

Why I liked the paper and recommended to accept:
The core idea in this paper is very elegant and the variational inference + z forcing to implement this idea is well done. As far as I can tell the use of Z-forcing and the variational bottleneck is novel in this setting. The results on partially observable and offline settings are impressive."	3
"While the paper studies an interesting direction of learning future conditioned policy/Q-function, which is quite exciting, I have several issues with the paper, namely:
1) The incremental nature of the work.
2) The complexity of the method.
3) Several concerns regarding empirical evaluation.

Taking these into account, I'm not sure the paper contributes enough in its current state to justify an acceptance. "	2
"The experimental results look impressive, but I was not convinced that the baselines were well-implemented. The experimentation also only looked at reward scores, so it is not clear why the performance improved. It was also not clear to me that there is any good theoretical reason why the method should improve the performance. Currently, I am recommending to reject. I hope the authors will provide strong arguments and evidence in their rebuttal.

---------------------------------------------------------------------------------------------------------------------
Update after author rebuttal
----------------------------------------
The authors have provided additional evidence on the correctness of their baselines.
They also performed additional ablations, such as checking the change in performance when increasing
the model capacity. I was satisfied with these changes, so I have increased the correctness to 3, and the score the 6.
My recommendation is a borderline accept, as I was satisfied that the method empirically improves the performance; however,
I think the authors could have done more to examine why their method improves the performance.

I still have some concerns with the paper:
- The experiments still only look at the reward curves. They could have explored points such as the prediction accuracy of the value function, the variance and accuracy of the gradients, or tried to do some experiment to examine the quality of the representations (although this one is more difficult, as I don't think there is a standard way to look into this).
- I am still not convinced by the authors' explanation of why ""looking into the future"" is useful. When the information bottleneck is removed, the value function would ideally just predict the Monte Carlo return. So the authors claim appears to be that ""looking into the future"" is good, but if it is done too much, then it stops being useful. I think this point could have been examined in more detail, e.g. by doing a study on how the information bottleneck coefficient affects the performance (admittedly, it would have been better for me to say this in my initial review). Previous works (e.g., https://arxiv.org/abs/2011.09464) doing related ideas provided longer more principled explanations of why their methods would be useful by arguing that it might reduce the policy gradient variance by removing the random fluctuations in the future.

Some more minor concerns:
- I still think the discount prediction requires more clarification in the appendix B.
- Regarding the discussion around equations 1 and 2. I think you still
need to give more clarification. If one differentiates this ""loss""
twice, one does not obtain the correct Hessian of the RL objective
(e.g., https://arxiv.org/abs/1802.05098).  I think this section should
be explained as an automatic differentiation trick to obtain first
order policy gradients.
"	3
"The paper leaves some key questions unanswered, and can be presented much more clearly. It is also missing very relevant prior work. Thus, I recommend a reject.

"	2
"Overall, the paper presents a novel and interesting addition to the randomized smoothing literature. This appears to be the first attempt at certifying complex semantic transformations, and therefore has merit. The paper still needs some additional work however, specifically in terms of quantifying the tightness of their proposed bound against semantic attacks. I also believe the authors should take a more careful look at the actual certificates for the compound attacks. An $ell_2$ bound for the entire parameter vector may not capture the worst case for each component of the transform. 

The paper in its current form is marginally below the acceptance threshold. However, if the authors show empirical evidence of their certificates against semantic attacks, I will be glad to improve my score."	3
While the ideas in this paper are to the best of my knowledge interesting and novel, many questions remain unanswered and key points in the paper remain unclear. 	3
"1. Strong theoretical and empirical results
2. The contributions are significant for expanding randomized smoothing to more complex semantic transformations"	4
I have some reservations regarding the novelty of this work. Moreover, certain empirical results do not back the claims made in this paper. Therefore, I am inclined towards rejection.	2
Authors present a clear investigation of how disentanglement of the domain factors may affect the performance of learned dynamical models. The suggested experimental evaluation is sound, but current results seem a little marginal. With additional results/modifications I believe this work could be useful to a wider audience, but my initial rating is marginally below the threshold. 	3
"In my view, this paper proposes a fair approach to a relevant problem. However, there are several concerns. 
- The benefit of disentanglement is not demonstrated. The long term generation is not sufficient to support the claim. If a fully unsupervised approach can work equally good what is the incentive of supervised loss? Therefore, I think it is critical to compare with some of the methods outlined above.
- The contribution is marginal as it simply introduces a regularisation term and provides empirical results. Simplicity is generally good and not a downside. However, it should be supported by proper justification and if possible perhaps by a theoretical claim. The choice of L1 in 3.2 and L2 in 3.3 is not properly explained.
- There are technical inconsistencies that leave room for ambiguities.

I have come to the conclusion this paper has concerns that need addressing. I, therefore, give a score of 5.

# Post Rebuttal

I have changed my score from 5 to 6.

"	2
I think the underlying technical contributions are quite small, while the empirical results are not particularly impressive. I thus find it difficult to argue for acceptance of the work.	2
"From the limitations that are described in this review, I think that this paper needs very significant changes to be accepted, especially because of the questionable claims and insufficient experimental results. Nonetheless, I am looking forward to discussing my opinion with the authors and other reviewers. I believe that this paper follows an interesting line of research and that further work could make it ready for publication at a next conference.

### Post-Rebuttal Update

I acknowledge the authors' response and thank them for their extensive answer. As explained in my follow-up response, I find that the proposed improvements are marginal and insufficient to raise my score. Therefore, I maintain my strong recommendation to reject the paper."	2
"The paper proposes to learn a new machine learning model, one that
simplifies the propositional encoding, and subsequent reasoning with a
SAT solver. The work could be of importance, but the current writeup
does not help to convey the key ideas. In its present form, I cannot
recommend acceptance of this paper.
"	2
I like the idea presented in this paper and hope it could be backed up in a more systematic and scientific way. Writing issues should be easy to address; it is just the current evaluation is a bit unsatisfactory (i.e., missing a highly relevant and important baseline), thus not very convincing. So I slightly prefer not to accept this paper, at least in its current state. I would be happy to adjust my assessment, if the authors could address my concerns about experimental evaluation during rebuttal.  	3
"Working with SAT solvers and Neural Networks is a very challenging task. [1] has already set the bar high and has paved the way for scaling NP-Hard solvers like MIP solvers to very deep neural networks. Unless the authors can achieve a similar feat where their approach scales to much deeper networks (ex. Inception model), I'm leaning towards rejecting this work because I see this work as incremental.

**citations**

[1] Subham Sekhar Sahoo, Subhashini Venugopalan, Li Li, Rishabh Singh, and Patrick Riley. Scaling Symbolic Methods
using Gradients for Neural Model Explanation. International Conference on Learning Representations, 2021.
https://arxiv.org/abs/2006.16322"	2
"- The paper addresses a very relevant problem in AI
- Developing deep architectures that are verifiable and interpretable by construction is a sensible idea in my opinion
- The presentation of the proposed architecture could be improved by adding figures and more examples
- Adding some definitions would benefit a large audience of non-experts in verification
- The trade-off between interpretability and predictive performance is not well characterized
- The claim that the model is more interpretable should be substantiated with experiments comparing TT-DCNs with other XAI methods"	3
This is a nice paper with a good idea. However, we believe that the introduced methodology is not fully developed.	4
I feel the importance of the work is not well argued in the paper, and hope the authors could address that.	2
"The paper is a solid approach to show feasible online update rules for learning representations that capture non-linear similarity.  Not all aspects of the theory or results are explored.  Key comparisons versus other scalable techniques Nyström are not made, perhaps to focus on the online nature. Nonetheless, some more comparisons and elaborations on the nature of similarities could make this approach more useful in practice. 
"	3
"I generally like the idea of the paper and think it is novel to express the low-rank approximation problem in terms of a neural network but I have the following concerns about the presentation and contributions of this work. 

The paper sells the result as an online algorithm, while the presented algorithm needs many passes over the input until the neural network converges. It should be stated clearly that your algorithm requires multiple passes.
Moreover, I think it would be much more compelling if the paper presented some convergence analysis for the training of the neural net. In particular, it is not clear if SGD on this network would find the optimal solution or how many epochs are needed for training the model. Basically, the paper presents an algorithm but it does not analyze the algorithm in terms of convergence, generalization, or runtime.
"	2
The paper is overall of good quality. While the empirical novelty is somewhat limited, theorem 1 is insightful. 	3
Even though the problem is an old one that has been well studied, the theoretical results provide a very refreshing and novel perspective. The experimental results are also precisely the right experiments to complement the theory. 	3
Overall, I think this is a well-written paper with an argument for the limitations of an important class of models. I did not find some of the experiments as convincing as the rest of the paper. 	3
"Overall, I find that this paper provides a useful extension to knowledge around VAEs. It provides a new connection between tight EF-VAEs and RBM-like models (EF-Harmoniums), and shows that the consistent subset cannot be enlarged by using a more complex encoder network if the encoder's sufficient statistics remain unchanged.  The experiments in sec 4.2 show interesting results about the ""unlearning"" of the true generator when the ELBO is optimized, and sec 4.3 shows that a more careful modeling choice (using word counts rather than frequencies) inspired by Theorem 1 leads to better ELBO performance.
"	3
This is an insightful paper on modeling the gradients that contains an extensive empirical evaluation. My main concern is in the presentation of the humanoid results.	2
"**Edit during discussion phase:**

I have increased my overall score from 6 -> 8 based on the ongoing discussion. I have also increased my ""empirical novelty"" score from 2 -> 3 because I believe the new ablations and clarifications yield deeper insight into the impact of using first-order information to constrain transition models in RL.

---

(Copied from above Main Review)

I am recommending to accept this paper because its algorithm construction is well-reasoned, the theory appears to be novel and provides appropriate support for the proposed algorithm, and the empirical results provide some minor support for the claims. Overall, I find that this paper contributes an important perspective on the role of using differentiable models, and the need to control the error of first-order information (gradient model) instead of only zeroth-order information (prediction model) when using differentiable models.

I remain on the fence, however, because (W1) the empirical results provide only marginal support for the proposed algorithm and (W2) there is a disconnect between the paper's stated goals and the novelty the paper _actually_ provides, leading to neither being done particularly well."	4
The paper proposes a novel solution to a well known problem in model-based RL. Though, the paper is not well written so it is hard to follow, even for an expert in the area. In its current state, I do not recommend acceptance because of its writing / presentation / lack of conveyed intuition, but these are problems that can be fixed in a moderate revision.	3
"There are some issues in the experimental results and in the justification of some moving parts of the algorithm, but the paper proposes a simple and seemingly sound solution for a very important problem in model-based reinforcement learning. I thus lean towards recommending to accept it.

---
The authors addressed my main concerns. Thus, I now recommend acceptance."	3
"In my opinion the technical novelty introduced in this paper is too limited and also its experimental validation should be improved by considering more relevant methods for comparison.
"	2
Overall, I think the proposed method is effective, but, especially given that the technical novelty is limited, it should be better positioned: if the main goal is imperceptibility of the perturbations it should be compared to other kinds of attacks beyond $\ell_\infty$ ones. Otherwise, the authors should better motivate why improving the $\ell_\infty$-attacks with such fixed threshold is relevant.	2
In general, I found the paper easy to follow. I also find the direction explored by the paper to be quite promising but found the experiments & the baselines chosen by the authors are lacking. I would encourage the authors to include a diverse set of baselines. For instance, I would also demonstrate how the optimization proposed in the paper improves gradient-free black-box adversarial example generation techniques other than the chosen few. Also, evaluate the evasion rate of adversarial examples generated by the proposed technique against a diverse set of detection mechanisms.	2
Overall, I liked the idea of this paper, and enjoyed reading it. However, it needs better justification on motivations, as well as improvement on experiments.	3
My main concern is that the original contribution of this work is not significant enough for being published as a conference paper.	2
The contribution is not enough experimentally and theoretically to be published. Although the proposed framework seems beneficial for the community, it lacks a detailed comparison regarding the usability or performance compared to existing frameworks, e.g., PyMC3. Regarding the technical contribution, the extension of stein mixtures seems promising, but it lacks the explanation, numerical experiments, and theoretical justification.	2
"The paper does not bring anything interesting enough research-wise and such a paper should be submitted to a software venue/journal like [JOSS](https://joss.theoj.org/).
"	2
"A nice implementation of an already known method that could be used for machine learners using prob. programs.
"	1
The paper is easy to understand, and has its contribution and novelty. Many experiments have been conducted, but theory is a bit lacking. I am willing to increase my rating if the authors can respond to my comments.	2
"Overall, the reviewer tend to vote for accept for this work since the proposed method is simple and it has conducted thoughtful experiments to demonstrate the effectiveness.

The reviewer encourages the authors to speed up the proposed method, make the comparison with previous methods fairer and try to test the method on different architecture."	3
The variance-invariance-covariance framework is insightful, but the experiments are not so convincing.	3
I vote to reject because the contributions of the paper are not well demonstrated in the paper.	2
This work proposes to alleviate ARGM existing problems by leveraging the EBM objective. Such integration, however, is not essentially new and this work does not demonstrate its superiority compared with other methods.  Despite three experiments performed in this work, the evaluation and analysis are insufficient.	2
Overall speaking the paper is well-written and well-motivated. The direction of joint training of AR model and EBM is promising. However, I'm sort of concerned with the computational cost and training instability introduced by the extra EBM module, compared to the performance gain it brings. 	3
"The proposed method is novel and interesting, but it only compares to traditional autoregressive models in experiments, and other energy-based model baselines are missing.
"	4
An interesting idea that can lead to improved performance on certain benchmarks. However, deeper insights about the effect that the model addresses (exposure bias mitigation) as well as estimations of the increased overhead are missing in the experiments, thus limiting the scope of the paper. 	3
There are some loose threads have been left by the authors. I would increase the scores if the authors address some of the points above.	2
The paper would benefit from more discussion on the related works and analysis similar to the one presented in the work by Balcan (2015).	3
The problem setting, the algorithms and the results require considerable clarification. The proposed methods can have large memory and computational complexities, which may be problematic for large state spaces. The technical contributions seem a little incremental.	3
Interesting paper with important theoretical contributions. There are issues about the motivation of the model.	3
A perhaps interesting theoretical result, but poorly motivated and very narrow. 	2
While I think this is a solid theory work, it is better to add some discussions for the tightness of these generalization upper bounds, and some empirical validation experiments.	4
"The paper provides new results in high probability that are valuable confirmation that standard algorithms will work well. The presentation leaves something to be desired, but is likely fixable in revision.
"	4
"Overall, the paper presents very nice high-probabilistic generalization bounds for minimax problems and improves the existing ones which are mostly given in expectation.  The main proof techniques are adapted from Klochkov and Zhivotovskiy (2021).  The paper is also well organized and related work is sufficient.  However, the claims about significant improvement over existing work and providing sharper bound $O(1/n)$ are not precise and need to be corrected.  The proof for Theorem 7 is problematic since SGDA is not uniform stability as far as I know:  it is uniform stable only after taking expectation w.r.t. the internal randomness of the algorithm.


**The authors have addressed main concerns on uniform stability of SGDA. It is a nice extension of the existing work. I only have minor suggestion on the final version of the paper about modifying the abstract in a modest tone and the caption to Table 1.   I will increase my score.**"	3
The paper is nicely written. There is a concern regarding the main theorem's dependency on the (1+eta) factor for the empirical measures. This also leads to a concern about the fairness of comparisons with related work. 	3
While I'm not fully convinced by all the experimental results (see comments above), this is an overall interesting paper because the method is conceptually simple and the results are looking promising (at least in terms of rank correlations). 	3
This paper is well-written and the analysis is insightful. However, some related papers are not taken in the comparison, such as BigNAS and Cream.	3
The idea is not so surprising and the experiments can not convince me. I tend to reject this paper.	2
I generally like this idea, although seems straightforward, applying meta-learning during training the supernet is a reasonable and correct strategy. The analysis and experiments are also very comprehensive. I vote to accept.	3
In general, this paper's proposed method has merits. But unfortunately, it missed some important related works and experimental results are not conclusive enough yet.	2
Overall, I think this paper proposed some very novel and insightful ideas with well-run experiments that would be beneficial to the community. I think this paper passes the bar for acceptance.	3
The motivation of the suboptimality is sound, but the algorithm to solve this problem is not very convincing.	3
In its current form, I cannot recommend this paper for publication.	2
"I really appreciate the promise of the paper and the direction pursued by the authors. However, I have some concerns regarding the theory-practice mismatch within the paper. Unfortunately, until Section 3.4, the writing is as if the authors will analyze AI-SARAH which is implemented in practice, however Section 3.4 introduces many modifications on the algorithm. On this front, it is not explained if the analyzed algorithm does really work well in practice or how much theory can be shown for the implemented algorithm. 

I am willing to increase my score if the authors reply to my questions in a satisfactory way."	2
"Although the authors make some contributions in this paper, the weaknesses above are very critical to evaluating the contributions of the proposed method. I would like to vote for 'reject' if they are not addressed well.

After the discussion, some of the concerns are solved. I will increase the score to 5."	2
"The reviewer does appreciate the efforts in the empirical study. However, the reviewer has a few comments about the gap between theory and practice. 

1. The strength of the proposed algorithm is ""tuning-free"" and ""parameter-free"". However, to theoretically guarantee the convergence, the author actually requires knowing the local Lipschitz constant in the set $\mathcal{W}_k$, see Eq(6). 

2. The authors argue that as the iterations approach the optimal solution, the curvature may tend to be flatter. Therefore, the global Lipschitz constant may be much larger than the local Lipschitz constant, resulting in a more conservative stepsize, i.e., the stepsize $1/L_{global}$ may be much smaller than the stepsize $1/L_{local}$. However, since $\alpha_{max}^k$ is already $1/2L_{local}$, doing a line search only results in smaller stepsizes. This contradicts the philosophy of using larger stepsize when possible. Therefore, the theory does not explain the strength of the algorithm. Moreover, if $1/L_{local}$ is known, why not directly apply SARAH with this local parameter? 

3. There is a small flaw in the logic of the proof. First, the authors define the set \mathcal{W}_k:= {w:||w- \tilde w_{k-1}|| \leq m\alpha_{max}^k||v_0||}. The authors argued that $\mathcal{W}_k$ is the set where all the iterations of the $k$-th epoch should lie within. This only happens when all the later gradient estimators $||v_t||\leq ||v_0||$ almost surely, otherwise the proof wouldn't work. However, the authors are not able to show this because all the descent results are in the sense of expectation instead of almost surely. 

4. With $\alpha_{max}^k = 1/2L_{local}$, the analysis is more or less the same as the original SARAH algorithm, with little modification. Therefore, the current analysis is not very interesting to the reviewer. However, a much more interesting thing is how well the proposed procedure approximates the local gradient, this has never been addressed in the existing works. However, with the current assumptions on $\alpha_{max}^k$, this challenge is left undone.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
After the rebutal, the authors have cleared my question in comment #3. The issue stated in this comment can be cleared with a few more explanation. 
"	2
I reject this paper mainly due to the above concerns 1-3. Once they are solved, I will change to accept.	2
Given the concerning technical correctness, poor novelty, significance and writing, I recommend to reject this paper.	2
I would suggest the authors choose another conference.	2
This paper targets on some important issues in generative modeling such as unstable training process and mode collapse. Novelty of this paper is low and evaluation is not sufficient.	2
"
Strengths:
1. Very detailed experimental setup is given for each experiment, which helps reproducibility.
2. It considers two evaluation tasks at one time -- causal graph extraction and TS reconstruction (not forecasting).

weakness:
1. The presentation really really needs improvements.
2. It only compares with one baseline ACD, only in linear data. In the spring data they did not compare the proposed method with anything but just show a sensitivity analysis on the number of states."	2
In summary the paper is promising, but not ready yet. A better embedding in the existing research, a few theoretical results (mostly about making explicit under which assumptions the graphs learnt are actually causal) and a more thorough evaluation would improve it a lot.	2
I like the approach, but I am not sure if it is ready enough for publication. I thus initially recommend borderline reject.	3
"Interesting setting, but essential theoretical results are missing, and the paper organization and writing needs improvement (some parts are inconsistent throughout the paper)
"	2
"Efficient supervised learning of deep SNNs is a problem that has resisted research efforts for many decades, so even incremental improvements are welcome in this area. The results of the paper offer a much desired step towards better SNN backprop algorithms that promise to unleash the many advantages of SNNs.  
The paper is lucidly written and provided experiments strongly support the claims. "	3
Based on the main concerns listed above, I recommend for reject for the current form of manuscript. 	2
"1. Missing reference: There’re some recent papers that should be discussed in the related works, e.g. [1]
2. I'd like to learn more about the details of data preprocessing. For example, does the real value in CIFAR and ImageNET datasets directly used as the inputs? In DVS-CIFAR10, how does the resolution reduced? How is the time length shrunk to 10 time steps? 
3. Performance comparison: In CIFAR10, can the authors also provide the performance of the 5 layers CIFARNet for better comparison with existing works? In DVS-CIFAR10, do other works also use data augmentation, e.g.[2]? Does the network adopted in this paper have a similar number of parameters to the existing works in the comparison?



[1] Yang, Y., Zhang, W., & Li, P. (2021, July). Backpropagated Neighborhood Aggregation for Accurate Training of Spiking Neural Networks. In International Conference on Machine Learning (pp. 11852-11862). PMLR.
[2] Zhenzhi Wu, Hehui Zhang, Yihan Lin, Guoqi Li, Meng Wang, and Ye Tang. Liaf-net: Leaky integrate and analog fire network for lightweight and efficient spatiotemporal information processing. IEEE Transactions on Neural Networks and Learning Systems, 2021."	3
The novelty is limited, as the core idea and the relative derivations are straightforward.	3
The results presented by the paper are not so surprising and lack some novelty. The evaluation also needs additional comparison experiments to better support the conclusion that smoother adversarially-trained models enhance robustness.	2
As the paper does not read well, and also considering that its novelty compared to prior works is unclear to me, it might not be relevant and valuable to the community in its current form, and as such I cannot recommend accepting it. That said, I think the paper does contain some good experiments that, if explained with more clarity and better placed with respect to what already exists in the prior work, can be helpful to the community.	2
"- The experimental results and claims are not novel while this paper does not provide theoretical results and new algorithms. .
- The paper's assumptions are not well defined and its claims and limitations are not clear.
"	1
My majorpr concern is the novelty.	2
The paper lacks novelty and writing need improvement.	2
In this paper it is proposed a deep learning based approach for video deepfake detection. In my opinion the technical novelty is not sufficient to warrant publication in ICLR and the performance improvement is marginal with respect to state-of-the-art.	1
I would recommend accepting this paper. The technique part looks sound to me. The two-branch network structure, the feature interaction between EfficientNet-B5 and Swin transformer, as well as the use of attention and new design of loss function for EfficientNet-B5 make the proposed method solid and convincing. My major concern is from experimental results and ablation study.	3
Interesting observation and good intuition, but there is a noticeable gap between the designed method and their intuition. I agree the proposed IAD empirically works, but the intuitions provided in the paper to motivate and interpret IAD is problematic. 	3
Basically, this paper is well written and the proposed method seems promising. I thus would like to lean on the acceptance side. 	3
This paper investigates an interesting problem about adversarial distillation, which has some distinct with the ordinary distillation. The authors provides an introspective adversarial distillation to solve the problem. Despite simple, it shows consistent improvement on the model robustness while does not sacrifices the natural accuracy too much. However, the writing about the motivation and the loss designed for each group as well as the experimental parts are not very clear and need to be further improved.	4
In general, considering the significance of the researched problem, this paper can be accepted by the ICLR2022. However, some points should be clarified and strengthened in the revision. I would like to strongly support this paper if my concerns can be fully addressed.	4
"The experiments on geometry value were interesting, and the paper asks an important question in trying to relate label noise and adversarial robustness. That said, for the results on adversarial training mitigating overfitting of noisy labels, I'm not sure I fully grasp what the key new conceptual insights are relative to prior work as I wasn't particularly convinced by the definition of or the experiments on the ""smoothing effect"" of adversarial training. So I'm a bit on the fence and leaning towards weak reject, though I'm happy to update my score if the authors could help clarify my confusions above."	2
An interesting connection is studied and the contribution is clear. But the work lacks theoretical insight and quantitative results. So I would recommend only a borderline accept.	3
Although the use of _PGD step number_ seems promising, there is only very limited evidence to support the claims in the paper. That is, there is neither a rigorous analysis/formulation nor a comprehensive empirical study to justify the use of _PGD step number_. Hence, I believe significant improvement is needed for this paper to back up its claims, leading my decision to a rejection.	2
Based on the questions and concerns above. Unfortunately, I cannot recommend acceptance of this manuscript to ICLR. I welcome the author to post rebuttals and will do a re-evaluation after the our discussion ends.	2
The problem is interesting but the paper suffers from some incorrect claims, insufficient motivation and justification, and underwhelming empirical results. 	2
The paper has a useful motivation and explores interesting idea, but the aspects of the execution and the missing details in the work make it difficult to recommend acceptance.	3
"Overall I liked the paper, as it solves 2 key challenges of program synthesis:
1. The training distribution of the synthesizer does not match the testing distribution
2. One would like to recover the target program with as few examples as possible

It solves both problem by ""mocking"" the active diagnostic process of (2) in training time, having a trained querying network to propose the most informative queries. To build a querying network, the proposed method embed both programs and examples in a joint F-space, and use contrastive learning to train the querying NN to propose query points that maximally distinguish the target program p from distractor programs p'. The construction of F-space is both intuitive and algebraically elegant.

Results give good evidence of the proposed method as well.

This paper would be stronger if the 3 points mentioned: exposition, other baselines, and related works can be addressed, I am willing to raise the score accordingly.

The author addressed the 3 points mentioned, so I am raising the score as I said I would. Although there is no score of 7, I meant a 7 instead of an 8 as the baseline (QBC) is still fairly close to the proposed method, and is derived in much less time (a week)."	3
Despite my comments, I believe that the idea of relying on Delaunay neighbourhood graphs is worth investigating. The paper could improve on many aspects - presentation of the experiment section, intuition behind the proposed method, discussion on hyperparameters and most importantly complexity - however its core contributions are valid albeit with limited novelty.	3
"All in all, I perceive this to be a good paper. The authors do a good job of highlighting drawbacks of prior methods to compare representation spaces and proposing a solution that mitigates them. The use of Delaunay graphs in lieu of k-nn based or eps-proximity based methods is an interesting way to explore the geometry and topology of representation manifolds. The diversity and comprehensiveness of evaluation is also quite good. 
"	2
The idea of using (pruned) Delaunay graphs to approximate continuous feature spaces is novel and technically sound. In my view, it has a lot of potential utility for analyzing high-dimensional embedding spaces, an important open problem. I am further convinced that this formulation is an improvement over related approaches such as IPR (Kynkaanniemi et al., 2019) or GeomCA  (Poklukar et al., 2021). On the other hand, several issues with the clarity of the presentation (especially in Sec. 4) obscure this main message. Overall, I am inclined to give the paper a more positive rating and hope that the authors can provide clarifications.	3
While the presented premise and the theory of abelian neural networks is quite interesting and may have interesting applications, the experimental section seems weak and needs additional results/motivation.	3
Novel and interesting idea of adding explicit Abelian constraints into neural networks with proofs that AGN is able to model Abelian relations. However, the scope is limited to word analogy and experimental results do not provide good support that AGN is advantageous than word vectors and MLPs.	3
This paper describes a simple and interesting method for modeling unordered data that has some nice theoretical properties, like generalization from smaller to larger sets. However, it is applied to only one problem, the word analogy problem, where the sets are always of size 3 and the importance of commutativity could be made more clear. Experimental results are mixed, with the proposed model performing the best on BATS but not on the Google analogies dataset.	3
"An interesting idea but implementation is problematic.
"	2
This paper is well-written, proposes a highly innovative model that is consistent with behavioral and neural data, and obtains excellent results. The paper addresses a long-neglected aspect of human vision (fixation drift) in neurocomputational models of human vision, and shows that it has efficacy in challenging conditions. This result suggests that it can be used in engineering applications where the stimuli are low-resolution. It has some confusing parts, but these can be fixed by the authors. 	4
"The paper lacks a clear demonstration of usefulness: either an improved fit to biological data (since the motivation starts from limited sampling in the retina), or a clear use case in computer vision. Since neither is demonstrated, I find it really hard to contextualize the work and cannot tell if the proposed model makes any improvements over previous models (see the main review for detailed suggestions). The use of a full-resolution teacher network is also not well motivated especially in connection to biology, and the second half of the paper is a bit hard to follow (i.e. what to take from the feature visualizations).

REBUTTAL UPDATE: I have increased my score following the authors' attempts at connecting to biology more directly, but I still believe key comparisons are missing: either a stronger link to biology and concretely relating model predictions to experimental results, and/or explicit comparisons to alternative models in ML tasks."	2
In summary, I found the core idea of author's network interesting; however, the author's claims are currently not justified by the results. Many more controls are needed to ensure that the DRC network performs better than alternatives. If the DRC network does perform better than all other control networks, then additional analysis is required to understand how the network is able to improve its performance.	3
Although authors proposed an interesting solution to a very hard problem, I believe proposed method and experimental work needs to be improved especially taking in to account missing references and supplying a detailed discussion. 	2
"Overall I feel this is a reasonable submission but seems not good enough for ICLR publication, so I gave "" marginally below the acceptance threshold "". "	2
While the extension of a supervised multi-manifold embedding to the manifold clustering problem is intuitive and interesting, there are several imprecise concepts underlying its description, and the carefully crafted implementation muddles whether the proposed approach can have wider applicability.	2
The paper combines two previous lines of work into a novel loss function which is then used in both supervised and unsupervised manners and achieves state-of-the-art results. It is currently below the acceptance threshold because the presented algorithms’ description and justification are lacking.	2
The proposed method builds upon the existing existing Maximum Coding Rate Reduction (MCR2) objective with a data augmentation as a constraint to enforce manifold learning. It seems that the performance is satisfactory comparing with other state-of-the-art methods.	3
The novelty and the contributions in theory are not obviously presented. 	2
Overall, both the theoretical results and experiments can be strengthened to make a better case for the proposed algorithm. 	2
"Although the current study discusses a vital topic in MARL, the current study focuses very limited class of MARL with quite strong assumptions. The theoretical analysis can be possibly considered novelties for the current study; however, the importance and the meanings of the theoretical analysis are not fully explained in the present paper. Finally, the experiment results are too limited to validate the effectiveness of the proposed method.

"	2
"The paper proposes a theoretical and methodological framework to
achieve robust  point set registration using scalable partial
Wasserstein based on the dual formulation. Their derivations somehow
derive from existing works. To be efficient, the  algorithm
relaxes the
Lipschitz constraint hence, the overall algorithmic scheme lacks
theoretical  guarantees on the matched distribution. Also the paper
lacks convergence, time complexity analysis of computing PW. Finally
connections to works related to minibatch unbalanced optimal
transport that can be competitors of the method are missing. "	2
The theoretical contributions are justified with experimental evaluation on the point set registration task. The experimental evaluations could be more extensive, for example, other related tasks.	3
An interesting paper deals with an important problem but lacks some appropriate references.	2
In summary, the work can be seen as well founded paper with clear theoretic contribution equipped with a practical method based on neural formulation of the registration problem. Good results on the selected datasets, though an evaluation with a more complicated datasets would improve the presentation. 	3
Overall, It's very nice to have a method to avoid the computation of dense matching matrix in an optimal transport framework. However, the limited experiments have not convinced me of its effectiveness on real-world datasets.	3
Good paper explaining the recent empirical results against DG algorithms	4
"

To summarize above, I find the theoretical results are somewhat unsatisfying due to limited domain numbers in practice. The experimental parts report results on small models, which contradicts some existing results on larger ones. All in all, I find the paper in its current form to be somewhat below the standard for ICLR acceptance, unless the authors address some of the points above."	2
"On the one hand, I think that this paper offers an interesting perspective for thinking about DG.  The theoretical work gives a new way of thinking about the accuracy-complexity trade-off in DG, and this leads to some interesting experiments.  However, I think that the paper tends to overclaim, in particular with regard to ""solving DomainBed"" and the implications of its theory for ERM.  Furthermore, I think that the writing is particularly unclear, and that the claims made in the introduction are not fully validated (e.g. the claims about the ""causal"" factor behind poor performance in DG).  Overall, despite the interesting insights, I think that this paper is not yet ready, and so I recommend that it not be accepted."	2
The paper has promising motivations but lacks of theoretical significance for DG community.	2
To sum up, the paper can be employed as a practical guidance for researchers to conduct AL for image classification. However, minimal analysis or insights are given for the results to be generalized to other datasets or methods. Thus, my initial rating on this submission is borderline reject.	2
Overall, the experiments are extensive and produce many findings that could help understanding of existing pool-based AL methods. The new codebase could provide solid benchmarks for making fair comparisons. Although some adjustments may make a better contribution, this paper is marginally above the acceptance threshold.	1
"This paper reproduces state-of-the-art results of popular AL methods with different datasets and settings typically used in the literature. It also highlights some approaches (sections 5, 6, and 7) that can influence the performance of AL methods by comparing only test accuracy with respect to a number of labelled points. It is not clear how many fewer labels a selection algorithm needs in order to achieve that test accuracy with respect to random sampling. They also provide a new Pytorch codebase that will allow future researchers to evaluate and compare different strategies in a fair way. This article lacks details on the labeling efficiency of different methods and their quantitative evaluation as done in Beck et al., 2021.
"	2
"I believe more thorough experimentation is needed for backing up conclusions in some parts like the effect of optimizers and backbone. It would be very valuable to compare popular semi-supervised methods in the section on using unlabeled data.  
"	3
"This paper presents a great and potentially very useful idea, but does not evaluate it in situations where the idea could reasonably be expected to do something more interesting than the original DT.
Better motivating the types of behaviors that could be expressed and produced via more flexible conditioning, and seeing that through by testing the method on higher-entropy datasets, would make this a much stronger contribution."	2
"The paper is interesting and presents novel methods which may in the future be important ideas leading to new work. Since the results are somewhat weak right now and there are minor clarity concerns, I recommend weak reject.

=====

After rebuttal (11/18):

I think the updates made by the authors during the rebuttal period are significant, adding writing clarity, experiments, and the Bidirectional DT. As I wrote below in my response: ""While the scope of experiments is still limited (as also pointed out by Reviewer Ffah), I think the experiments are thorough and the paper contains interesting insights that will be valuable for future investigation, especially given the early state of literature on transformers for RL; I will thus increase my score from 5 to 8."""	3
"The paper is not ready for publication. The unifying perspective presented in the paper is very interesting, as well as the proposed distributional and unsupervised extensions of decision transformer. However, the evaluation is not sufficient. There are too many variations of the algorithm considered and the experiments are not expressive enough to illuminate them. Generalization only studied in a very limited scenario. No comparison to distributional RL is provided. As a result, the paper is trying to cover too many things at once and it is recommended to restrict the scope and provide more thorough analysis.

====

After rebuttal

The authors addressed my concerns with evaluations and baselines. Furthermore, the connection to distributional RL was clarified. The updated paper is 80% new. With the improvements made by the authors, I have no further issues and raise my score to ""8 accept, good paper""."	3
As a final comment, this work does some contribution for learning and evaluating syntactic disentanglement for sentence representations, and will be helpful to the community. However, the writing of the paper should be improved. For detailed comments please refer to the main review.	3
"To summarize, even though this paper demonstrates that Transformers with $\beta$-VAE can be an attractive option for generating latent variables representing some syntactic roles, its novelty is generally limited to the fact that it proposed a new evaluation protocol aimed at estimating the extent to which each latent variable is mapped to a specific syntactic role.
Therefore, my suggestion is a weak reject."	2
"While the research presented in this study is very exciting, it is not quite convincing enough yet for me to trust that the results are actually due to the modeling decisions presented. Moreover, a similar work (Behjati and Henderson (2020)) is not discussed, which casts doubt on the novelty of the approach. I therefore recommend to reject the paper in its current state, but I am happy to change my scores upwards if my concerns above can be resolved.

* UPDATE:  I raised my score slightly as a consequence of the rebuttal phase, where authors conducted an ablation study that now more clearly shows that the improvements over standard VAEs are due to multi-vector nature of the representation rather than just because of using Transformers."	2
"Overall, although I like intuition and motivation, however, I feel the empirical support is lacking. I would be willing to raise my score if experiments on other aspects or in other languages were done. I believe this would further reinforce the claims made in the paper regarding unsupervised disentanglement.

---
Post rebuttal:

I appreciate the additional experiments the authors provided and their time and effort in writing the rebuttal. This has clarified some of my concerns and added extra evidence to the paper. For this reason, I will raise my score to a 5. Despite this, I still feel that a more general approach, able to disentangle different factors, or in different languages would add more substance that I feel this paper is lacking."	3
In summary, this paper studied how to learn DQN in a fully Bayesian way and is interesting. However too many parts of the paper related to its core contribution is unclear at this time. Unless it's significantly updated then this paper is not ready to published.	2
The paper presents a potentially promising method for Bayesian deep RL that can scale to large states spaces like Atari problems. I do not however believe that the paper is ready to be published as is since the experimental validation and comparison needs expanding, and further discussion on why exactly this method is so useful compared to the current literature would be useful.	3
In summary, the idea of applying Bayesian inference as an alternative to backpropagation-based algorithm is interesting. However, it requires more complete experiments to demonstrate this, along with the other assumptions that simplify the computing complexity. 	2
The paper explores using Bayesian methods to guide action selection in deep Q-learning, and it makes some progress in that direction. I appreciate the time and effort spent in preparing this submission, and I do see some value in the proposed approach. Unfortunately, in its current form, the paper does not have a clear contribution. The paper attempts to answer too many questions and does not sufficiently answer any of them. Additionally, the main derivation is hard to follow and appears to be incomplete or incorrect. I recommend rejection.	3
"Overall, it is a solid paper with clear introductions on the proposed method and good experiment design supporting the main claims. However, the fact that experiments are all based on one kind of CNNs is concerning. I would consider it as a board-line paper with the current version.

-------------
The author feedbacks are satisfying. The improvements on the paper with answers to Q1&3 are substantial. I've raised my score."	3
Overall, I think the paper is interesting and the ideas are worth pursuing. The writing needs significant improvement, the experiments are still limited.  I am not sure also not sure about the impact of the paper, in its current form, and its utility for enabling improvements in design of neural net architectures.  	2
Overall, the paper is interesting and the proposed approach of measuring unit importance is well motivated. However, as mentioned above, I do have some concerns regarding the empirical evaluation of the proposed method.	3
"The effects suggested by the proposed method are very useful and can be considered a sufficient contribution. On the other hand, the proposed method is heuristic and limited, and there are some questions about the theory behind feature extraction, which require deeper investigation in the future.

[Additional comments after discussion]
Through discussion and revision, some of the unclear points were clarified and the paper became better. The proposed tool has the potential to be a very useful tool and its contribution is appreciated. On the other hand, this is something that needs to be developed in the future. These points were taken into account at the beginning, and although they are relatively positive, the score remains unchanged.
"	3
The main reason that I give a slightly negative score in the current is that I think the experiments are not supportive enough. Different from some theoretical causality paper, this paper tackles a practical and widely researched problem. Hence I think the experiments need to be designed carefully. I look forward to more clues from the authors. If they could address my concerns and questions, I am very happy to increase my score.	2
Overall, the proposed method is interesting and novel. However, since the proposed method is based on a heuristic without theoretical analysis, as also acknowledged by the authors, I think it is important to demonstrate empirically with a fair experiment setup that the proposed method works well compared to the existing baselines.	3
Ultimately my opinion is that without additional theoretical results or better justification and exposition of the proposed approach to advance the state of the art, the contribution is limited.	2
As i mentioned in the main review, I believe that the main ideas of the paper are interesting. However I find that the presentation of this work is not very clear and somehow confusing. In particular, the structure of the paper and the results presented in sections 2 and 3 are difficult to absorb. See my comments below. 	3
This paper provides new insights into the performance of Adam, and proposes a novel optimizer that both converges fast and generalizes well. Further improvements can be made by comparing the proposed method to SGDM more thoroughly.	3
This work provides some new theoretical insights for momentum SGD and Adam, which are interesting and important. The authors then propose adaptive inertia based on the insights, which shows good performance. Some revision is needed to make the story clearer (see main review).	3
The presentation and statements are confusing in my opinion. Some steps in the analysis are not transparent. 	2
The review was written after the rebuttal phase, and authors' responses to other reviewers' comments were taken into account. I find this paper to be a sold work, a simple idea with good empirical evaluation. In my opinion, the authors also did a good job answering the reviewers' comments. However, I still have some other doubts, so I believe that right now, this paper is marginally above the acceptance threshold - I recommend it for acceptance, but I will not be upset if the paper is rejected.	3
Overall, the idea of utilizing the contrastive loss during the training of multi-label image classification network is appealing, however the limited novelty and experiments are the main concerns. Authors should carefully address above concerns (weaknesses) during their feedback, the final recommendation will be made upon the feedback.	2
"This paper presents new state-of-the-art results on MS COCO and NUS-WIDE for multi-label classification. It leverages contrastive learning to enhance distinctiveness for great performance in multi-label image classification. It is also well organized and written. 

The proposed label-level embedding network is conceptually similar to visual transformer or other attention variants. I am not sure about the novelty at current point. The results were demonstrated on MS COCO and NUS-WIDE only. I am not sure if it is scalable to larger datasets (like open images dataset)."	3
Considering the limited contributions and unclear sources of the performance gain, I think this paper is marginally below the acceptance threshold.	2
The proposed bound is non-trivial and potentially useful; however, it would benefit from additional context to show whether it is truly significant. The empirical results are interesting, but rely on a heuristic that is not well-motivated and may be domain specific.	3
This paper provides rigorous analysis for using reinforcement learning in imitation learning. The result is novel and shows the potential to accomplish imitation learning in a straightforward way. The intrinsic reward function reduced by the expert dataset is simple to construct. By setting a constant one reward for matching the demonstrated action in a demonstrated state, any reinforcement learning algorithm can provide policy for such a MDP problem. The result in this paper gives intuition of how this method performs in terms of expected return. And sufficient experimental results are shown to verify their theoretical results.  	3
"In sum, the paper seems to contain potentially interesting results. 
The overall organization and presentation can be further improved to improve the readability of the paper. "	2
The paper shows that, for deterministic experts, imitation learning (IL) could be reduced to RL with a stationary reward. The paper thus provides theoretical grounding for several existing works. In addition, the paper also draws connection with f-divergence minimization commonly found in adversarial IL. On the other hand, the empirical results are consistent with those from existing works, but are limited in its scope. Despite the flaws, I think the work is still interesting to the community. It would be nice if the paper could include more experiments to further support the theoretical analysis.	3
Writing is clear, problem is significant, and proposed methods are novel. I did not find specific issues in this paper (although I did not go through the details of appendix A).	4
While the theoretical derivations and the idea of Brownian bridge are interesting, and potentially a breakthrough, I feel that the theoretical results alone are not sufficient for publication. Changing the reversible SDE to BB is still somewhat incremental proposal, and thus the experimental demonstration would have been necessary to make the paper strong enough for publication. Furthermore, the presentation of the paper not good enough for a publication.	3
"The main idea of the paper is sound and original. However, I think that this
work might not be mature enough. I would like to see more theoretical and
experimental comparisons with existing works to assess the efficiency of the
proposed method. For these reasons I recommend the rejection of the paper but am
ready to raise my score if the authors provide compelling justifications for
your method."	3
I found the paper hard to read - but found the ideas interesting.	2
"This paper combines the use of a replay buffer and a model to lower the on-policy model error. The experiments seem convincing and the method is very well analyzed.

While I am not familiar with the related literature and I might have missed something important (hence I lowered my confidence accordingly), I have the impression that this work is solid and would bring a nice contribution to the conference."	3
Overall, I'd say this is a relatively thorough examination of a simple but seemingly effective technique. As far as I'm aware, the approach is novel in the context of MBRL, and I can see it being widely used by the community, so I recommend acceptance. I have concerns regarding Section 3.1 which is the main motivation of the method, which limits my score. 	3
"The paper considers a straightforward combination of two methods which does not seem highly significant. 

The paper is also somewhat lacking in its analysis and empirical claims."	2
While this looks like it is a solid paper on what is arguably a somewhat incremental idea, I have some issues with the experimental results that need ironing out before I can accept. 	2
Empirically, applying CycleGAN to reveal the mapping of pre- and pos-learning neuronal activities shows good results! Although the architecture or method novelty is not significant, and there is some unclarity of the writing, it can be a good starting point for further explorations.	1
The paper addresses an interesting neuroscience problem from a unique perspective; however, I am still unclear exactly what the method is learning, and how it can be used to gain additional insights into the data. My initial assessment is to not accept this paper.	3
The motivation for the methods presented in the paper are not clear, the conclusions are not very convincing and consequently, it is hard to judge the contributions of the paper. A lack of clarity in the methods and results section exacerbates this.	2
I lean to accept this work since it provides a new view of pretraining and the proposed approach receives significant improvement on four datasets.	3
This paper is well-written and very informative, introducing `TAPEX`, a pretraining method learning SQL executor, with significant performance improvement. The novel design of pretraining is very extensive and proven to be effective on TableQA setting. While some more experiments could be conducted to further understand the performance gain, the current paper has done a lot of ablation study, showing the impact of pretraining data. I recommend this paper to be accepted.	4
"To sum up, the paper has strong empirical evidence to support its simple yet effective approach. Though the wording and claims are sometimes not quite accurate, it's still a very good paper. I believe it can arouse lots of interest in the community and persuade more people to abandon ""mined large noisy corpus"" and turn to ""accurate and small corpus"". Based on these contributions, I would recommend accepting this paper."	3
A simple and effective pre-training strategy for Table QA. The authors proposed TAPEX, a model trained on executing SQL query rather than MLM, a new SOTA in 4 benchmarks, and many analyses and visualizations. 	3
Overall, I think the paper is not ready for publication due to the reasons I listed in the previous section. 	2
"The paper explores an interesting idea for compressing DNN networks, e.g., through lossless compression. This could be potentially quite impactable but the current state of the paper raises several concerns:

1. There have been significant missing discussions related to model compression. In particular, if we look that the related work section, it is very weird that most of the cited papers in Section 2 are work done prior to 2017, whereas there have been vast advancements for model compression in the past several years that the paper seems to choose to ignore, such as [1-3]. Of course, many existing compression techniques are not lossless in theory, but they often achieve comparable performance as the original model, e.g., quantization, that cannot be dismissed completely. 

2. The evaluation section is weak. The comparison is made primarily with the Huffman coding method used in Deep Compression [4], which was done six years ago and no longer represents the state-of-the-art. Furthermore, the evaluation was also done on AlexNet and VGG, both of which have been heavily studied. To be more convincing, the paper should compare at least with some lossy compression techniques such as integer quantization and one more advanced model architecture such as ResNet and Transformers. 

3. The paper lacks descriptions of implementations and how speedups are calculated. Is the speedup measured through proxies or measured execution time? Is Succinct Data structure GPU hardware friendly? How does it perform on GPU in comparison with models served via cuDNN?  What are the implementations used for the baseline? 

4. The paper would benefit from thorough proofreading.

Typos:
1. Missing references, e.g. Huffman Coding (?), in the first paragraph of Section 6. 

[1] Dong et al. HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision, https://arxiv.org/abs/1905.03696, 2019

[2] Choudhary et al. A comprehensive survey on model compression and acceleration, 2020

[3] Deng et al. A comprehensive survey on model compression and acceleration, 2020

[4] Cho et al. On the Efficacy of Knowledge Distillation, 2019

[5] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, Han et al., 2015, https://arxiv.org/abs/1510.00149"	2
Overall, the paper presents an interesting idea. It is relatively easy to follow it, but the clarity of the paper needs improvement. The experimental part needs additional work, as already discussed. There are major claims which might need to be reformulated. There are also missing references at different points. Overall, the work is not ready for publication yet.	3
I feel that the description of the work needs to be improved. The evaluation also needs improvement. For these reasons I am recommending the paper is rejected.	2
The paper proposes TRAM, a method of integrating privileged information in the network weights during training. Experiments on both realistic and synthetic datasets show that TRAM can help improve the model accuracy on noisy data. However, the improvement is significant. Further, the lack of detailed investigations about how the privileged information is integrated into the network weights and how the privileged information help explain away the label noise in realistic deep neural network models limits the contribution of the present work.	2
I think that the main idea, directly approximating the conditional marginal distribution without requiring any sampling methods or additional specific architectures, is interesting (although I think the proposed methods are not super novel). I also like the way the authors build their methods from theoretical analyses and motivating examples. However, I at the same time think that the manuscript could be improved. Please see the detailed comments above. 	3
"This paper proposes an intuitive phenomenon that using PI is helpful and improves the performance of the model. However, the authors have not given any proof for the general case (not the edge cases) theoretically or empirically successfully.
"	3
"The paper proposes a new perspective on the noisy learning literature. The proposed framework is simple but useful.
"	3
Overall the paper gives interesting generalization bounds for the metric learning setting but does not seem ready for publication.	3
In a word, I think this paper does not provide a new theoretical guarantee for nonlinear metric learning, and its results are basically the same as the existing generalization theory of DNNs.	1
"Paper is not fit for publication in its current form: it should be structured more coherently and the results discussed in more context.
"	3
Overall the paper presents a nice adaptation of the bounds from Bartlett, Foster, and Telgarsky for the problem of metric learning. I think understanding metric learning is an important problem in machine learning. However, I am not convinced that the improvement in the bounds is significant. However, my opinion on this can be changed. 	3
The manuscript proposes an interesting point of view on graph convolutional operators. The proposed idea is clearly defined and theoretically grounded. The empirical evaluation of the operator shows interesting results, but some points about the experimental methodology have to be clarified since the comparison with the baseline models seems not completely fair.	2
"
While the experiments are well designed, given that there are very few comparisons to similar works that claim to solve the problems of oversmoothing, and limited theoretical contribution I lean towards rejection."	1
Overall, I recommend marginally below the acceptance score. My concern is about the limited novelty and not enough experimental results. Hopefully, the authors can address it.	2
"My concorns are mainly from two aspects:
- The novelty of the proposed method is limited. The contribution of this paper needs further discussion between BankGCN and other efforts on designing filters from K-order polynomial function space. 
- The current experimental analysis is insufficient and can be further improved to make it more convincing.
"	2
Interesting work. However, there are several related methods in the literature.	3
Overall, I think that the paper is quite strong and the method proposed contributes some quite interesting ideas for effectively addressing the challenging problem of unsupervised pose-aware generative part decomposition. The experimental evaluation is comprehensive and follows correct practices.	3
While I like the paper, some aspects are unclear to me and I expect the authors to clarify them during rebuttal. Regarding the paper's clarity, I am confident that the paper can be significantly improved if the authors spend some time restructuring and rewriting parts of the text. Finally, I believe that showing additional qualitative results and ideally also providing results on a more challenging dataset would make the claims of the paper significantly stronger. Therefore, I think that the paper in its current state is below the threshold of acceptance. However, if the authors address my concerns I am happy to increase my initial score.	3
I think this is a nice written paper with a good technical contribution. The paper is well written and the results look convincing. Due to the specific scope of the paper (i.e., articulated objects) and the lack of works with this specific goal, comparisons to SOTA methods are somewhat difficult to parse. Nonetheless, seems to propose a step forward in the scenario that is designed for. 	3
Overall, the proposed unsupervised parts segmentation approach from disentangled shape and pose is novel. However, the paper is not well-written. Details are not clear to the reviewer. The results are not quite promising as demonstrated in Table 4. 	4
I think the paper proposes a new algorithm that tackles a somewhat new problem, but the methodology lacks originality and the hyperparameter details are missing. Therefore, I would vote for a weak reject.	2
This well-written paper studies the important problem of transferring knowledge to improve sample efficiency for offline RL. The proposed reward augmentation framework is well justified for both the model-free and model-based settings from the perspective of variational lower bound and model-bias when the dynamics shift, respectively. Also, the experiment results support authors' claims well. Overall, this is a good submission that can inspire future works along this direction.	3
I recommend acceptance. The paper introduces a framework to adapt to the target environment with a relaxed requirement on data from the target task. I believe this work contributes to improving the data efficiency in the offline reinforcement learning setting, even when the access of the reward function in the target environment is limited. There exists shortages and limitations as listed, but overall, I consider the positives to outweigh the negatives.	4
"Overall, the given work is theoretically grounded, providing incremental improvement over the past work in this area of tackling distribution shifts for offline RL learning. The results seem promising. However, the paper is not the easiest to read and interpret.  I'm willing to reconsider my decision, based on the feedback addressed by the authors.

Edit: Updated score based on reviewer feedback
"	2
The work is incremental and uses already existing building blocks	2
The three important aspects of a good paper, i.e., contributions, novelty and performance, are missing in the paper.	1
The proposed model seems to lead some performance improvement in some benchmarks. However, the proposed methodology is not entirely convincing, requiring deeper analysis and justifications compared to recent GAN-based models. Also, it is hard to get a clear idea on their methodology in detail, as the paper is poorly written. 	2
This paper works on unsupervised low-light image enhancement. The novelty is weak. The organization and writing are poor. The validation needs improvement.	1
The novelty of the proposed method is limited and the empirical evidence is not strong enough to justify the method.	2
"This paper adapts DynSparse training to BERT to demonstrate its effectiveness. However, evaluation is somewhat narrow and it's not well justified why random re-allocation of weights works better than gradient-based re-allocation for BERT. Also, the performance gains in terms of end-to-end training time on the commodity accelerator like GPUs are still questionable.
"	2
"Extensive experiments are conducted to demonstrate the effectiveness of the proposed method. However, I am not fully persuaded by the methodology. More details like complexity analysis and latency reduction experiments could be given.
"	2
"My main concern with this paper is that it is merely a parameter study of an existing method being applied to an existing architecture. While these kind of studies can still be of high scientific value, they need to provide new insight into the method or the architecture and yield some translation/generalization in terms of ""lessons learned"" for other applications in order to do so. I do not see this to be the case here. "	1
"This work did a comprehensive empirical evaluation on applying DynSpare to BERT,
in both structured and unstructured settings. The novelty of algorithm and theoretical understanding is limited. The empirical results could be more thoroughly conducted, e.g., the scale of BERT, downstream task evaluation. 
As an initial step of exploring sparse training of BERT, it provides some promising results. The tool this work developed could benefit the community."	2
"1. Strengths: interesing and important problem, the MI-based framework is reasonable and elegant, most parts of the theoretical proofs are reasonable.
2. Weaknesses: some problems related to method design and/or paper presentation, weak evaluation, insufficient discussion on (or comparison with) some related works 

==After rebuttal==

Thank the authors to carefully consider my suggestions. I appreciate the authors' great efforts in improving the paper, e.g., improving method description, adding details missing, and providing three additional numerical experiments. Most of my concerns have been addressed now. However, I can still see that some experimental results are not very good (e.g., Sec. 4.2.3). Considering all these, I would like to raise my score to 6."	3
An interesting approach to study the knowledge integration process, but the proposed method and empirical analysis are both quite limited to particular settings and models. 	2
"=== **after rebuttal** ===

While the theoretical discussion appears sound, I do not think that contribution in itself warrants acceptance at a venue like ICLR. I have an issue with the expressivity of the proposed implementation of GCS since it does not encode relation type information, which is essential in the definition of a triple.

However, my main concerns are with the experimental verification part, where the claimed ability of the proposed method to identify which knowledge is learned is not acceptably verified and the experiments may suffer from other issues.

While I appreciate the efforts of the authors, and think the paper now looks better, I think it does not deserve publication at ICLR in its current form and I will not be raising my score. My concerns with some critical issues of this work were not resolved by the author's reply and the updated paper.

=== **before rebuttal** ===


While the paper focuses on an interesting question, due to the above issues, in my current understanding, I don't think it deserves publication in its current form at a venue like ICLR. While it's different from previous work that I'm aware of and might be a neat idea, I found the claim of using attention weights of a GAT-based network simulating the KI process to inspect what is being learned not sufficiently justified. In my opinion, the proposed simulating probe has not been properly verified, and it is not clear why it has not been compared to more basic methods and straightforward adaptation of previous work."	3
While some changes to the presentation of results would further confirm the interpretability of the attention weights learned by GCS, the current set of results still seems to present a valid and useful method of probing for integrated knowledge in pretrained language models.	3
This is an interesting analysis paper discussing the Knowledge integration process by proposing to measure their Mutual information (MI) and in turn model it as graph convolution. The claims are solid and thorough analysis are made. Good paper. Very enjoyable to read.	4
* The paper reads a set of subjective observations about the meaningfulness of explanation and relationship with data manifold + tangential theory. The paper does not have a coherent story. However, the central question is interesting. 	2
Overall, the weaknesses of the paper outweigh its strengths. While the hypothesis is well-posed and the experiments are thorough, some major weaknesses are: (i) insufficient/misleading evaluation of explanation correctness/quality,  (ii) limited novelty vis-a-vis methodology and results on robust models, (iii) missing discussion on why robustness improves alignment,  justification of synthetic dataset and theory, and connection to previous work on sanity checks.	2
The paper constructs a clever setup to test its central hypothesis and provides some convincing results that the hypothesis holds true. However, there is no analysis of stochastic variation in the quantitative results, so they may not hold water as well as the central tenet that the gradients in line with the tangent space of the data manifold are qualitatively different from gradients orthogonal to it.	3
Overall, while I certainly think that this paper makes a hypothesis that is interesting and is at least partly true, it does not make its definitions (see point 1) and experiments (point 2) precise, which makes it impossible to prove or disprove the hypothesis. I would be willing to accept this work only when the paper makes a more clearly stated hypothesis, and designs similarly clear experiments. 	3
"Overall I think the idea is interesting and in particular the direction of a latent bipartite graph as a graph neural net way of making multivariate forecasting more scalable is novel, however it could be useful to point out and contrast to related alternate approaches for scalable multivariate forecasting, which are similar but not using graph neural nets.

However there are several questions about the method and questions and concerns about the experiments that I feel need to be addressed before I can recommend the paper for acceptance - i.e., specifics of the method so it could be reproduced and understood by the readers, and details of the experiments to determine how hyper parameters were chosen and if the results show significant improvements.

Without additional clarifications and details around the experiments and the method I am leaning on the side of rejection.  However, if these can be addressed I could change my recommendation.

 

***Update after responses and discussions:***

I appreciate the responses, and read the author responses, updates, and other reviews and comments.  I tend to agree with the other reviewers' points and don't feel all concerns are addressed with the updates. 

As pointed out the key novelty is really using the bipartite graph to increase computational efficiency, and as other reviewers have argued little analyses around this is done, larger data sets should be included, and training time scaling reported.  Despite the authors claim that larger data sets do not exist, and aside from simulated data, some larger ones exist and have been used in several other papers. E.g., the large wiki dataset has > 100,000 time series (and used in one of the scalable time series forecasting papers I cited and other past work it cites), and the recent M5 dataset and competition also has 30,490 inter-related time series at the lowest level, and there are several forecasting papers published about it as well.  Additionally more can be found with some searching / reading. The authors would have at least seen the wiki dataset if they looked at other prior papers including what I mentioned in my review. 

In particular, as the main benefit they are claiming from their method is enabling scaling for multivariate forecasting to large amounts of time series, they ideally should consider related work on scaling forecasting for large collections of time series and at least discuss this area of work and contrast to their work, which they did not, despite the recommendation. They also should compare to the state-of-the-art multivariate forecasting methods (including without considering graphs, to demonstrate the graph approach is helpful), and especially need to compare to those designed to scale to many time series, which also was not done.

I also agree with the other reviewer - to only compare and report forward evaluation run times leaves uncertainty about practical performance, as training is the biggest issue in most cases.  Further, there seems little reason not to report it and explain what it reveals unless the benefits are not as much in this case - which would also be interesting and fine just needs to be reported and some analyses provided. Ideally the analyses around training and forward time should be performed for varying number of time series across multiple datasets, e.g., plotting on x-axis the number of time series, and on y-axis the training and forward time (and nice to also see the error metrics as well per number of time series).  It could even be combined in the same tables or plots for forward time by showing average training time per step and average forward time per step, for example, on dual y-axis if space is a concern) vs. number of time series.   This could potentially also be generated from larger datasets if needed by sub-sampling series, or just using the natural number of time series in different datasets.

The fact training time isn't reported makes one wonder if there could be some reasons it's not providing as much scaling benefits for training, e.g., maybe the train time is actually not much if any better, due to the extra propagation steps having more of an impact at training time as perhaps slowing down the back propagation updates (which we don't see from forward time alone), and also from the fact that all time series still must be present at once so there could be some dominating factor on the data side that doesn't change.
"	3
The paper has some strengths that could be useful in applications of multivariate time series forecasting. The graph representation could be useful not only for prediction but also for analysis and interpretability of results. However, I feel that more can be said about the architecture and how it affects the type of relations that it is capable of represent.	3
Overall, I think that this paper would need to be extremely solid on the experimental side with potentially further experiments, e.g. using a larger number of features, robustness to missing data, etc, in order to make up for the somewhat limited contributions in terms of novelty.	2
Novel method which gets improvements on SOTA	3
 The problem this paper aims at solving is an important one and I can see that it will make a difference in many applications.	3
In summary, it seems that this is a novel and interesting paper for learning temporal logical rules with solid experiment results. I am familiar with logical rule learning but not the temporal point process, so I am not sure about my evaluation of the temporal process part. But in general, learning temporal logical rules is very interesting. 	3
"In summary, although the paper presents a solid theoretical discussion and a comprehensive verification, it still has defects in the overall structure, the clarity of innovation, the challenges of the problem scene, and some method details. As a result, I do not recommend accepting the paper even though it shows many merits.

Revision Notes (2021-11-26):

Most of the problems have been appropriately resolved or explained in the revision. The quality of the paper has been significantly improved and exceeded the acceptance threshold. Therefore, I have raised my score to 6."	3
The contribution of algorithms which heavily rely on regression / classification to guide sequence design is interesting, especially since they are the product of a general mechanism for producing sequence optimization methods. The empirical results seem sound. The exposition of the model could use work to help readers have a crisper sense of how probabilistic modeling gets linked to a problem setting where the only randomness is in experimental noise.	4
The paper has some interesting methods, and the connection to LFI is helpful. However, it does not have a clear empirical recommendation for what algorithm readers should use going forward and the paper does not provide adequate details to understand how to go about actually using these methods in practice.	3
The paper is clear and well-written and provides a very useful conceptual framework tying two subfields together, with sufficient empirical evidence to show real gains from this approach.	4
"* The outlined relation between likelihood free inference and sequence design (blackbox optimization) is interesting; I am not aware of any existing papers with this insight. However, I am not sure about the impact of this insight on how sequences are designed in practice.
* It is not described clearly enough how the proposed methods differ from existing methods for sequence design such as Bayesian model-based optimization, GANs, or RL.
* Important details about the proposed methods and performed experiments are missing, which makes it hard to understand and assess."	2
Although the reviewer provides some negative feedback in Main Review, the reviewer still likes this paper. Therefore, I will give a positive rating.	3
This paper proposes the non-contrastive representation learning method for deep clustering based on BYOL. It is a combination of two existing methods. The novelty is limited.	2
"I think this paper is well written in that the motivation, the description of methodology and the analysis of the results are all reasonable.
Though I feel more details are needed for some factors that i described in the main review section, I think this work is good since the proposed method has pointed out the weaknesses of the previous methods and improved it experimentally."	2
The paper shows promising results, with a great motivation. However, there are some issues that can be easily fixed to greatly improve the quality of the paper. Specifically, ProtoNCE, or PCL, should be included in Table 2, and an additional baseline that empirically shows the negative loss harming performance would be great. Presentation could also be improved to be more clear. 	4
Overall the paper appears to have good quality, with a new time series prediction model proposed. 	3
The paper solves the periodic time series forecasting problem; the proposed DEPTS model is intuitive, effective and interpretable; the experiment is solid and extensive; the improvement is significant. 	4
"A well written paper with a small, but interesting contribution. 
Some aspects of the evaluation still can be made more clear."	3
"In this paper, the authors proposed a customized deep learning (DL) architecture for periodic time series to explicitly take account of the periodic property. The proposed solution is technically sound and effective based on the extensive evaluation. The periodical module sounds novel and reasonable, and parameter initialization resorts to a two-stage optimization problem, which is also interesting.  However, I have the following comments that the authors should take into account to improve the paper.

1. the writing should be improved. For example, Section 4.2 is not easy to read and the left part of Figure 2 is not very easy to understand. The authors are suggested to overview the framework before delving into the detailed description. The figure should be made more clear to understand. 
2. a few technical parts are missing. For example, how to design local block function f^l and periodic block function f^p. 
3. some motivations are not very clear. For example, why instantiate the period function as a series of cosine functions and how such a formulation can lead to the modeling of periodic patterns. 
4. Though the results of the proposed method are good, but in some cases, the improvements are small. The authors are suggested to include significant testing and show some standard error in the result table.
5.  Though the proposed method performs well, it is unclear the efficiency of the proposed method compared to baselines. The authors are suggested to add detailed complexity analysis to the proposed methods and some related baselines."	3
"I think this paper overall contributes an interesting pipeline for automatically extracting relevant concepts for the concept bottleneck model. Several concerns are listed above.

-- after rebuttal --
Thanks for providing the rebuttal and explanations. I've read the authors' responses and other reviews. Overall I still feel the paper is addressing an interesting problem, but agree that the method might limit its applicability. I'll keep my rating (borderline-ish)."	2
In summary, the paper is well-written and addresses an important problem of interpretable video classification models. The proposed method does not rely on manual concept labelling, but rather extracts these concepts automatically from natural language descriptions provided by dataset annotators. My concern is that the proposed method does not seem general to me, and I feel that it only works for the specific datasets that the authors have chosen. And although the concept extraction process is automatic, the method requires obtaining natural language descriptions for each video in the dataset, and the annotation cost of this is not discussed at all in the paper.	2
"This paper proposed an end-to-end pipeline to train an explainable video classification model. The paper is targeted to study three interesting research questions, and I particularly liked the discussion section at the end of the paper. However, the experiments lack in some aspects resulting in a less convincing story. 


######## Post-Rebuttal ########

I appreciate the authors made a good effort to improve their paper based on review comments. However, I still find that the contribution is weak -- experiments are done on only two video datasets with limited classes but given that the method itself is generic. Therefore, I stick with my original rating. "	2
I think the paper is well-written, but the merit of the method is not very clear. Since textual descriptions may require a similar burden to handcrafting a set of concepts, the proposed method may just add noises in the set of concepts. Regarding this point, the merit of the method should be experimentally verified (w1). Also, the necessity of pruning is not very clear (w2), which I guess involves the first contribution of the paper. I think the paper will be stronger if these concerns are resolved. 	2
There are novel contributions of the paper both technically and empirically, however some major analysis sections are not convincing and a significant portion of the presentation needs to be improved.	3
The studied problem is important and novel. The proposed method is simple and clear. The experiments justify the effectiveness of the proposed method. 	3
"The motivation, experimental settings and results look good overall.

One major caveat however is that it is not clear how flexible the inference setup is. This is critical as the predictions for a datapoint at inference depend on the other datapoints in the batch. Currently, it seems like the inference is being done with a batch of all the same attack. These assumptions are simply not realistic as a threat model. Without understanding how this performs under a more realistic threat model, I cannot score this paper higher. I have highlighted experiments that would provide more realistic results. 
Despite my ""marginally below"" score, I do not think the paper can be accepted without an answer on this point. Reject is too harsh for a paper that is otherwise promising.  

The paper would also benefit from another writing pass, mentioning missing related work and clarifying some properties of the dataset. "	3
"
The paper studies the very timely topic of adversarial attacks against prefix tuning. The paper proposes a method that maintains the advantages of prefix-tuning against finetuning, a formidible problem. The experiments are OK as of now, though expanding the scope to generation tasks could make the impact substantially larger. The characterization of the method could also be further improved by providing finetuning (with and without defenses) as baselines. Overall, I lean towards acceptance, though I am not an expert in adversarial attacks/defenses."	4
The proposed approaches are novel and significant, and clearly evaluated. They are shown to be a promising new approach for ASR. However the clarity and quality of writing of the paper are quite below the standards.  Hence in the current shape i cannot recommend acceptance, it's just below the threshold. I will be happy to increase my score if the writing is improved.	3
"The paper contains many typos and ungrammatical sentences which make it difficult to read. To list a few:
* Page 2: ""The long range dependencies The latter allows to model long-range dependencies just like self-attention would.
* Page 3:  ""MLP-Mixer simply applies the MLP across the token dimension gMLP has a module called Spatial Gating Unit (SGU)""
* Page 6: ""For Librispeech, We tokenize""


* The related work section is very short, focuses only on models for images and gives no reference to ASR. 

* Section 2
    * Equation 1, `Linear_{channel}` is referenced in the following sentence as `Linear_{token}`
    * Equations in 2.1 are wrong or wrongly written. Some variable are not explained, and the dimensions don't add up when you consider all the projections.
    * SGU is presented in section 2.2 but has been used in equations of section 2.1
* Section 3
  *  C-MLP', which will be the best model, is not clearly descibed (only equation 8)
  *  ""This operation to obtain filter HCGU is independent of the channel dimension."" : this is unclear
* Section 4
  * In the text, reported WER seem to have been copied from one to another, and don't match the table.
  * `C-MLP achieves competitive performance with Transformer-based model with only 57.1 % of its parameters`, but after computing the value, it seems to be `57.4 %`
  * Should be `Table 2` not `Table 1` in section 4.4
  * C-MLP' is the best model based on table 1 but is not presented in  table 2
  * In table 5, RTF is not defined
  * In the claims (abstract), it is said that the proposed architecture reduces WER by 1.9/3.4 % but this result seems to be mainly due to the tiny attention module, which is not clearly stated.
"	2
Although the problem that this paper tries to solve is an important problem, the novelty of the proposed solutions seems to be weak. Experiments could be more powerful with commonly used benchmarks (LibriSpeech-960h), and details are missing.	2
While it seems the paper presents technically interesting topics, it requires major revision in terms of writing. It includes many grammatical errors, wrong references and punctuation are not respected. Unfortunately, it makes the paper difficult to follow.	2
"Correctness: 4
Technical Novelty and Significance: 3 (I am debating between 3 and 4 for the final review. I think the NAS problem is clearly identified and the datasets curated for it are novel but the problem itself, of OOD, is not novel. There are more empirical insights than theoretical ones for the paper. I might change this rating based on the other reviews / discussions).
Empirical Novelty and Significance: 4 (I am debating between 3 and 4 for the final review. I think the total sum of contributions: detailed analysis, identification of the NAS scenarios, and additional loss terms to Mahanalobis detector make it novel & significant even if some parts may not be. I might change this rating based on the other reviews / discussions).

Overall Rating: Weak Accept
Given the strengths and weaknesses of the paper mentioned above, I recommend an Weak Accept for the paper. It has good contributions with the NAS problem & relevant datasets to test it. It also presents a good analysis of existing OOD methods, their flaws and a potential improvement in one (for the Mahanalobis detector). My main concerns are about the Pose component of NAS and scalability of the proposed modification (new loss terms) which if addressed/responded to in the rebuttal discussion will help me change to an Accept."	3
Overall, I like the idea of this article, the proposed new task is valid. However, there are some weaknesses, especially the experiments that are not comprehensive and convincing enough.	4
The paper is well written with extensive experiments. I think if the author could address my concern in the weakness listed above, the paper is good enough to publish in ICLR.	3
The contribution on proposing discrete-time GMFG and establishing existence and approximation results are good, but the algorithmic and numerical contributions are limited. 	3
The authors provide theoretical results showing the game is well posed, and they provide interesting numerical results (also in the appendix). But overall I am not sure whether the “learning” part is really crucial in the paper. If it is the case, more explanations on the advantages of the proposed approach should be included.	3
This paper provides some nice contributions to the theory of graphon mean-field games in discrete-time settings. It also provides some heuristic learning algorithms, though without theoretical justifications. Some preliminary numerical results are also provided, but without comparisons with the existing MARL algorithms.	3
Overall, I think the paper makes a strong and important contribution. The results in this work are novel and important. The paper is well-written. 	3
Generally, this paper proposed a novel programming-based adversarial perturbation and it is effective and competitive in various benchmarks. However, this paper aims to generate a physical imperceptible perturbation but doesn’t well-supported by experiential results.	3
The reviewer cannot recommend acceptance because this paper is lacking in novelty and empirical evaluation.	2
"Based on the three issues listed below, I would recommend rejecting this paper.

1. The problem is not well motivated and justified (see weakness I), and it might be trivial to succeed in this attack (see weakness III.2). I would suggest reformulating the problem setup and algorithm (see the main review for detailed suggestions) and clearly stating the challenges of this problem.

2. The proposed solution seems to be a very straightforward combination of several existing algorithms. The simplicity of the algorithm is not bad; however, I would encourage the authors to clearly state (1) why this problem is challenging; (2) why their proposed approach is non-trivial and novel; (3) how effective the proposed approach is (requires more baseline comparisons).

3. The evaluation is incomprehensive, and the performance comparison chooses an unreasonable baseline (see weakness III). I would suggest adding more experimental comparisons with other baselines to demonstrate the effectiveness of the proposed approach (see the main review for detailed suggestions).
"	2
"However, I have 2 minor concerns:
1. The title of the paper boasts “physical”, and yet figure 6 indicates the experiments were mainly carried out digitally. What’s the real physical performance? I.e. how effective is the proposed method when deployed in the real physical world?
2. This work mainly used l2 distance to measure distortion, but l2 distance sometimes could be flawed itself, how about measure the distance between 2 distributions? Would that change the results?
"	2
This work frames the adversarial attack as a program synthesis problem, which is interesting. However, I don't think this problem setup is very well-motivated. Specifically, there might be simpler adaptions of existing adversarial attack algorithms that can achieve similar or better performance, while they do not require a computationally expensive program search process.	2
Some of the technical details need to be provided. The novelty need to be emphasized. I'll consider raise my score if my concerned can be properly addressed.	3
I would recommend this paper to be accepted if other issues do not arise.	2
While the paper is clearly written and the empirical evaluation is convincing, it is not clear why and how the over-smoothing issue could be resolved or what kind of equivalent regularization behind. 	4
Overall, this work is well motivated. However, the theoretical complexity analysis is missing and the methodology part remains confused. The technique contributions are limited.	3
"The proposed AF2 tries to solve HSR aerial imagery semantic segmentation by making use of multi-scale representations. The whole  Adaptive Focus Framework (AF2) is simple, including Hierarchical features extractor, Preliminary predictor and Adaptive confidence mechanism for prediction selection (ACM).  
As the first two modules are popular in semantic segmentation and other related works, the main contribution of this work is the proposed ACM, which judges whether the prediction for each pixel is sufficiently confident or not in each level.  But actually, the proposed ACM is simple lacking novelty. The experiments in this work cannot verify the effectiveness of proposed method solidly. In Table 1, AF2-AFPN brings slightly better performance compared with PointFlow. On Potsdam, its performance is even worse. For general segmentation benchmark, only SFPN method is reported and compared, but in PointFlow, it has mIoU 80.3% on Cityscape. 

All in all, i think this paper is not novel and solid enough to be accepted by ICLR.
 "	1
"The paper proposed a method to selectively merge multi-scale predictions, but failed to compare to any other methods aiming for similar goal (e.g. max-pooling, attention). There is no experiment to validate the ""adaptive"" design of the mechanism either (compared to fixed threshold or top-k). Therefore, I recommend reject."	2
Compared with exsiting methods on scene understanding, the proposed Adaptive Confidence Mechanism is little weak.	2
"
The proposed method looks promising in improving segmentation results in remote sensing application. 
However clarity has to be improved."	2
The paper presents an interesting approach to a difficult problem, yet also presents opportunities for improvement in terms of motivation, breadth of experimental comparison, and scope of experiments.	3
Based on my comments above, I suggest that the paper is not ready yet for publication, despite having some interesting ideas. I look forward to the authors' addressing my  comments above.	2
The method proposed in this paper is interesting. Two techniques used in this new algorithms and analysis of this algorithm are non-trivial. This algorithm also achieves a good performance in experiments. 	3
Overall, the paper has strengths as I highlighted above that merit publication as long as the clarifications on the policy for dealing with cycles and the clarifications on the proofs is provided. I recommend also including some related work on betweenness centrality for random walks and random walk betweenness centrality.	3
"Overall, I think the paper proposes an interesting evaluation but is still missing empirically in multiple aspects that could strengthen the paper:
* Tracking potential reasons for the failure source (perceptual or reasoning) and making sure to reduce ways for the adversarial model to place objects in ways that make them too hard from the perceptual side to answer correctly.
* Comparing adversarial to randomly-sampled scenes to measure the degree of consistency given a question rather than penalize the model for the existence of a single way per question to fool it. 
* Explore the model at least one more additional environment (e.g. 2d)
* Explore additional indications about models behavior when they do and do not fail, to give further insight about potential reasons for why they are in/consistent in different cases.

I therefore give the paper a marginal score, and encourage the authors to update the paper to make the evaluation more extensive! "	2
"Overall, this is a solid paper with really interesting results. There are a few issues, noted in weaknesses, that prevent me from giving it a higher score. I will happily assign a higher score after rebuttal if those points are clarified.


UPDATE: I have read the author's responses and all other reviews carefully. Like others, I had some concerns about scene occlusion, etc. issues, but the authors have satisfactorily answered that. I also appreciate the authors' commitment to update the paper according to the reviews. That being said, I do not think the revisions have made for any *new* information for me to change my scores. I still think this is an interesting paper, albeit a bit limited in scope. I tend to keep my score as is ( ~ a weak accept)"	3
Overall, this paper proposes a feasible way to evaluate the reasoning abilities of any black-box VQA model. Although the experimental results haven't revealed new problems, the proposed method can inspect current largely pretrained vision-language models to see if they perform reasoning on various tasks. Thus I endorse acceptance.	3
The motivation of this paper needs more clarification. The proposed model is a simple hybrid dynamic system and the training method is based on existing VI and BPTT and thus the paper lacks novelty and technical contribution. The experiment is not solid enough to support the claim advantages of the model, i.e. interpretability and approximating ability. 	2
Overall, I vote for accepting. The paper is well written. The method is evaluated on various data and compared to other methods. Hopefully the authors can address my concern in the rebuttal period. 	4
The spline basis expansion works empirically but the technical novelty is not strong enough. 	2
In short, the paper has some merits in comparison to the literature of differentiable decision trees. However, its performance is not so strong and some experiment results are not solidly justified. Nonetheless, I feel the cited papers by this submission are also somewhat incremental to their literature. 	3
Despite the theoretical efficiency advantage of the proposed method, there are still remaining questions as mentioned above. I am leaning towards rejecting given those questions.	2
"Overall this paper proposes a similar idea to previous work [1] that also sparsifies the NODE without mentioning it. There is no empirical measurement of the claimed speed up, and the selected datasets are not compared to the datasets used in NODE, and no baseline including sparsified neural network like [2]. The accuracy results are somewhat lower than CatBoost in 4 out of 6 datasets. 
To summarize, there is very little novelty in the architecture, no empirical time measurement to back up its claim, and the datasets selected are not comprehensive, and some missing important baselines. So I recommend rejection for this paper."	1
Based on major concerns regarding technical novelty and experiments, I am leaning towards rejection of this paper.	2
Not recommended as its current form since it seems to have missed a deeper connection and hence a much nicer solution. The batch size observation also doesn't seem to be firmly supported. However, the question is interesting so I look forward to its future form.	1
"1. In assumptions 1-3, it requires the empirical loss function Q to be differentiable. Is that possible to extend current results for non-smooth loss? Say, the absolute derivation loss.
2. As it has been claimed in the abstract, ""by carefully re-tunning the learning algorithm"", how to choose the learning rate $\gamma_1,\dots,\gamma_T$? Is the proposed method robust with a certain range of learning rates?"	3
"
The paper is mostly sound and brings a contribution (even if not very surprising).
"	2
The paper makes some good progress towards an understanding of DP and BR in SGD via Proposition 1 and 2. However, the convergence results are not very clean and not properly contextualized. The writing is rather poor and the $\epsilon$ issue is a big one. I cannot recommend acceptance in the current form. 	2
This paper proposes a new sampling procedure to tackle the challenge of dataset heterogeneity in medical imaging and illustrates its generalization performance on breast cancer prediction with mammography images. However, the paper is limited in its narrow evaluation and lacks a thorough analysis to demonstrate its significance or novelty, especially given the fact that the proposed procedure has many heuristic components, so I would suggest a rejection for the paper.	1
"The authors present a well written paper of a sampling method to tackle the prevalent issue of skewed heterogenous data in medical images classification.
However, they do not provide comparison of their method to a so called state of the art to handle these robustness issues. Their conclusion of the contribution based on their experimental evidence is lacking and therefore questions the contribution of this paper to the field."	2
"The authors in this paper propose a novel approach of using periodic sampling with DNNs to efficiently train a DNN for breast cancer prediction from mammography images. They also perform binary classification with an SGD-like optimizer. 
There have been only a few research works in using periodic sampling, which helps in understanding the novelty of this approach. The idea is well-motivated as well. The authors have described the dataset, methods used, and evaluation well. 
The authors could improve some sections by providing additional explanation on their experiments and choice for the heterogeneous dataset. Also, additional evaluation metrics would help the reader understand just how beneficial periodic sampling could be. There are some typos in the paper that would need to be corrected.
All in all, the paper carries a novel approach to an existing problem, however, the authors could have expanded some sections with a more precise explanation, such as why they performed only 2 large experiments, instead of multiple smaller ones or why periodic sampling has not been compared to other relevant existing methods, even though the approach is novel but the method is well known. "	3
Clear well-written paper on an important problem. However, empirical results are on an unspecified and unavailable dataset which reduces reproducibility and it is unclear whether the results generalise to other clinical imaging problems or other problems in general. It in unclear what is the state of art being compared to.	3
"A simple approach for balanced sampling for mini-batch SGD training is shown to provide modest gains in the case of extreme label imbalance across segments of the data. More experiments are needed to quantify its benefits for other datasets. Still, in some real-life scenarios (not the ideal, unbiased case) it provides a principled way to favor rare but 'deemed-important' examples. However, it lacks an analytical and empirical study to quantify the pro and cons of the method based on the label imbalance within data segments.

"	2
The authors consider the challenge of improving sample efficient HRL in sparse environments. They identify several drawbacks in RUDDER and propose effective modifications to the learning algorithm to improve performance. While the experiments are promising they are restricted to navigation style experiments, it would be useful to have comparison to more imitation learning based baselines with diversity in learning environments.	2
"A solid approach to the difficult and important problem of learning from scarce expert demonstrations. An analysis of the success rate degradation in the last part of the minecraft task, and an improved explanation of Align-RUDDER's ""five steps"" would strengthen the paper significantly."	3
Given all the drawbacks, the paper is not ready for publication. I would recommend the authors to focus more on the in-depth comparison with Rudder, save some space by moving the figures regarding Minecraft to the appendix, rewrite the key concepts in more depth, verify the key assumptions and demonstrate WHY the new method is better.	3
In summary, I think this paper presents strong results, and is a valid technical contribution, but the main reason I am currently voting for reject is that it needs some rewriting to address my clarity concerns. I am willing to adjust my score if the authors can improve on this.	2
"The paper is well written and presents a novel p-GNN architecture on top of other PageRank based GNN architectures. I found the theoretical justifications and connections drawn between the message passing scheme and the polynomial graph filter particularly interesting. Also, they had an interesting bound on the risk (Theorem 3). I am overall quite positive about this paper's acceptance, as it makes novel contributions to handle a difficult scenario like heterophilly in graphs and also the fact that they challenge the implicit assumption made by previous works that the ""shape of node neighborhoods"" helps distinguish nodes.. of course this can be extended to graph shapes too."	3
See the main review. 	2
I have mixed feelings about this paper. On one hand, I like the newly designed message passing scheme and their implicit bias demonstrated in Proposition 1. Also, conducting experiments on cSBM is great for studying problems of heterophilic graphs. On the other hand, the drawbacks of the paper prevent me from supporting acceptance of it. I think the paper has great potential but requires a major revision.	2
"Overall, I think the paper addresses an important problem in RL: discovering diverse options when there are no external tasks. The paper presents an adaptation of Termination Critic. 
However, some of the design choices made by the approach does not seem to be reasonable. Specifically, why use Variational Intrinsic Control in combination with the proposed approach when there is a direct way of learning option-policies from the discovered termination functions. Also, using VIC in this way makes it unclear as to whether the performance gains are due to the proposed approach or due to VIC.
"	2
In all, this paper studied an important problem in reinforcement, learning diversified options that can be reused in various tasks. Their empirical study is able to justify the diversify of the learned options and shows superior performance in transfer learning settings comparing to some baseline.  However, there are still some key references missing and should be compared against. Also, the method seems trivial extension of many existing methods. 	2
"Well written paper with an interesting objective that makes intuitive sense.
The results look promising, but the discussion and analysis could have more depth; there are questions left unanswered on this paper."	2
Nice idea, but the connection between the theory and practice is somewhat missing, resulting in an algorithm that might be too similar to VIC, and the differences are not explained clearly enough nor supported by the experiments. In addition, including more baselines would have made the empirical contributions stronger. 	3
Overall speaking, the assumption used in this paper is too strong to simplify the real cases properly, so the theoretical interpretation established on this assumption is unconvincing. Besides, the large training part lacks justification on some claims, while there seems to be some errors in derivation on the proposed method SWD. The experiment results are not convincing either. The current quality is below the acceptance threshold.	2
The paper is interesting, free from technical errors, useful and fairly rigorous.	3
It seems like AdamS is only beneficial to training and I think it might be helpful to add this modification whenever using Adam in combination with weight decay. The improvements are not superstrong but there does not seem to be any drawback.	2
Despite the effectiveness of the proposed AdamS in small-scale datasets, the theoretical analysis about weight decay does not respect the scale invariant property of some important neural networks, which are used in the empirical validation of this paper to demonstrate some results of its analysis. This results in a severe conflict between theory and practice in this paper, so I would suggest a further revision for this manuscript.	2
The paper is generally clear and well written and makes several interesting and worthwhile observations about MMD and its use in recent work.  However, there is an opportunity to better support some of the claims--like what is considered status quo (through citations) and about recent experiments (which paper used which settings and, in the case of (Niu et al., 2020), where those setting can be found--and to clarify the perturbation based desiderata and experiments/analysis.	3
The paper mainly focuses on the negative problems of the MMD, which are explained and in several cases empirically proved. This makes the paper strong and it should be accepted. However, the solutions are not clearly explained, decreasing the score for this paper. 	3
"The authors focus on an important problem in the graph based generative modeling domain and provide valuable insight and learnings via carefully designed empirical results and clear and unambiguous explanations and intuitions. The recommendations from the authors is insightful and would be useful to researchers working in the field of graph based generative models. However the current scope and focus of the paper is a bit limited. The authors could have expanded this work via considering i) other metrics apart from MMD, ii) other descriptor functions as well as iii) working with non-linear variants for correlation, i.e., mutual information. I would like to congratulate the authors for this work and ideally would like to see this paper accepted. 

----UPDATE----

After revisiting the updates made by the authors, I am updating my score to Accept. I would like to congratulate the authors for their excellent work which would benefit researchers working in the field of graph based generative models."	3
"I am inclined towards accepting the paper for its much-needed practical guidance on the use of MMD in the context of graph generation, which could have very positive implications in this research field. However, I acknowledge (and the authors should, too) that what is discussed in this paper is only one of the many aspects to consider when evaluating graph generators.

----EDIT----
I find the response of the authors to my inquiries satisfactory. For this reason, I am raising my score from 6 to 8 and vote for its acceptance."	3
"The paper proposes a novel CAV-based method to solve PLL problems and shows a promising performance than several state-of-the-art methods. Based on CAV, they propose a CAV learning method to select a potential true label for training and thus transfer PLL to supervised learning. Extensive and solid experiments prove the effectiveness of the proposed method. The paper provides a new way to tackle the PLL problem. The  main concern is the CAV is similar to the attention mechanism, and some of paragraphs are still need to explain.

"	2
This paper investigates partial-label learning. This paper makes two key contributions including an interesting empirical observation and a novel methodological contribution. To the best of my knowledge, the empirical observation is disclosed by this paper for the first time and the proposed method is simple yet effective. Comprehensive experimental results support what the authors claimed in the paper.	3
This paper introduces the class activation map (CAM) for handling identification in PLL. But the effectiveness of CAM for PLL is not validated. Several of the paper’s claims are incorrect or not well-supported. The contributions are only marginally novel.	2
The paper presents a solid and well-motivated generalization of prior work. There is some room for improvement in terms of the writing as well as some additional experiments, however, I would already consider this a solid submission as it is. Therefore I am **leaning towards an accept**	3
"**Recommendation:**

6: marginally above the acceptance threshold; Tendency towards 8, if concerns addressed. 

**Summary:**

The paper presents a novel method and provides empirical and theoretical evidence for its effectiveness. There are two parts of the paper that are currently insufficiently explained. Firstly, the technical setup of the normalizing flow and how it is used to create epistemic uncertainty is underexplained IMO. Secondly, the toy experiments in Figure 1 should be explained and discussed in more detail. I'm specifically currently uncertain why the epistemic uncertainty is not shown and presumably quite low in this setting. 

I think these concerns can be easily addressed by the authors and would be willing to increase my score to an acceptance (8) if they are addressed well. "	3
"The paper is well written and seems technically correct. The proposed method NatPN extends the Posterior Network to the exponential family distributions and has the advantage of using a single normalizing flow rather than C flows for classification tasks. The empirical results are backed up with an interesting theoretical guarantee under what seem to be reasonable conditions.

A number of aspects of the paper are unclear to me, I have listed these questions above.

Given the slightly incremental contribution of the extension of Posterior Networks to the exponential family distributions the paper would benefit from additional empirical validation e.g. with the above suggested ablation, the addition of SNGP and DUQ baselines and the inclusion of OOD performance metrics. Additionally, given that I see the use of a single normalizing flow as a main contribution of this paper I would like to have the question regarding the modified training procedure clarified and I would like to see empirical validation of the scaling to CIFAR-100 and/or Imagenet.

Nonetheless, the paper is a solid contribution and I believe worthy of acceptance given the information currently available."	3
There are a few concerns which are highlighted in the sections above, but overall the paper is a strong submission which solves a relevant problem in a novel and intuitive way. My score reflects everything stated in my review and is likely to be updated based on the authors response to my questions above.	4
"While the paper provides a useful extension and next step to the prior work by Charpentier et al. (2020), the theoretical contribution seems minor and without great empirical improvements. Explorations in areas where the proposed setup with the uncertainty decomposition becomes relevant could strengthen it a lot.
"	2
I think that this paper is useful since it evaluates model-based predictors on the same open-source framework, enabling a fair comparison of different predictors. However, I think that the experimental evaluation conducted by the authors is not enough in order for me to give an acceptance score. There is also quite some work to be done in order to improve the structuring and clarity of the text in my opinion.	1
"- **Novelty is limited**: Most of this work is simply using the existing hardware performance predictors and running them to collect results on the existing hardware dataset. Too much focus on introducing the setup and configurations of these hardware performance predictors and dataset, and simply numerating the results from the conducted experiment, while the proposed insights are overshadowed. 

- **The insights are shallow and obvious**: Even though the paper presents multiple insights from the experiments, most of them are rather shallow and obvious. For instance, the paper stressed that the model-based can benefit more from more training data as compared to the lookup table based predictors, which is rather obvious as model based is data driven while lookup table is deterministic. However, some specific necessary and useful  insights are missing. For instance for page 5, most of the texts are devoted to reporting which methods perform better and which methods scale better  with more data. The underlying reasons or hypotheses are not provided.

- **Unclear on the aspects of how to counter the imperfect predictions on guiding the network architecture selection**: One of the most interesting aspects about this work is that it proposes imperfect predictions from the hardware predictors can also lead to decent selected architectures. However, it is rather unclear how to achieve this and why. For instance, in page 7, the authors mention “verifying the hardware metric predictions of the 13 architectures”; which 13 architectures? It was never mentioned elsewhere. I really hope the author can stress this part more.
"	1
Given the limited novelty and lacked discussions elaborated in the weakness part, I tend to deem this paper marginally below the acceptance threshold. I'm willing to adjust my scores if the concerns are properly addressed.	2
Overall, this paper is important and useful for the community, and has pretty good and thorough analysis. However, it could have more interesting insights, and there are some questions about Section 6. Overall, I currently will give a weak reject and look forward to the authors’ response.	1
It was an interesting paper because of the way that authors bridged from geometry to solve an applied problem with neural networks. But it has some room for improvement as I mentioned in the detailed review. Although the approach is interesting the technique is not that novel. 	2
"The paper is well-organized and easy to follow. The idea to use multiple Givens rotations seems novel. 

However, there are some flaws in the technical and experimental parts. One is the lack of discussion of the number of rotations in one step. It seems the authors fail to provide convincing explanations. Another problem is the missing comparison of consuming time. The method that uses only one Givens rotation is also necessary in Section 3. "	3
This paper is overall well written and the contributions can be clearly recognized. The method part is clear but the experimental part is not solid as expected. I tend to accept this paper at this stage and am looking forward to the authors' feedback.	3
"I do not know much about product quantization but I know a bit about rotation matrices. For me, necessary experiments to prove the point of the paper in product quantization are presented, but the gain seems marginal. 

Regarding rotation matrix learning, I think the authors may have overlooked the standard approach of projecting rotations to a tangent space/Lie algebra. In this particular problem when the rotations are small, Lie algebra can be very helpful. The paper would be a lot more interesting if there were deeper treatment on this front."	3
The paper takes on an interesting perspective in trust modelling while connecting the biological, humanities, psychological and societal studies inspirations to the proposed method. The technical formulations are easy to follow and  sound.	3
I am not an expert in trust modeling, but even so I think the paper as framed is mistargeted for the conference. It needs to have much shorter / pithier motivation and review, and much more detailed and precise explanation of what was actually done. In addition, there need to be some sort of payoff (i.e. new results / insights) from all this framing -- as-is, there is the DK and OC effects, but to the extent there's a prediction of new effects (e.g. the location of the first confidence peak) the paper dismisses it as a potential artifact of the setup, so I'm not sure what to make of it. 	2
The paper nicely presents a simple function for defining trust. However, the conceptual and experimental shortcomings need to be addressed. Specifically, appropriate experiments need to be run to evaluate the observed effects and their implications on understanding human behaviour. Ideally, the paper would simulate previously evaluated human experiments and compare those results with their simulation analysis. The metrics used would also need to be comparable. 	3
"The problem of trust is interested, but the paper actually focuses on ""self-confidence"". The assumptions and formulation of the mathematical model are not justified. Several concepts are introduced in the paper, but they are not really used in the model. The findings of the simulations are not insightful, since the model is based on assumptions that are not based on a clear rationale."	1
The authors propose a novel new approach to reduce directly the power consumption for inference. The originality and good results are the main strength of the paper while the weak points are mostly centered around some assumption and justifications of the power model and the limitations of the proposed approach. Overall both side are fairly in balance.	4
Although power-aware DNNs are interesting and this topic is an important research direction, this paper has some fundamental flaws that prevented the reviewer from recommending acceptance.	2
The idea is of intellectual merit, but the evaluation ignores too much important aspect of real energy/power modeling. The results are far from convincing in that regard.	2
The idea of finding the optimal quantized model in terms of the power budget is an important issue for real-world applications. Although some part of the paper is ambiguous to me, I think this paper could be a bridge to reduce the gap between the model researchers and hardware engineers.	3
"It is important to show SGLD might not always give (\epsilon, \delta) differential privacy guarantee. But the text should be improved to clarify the points that I mentioned above. Maybe the title and the introduction should be revised, or some analysis could be added to show this is also applicable to deep learning.
"	3
"Overall, I consider the contribution of the paper is quite restrictive. 

By definition, it is sufficient to find a pair of neighboring data sets to counterprove the loss of privacy. But the results also depend on the specific setup of the SGLD algorithm, which I believe is not very proper.

In the common privacy-preservation algorithm, one typically injects Laplace or Gaussian noise. To show it works, we always need to have some lower bound of noise variance. Similarly, if SGLD preserves privacy, there are potentially some requirements on the algorithm implementation. To counter-prove that, I suppose one needs to show that no matter how one tunes the SGLD algorithm, the privacy breath is inevitable."	2
This paper analyses the differential privacy of the SGLD algorithm. It uses Bayesian linear regression as an example to demonstrate that while differential privacy holds at the beginning of the SGLD updates and similarly at the convergence, but it may not hold during the intermediate steps of SGLD updates. The results seem convincing.	3
"The paper explores the privacy-preserving performance of SGLD and shows its privacy loss can be unbounded in the middle regime of sampling. The finding is interesting and useful. 
However, the authors should make their contributions clearly and some sentences are misleading. The paper should also be re-organized and proofread before it can be accepted. "	3
Overall, this paper is clearly written and easy to follow, but the methods may not apply to the more complex scene with less latency. My concern is that this limitation is from its setup. 	2
"The proposed method effectively handles several obstacles while adopting the existing IDE method with spike-based implementation and provided convincing experimental results.
However, I doubt that the theoretical improvements of this paper is significant enough especially with my poor understanding on the mathematical derivations in the paper."	2
The paper presents a nice step in an interesting direction. But it does not manage to clarify what exactly its innovations and possible applications are.	3
"The paper proposes ""big prototypes"" for few shot learning, which represents the prototypes by hyper-spheres with dynamic sizes. The new method is easy to implement and can reduce the influence of biased data. Extensive experiments in both NLP and CV demonstrate the effectiveness of the proposed method. More details or analysis are needed to improve the paper."	3
"The paper lacks  thorough literature review. To decide whether this solution indeed works, the authors should fully discuss and empirically compare with existing works. Just comparing with prototypical network is far from enough. 

==== After rebuttal

Thanks for the new empirical results. I raise the score because the empirical part is now more completed. 
However, it is still unknown why using hyperspheres can obtain better performance than others (especially those using distribution)  consistently. More insights and theoretical analysis are still required.  

"	1
"As the author stated in Section 4, the experimental goal is not to lead in all the leaderboards, but to verify the effectiveness of the model only with baseline. Therefore, from this point of view, the proposed model is indeed better than the baseline model for comparison.

For me, the method proposed in the article is simple and has aroused my thinking. At the same time, it also brings some more worthy questions. I expect the author’s answer. At present, I think the proposed model is effective and enlightening. Therefore, I tend to suggest acceptance. Of course, I am also happy to see different opinions from other reviewers, and further adjust the score according to the feedback of the author."	3
"A good idea with an interesting multi-domain evaluation. However, the lack of comparison with modern approaches and the use of a single backbone from 4 years ago makes the method not as convincing as it could be. Ultimately, this is not enough for ICLR.

I strongly encourage the authors to adapt their simple idea to a wider range of methods (including hyperbolic prototypes) and add more baselines (and also significantly polish the writing and rigor). There is a potentially very good paper to come from this.

Post discussion: the authors have addressed my concerns and have added many experiments to better substantiate their claim. In the end, a simple yet interesting idea.

"	2
The paper is generally well written and the problem is motivated clearly. The paper also highlights contribution with reference to the compared methods. The experiments (empirical) are exhaustive, however, a few qualitative results on explanations of a few disparities in the performance of the proposed method (as indicated in the detailed review) would help understand the impact of the proposed method better. 	3
Totally, I think the paper is still not well-prepared for publication.	3
"This work introduces a simple mechanism to add relative attention / locality bias to linear attention to significantly improve their performance. The results showcase that using the proposed cosine re-weighting the linear attention achieves similar performance to vanilla transformers while being significantly faster.
"	3
"Please see the comments in ""Main Review""."	3
I vote for this paper to be accepted. Apart from a lack of polish, there are no glaring errors in the paper, and the key ideas in the paper are easy to follow and appreciate. 	3
My biggest concern with the paper is about the significance. I believe that the idea behind the paper is simply optimizing a convex combination of functions, and that the search formalism that was imposed on top is mostly an implementation detail from a theory point of view. I also find the experiments unsurprising and not particularly insightful. Other than that, I do not believe the paper to be technically flawed or problematic in its claims. However, I find the significance concern is serious enough to prevent me from recommending acceptance. I look forward to a robust discussion with the authors on that point.	2
"I would like the authors to address the comments above. Should they do so, I am willing to reconsider my score.

---

After discussions with the authors, I raised my score from a 5 to an 8."	3
Well-written, but in my view not very novel/just incremental work over past work on subgame solving. 	2
Overall, I think the contribution would have been stronger with a proper grounding of why the problem is ecologically valid, why the solution proposed is an appropriate one, and why specific design choices (box embedding) were made. There are also missing ablations and rigorous evaluation of the model.	2
I think this paper is a very interesting paper for concept learning from image and natural language. The motivation of this paper is very natural and the architecture is chosen appropriately. As long as the author could address my questions for the setting, datasets, and details of the approach, I would recommend accept.	3
The paper proposes a novel framework for fast visual concept learning that builds on a neuro-symbolic program executor, box embeddings, and graph neural networks. Even if the individual components might be from the existing work, the overall approach presents a novel technical approach. The evaluation based on downstream question answering shows the competitive performance of the proposed model over baselines in multiple benchmarks. Given the novelty and the effectiveness, I believe the paper makes a solid contribution.	3
"I find the paper interesting to read, and believe there are enough novelties in the proposed paper. I recommend a rating of “weakly accept”.
"	3
"At this point, I cannot vote for acceptance, as 

* the provided theory is helpful to get an overview over the various general ideas of tackling the exploding and vanishing gradient problem, however, it is often prohibitively hard to prove (or it is simply not true) that the RNN exhibits specific dynamics (for instance limit-cycle behavior), due to the role of the external input, for which preferably no prior assumption should be made. That being said, the provided framework might be very limited in its usefulness, when it comes to constructing new RNN architectures which mitigate the exploding and vanishing gradient problem.
* the suggested training method seems to be a minor deviation of very standard approaches
* the empirical evidence is not sufficient (more diverse datasets should be considered) and the presented results lack more useful comparisons. 

I am happy to increase my rating, if the authors can resolve my concerns.


=====POST-REBUTTAL COMMENTS========

In general, I'm satisfied with how the authors addressed my minor concerns, in particular I appreciate the additional experiments.\
However, my main concern that the theoretical framework provided is very limited in its usefulness remains.\
I raise my score to (very) marginal acceptance."	2
Paper has two main contributions: (a) theoretical connections between loss gradients and Lyapunov spectrum that help in analyzing various RNN dynamics including chaos, and (b) sparse forced BPTT. The latter has been explored in the literature and the paper fails to mention these works in the literature review. Theoretical contribution is of some novelty, but it would be another way to explain EVGP, albeit with dynamics in mind. 	2
Overall solid paper. Please elaborate on the weaknesses mentioned above.	3
Based on what is provided it is difficult for me to believe the claims are valid.	2
The paper is in a worthwhile direction, but ultimately has too many oversights and errors to be accepted as is. The attack methodology relied upon is known to be incorrect and suboptimal. The regularization introduced is a particular flavor of a defense which is known to be suboptimal and easily attacked. The claim of state-of-the-art performance is outdated for BNNs. 	1
"The posterior $p(\theta \mid D_{adv})$ in Eq. (5) is not mathematically rigorous because it does not contain $D$ on the left side, but $D$ is involved on the right side. 

I cannot see the definition of the information gain and it seems to be identical to the mutual information proposed in [1,2]. Moreover, the first term on the right side of  Eq. (8) is $H(y \mid x, D)$ because it is independent with $\theta$. The second term should be $E_\theta[H(y \mid x, \theta)]$ according to your derivation in Appendix.

SVGD is not proposed to minimize the loss function directly. Although the update of the particle models in Algorithm 1 is fine, it requires more effort to define the distribution for which we only know its unnormalized version. Furthermore, Eq. (9) needs to have the absolute value for the difference between $IG(x)$ and $IG(x_{adv})$. In addition, the notion here is inconsistent with the lack of $y$."	2
I recommend to not accept this paper. The theoretical analysis of the Information Gain term appears vacuous and distracting, although I could be missing something. The contributions (SVGD and Information Gain for BNN posterior sampling) are not significantly novel, and their relative importance is not properly evaluated. The experimental results appear strong, but it's difficult to judge the quality of the defense experiments without being able to inspect the to source code (in particular, the implementation of the EOT attack).	2
The paper is easy to follow, but the authors are expected to clarify their technical contributions and make more comprehensive experiments. 	2
A good paper with a novel ideas on oversquashing in GNN. The main limitation of the paper is the limited of empirical support.	4
I found the paper very interesting in that it analyzes the over-squashing problems through a new, geometric lens. The theoretical motivation for using curvature-based tools as opposed to random-walk-based rewiring is well-done. My main concern is that the authors’ notion of graph structure preservation is not convincing to me. The experiments could be more comprehensive. Please see detailed comments.	3
"Despite my many comments, I think that this is an important paper, which deepens our understanding of the over-squashing phenomenon from a geometric perspective.
While many recent GNN papers introduce new application domains, new features, and new GNN architectures, this paper improves our understanding of the foundations of existing GNNs and their limitations, which is even more important.

I will increase my rating if the authors would answer all questions in the ""Writing"" section above, in the ""Additional Questions"" section above, and provide more evaluation datasets (as detailed in the ""Evaluation"" section above).

===== Update =====

I have increased my rating to 10, good work.

Note that Table 2 is now too wide and goes beyond the page's borders."	4
This is novel work on a seemingly important effect hindering GCN performance in general. Giving a formal definition of over-squashing and providing an analysis based on curvature is novel work, and potentially very useful to the field. 	4
I do not believe this work is ready for publication in its current form due to significant clarity issues. I was not able to fully understand all the details of the proposed approach, so it is possible that I've missed some crucial details, but I don't think its current presentation is sufficiently clear.	2
This paper proposes an attention-based architecture for actor-critic algorithms. While the idea is novel and has interesting elements to it, this submission is not ready for publication. The presentation is imprecise, to the point of obfuscating what is being optimised, while the motivation is either non-existent or makes sweeping claims that has no clear support. Empirically, the results do not sufficient to conclude that the proposed method presents an improvement for actor-critic algorithms.	2
The intuition and meaning of the attention mechanism need more explanations. The advantage of the proposed method seems to rely on an accurate critic network and this advantage may become a disadvantage when the critic is not good enough.	3
The proposed method brings an interesting perspective combining RL and meta-learning, and demonstrates better empirical results. However, the model design is not well motivated, and the optimization objective does not match the practical algorithm. Lastly, the empirical results are too concise for detailed performance assessment. I think the current paper would benefit from another revision. 	3
"I would recommend accepting the paper for the conference. The text can be streamlined (there is a couple of typos) but the results are pretty solid, the problem is relevant and the idea is novel. 
"	3
"The paper proposed a method to solve hybrid action control problems by encoding action to a continuous latent space via representation learning. The motivation is sound and the writting is clear. The technical contribution is significant. The experiments are comprehensive and support the paper's major claims. Overall, I recommend acceptance.

================== post-rebuttal ==================

Although I believe this work has some limitations that can be improved in the future, I feel the contributions of the current paper are already significant and I vote for acceptance."	3
"This is a very interesting paper that could be excellent with a bit more transparency in the method’s limitations as well as a bit more care in the experimental validation. I would increase my score if authors can provide details on the limitations of the method and tackle the major points I mention in the ""experiment"" part.

================== POST REBUTTAL 1 ================

I would like to thank the reviewers for the detailed answers and the revision of the manuscript.

The additional data provided by the authors in Table 7 and Figure 12 provide convincing proofs of the method's efficiency that were lacking (or at least that we could doubt) before the revision.
The additional ablation in Figure 13 brings nice insights on the methods. I even personnally would prefer to see it in the main text in place of the t-SNE visualizations as I find it more informative (but it is also fine like this!).

Insisting on these minor points:
I would rather not have footnote 2 and be more precise in the text.
I would also detail in the HP section of the appendix HOW hyperparemeters were selected and not only which ones were selected (even if the sentence just says that not much tuning was done, I think it is important.)

I am now recommending acceptance and raising my score to a 6 (it could be a 7 if it existed).
The work would be a clear 8 with an additional experiment on a more challenging and less toy-ish environment.
Yet, I understand it can be left for future work. 

"	3
"Overall the paper is well organized and written, and the provided code ensures the reproducibility. 
"	3
Overall, the paper proposes a self-supervised character generation and parsing by reinforcement learning. However, there are still some issues on modeling and training details that need to be clarified. Also, since the results are not comparable to the existing methods, it seems not significant for the proposed methods.	3
Generally speaking, I am not an expert in the field of stroke-based image generation and remain lukewarm about this project. Due to the strengths and weakness I mentioned above, I rate this submission as weak reject.	2
"Overall, I think there are potentially some interesting ideas within this work, but there is a significant lack of context with respect to recent works in this space that would need to be addressed. Further, the results suggest in some cases the proposed methodology performs significantly worse than other approaches.
"	2
The proposed approach presented as a deep-learning extension of AFT optimizes a rank-based loss function very similar to that in Raykar et al. (NeurIPS 2007: On Ranking in Survival Analysis: Bounds on the Concordance Index). The experiments results show that the proposed approach does not outperform standard Cox in 3 out of 4 datasets.	2
While there is a large number of machine learning methods for survival, by considering a (for the machine learning literature) novel loss function in the AFT setting, this paper can be seen as a meaningful contribution to the machine learning for survival literature. The theoretical foundation of the loss function is however not perfect and the influence of loss function and architecture are hard to disentangle. Due to the form of the loss function, a formal scaling analysis would also be needed. I would recommend a weak accept. I could be convinced to improve of the score with a more convincing argument why the loss term in the non-linear setting is a good loss term, maybe through some upper bound on the loss term.	3
While the idea of combining Gehan model with deep learning is new, the results show that the proposed model does not outperform SOTA systems on several benchmarks.	2
This paper has extremely limited technical novelty, spends too much text on explaining existing standard results/definitions from survival analysis, and has experimental results that are suspect (inconsistent with existing literature).	2
Apart from empirical competitive quantitative results against baselines, the technical contributions are not clear. The semiparametric AFT model based on Gehan’s rank statistic objective function has been previously proposed. Additionally, while the paper claims to make time-to-event predictions, the conditional hazard transformation of model predictions seems to indicate the model predicts hazards instead.	1
"Technical contributions are very limited since the GO embedding module uses the same structure weighting, architecture, and loss function as existing method DeepGOA [1] and replaces its sequence embedding module with an existing protein sequence encoder, SeqVec [2]. Empirical results do not significantly improve, or in some cases even match, the previous method TALE [3], with the exception of prediction on the BPO.

The main difference is in the dataset in that it uses all GO terms that have a `is_a’ relation in the DAG, rather than the common approach of removing terms that are not seen or seldom seen in training. However later in the text it states that more deep BPO terms were also trimmed when the maximum depth was cut off to 80, so it is not clear which terms are used to evaluate after all. 

Motivation and comparison to prior work is very limited as the paper does not properly discuss differences to prior work or motivate the decision made. In particular, the choice of term “cut off” (which terms are used in training) are not empirically validated; the use of these particular architectures are not well-supported as the component modules are not used as baselines in experiments.
"	2
I find the paper not particularly novel. The paper needs more discussion and comparison with DeepGOA. The method itself is not novel enough for ICLR. 	1
This article proposes an interesting architecture for PFP, but the evaluation is not very complete, and the explanation of the method is very confusing.	3
Considering the above pros and cons, I think there is still a lot of room for improvement in this paper.	2
Thorough empirical study on whether BPE affects memorization in transformers. While the final conclusion, shorter input sequences lead to better memorization, is not well supported by experiments.	3
I set the score as marginally below the threshold given my concerns above where the main one is that the overall conclusions represents empirical findings without much of a contribution towards specific takeaways or practices which would be handy for the community. Nonetheless, I am open to discuss how this work can be positioned in this conference if there will be strong positive feedback from others.	2
The research questions explored in the work are very interesting and important. However, the experiments are relatively weak, and it is not clear how to apply their findings in practical tasks.	3
Good paper but I expected a bit more in terms of formalizing quality. 	3
"In summary this paper presents a series of interesting empirical results for smoothing techniques. However, I find the empirical results for the goodness of the smoothed attributions are not convincing to me, and the conclusions from the non-Lp attacks are not very new to the community, I remain negative for this paper.
"	2
"While the results of this work are potentially interesting, this paper has many
short-comings in its empirical evaluation, which is especially problematic
since it is the paper's main contribution. With small sample sizes, most errors
unreported, only a single model and dataset, and only a single run, there is a
lot of room to improve this work. As these changes involve running many more
experiments, for the sake of improving this work, I recommend a score of 3:
reject, not good enough."	3
The novelty and significance of this paper are in the experimental evaluation of robustness of the smoothing explanations that were known to be robust and any quality degradation trade-off. However, the experimental evidence is not fully supporting the authors' claims. The paper should provide experimental evidence showing significant robustness reduction. Also in-depth discussion about the experimental results is needed.	3
The paper gives a new method for zero-shot action recognition by learning the transduction from unseen action to test videos in hyperspherical space. The performances are good when compared with other state-of-the-art methods. However, the details of the technique  is a little hard to follow for me. In my opinion, the paper forms well and the writing is very good. I give accept currently because of my unfamiliarity of such domain. By the way, I will follow up the work in the following reviewing sections. 	3
The paper can be further strengthened by demonstrating fairer comparisons and adequately evaluating the importance of the critical components.	2
This paper proposes a sensible solution to reduce the bias between the source domain and the target domain for the task of action recognition. But novelty seems incremental. Also, some experiments seems a bit unconvincing and the approach seems not to scale to general settings. 	2
The idea of using transductive universal transport for zero-shot action recognition is new, and the performance is good. But the core setting that the entire testing set is available during training to get the distribution information is unrealistic. The writing, especially the math part, needs improvement.	3
The idea of this paper is conceptually simple and I quite like its application to this challenging setting of altruism without true rewards, as well as the analysis that the paper presents. However, mostly because of the choice of environments in the empirical evaluation, I feel that the paper hasn’t provided enough evidence for the claim of the applicability of this altruistic objective, and I think this is a borderline case.	3
"I think the paper tackles an important and rather neglected problem, proposes a novel and quite general method for this setting, and demonstrate significant empirical gains on multiple tasks. 

My main concern is that some of the potential failure modes of this approach are not discussed in great detail. Because the method is so general, there are many settings where it may not be the most effective approach (which, from my perspective, is not a problem in regards to publishing the paper as long as this point is clear to the readers). I would really like to see an experiment (or more) that aims to tackle a case where this method isn't expected to shine to 1) either openly illustrate the method's failure modes and where it shouldn't be used or perhaps 2) show that it still works decently well even in such settings. Another big concern is the comparisons with other baselines, including the tuning of AvE, and the lack of comparisons with AvE and the Supervised baseline for some of the tasks.

In conclusion, I am leaning towards rejecting this paper, but I am willing to increase my score if the authors can successfully answer the questions above and address my concerns (especially the ones mentioned in the above paragraph).
"	3
The authors are trying too many different(existing) methods to prove the central concept around how one agent can be altruistic to another by maximizing their reward. The paper is very hard to follow through. Although, the experiments are extensive. 	2
I enjoyed reading this paper and I think the core idea is a valuable contribution to the multi-agent RL research. My main concern is about its comparison with empowerment and other related multi-agent RL work, which is detailed in my main review. I would appreciate some clarification from the authors and would be willing to increase my rating if my concern is addressed in the rebuttal and the revision.	3
A novelty work, worth accept.	4
I feel the paper in its current form can be significantly strengthened in terms of the presentation of the empirical results by clearly explaining the trade-off achieved as well comparing with more algorithms from the literature as baselines. 	3
"The paper seems well written. Here are a few typos.

1) The first line on the first paragraph of section 2: $S=...$, a comma is missing after the symbol $\mathcal{G}_2$.

2) On page 6, in the sentence just above equation (12), *we drive* should be *we derive*...

"	3
My main review is based on the weaknesses I stated above. I am not totally confident about the importance/merit of the approach provided in this paper. Thus, I am giving a weak reject. 	3
The paper has various strong points, but the write-up can be improved. I found overall the paper hard to read; e.g., the choice of the dual GNN is not well justified (what issues is it resolving from GAP), reading the theorem 1 was kind of confusing, as the statement feels more as an observation derived from the definition of a partition etc.  I also found the term of permutation invariant for labels (0,1,1) vs (1,0,0) a bit confusing, as up to that point, I was expecting labels to be indicator vectors of which community a node participates in. 	3
The presentation of the results is extremely poor, making empirical results very hard to interpret. I also have concerns regarding the problem setting and evaluation, so I do not view this paper as ready for publication at this time.	2
The paper proposes a new GNN-based framework for inductive graph partitioning. The authors conduct extensive experiments that cover a reasonable range of scenarios that showcase the usefulness of the proposed method. My concerns are (1) there is no generalization guarantee, and (2) the method may fail due to distribution shifts, as shown in the experiments.	3
The results look promising, but the novelty of this approach seems limited since the use of learnable masks, gating mechanisms, and layer-wise normalization have been previously proposed in the continual learning context. Moreover, the experiments are limited in terms of comparison with other state-of-the-are continual learning methods, evaluation metrics (such as backward and forward transfer) that characterizes the forgetting behavior.	3
The findings are interesting, suggesting that research on continual learning should pay more attention to designing neural architectures that are natively suitable to handle catastrophic forgetting. However, the novelty seems to be limited as it is based on well-known architectures and ideas.	2
I think this is a strong paper with compelling experimental results. I vote to accept it. Some small grammatical errors should be corrected.	4
The paper is hard to read and uses strange notation. The proposed modification seems to be small and not significant. The experimental section looks nice and is convincing, but authors should use some additional architecture and show how the method works on adam optimizer.	3
It's a decent paper - it would be better if there was more theoretical explanation for why the proposed changes to the architecture help with continual learning, but the provided empirical evaluation seems pretty good, and the proposed architectural changes to HCN seems quite effective.	3
While the proposed algorithm improves upon existing algorithms for learning linear SEMs, it comes at the cost of increased sample complexity. Unlike existing algorithms, the proposed algorithm is not applicable in the high-dimensional regime since the sample complexity grows as $O(p \log p)$. Given that existing algorithms are already polynomial time and the proposed algorithm doesn't improve upon them in terms of required identifiability conditions and has significantly worse sample complexity, I feel the contributions are not strong enough.	3
I find that the improved theoretical guarantees are not well motivated, and comparison to existing algorithms not clear enough.	2
The paper is well written and the contributions and claims are well supported, while the proposed method focuses on a restrictive class of SEMs.	2
"This paper has a good idea of learning linear SEMs. However, this version of paper includes some unrefined conditions and requires better simulation settings. 

I believe that this paper has a great potential and it would be a much better paper after modifying some points of the paper. 
"	3
Overall an efficient algorithm to learn causal relationship. The paper is held back by strong assumptions.	3
The paper proposes the integrated gradients and the method (TAIG)  to attack black-box models, which has shown good performance. But there are still some place to improve.	2
The paper is overall quite good, with thorough experiments demonstrating good performance.	3
In general, the paper explores an exciting direction, but I found the experiments a bit lacking. I would encourage the authors to address the questions listed above	3
This paper proposes a novel algorithm to generate adversarial examples, and the results are good. However, some analysis and experimental results are confusing. The most important weakness is the high computational cost. Please address issues listed in the detailed comments during the rebuttal and make the paper clearer.	2
Noteworthy application of variational auto-encoders to improve classification in the presence of missing data. Overall a solid paper, I recommend accepting the paper in the conference.  	3
The addition of supervision and joint training to MIWAE seems novel, and while the work improves upon the prior methods of using DLVM for missing data, it falls short in the claim of scalability and overall gains in performance across different types of datasets. 	2
The work is good but not novel enough. Need to show results on larger datasets. Please also highlight the fail cases in the paper. 	2
A good paper including very clear and technically sound principles for solving the problem of supervised learning with missing data. Experiments are illustrative although it would be needed to implement them on bigger datasets and deeper neural network architectures as well as to compare to other methods in the literature.	3
"Post-rebuttal: 
After carefully re-checking the paper with the feedback from the authors, I have fully convinced by the methods and therefore changed my recommendation to acceptance. 

==================================

The paper proposes a seemingly neat algorithm and presents a good amount of experiments. However, some critical correctness flaws cannot be rigorously justified based on the current content of the paper due to too many hand-wavy arguments. The readability of the paper should be improved as well.

I cannot recommend acceptance based on the current form of the paper. But, I would still believe in the potential of the algorithm based on the experiment results. That being said, the paper still has the potential to become a really strong one if the algorithm can be rigorously (or mathematically) justified."	3
-The proposed idea is novel, but the author should provide further explanation on collaboration criterion and further experiments to see where the performance gain comes from. 	3
I think the submission overall is good for its novelty and experiment results. I only have some minor concerns about the theoretical backup of the proposed method. Thus, I suggest accepting the paper.	3
"In summary, this paper has some novolty but stays at the application level, though this is not my concern. My main concern is the effectiveness of the main contribution claimed in the paper, i.e. the maximization-minimization MI. I concern that whether the performance improvement mainly comes from the specific architecture $T$ used in the framework.

For the current version of paper, I can only recommend reject (but marginally below the threshold) due to the weakneses and concerns above."	3
The paper seems to have a fundamental misunderstanding of the Wasserstein gradient flow for the relative entropy, and the experimental evaluations may not be appropriate. 	2
This paper leverage the microscopic equivalence of the Wasserstein gradient flow of the relative entropy to sample from an unnormalized distribution, but the derivation of the key step in the proposed approach is not well explained. 	2
The idea is elegant interesting but the paper lacks evidence for its usefulness, both from theoretical and applied perspectives.	2
I find the approach interesting. However, there are some issue with the paper as it is, both theoretical and empirical. From a theoretical point of view, there is no discussion about conditions for convergence of the algorithm. From an empirical point of view, the numerical experiments are not complete enough, with respect to the comparison analysis that is carried out, but also with respect to compensating missing theoretical analysis. Overall, the paper is interesting, but in its current form is not ripe enough for publication.	3
I have mixed feelings on this paper. Overall, I think the technical novelty and impact of theoretical contributions is potentially limited, but some of the experiments in distribution shift/MDP ambiguity are interesting. Currently I'm not comfortable accepting the paper on account of dubious theoretical contributions and limited scope/technical novelty, but if the authors can convincingly argue why their bound in Theorem 3.1 is meaningful (motivate the supervised version of the loss), add additional baselines to make the paper less specific to just FOCAL/COMRL, and improve the writing in the theoretical sections, I am open to accepting it (but could be convinced otherwise).	2
"In general, the authors have proposed a modification to an existing algorithm, FOCAL, which incorporates attention and contrastive learning.
They evaluate on only one dataset. In spite of these weaknesses, the approach has strong mathematical foundations, and results on the Mujoco dataset look promising. "	2
Overall, I think this paper proposes reasonable modifications for context-based offline meta-RL methods. But, I worry that their results focus too narrowly on FOCAL to be of wide interest to the ICLR community.	2
The architecture improvement on FOCAL is a good contribution in practice, while the theoretical analysis part requires some work for more solid discussion and proof.	3
This paper study a new research topic for attacking neural models, i.e., efficiency attack, but it is not as meaningful as accuracy attack. 	3
The paper is very interesting and the contribution is clear, but the claims are not well supported by the existed experiments. Therefore, I give 5 (weak reject) at this moment and would like to increase/decrease my review score after the author response.	3
The paper proposes an interesting idea of a new attacking target of NMT models, but has some major flaws such as limited contribution and inadequate evaluation. Therefore I vote for a rejection.	2
The paper explores an important aspect - efficiency robustness of NMT systems - that is clearly understudied in the literature. Some missing details and experimental setups prevent me from giving a stronger acceptance recommendation.	4
Overall it's a very interesting paper and would recommend acceptance just for the new variance based metric for coordination edge selection. 	4
The paper is well-written and seems to be of significant contribution to this research area. There are some limitations that are a bit concerning and it would be significantly better if the authors could provide more comprehensive study and explanations on those main limitations, but if not possible due to time limitation, I won't object for acceptance of the paper based on the current results and leave those potential improvements as future research work.	3
"I reviewed this paper before, and the authors improved the manuscript and the paper reads much more nicely now. Empirical results are good, and the authors hypothesise on why this or that experiment went that way. 

However, I give it a 5 (marginally below the threshold) due to the fact that the authors decided not to include the related work that the reviewers suggested, e.g. Deep Implicit Coordination Graphs for Multi-agent RL by Li et al. I am willing to increase the score if:
* authors provide a legitimate reason for not including relevant work into this submission, and
* include the relevant work and compare to it / explain why such comparison is not meaningful."	3
"The introduced method is novel and relevant. Although the reviewer was not able to follow the entire proof, the edge selection seems well motivated. Experiments are sometimes weirdly evaluated, but clear enough to say that in some cases using sparse graphs is an advantage. The reviewer would have liked to see more intuition when this is the case though.

In summary, this is a good paper that merits publication. The reviewer would consider increasing the score if the authors fix point (1) or convincingly explain why number of scanned targets is the correct evaluation metric. "	4
"The reason why I can't recommend this as one of the ICLR papers is
1. The technical novelty is almost none.
2. Many other experiment results should be provided in order to assess various aspects of the proposed model."	2
I vote for rejection. The method itself is not novel because it is a standard application of genetic algorithm. The results show that the proposed method did not outperform existing baselines. The main claim of the paper (benefit of two-stage procedure) is not supported by ablation study.	1
The method lacks novelty and experiments are not convincing.	2
The paper explores reasonable techniques, but the experimental evaluation is limited, and the overall setting is not very relevant to real-world tasks. Moreover, many things are left unclear. I believe the paper needs more work to be made ready for publication. 	2
Overall, a good mathematical study is important to have in a fast-moving field that certainly lacks theoretical results. However, I find that this study might not be that helpful practically as the setting is limited in linear RNN encoder-decoder setup, which is unlikely to be considered as a main choice of architecture these days. Also, as an ML researcher, it is a bit hard to follow a paper that is written in a formal mathematical style with a very small experiment at the end.	2
Generally a good paper that tries to theoretically explain the approximation power of RNN encoder-decoder architecture, although in linear and continuous-time setting. It provides intuition for the required characteristics of the target to ensure low approximation error and how to control number of parameters vs approximation error. A few places in the text and equations need authors' attention and better explanation of some assumptions are required.	3
This paper gives interesting notions and discoveries on the approximation properties of the recurrent encoder-decoder structure, and the statements are rigorous.	4
Similar to some prior work, the paper exposes the inability of (a subset of) pretrained language models (BERT, RoBERTa) to do complex logical reasoning, showcasing that their performance is much lower than what humans score. This is done through the introduction of a new task called naive logical reasoning. Overall, while the NAIL dataset can be quite helpful in guiding future advances on equipping language models with knowledge and methods required to solve such logical 'puzzles', the paper should have done a much better work on the aspects of motivation, discussion, analyses, and higher-level implications of the main results. I feel that more work is needed (see the main review).	3
"Please provide a short summary justifying your recommendation of the paper.
Good paper
Very useful dataset
All reasoning steps in the development are properly justified
Experimentation is reasonable and accurate
"	3
"The authors manually constructed a dataset that aims to train and evaluate the model’s capabilities in naïve logical reasoning, which consists of 10,296 instances. The instances were extracted from Chinese National Civil Servants Examination (CNCSE) and Law School Admission Test (LSAT) and those instances were re-written by humans to increase the size of the dataset. However, all the instances in the dataset were selected or written by humans, and there is no valuable or useful method proposed in the dataset-construction process. Besides, it is not clear how they guarantee the quality of translation. There are even many grammatical errors in the examples given in this paper. For example, “Selecting Prince Charming”, “Xiao Li’s ideal gift has following characters”, etc. It seems that the main difficulty of answering the questions in the constructed dataset lies in semantic parsing rather than logical reasoning.

"	1
The proposed dataset is carefully designed and may be useful for pushing the boundary of assessing the logical reasoning capability of systems through the reading comprehension task, but experiments are insufficient for empirically showing the usefulness of the dataset.	3
Overall, I think the paper presents an interesting approach that shows significant improvement in multi-label classification. However, the experimental analysis could be strengthen by improving the presentation of the results, and by considering more benchmarks.	3
"Strong:
1.	The idea of SEAL is novel and effective to a number of tasks;
2.	This paper is well written and the direction of using of dynamic loss functions is an interesting direction.

Weak:
1.	Less aware of the signicient technical aspects of using energy network for the target tasks. Any other baselines of not using energy related networks in table 2? [thanks the authors' responses for the detailed explanation of this point. Now I have a better understanding of the benefits of applying ""energy-based methods"" to the corresponding tasks.]"	3
I am more inclined to marginally accept the paper given its current status.	2
Due to poor presentation, a lack of empirical comparison, and no theoretical analysis in the underlying paper, I believe this paper is not in a suitable form for publication right now.	2
While the paper was well written and the ideas made sense as presented, however, my concerns with the amount of theoretical novelty combined with an empirical situation in which I have some questions is going to make my recommendation at 5, marginally below threshold.  	2
Overall, the significance and novelty of the work is not so clear to me, since it seems like it's a straightforward application of an off-the-shelf RL algorithm for choosing parameters of online continual learning. On the other hand, it is interesting that this setup does give performance improvements. But the biggest reason for my score is that the tradeoff between adding the complexity of DQN for choosing hyper-parameters vs performance gains is not discussed or quantified.	2
Unfortunately, I recommend the rejection of this paper. I believe that the problem that the authors are proposing is insightful in itself, and certainly captures one current issue with most CL evaluations today: that most approaches tune hyper-parameters in an off-line setting, which is inconsistent with the CL problem formulation. However, in my opinion there are a number of shortcomings (primarily empirical) that should be addressed for turning this into a complete conference publication. Most critically, there are a number of issues in the evaluation setting and in the results themselves that should be addressed prior to publishing the work. I do strongly encourage the authors to continue improving upon their work, as I believe that it could be highly impactful.	3
The paper addresses an important variant of continual learning, and the proposed method is simple and applicable on top of many existing approaches. On the other hand, the application of RL isn’t well-motivated, and some crucial details are missing that may warrant reevaluation of the proposed methods, such as by achieving a computational parity with competing methods.	3
A solid work in general, yet several aspects of the empirical evaluation could be improved and some details are missing.	3
"### Recommendation

This is a good paper. Although I wouldn't label as ""significant"" the contribution of going from a Butterfly to its blocked counterpart, the fact that the Authors also demonstrate that the product of Butterfly factors can be replace with a sum of factors, makes it a substantial contribution. Experimental evaluation includes a variate of datasets, that is a big plus. There are other recent works proposing methods to accelerate training with sparsity. However, they require specialise hardware. Authors could consider adding them to their related work or introduction: [1, 2] (note, I’m not an author of these works). I wasn’t familiar with NTK, glad I know about this now. 

- [1] https://arxiv.org/abs/2001.01969
- [2]https://openaccess.thecvf.com/content_CVPR_2020/html/Goli_ReSprop_Reuse_Sparsified_Backpropagation_CVPR_2020_paper.html


### Supporting my Recommendation

Please see points above.


### Minor points

- I believe Figure 4 is not ready for colourblind reader"	3
This is an interesting, well explained and soundly tested method. Sparse training for large neural networks is an up-to-date and very interesting research topic, and the paper provides an innovative approach towards this research field. 	4
"Good paper with some question marks around the utility, configurability and practicality of the presented approach, and the methodology of speedup measurements.

-----------------

All of my comments/questions were addressed. I would've liked to see more ablations regarding the accuracy-efficiency tradeoffs but the presented response is more than sufficient. I will therefore increase my score from 6 --> 8."	3
As mentioned below, the overall framework is not convincing to me and hence I recommend rejection.	2
"The paper introduces good albeit ""unsurprising"" conjecture. The paper doesn't support the conjecture with enough theoretical analysis and even the conditions are not fully defined and just relegated to ""natural"" settings. The paper stands as a good initial study but needs more analysis. "	2
The empirical phenomenon reported by the paper is novel to the best of my knowledge. It is also an important observation to be made in the behavior of deep neural nets. The paper presents a possible reason behind this behavior and this is formalized in a clean manner. Overall, the paper is very well-written and performs a comprehensive set of experiments to back their claims. I am not entirely convinced the observation holds in the strong form it is stated in but nevertheless I feel the paper’s contributions are sufficient for it to clear the bar for acceptance. Some changes in the empirical experiments which would help make the paper’s case stronger are (i) error bars on plots of label noise experiments (ii) label noise experiments on ImageNet data, (iii) including code in the supplementary material.	2
The paper is clearly written and adds a contribution to the GNNs society. Also, the numerical experiments suggest improvement over other data sets. Some of the experiments can be improved, and since topology is of high interest geometrical datasets and application, I think that some discussion should be added, and will strengthen the paper.	3
Motivated by topological data analysis, the paper proposed the TOGL plugin for GNN to boost the ability of topological structure detection. The idea is insightful and is analyzed theoretically and empirically. Though I have a few concerns as listed in Main Review, I would like to weakly accept the work. Surely, I will appreciate it if any of my concerns can be addressed.	3
The paper roots in a nice theoretical framework but shorts fall on experimental side.	3
This is a competent paper that builds upon existing works and provides an interesting application case of TDA and may be a good addition to GNN toolboxes.	3
"
This is an interesting paper with good ideas, but is held back by an
overall confusing experimental evaluation. The theoretical arguments
that motivate the proposed algorithm are well thought out. The empirical
analysis, however, does not give confidence that the proposed algorithm
is an improvement in the action-less regime. I would like to see an
environment that clearly shows that densified rewards with the proposed
algorithm is superior to densified rewards with a learned value function
via TD0. I also think that the correlation performance criterion needs
to be better justified. I will currently leave my rating at below the
acceptance threshold, but I am open to increasing my score if some of my
concerns are addressed.

Edit: After discussion with the authors, I have increased my score to a 6.
"	3
"The paper tackles an interesting topic and proposes a simple approach which performs well empirically. I don't think the theoretical result is very strong, but the paper focuses more on the empirical aspects so it's not a major concern. Also, the paper is generally well-written although there are certain areas that require more clarification, which is easily addressed.
Overall, given the thorough experiments and the interesting problem, I'm leaning towards acceptance.

"	3
This is a clean idea, presented carefully, with interesting theory and well conducted, insightful experiments. 	3
While the problem setting, i.e., state-only offline RL, is realistic and practically relevant (cheaper that classic RL), and the paper does contain some interesting ideas in terms of theory and algorithm, the paper is too premature. The writing sometimes lacks motivations and connections, and the theory, I think, can actually be extended (be made more systematic) already within this paper, this would be a deeper contribution.	2
The authors have conducted a careful experimental study to build better MLPs. However, in its current form, I believe the paper's contributions are marginal. However, I think the direction of study is pretty exciting and can be extended to study many related exciting directions.	2
The paper is below the ICLR standard.	1
Since the experimental evaluation of the proposed network architecture was not convincing for us, and since there are no new theoretical analyses in the publication, we have doubts on the relevance of this paper for ICLR and therefore recommend rejecting the paper.	2
"A (somewhat)novel, generic MLP block is proposed. Unfortunately it is not shown to outperform other blocks empirically.
"	2
"On the whole, the work looks plausible for voice conversion, even if it only used the base Euler-Mayurama scheme. The derived scheme produces better samples, or does so allowing use of much larger timesteps (~ N = 6). Nevertheless, I am not totally convinced because
1) It is not explained why the optimization 'works' in an intuitive sense. I assume that it provides a path with better likelihood since it has been optimized
2) The derivations ought to be gone over very carefully, and it is crucial that the details are correct. I would like the other reviewers to take a careful look at the derivations, especially so as I feel that there is a mistake in equation (40), which could carry over to the other steps as well. 

I rule this work as a marginal reject, but can change my decision after discussion and clarification. "	3
Based on the above main review (especially the strengths of the paper), the proposed method is novel enough and the experiments can well support the effectiveness of the proposed method.  think the paper could be accepted for publication on ICLR.	4
A paper of high quality, featuring substantial contributions that can be valuable to anyone interested in diffusion models. Code and models are available and may have practical usages.	4
"As mentioned in the main review, I'm not exactly satisfied with the lack of many potential discussions and further analysis of why and how RPG works. There clearly are a lot of things that can be studied here, and mere empirical evidence may not be the only thing that we need. Also, while the results are good, the method is not too different from the canonical hypernetwork idea which is also trained end-to-end. On the other hand, I feel the paper has a strong set of empirical evidence demonstrating the effectiveness of RPG and I'm surprised by the results.


-------------------

Post-rebuttal: See my comment below. I keep my score of 6."	2
The paper presents interesting results and shows that the proposed technique offers efficient parameter utilization. Applying the proposed technique in the subsequent iterations of the same network outperforms the original model. While parameter reduction is one of the main contribution claims of the paper, the experiments lack direct baselines focusing on this task. 	3
"

Balancing the strengths and weaknesses of the proposed method, I would like to recommend a rating of weak rejection for this paper unless the authors provide a persuasive response to the above negative comments.
"	3
"Inherently, this recurrent parameter generator (RPG) is one kind of vector quantization methods, which have alread studied in some papers. Such as the citiation ""LegoNet"" in the paper and ""Quantized Convolutional Neural Networks for Mobile Devices"" in CVPR2016.  "	2
This paper introduces a new perspective in Byzantine defense in comparison to typical robust learning defense or other detection defenses. While the algorithm and experiments done are relatively simple, the paper serves more as a starting point for research and application in the new domain. Claims are properly supported and the method's competitive performance compared to other methods is promising.	3
I recommend accepting this paper, as it provides a novel technique with sound theoretical and convincing empirical justifications for attacking an important problem in the application of federated learning.	3
I do not recommend acceptance because formal robustness guarantee is missing.	2
"This work proposes an extension of the method ""SplitFed"" of Thapa et al. 2020, where a common transformer based backbone is trained across *different* tasks; this is a natural extension of the prior work. This method has been shown to achieve similar/marginally better performance across all the five tasks considered in this work. However, the scale of experimentation is small (only 5 clients), and idea itself carries incremental novelty. In view of the above, I can recommend this paper further, but perhaps with some reservation.

-------------------------------

Post authors' response:

Thank the authors for taking the time to address the concerns raised in the review. However, as detailed in individual comments, their response is not convincing. The key points are: (1) no fundamental reason to favor Transformers over other neural modules, (2) no guarantees of privacy preservation, despite claims to this effect, (3) no large-scale distributed study to validate their design. Hence, I urge the authors to address these, and revise the paper. I am unable to recommend this work further in the current form. "	3
"The submission presents a specific system that combines split and federated learning for multi-task learning of various image processing applications. It offers a good proof of concept of the proposed architecture decomposition, but lacks a robust discussion on communication cost/overhead as well as privacy guarantees. In particular, the ethics statement relativizes what the title claims (""privacy-preserving"")."	2
"This paper is well-written and presented. However, some key experiments and designs confuse me a lot. Overview, it approaches the borderline of the ICLR community. The authors should address the above concerns (# Main Review) in the rebuttal period. 

After reading the responses from the authors, the authors partially solved my concerns. Thus, I decided to increase the rating."	2
I vote for accepting this paper. Its technical novelties and contributions are sufficient, and the given system seems practical and effective. It applies federal learning to image restoration tasks. It leverages ViT for universal representation learning, and task-specific heads and tails for training different tasks. The results are convincing. I think it is worth giving a brief study about how this framework reacts to privacy attacks.	3
While this paper studies an important topic of deep learning, generalization capability of deep neural networks, the key findings of this paper are either not novel or lack comparison with existing works. I suggest the authors perform a thorough literature review in the recent progress of theoretical research in deep learning, and then carefully place their work under the context of the literature.	1
I am not very familiar with the recent prior work that this paper cites and builds upon its approach and I have not completely checked the mathematical correctness of the claims of the paper. But, overall, given the importance of the problem analyzed by the paper, i.e. generalization performance of neural networks using NTK approach, and due to mix of theoretical results of the paper with empirical results, I am inclined towards accepting the paper.	3
"The paper should have focused on its strengths which is the empirical study that links the NTK spectrum to generalization in neural networks. The theoretical results apply to kernels in general and, thus, do not see much novelty in this regard. The related work does not adequately cover prior developments in kernel methods and fails to place the claimed theoretical contributions within this scope.
I would recommend a major revision with a proper review of prior work on kernels. The focus of the paper should be empirical and presentation of its empirical findings should receive more attention/pages. I also fail to see sufficient theoretical contribution relative to prior work on kernel methods."	2
"On the one hand, this is a really nice theoretical paper about generalization for kernel regression. I think the results are useful, intuitive, correct, and bring new insight that let us understand better how kernel regression works. On the other hand, I do not think the analysis supports that this is a first-principles theory of generalization in deep learning. The results may very well be relevant for explaining generalization in deep neural networks, but the theoretical analysis and experimental evidence for that are not particularly convincing as of yet.

My recommendation that this is marginally below the acceptance threshold is based on the framing of the paper and statements that about deep learning that are not as careful as the really nice analysis of kernel regression is. If some of these statements are revised (or perhaps supported with more evidence), and/or if there's more discussion of when they expect their results to apply to DNNs and when they expect it to break down, I will substantially increase my score and recommend an acceptance.

### After Author Responses

The authors have addressed a number of my concerns, and I have updated my score accordingly."	4
"1. I see very little novelty in this work. Most results follow by combining some simple identities for expectations of random variables with convergence results for Local SGD from prior literature (Wang et al., 2020a), (Li et al., 2020c). Adding a global stepsize also does not change almost anything in the proofs.  
2. The results do not have a clear significance. While Federated Learning is an important and challenging problem, and client sampling may play a big role in its efficient use in practice, the results in this paper provide us with little new insight."	2
This paper proposes a new framework to study the effect of client sampling strategies based on weighting of clients/samples in a federating learning setup. While the framework is new and provides new insights, there are some clarifications required in terms of the theoretical convergence results. The experimental results are also incomplete and not up to the mark. Additional experiments are required. The paper is in a good shape, but without the improvements above it's not a strong paper yet.	3
The paper is well-organized. But I am not convinced by the significance of the theoretical analysis of this paper.	2
Overall I appreciate author's effort in formulating and understanding the nuances in client sampling. My main concern of this work lies in the significance of the results as well as the interpretation. As this time I think the paper is marginally below the acceptance threshold, but I am happy to re-evaluate the paper if the authors can address my concerns.	3
Overall, this paper introduces interesting ideas, but it needs significant revision. The assumptions are limited and analysis can be significantly generalized. In this paper, there are no theoretical and experimental comparisons with other methods. Moreover, only two distributions are compared, additional comparison with other distributions is needed. 	3
This paper focuses on fairness in FL from an interesting angle. However, the current comparison between UFL and FFL via FedAvg doesn't make sense to me. More explanations are also needed for both algorithm and empirical results.	2
"The authors proposition appears to be novel and with a genuine interest. Authors did a great job at scoping their work and rigorously used a mathematical definition for fairness evaluation.
However, paper structure needs some rework: some keys components, required for the reader to understand the paper, are not in the main content but are in the appendix. Some elements (e.g. UFL) are presented as keys, but their impact on the overall demonstration is not clear and may be moved to appendix. Also, some claims are not fully supported yet.
Finally, experimental cases, and some proofs, are not straightforward to generalize enough mainly due to binary class problems only and federating only two clients."	1
"1. The paper is not well-written and hard to follow. Some of the details are missing, which makes me hard to justify the claims.
2. The complete convergence analysis of the proposed algorithm is missing.
3. The experiments are not performed extensively. Thus, the claims in the paper are not fully supported."	2
Overall, I recommend the paper. It considers an important problem, fair federated learning, analyzes and compares existing approaches, and proposes a convincing framework. The writing is nice and clear, especially the discussion in Section 3.	4
While the idea of this paper is neat, there are a few details missing. The empirical studies lack comprehensiveness to support that the proposed method can generalize. I would be more than happy to change my score if the concerns are addressed well.	3
"This is an interesting work to utilize the grounding between image regions and text words, as a prompt tuning way for pre-trained vision language models, and shows promising results on three refCOCO tasks in zero-shot and few-shot settings. One concern or question is to verify the generalization capability of this work in more broad settings or diverse tasks, or it is just task-specific for refCOCO.

Btw, I am not sure whether it is okay for section 8 & 9, which are on the page 10 (beyond 9 page limit)."	2
Pls see the details in main review.	3
"I think the paper is good and has the opportunity to be in ICLR. I would appreciate it if my concerns can be addressed in the rebuttal session.

-------------------------------------------------------------------------------------------------------------------
After reading the rebuttal, my concerns have been addressed partially. I will keep my original score. "	3
"Overall, I recommend accepting this paper because it provides a theoretically clean analysis of a novel insight that is relevant to important problems. The key insight is that one can use the Boltzmann distribution over policies as a prior, to make behavioral cloning more data efficient. The experiments and presentation are overall convincing. I have concerns about the behavioral cloning baseline and the relevance to learning preferences.


UPDATE: Although we were not able to resolve some confusion during the discussion period, my substantive comments have been addressed so I have updated my score now. "	4
I’m overall positive about this paper. The idea is interesting and the experiments illustrate the effectiveness of the new model. The model has the potentials to be applied and further adapted to other settings.	3
The paper addresses and motivates an interesting problem and further solves modifies the previously known Boltzmann rationality, introducing Boltmann Policy Distribution, in order to adapt to the human behaviour over time. The technical details of the approach are well written and supported and experiments are well designed to give illustrations of the cases where the Boltmann rationality will fail while BPD manages to predict human behaviour. I believe the paper is a good paper and authors have taken time to write it in an elegant way and design insightful examples to motivate their problem and show the advantages of their method. 	3
"* I think it would be great if the connection between the proposed method and GAIL could be stated more clearly (if there are any). And empirical comparison with GAIL could be helpful, too. This is why I currently give 2 for the `2: The contributions are only marginally significant or novel.`

# Post-rebuttal
Thank you for your clarification! I have adjusted my score."	3
Overall, I think the required assumptions are strong and the theoretical results are not significant. 	2
Both the theoretical and experimental results are not sufficient. The assumptions of theoretical results are too strong and experimental results lack the comparison with the same kind of methods.	2
"In this paper SLIM-QN  is proposed. Converegence   results are provided in a stochastic setting. Numerical evaluations on real data  shown the advantage of in terms of  speed and accuracy.  I not quite sure why SLIM is faster than sgd (as reported  the wall-clock time) ? What is the stopping rule the authors used  for SGD ？
"	3
This work provides interesting empirical results that a second-order optimization method (other than K-FAC) can achieve fast training in the ImageNet scale. Also, this is the first result that a Vision Transformer model is trained by a second-order optimization method faster than SGD. These results can be a data point for future studies on more efficient optimization methods for deep neural networks. However, the motivation for small-batch training, which is the main target of this study, is not convincing. Moreover, the validity of the comparison between the proposed and existing methods (K-FAC) is questionable. 	2
Overall I feel that both the novelty and impact of the paper are somewhat limited. The key techniques proposed have been studied before, and the resulting method is only practical for small problems. Further the gains from this method are rather small compared to current optimizers, and the authors do not compare against the best optimizers.	2
The promising results confirm the effectiveness of the proposed SSA to attack the ASR system. The authors also provide a comprehensive analysis of hyper-parameters setups to the achievable performance. Moreover, demo audio samples are provided by a website link. The paper is well-written, and the theoretical part should be correct. However, we think additional experiments should be conducted before the paper can be accepted for publication. Moreover, the authors are suggested to summarize the major theoretical contributions on this study and discuss potential impacts to related research fields	2
Technically this paper is sound. However, the contribution here is rather narrow.  It is interesting, but not especially consequential, that CVAE manipulation can be used to localize the contribution of an adversarial attack.  It is unclear that the presence or absence of a carrier speech signal in favor of a carrier tts utterance is a consequential modification of this attack.  	3
"I found this paper novel (to my best understanding) and interesting in that it provides a new adversarial attack mode. I wish that the paper is better justified as to when this kind of attack can be a real threat, instead of relying on the reader's own conjecture. The paper is missing discussions on the comparison to the other AIA methods. The demo signals sound natural, but it might be better to include some subjective test results as the optimization on the ""naturality"" part isn't too clear. "	3
Well written, novel contribution, good references, and compelling and complete experiments.	3
Overall, this paper is in good shape. The most critical concern is motivation. If the authors can provide a reasonable explanation on the motivation as well as other concerns mentioned above, I would like to consider recommending acceptance.	3
"Overall I like some ideas of this work but it seems the authors should provide more analysis to support their claim. More discussion on how the proposed supervision works is also beneficial.

Overall I think this paper is a good starting point. I may raise the score if the authors could solve the concerns on loss derivation (weaknesses 1) and provide analysis on complete bounding box prediction (weaknesses 2). I guess weaknesses 2 cannot be fully avoided, but a detailed analysis can back up the authors’ claims much better."	3
The paper presents interesting ideas for training monocular 3D object detectors with just LiDAR point clouds supervision, and the proposed solution is reasonable and achieves decent performance. Therefore I am leaning a little positive despite there are several concerns that I invite the authors to address in the rebuttal. 	3
This paper proposes a method named WeakM3D to use object-LiDAR points to supervise the 3D learning from monocular images. In order to align the object-LiDAR points with monocular 3D predictions, the paper analyses the challenges and presents corresponding techniques or loss constraints. The method can train the models without manual labels, which is practical for monocular detection tasks. 	3
This paper proposes an effective method for an important but under-addressed issue. However, the novelty of the task is limited and the comparison with prior methods does not seem sufficient.	2
This paper is easy to follow and well written and the hypothesis regarding the difference between PGD and TRADES is well analyzed. They present the problem of memorization in AT by robust overfitting and propose the regularization term by temporal ensembling approach with their hypothesis, also show the effectiveness of the proposed method in systematic experiments.	3
Overall, the findings of this paper lack some clarifications about their significance. Moreover, the proposed adversarial training method does not have enough novelty beyond currently existing methods. 	2
Overall, the paper is very well written, what the authors do is clear, makes sense, and is supported by a significant amount of data. There are really few weaknesses with the paper.	3
This paper presents a very rigorous empirical investigation of the dynamics of memorization in adversarial training. The provided insights are novel, properly isolated and clearly explained. Besides, the results are not only insightful from a theoretical perspective, but they also provide actionable insights. The inclusion of a few additional experiments could strengthen more the paper, but in the current form, I believe the paper is good enough for acceptance.	3
Authors propose 8-bit optimizers for training neural networks, and demonstrated its usefulness in one regime. There are several important ablations (mentioned in the reviews, and I will revise the score accordingly) that are missing which are important it ascertain its wide utility, as well understand its limitations.	3
"The proposed paper proposed 8-bit quantized optimizers counterparts for saving memory footprints during training. 
Empirical results show the proposed method's efficiency over a wide range of tasks and architectures. 
I believe that the proposed method introduces an important contribution towards the efficient training of NNs.
Hence I vote for its acceptance in a pre-rebuttal phase.
***********
Post-rebuttal discussion: 
I have fully satisfied with the author's feedback and revision version of the paper according to my and other reviewers' concerns. 
My score remains ""8"" and I am convinced that this manuscript would significantly contribute to ML community, hence I vote for his acceptance. "	3
This paper has very valuable shared resources, and it's worthwhile for the research community to notice.	3
"I have some questions that could influence my understanding of the basic blocks. I cannot make a clear decision until seeing the answers.

==========================

Based on other reviewers' comments and authors' responses, I would like to lower the score to reject this paper. The false claims and equations give me an impression the authors do not understand what they are doing. I would recommend another round of revision and rebuttal for authors to build a solid story instead of rushing this work into publication."	2
The submission is commendable in joining a set of recent papers [1,2,5] in pushing for properly benchmarking and defining OOD detection. The novel perspective in this particular submission seems to be the suggestion to disentangle texture from object-identity in a way that detecting one does not imply detection of the other, which is an interesting and thought-provoking view, albeit somewhat narrow. The idea of using density models on PCA-reduced features is demonstrated to work well. The submission scores low on clarity of notation.	2
On the good side, the proposal of OOD problem with texture and semantics is reasonable and the proposed method achieves SoA performance. However it still lacks comparison with some latest methods.	2
Overall the paper is good and ready for publication.	3
"The paper does not clearly explain differences between GP in this paper and traditional GP methods. The proposed MT is not a sufficient improvement. So my rating is ""5: marginally below the acceptance threshold"""	2
The simplicity and effectiveness of proposed method are important and interesting. The findings will inspire more studies on the nature of network pruning. However, it is not easy for readers to understand where the effectiveness of proposed method comes. 	3
"The paper does not propose anything novel (which is fine) but the observations are also not new while the paper ignores most of the issues and tries to underplay other baselines. 

The paper is not ready to be published but is an excellent starting point for new readers. 


=---------------------------------------------------------------------------------------

After an extremely long discussion with the authors and providing them with things I felt were necessary for fixing the experiments, claims etc., I think the paper is not ready to be published. The authors should revisit most aspects of the paper if they want to make it a benchmark paper for GP and claim so. None of the insights and contributions are novel given my discussion and experience. I  hope the authors understand that it is better to publish a ready paper than a paper changing around a lot at this point. 

I vote for rejection.



"	2
"The proposed metric in the paper is well designed and supported by:
- intuition
- theory
- experiments vs. adversarial attacks and compared with other metrics

--------- UPDATE ---------

I have read the other reviews and rebuttals. The authors clarified some details and notations. I keep my score and recommend the paper for publication.
"	4
I think this is a good submission and the proposed information-theoretic framework is persuasive. However, I am on the borderline between acceptance and rejection. The main contribution seems limited and the ablation study including entanglement attacks only considers a few simple cases of entanglements. 	3
"1) Novelty:
- The idea of an information-theoretic analysis framework for disentangled representations was proposed by Do & Tran [1] but is not discussed in the paper.
- The metric UniBound proposed in the paper has a very similar formula to those of the WSEPIN and WINDIN metrics in [1] but with the input image x replaced by the label y. I think the authors should discuss this similarity in their paper.
- The authors should also discuss and compare their metrics with other up-to-date improvements of MIG such as JEMMIG [1] or MIG-sup [2]. Please check [3] for more detail of these metrics.

2) Clarity in the presentation:
- I think the authors abuse notations in their paper which causes difficulty in reading. For example, if $\mathcal{R}$, $\mathcal{U}$, $\mathcal{C}$, $\mathcal{I}$ are just mutual information, the authors should write all of them as $I$. I think the variables inside these terms are enough to define their meanings. 
- I don’t understand the meaning of the symbol $\backslash$ in the mutual information. Is it equivalent to the symbol $\|$ for conditional probability? If they are equivalent, I think the authors should write $\|$ to make the mutual information more familiar to readers. Otherwise, they need to write explicit formulas of $\mathcal{U}(y_k; z_{\ell} \backslash z_{\ell '})$ and $\mathcal{U}(y_k; z_{\ell '} \backslash z_{\ell})$ since I cannot guess what they are.
- The derivation of the proposed metric UniBound in Eq. 5 only uses Unique Information ($\mathcal{U}$) and neither Redundant Information ($\mathcal{R}$) nor Synergetic Information ($\mathcal{C}$). I wonder how the authors can relate their metric to “redundancy” and “synergy”?
- How the lower bound and upper bound of each term are related to disentanglement is not well discussed. I am almost unable to deduce anything when looking at Fig. 3b.

3) Correctness and practicality of the method:
- I don’t really understand why the latent variable $z$ is concatenated with a new vector in case of Redundancy and Synergy attacks? Doesn’t it double the length of $z$ and make $z$ no longer a suitable input for the decoder?
- I am concerned about the practicality of the proposed metric as it is computed by summing over many small probability density values ($\log\sum_{x} p(z|x)$ in Eq. 8), which often leads to numerical instability if $z$ is high-dimensional. This is serious because incorrect results can cause incorrect interpretations. MIG and its improvements (JEMMIG, MIG-sup), however, do not suffer from this problem since these metrics use the conditional probability of a single factor $p(z_{i}|x)$, not all factors. This problem was analyzed in [1] (Appdx. A5). I would like to hear the authors’ explanation about this. I also would like to know the dimensionality of the latent code z used in their experiments as I cannot see this value in Table 3, Appdx. C.
- Clipping $I(y_{k}, z_{\ell}) - I(y_{k}, z_{\backslash \ell})$ to be >= 0 makes the metric unable to compare between models that have $I(y_{k}, z_{\ell}) \ge I(y_{k}, z_{\backslash \ell})$. This is problematic because $I(y_{k}, z_{\backslash \ell})$ is likely to be larger than $I(y_{k}, z_{\ell})$. An example of this is the UniBound values for JointVAE in Fig. 3a.
- In Fig. 3a, UniBound produces very different results for FactorVAE on dSprite and on 3DShapes. I would like to hear the explanation for this.

[1] Theory and Evaluation Metrics for Learning Disentangled Representations, Do & Tran, ICLR 2020

[2] Progressive Learning and Disentanglement of Hierarchical Representations, Li et al., ICLR 2020

[3] Measuring Disentanglement - A Review of Metrics, Zaidi et al., 2021"	2
The main idea (applying PID for disentanglement analysis) looks interesting, and the underlying technical contributions seem good. However, the empirical results are very weak and not impressive. I thus find it difficult to argue for acceptance of the work.	2
Overall I think this is well-written and an acceptable paper with solid empirical results. I am happy to increase my score if the response clarifies my concerns or I misunderstood some points. 	3
The proposed method can be considered as an extension of Tent. Essentially, the optimal BN parameters ( $\mu$, $\sigma$, $\beta$, and $\gamma$) can be obtained by optimizing only the BN affine parameters ( $\beta$, and $\gamma$). The proposed alpha-BN actually provides a better initialization of the BN parameters for DA, which is why it can obtain better performance than T-BN. But, there is no proof that the proposed $L_{core}$ can necessarily achieve better results than Tent. I consider the proposed method is incremental. The authors should provide more explanation & discussion of the $L_{core}$ in the rebuttal. 	2
"Overall this paper proposes a simple and effective method for test-time adaptation that seems to outperform the previously described TENT method. However, key questions about how the $\alpha$ parameter should be set in practice and issues with the experimental evaluations should be resolved before the paper is ready for publication. A more thorough examination of the effect of $\alpha$-BN and CCO components will also provide more confidence in the contribution. 

**Post response update:** I have read the other reviews and responses from the authors. The additional ablation studies address my concerns about the effect of the individual components, but as pointed out by the other reviewers as well, technical novelty is somewhat limited as both components have existed in the literature although they are not applied to this particular setting. Also, it is unclear how the $\alpha$ parameter can be set in practice. Thus, while this work presents an interesting empirical study, I think it is borderline and I lean towards rejection."	3
"- The main concern is the lack of technical novelty. I would say both of the two main components in this method have been proposed by others previously. In this case, this paper over-claims its contributions. It will be better to reshape the paper as an empirical finding paper instead of a method-proposing paper. 
- Also, semantic segmentation is so different from image classification, so that I am not sure if the conclusions we get from image classification can safely transfer to semantic segmentation. For example, the mismatch between Table 2 and Figure 3(b), as I mentioned in the weakness section as well. And it is also quite weird that another important component, class correlation minimization, is only applied on classification but not segmentation. So I would suggest the authors either discard the semantic segmentation and fully focus on image classification, or conduct additional experiments and analysis on semantic segmentation to have a deeper understanding.
"	2
In summary, this paper studies a robust GNN against adversarial attacks on both graph structure and node features. However, the methodology lacks novelty and requires further justification. Some important technical details are also not covered. Though the experimental results on the small datasets are good, some more experiments on large datasets and the details of the experimental settings are required. 	2
The authors propose an interesting idea, to jointly sanitise the graph structure and the features as a defence against adversarial attack. The experiments are extensive and thorough and the method appears to perform well. However, the paper is let down by two major factors. The first is that it is confusing how their framework is general and how it relates to prior works. It’s also not obvious specifically how they are innovating compared to prior works as parts of their method are similar to some previous studies. The second is that the derivation is lacking details and references and, as it is written, is not at all easy to follow. I believe this paper has potential but in its current state, is not yet ready to be published. 	2
The model definition of the paper is not convincing and the experimental results needs a more careful investigation. In particular, the novelty seems to be limited to the graph cleaning since the remaining parts are an existing model (PPNP).	2
Please refer to the negative points in the detailed review.	2
I think this paper has an interesting motivation of learning 3D reconstruction from incomplete raw 3D scans / open meshes, but there are major flaws in the method description and the experiments (detailed above). I also don't think there are either sufficient novelty or insights in the paper. I think the submission needs much major revisions with additional experiments to validate the effectiveness of the proposed DSG.	3
This paper has novel and interesting contributions to the field of single-image 3D reconstruction.  They provide convincing experiments to validate their contributions.  The paper is also well-written and nicely presented.	3
Overall, I think the proposed method is novel and with reasonable performance.  I am in favor for acceptance if the authors can provide some discussion about the cons listed above.  	3
"The paper is well written and presented overall.
The essential experiments are performed and the results are well presented.
However, I have a question regarding the major contribution (see weakness)."	3
The purpose idea of the dataset is valuable, as well as the problem is proposing to address. However, there are some problems that do not allow me for acceptance. This includes problem statement unclarity, claims not well supported by results, and ethical concerns. 	2
Putting together all the strengths and weaknesses I believe the NLP and QA community will benefit from the insightful outcomes of this work. However at the same time, it does leave things to be desired. Nonetheless, I am inclining to accept this work.	3
"The paper proposed a novel study of QA system robustness under contradictory information. It's well written and provides a new dataset. However the discussion around how a classifier could tell truth from noise and why don't we leverage source authority are missing. Some smaller issues exist too. Reviewer would like to see the two bigger questions answered. 
"	2
"The paper's experiments do not back up the claims of reasoning for misinformation. It is about reasoning with contradictory information and its no surprise QA models dont know what to answer when that happens. Why is this surprising? What is the goal of the work. 
I apologize if I have badly misunderstood the work and I'd encourage the authors to discuss these comments in the rebuttal. "	1
"I recommend reject, as the paper's core story isn't well placed compared to related works. The paper is placed as ""RL"" so we're missing important baselines and connections to other gradient estimators through discrete processes."	2
The technique while sound is currently quite limited in scope in that important baselines and algorithmic elements (e.g. locally non-constant models and local kernels) that need to be considered in the context of doing high fidelity data augmentation for regression problems have not been adequately investigated or discussed.	2
I'd like to see the other reviewers' opinions about the empirical results.	3
The paper presents an interesting idea for privacy-preserving prediction for fine-tuned models. However, as written now it is not clear if the benefit in experiments is due to fine-tuning or due to the new method of training/prediction proposed in the paper.	3
Overall, this is an important research direction. The proposed approach is interesting. However, there is room for improvement.	1
"To summarize the points from above, I believe the main shortcomings of this paper are:

1. unrealistic assumptions in the experiments and about the problem setup in general (relating to the size of data and correlation between users).
2. Huge computation and storage costs which are not explicitly studied.
3. Not providing any comparisons with prior existing work in NLP.  

If the authors show that this method is effective in a setup where the user data is scarce, address my concern on corrolations, and provide comparisons (qualitatively) with prior work I am willing to update my score. 





"	2
Sentence-level privacy in the language model is a very challenge problem since LMs applied to many tasks in NLP. To this end, Submix provides a scalable and flexible approach to address the issue by focussing on next-token prediction, which is popular for many downstream NLP tasks. Hence, I recommend to accept this paper.	3
"The paper targets at the interesting problem of few-shot learning and presents a unified framework and a meta-dropout method. Although the experiments are widely validated on several benchmarks, the main concern is the comparison of the experiments miss several State-of-the-art paper results. 

-----------------------------------------------------------------------------
Update after the rebuttal:
The rebuttal does not well address the concerns mentioned in the previous round of review. For example, the experiments can list the setup using a fixed selection of novel samples for a fair comparison with state-of-the-arts besides from the multiple rounds of results listed in the paper. "	3
The paper is hard to read and the novelty and impact of the proposed contributions are not clear. Results lack a clear comparison with the recent literature. 	1
This paper is not easy to follow and lacks novelty in the method. 	1
The pros of this submission is to validate the algorithm on different few-shot tasks but the contribution is unclear.	2
The paper shows hope for using ML and RL techniques to solve complex combinatorial problems like VRPTW. The proposed attention based encoder-decoder architecture is a good contribution that shows how to handle domain constraints of VRPTW problems. Having said that I think the paper is still premature in terms of both technical and empirical novelty. The technical contributions are incremental and its performance is not validated against state-of-the-art methods. The empirical evaluation is also not detailed enough to validate the efficacy of proposed method.	2
The paper, in my opinion, is a purely applied paper with almost no change in the method proposed by Peng et al. 2019. Most of the technical contents are copied and pasted from Peng et al. 2019. It's clearly a rejection.	1
See strengths and weaknesses described in the Main review.	2
The numerical experiments do not support the claims of the paper and the recent papers and algorithms need to be added in the literature review and the benchmark list. 	1
Although there might be some good core contribution, the paper in its current state is not quite ready for publication. See details in my Main Review.	3
"The paper is motivated by an interesting problem to the community, proposes a fairly novel method for leraning better representations, but there remain certain gaps in my understanding currently. I hope the authors can clarify this during the rebuttal. In any case, the experimental results are not correct in my prior experience of running these exact methods. For this reason, I am advocating for a reject. I'm happy to revise my review if these issues can be addressed. 


-------------------------- Post Rebuttal ----------------------------------

The new results provided by the authors convince me fairly that the method is somewhat useful. I am still skeptical about the RAD results and the overall scores reported for DMC tasks, since the absolute numbers are not state-of-the-art. This might be due to a different evaluation setting where the distractor videos are changing across training, and at test time a video is drawn from a different distribution. I am unsure how much this sort of an evaluation scheme affects RAD's results, which should produce much better numbers than reported. I am giving the benefit of doubt here to the authors and hoping that they have done a fair evaluation for the baseline RAD method. "	2
The approach is novel. The paper shows good empirical results on DM Control with natural video background and procgen. However, it's not clear to me why the procgen-type of generalization and the procgen experiment are relevant to the main story and I have some concerns about the mathematical correctness. I'm slightly leaning to acceptance at this point, but I hope the authors would be able to clarify. 	3
"In general, I think the proposed idea of this paper is interesting, and the theoretical analysis seems correct to me. However, I still have questions about the details of how to implement the multi-view settings (In my general question (1)). Also, the experiment part seems a little weak since the two environment suites are very similar. 

I think this paper is marginally above the borderline. I may raise my score if the author provides reasonable answers to my question.

"	3
Because of the empirically validated generalization properties of the presented model when the 2D scene layout & goal inputs are switched, I am leaning slightly positive, but currently the concerns listed in my review are stopping me from giving a higher score.	2
This is a well-written paper that provides an interesting solution to an important problem. I am not too familiar with the field of motion planning, thus a low confidence in the assessment. From a more general perspective, the work seems technically sound, and the experimental evaluation, including baselines seem reasonable. With all that in mind, I am quite positive about this work, and recommend accept. 	3
While the general idea is viable and something being researched, what is being proposed in this paper is simply too unclear and vague to be assessed.	1
Overall, I am at the borderline for this paper; however towards more accept side. I could move towards the higher score, if authors could rebut my points effectively.	3
"I think the idea is good. But related works are not discussed thouroughly. I also have concerns in terms of the baselines and ablation studies in experiments. 
"	3
"The paper shows **impressive results** for keypoint-based editing of synthesized images. However, there are **too few qualitative and quantitative comparisons to existing approaches** which makes it difficult to assess the quality of the proposed method in practice. In particular, it is not clear if the proposed method indeed outperforms existing approaches wrt. generalization, i.e. being easier to tune, and disentanglement of pose and appearance. Therefore, in its current state, I lean towards rejecting the paper.


**Post-Rebuttal**:
After the rebuttal, I lean towards accepting the paper as image fidelity improves over the baselines. However, my concerns regarding better performance wrt disentanglement and easier tuning than the baselines remain."	3
As abovementioned, I think the paper is well-written and polished to a high level. However, its contribution is a bit thin and it lacks an in-depth analysis of the two deterministic hyperparameters: the number of key points and their influence range $\tau$ (the discussion in appendix F.2 is not sufficient). Thus, I feel slightly negative on the submission.	2
This paper absolutely feels like a simple repackaging of old methods with a new name (C+1 loss) and there are almost no good enough contributions to make a case for acceptance.	1
To summarize, the paper covers an interesting aspect, however, the overall novelty of the paper is very slim and the quality needs to be seriously improved. Thus, there is the clear recommendation not to accept the paper for ICLR!	1
"I enjoyed reading the paper because it tackles a well identified problem with clarity and concision.
The problem is well defined, the solution is simple and explained with clarity. 
I believe that it belongs to the category of papers that I read once, keep it in memory, and come back to it regularly. 

---------------------------------------------------
Updates: Thanks for the authors' response, which partially addresses my concerns.  I appreciate the fact that the authors specifically point out in their paper that the background class should be treated in a differentiated way wrt other classes. This observation, in itself, in my view, constitutes the originality of the work --even though the technical solution is surprisingly simple. I encourage the authors to re-submit to an other venue."	2
Overall, the paper severely lacks novelty. The method simply adopts a loss function introduced in the existing method and a slightly modified version of it to improve the performance. The claims are not well supported by the experiments. The writing is unsatisfactory.	1
"While the paper proposes an interesting idea with solid improvement over the baseline, I do not see the practicality of using a learned optimizer that seeks a wide solution when there exist much better alternatives that don't require meta-learning. I believe that a similar idea could be useful for something other than sharpness but in the current form, I don’t feel the paper is good enough for publication.
"	3
This paper introduces flatness-aware regularizer to L2O objective to improve the learned optimizers, which is a novel contribution. The empirical result looks promising and it could be improved by adding more insights of the learned optimizer, some discussions of the limitations (or some results applying to a more realistic model) and confidence intervals to the results.	3
Given this paper’s lack of practicality and questionable experiment results, I recommend rejection. 	2
The current version needs some modifications on the experimental part to include more important information like running time and variance, as well as more experiments comparing the proposed algorithm with traditional optimizers. Also, it lacks the description of connections between two different regularizers. Thus, I think the paper is slightly below the acceptance criterion.	2
"On the whole, I think that while I don't find the scenario posed by the paper convincing, I do find the concrete setting itself to be so. The scheme proposed for the same is interesting, and, taken at face value, very effective. Nevertheless, I right now am not convinced that the baselines are being compared to correctly, which needs to be justified, and further, I think that ablative experiments illuminating how the aspects of the proposed contribute to performance are needed. I think without these, though, the theoretical study ultimately produces a sample complexity analysis for a method that uses the knowledge of a key underlying parameter, and even then is not shown to be tight, which is a bit too weak, and I think needs more work.

With the above consideration, I have reservations about recommending acceptance as the paper stands. I am willing to budge on this if the experiments are clarified, or if the analysis can be improved. 
"	3
The paper is well written in general. The descriptions on related literature need some improvement. 	3
This paper proposes a novel formulation supervised learning with abstention. It assumes that feature space can be split into an informative region, and a completely unpredictable region.  It offers detailed theoretical analysis showing that recovery is possible, and a heuristic practical algorithms to both recover the partition of the feature-space, as well as learning the model over the predictable region.  The setting is interesting -- but I suspect it's very artificial, so I'd be very curious to know to what extent this has relevance to applications which have no clear-cut partitions (or to traditional selective inference).	4
The paper studies identifying uninformative data in datasets which is a very relevant problem for the machine learning community. The algorithm analyzed in the paper is supported by interesting theoretical results and can be revealing for more realistic algorithms.	3
"The paper has a very interesting theme but I find the analysis and conclusion unsatisfactory. Furthermore, the empirical evaluation is not more convincing. 
"	2
"Overall I would recommend an accept. The simplicity of the approach and the improvements shown in the experiments show that the global clipping has promise. However, there are a couple of aspects in the paper which need to be addressed. In particular, I believe that the text of Theorem 3 is worded too strongly.

---

Edit: I have changed my score from 6 -> 5 as a result of reviewer discussion. In particular, due to how the assumptions of Theorems 1 & 2 are not the same due to the assumption on the gradient norm bounds. Further discussion / analysis is needed for the violation or removal of this particular assumption (following the critics presented in LL2P and b1Rw's reviews)."	3
"
Overall, while I think this paper explores an interesting question, I also think it overstates its contributions and impact. In particular, I think that the utility of global clipping is not sufficiently well supported by theory or experiments. In addition, I think that the use of gradient flow to analyze the dynamics of DP-SGD is interesting but lacks clarity at this time. For the above reasons, I will argue for rejection.
"	2
I am in favor of this work. The global clipping is simple but elegant. Furthermore, the NTK analysis is nice and helps support the intuition behind the method. I think that using NTK to analyze differential privacy is actually a method with potentially widespread applicability. Furthermore, the empirical analysis of the method seems sound. The issue of calibration with DP-SGD is also interesting and understudied. I find few faults in this work, and the brevity of this review is a consequence of having little to critique. 	4
"The research problem is interesting.  Novelty on the idea is modest and the performance compared with the existing method is also modest.

The presentation of the paper is good but it lacks a clear insight on the motivation for hierarchical architecture and also the way cross-correlation is established. What would be the performance when cross-correlation has been established in a different manner than that present in the paper? As there is potential to have different combinations of cross-correlation dependencies between the layers."	3
Although the experimental results show some improvements, the contributions of the paper is incremental from previous works and the intuition for the improvement is unclear.	2
The approach proposed in this paper is straightforward and well implemented. The DNNs trained by this approach benefit mostly in the case of small training sets and epochs. The advancement over the full training settings is still small due to the natural limitation of the consistency assumption mentioned above. 	3
"The authors proposed an interesting hierarchical comtrastive learning approach that achieves noticeable improvement over serval benchmarks. 

However, it would be more convincing if some theoretical analysis or empirical interpretation could be provided to help the readers understand why hierarchical project heads and cross contrastive loss works better. Additionally, some justification regarding downstream task training protocols should be provided for the sake of fair and rigorous comparison with previous work."	3
"The paper studies a new contextual bandit setting motivated from real-world scenarios and proposes a new algorithm and presents theoretical guarantees on the same. There is experimental evaluation via simulations but I’m unsure if the baselines considered are competitive. The paper is well-written generally. 
"	3
The paper is clearer compared with an older version I have reviewed but I still have questions regarding how the knowledge of a causal structure helps with learning.	3
"Overall I think it is an interesting problem to account for context variables in interactive decision making. The proposed method is intuitive and the authors provide theoretical and numerical evidence for its performance. Still, I would be more convinced if there is a direct comparison between the proposed method with an existing method that does not account for context variables. 
"	2
The paper has a certain interesting problem with an intuitive solution. Yet, the theoretical and empirical analysis seem a bit lacking. Further, the assumptions need to be further clarified.	2
The main concerns are first on the analysis, which I couldn't tell the correctness at the moment. Second issue is the empirical results, which seems not consistent with other works. 	2
I think the paper presents an interesting analysis of TAC-GAN, AC-GAN, and other cGAN methods. My concern is that the proposed method has the same actual implementation as existing method (TAC-GAN). The paper also lacks results on high-resolution generation. I am willing to raise my score if my concerns are resolved.	2
The authors propose a new type of ACGAN named ADC-GAN to address an improper optimization process of ACGAN. They apply adversarial training not only for the discriminator but also for the auxiliary classifier to eliminate a contradictory divergence and conditional entropy in ACGAN training. In the experimental results, they prove the effectiveness of ADC-GAN using synthetic datasets and various benchmark datasets. However, I think Theorem 1 has already been addressed in TAC-GAN paper and experimental results do not fully demonstrate the effectiveness of the proposed method. 	1
The author proposes a simple but seemingly promising classifier-based cGAN. I hope the author can answer my questions above in detail. I will improve the score based on the author's answer. 	4
"The attempt to assess the human-nature of the image representation of networks which are robust to adversarial attacks is very interesting both for machine learning as well as for computational neuroscience. However, a number of points need to be clarified before the work can be published at ICLR:

1. Representations with shared null-space (the central idea that is checked in the work) are not necessarily the same: simple linear examples  tell us that they can differ in arbitrary rotations or extra linear invertible transforms, and these may represent quite different qualitative nature!

2. Authors should give additional evidence for the suggested connection between the use of summary statistics and robustness to adversarial attacks (fig. 8).

3. Authors should mention other ways to assess the similarity between the robust networks and humans which may be centered in the response space rather than in the null space.

See extra minor comments at the detailed review."	3
"I think this paper is marginally below the acceptance threshold, but this may be due in part to the exposition of the work. I like the effort to try to link these two areas of human peripheral vision and robust representations, and I think the paper does quite a good job to describe the experiments undertaken. If the authors can respond convincingly to the various questions I and other reviewers may have, I would be willing to upgrade my rating.

EDIT: following the rebuttal from the authors and taking into consideration their response to other reviewer comments, I am upgrading my score."	3
"The paper makes a few interesting arguments linking robustness to foveated visual systems. I find the evidence that the texform and robust models are doing similar / the same computation to be only weakly supported. The story of the paper could also be more clearly constructed.

EDIT after discussion: Given report of new experimental evidence that the texform and robust models generate perceptually-similar images, I am raising my score from 5 to 8."	3
Although this work has some weaker aspects, this is a mature research project whose acceptance to ICLR would promote more important research into the connections between CNNs, robustness, and peripheral vision. Therefore, I recommend acceptance.	3
Overall, I like the solution proposed by this paper to address the issues in marginalized distillation and local distillation. However, I believe the authors did not fully investigate the failure reasons for local distillation. The argument in the current paper is kind of vague and not well-supported. I believe a more rigorous analysis and carefully designed experiments are needed to illustrate the argument (e.g., as I mentioned above). I will increase my rating if the authors can provide more convincing results.   	2
"* The paper is relatively clear.

* The experiments are not well-chosen and the results are not convincing. 

* The method section does not explore the idea of knowledge distillation for DGM deeply. There are questions about the generalizability and scalability of the proposed method."	2
I am mainly concerned about the novelty and clarity of this paper. At the current stage, I don't recommend the paper for acceptance. 	2
The work is good and the paper is well written. I feel the contribution of the work is not that novel. My confidence in evaluation will increase once the authors address the concerns raised above. 	2
"I currently believe that while the work is useful to the community (see 1st point in ""Strengths""), I think in its current form the paper makes a few considerable claims that are not supported. 

If the authors can address my main concerns, I will reconsider raising my score.
"	2
The authors experimentally showed that PTQ can be used to prevent data corruption of learned image compression, but the value of this research is not clear due to the unclear target issues, insufficient comparison with previous methods (e.g. Balle ́ et al. (2019) and Sun et al. (2021)), and some unclear points, so I considered that currently this paper did not meet the conditions for acceptance.	2
I tend to accept this paper though I have some concerns about the technical novelty. 	2
"This paper presents a simple practical solution, post-training quantization, for robust learned image compression using neural networks.
However, the contributions of this paper are marginal since the problem statement and solution are overlapped with the previous work (Balle et al., 2019).
Moreover, this paper presents limited comparisons to the work of Balle et al. (2019) while Section 1 describes its failure cases.
The authors should thoroughly state the differences and improvements of the proposed method to the work of Balle et al. (2019).
"	2
This is a well written paper that shows the effectiveness of existing post training quantization techniques when applied to multiple neural image architectures. In addition to showing how well this PTQ techniques perform, a more computationally efficient entropy parameter discretization technique is introduced.	2
"Although the imbalanced regression problem is very interesting and import, I have concerns on the technical contribution and effectiveness of the method. Therefore, my current rating is: ""5: marginally below the acceptance threshold"""	3
I think this paper in its present form cannot be accepted for publication in ICLR.	3
I recommend a borderline acceptance. Although the motivation of the paper is not fully justified and some discussions are too limited to a specific domain (random noise augmentation), the paper explores a new topic and provides a technically sound solution. My overall attitude towards the paper is positive.	3
Overall, I like this paper because of its task importance and the new formulation of self-supervised learning for long-tailed regression. If the authors can fix the negative points mentioned above, this paper would be more solid and thorough. 	3
The paper presents a complete and sound methodology for the provable repair of neural networks while minimizing the side effects of the repair. The ideas presented in the paper are new, come with solid theoretical guarantees and advance the state-of-the-art. Based on the contributions, I would recommend acceptance of this work.	4
Interesting ideas for neural network repair but there are concerns regarding motivation and scalability.	4
"The paper alleviates some significant limitations in neural network repair in a
method that is adequately evaluated, thereby making for a solid ICLR
contribution.
"	3
Overall, this paper is well-written and interesting. However, considering the limited application, the little discussion on the point-wise repair and the weak experiments, I think the paper should be still improved before it is accepted.	2
A clever paper using a relatively uncommon mathematical trick to make progress on a niche, but important problem. While the paper could be improved, I do think the ideas here could be interesting and useful to the community. The experiments seem relatively compelling and code is included with the submission.	3
"**Recommendation**

My sense is that in its current form the paper should be rejected. Especially given the lack of comparison to existing methods. I am not an expert on phase retrieval, so I am happy for the other reviewers and rebuttal phase to change my mind."	3
In general, the idea to use neural networks to parametrize Blaschke polynomials for predicting phase functions is well motivated and shows promise based on the experimental results. The architecture and theory are presented in a concise fashion. However, the evaluation of the model is quite minimal and could profit from a more in-depth analysis in the form of additional experiments. Nevertheless, I tend towards accepting the work.	3
I believe this manuscript has a promising idea of using neural networks to learn the parameters inside of a Blaschke product. Since this is fundamentally a technique for approximating functions of one complex dimension, the application to phase retrieval seems very natural. I think the authors have a half-baked idea, though. In my opinion, they fail to fully compare or motivate their choices. In particular, I wonder how the author's approach compares to other rational function approximation schemes that do not use neural networks such as the AAA algorithm. 	4
"The parametrization of weight matrices by Householder matrices is interesting, although the experiments are quite preliminary.
However, the whole derivation of proxies for expressivity or trainability is not adequately grounded theoretically, nor demonstrated empirically beyond a small contrived example.
I do not think this paper should be accepted."	1
"Notion of variability maybe a step in the right direction to explain generalization and trainability of DNNs. While the Han-layers do seem to outperform FCNets, many questions remain unanswered in this work, namely, 
- Are Han-nets compute hungry (train, inference) when compared to FCnets?
- Is the additional depth necessary?
- Is it harder to train Han-nets in comparison to FCnets, due to orthogonality and additional non-linearity?
- Are there simpler definitions of the notion of variability that extend to higher dimensions than the toy examples illustrated in the paper?
"	2
"This paper is easy to read and provides some interesting discussions and visualizations. However, concerns exist regarding
- the usefulness of the proposed concepts;
- the design of the landscape experiments; and
- the effectiveness of the proposed model on more complicated applications."	3
The paper aims to propose new a measure associated with generalization, use it to define new models, and gain some insights into the behavior of DNNs.  However, all the parts of the work feel half-baked.	2
The method is well motivated and quite intuitive, the paper is easy to follow. Extensive experimental results are provided on several tasks and datasets, but statistical significance of improvements is missing, which is needed given large confidence intervals reported. 	3
Though this paper is well-written and studies an important and popular problem in graph learning. I still have concerns in different aspects.	2
Both AutoSSL variants (evolutionary strategy and the gradient descent version) seem ad-hoc and straightforward combinations of existing tools. This is not a problem on its own, however, the empirical gains are not convincing. 	2
 homophily datasets is used and authors shows theoretical analysis.	2
I think, overall its a good paper which solves an important problem that in turn broadens the applicability of weak supervision. The work is novel and technically sound. In my opinion, it can be a clear accept.   	3
"I feel the paper is really at borderline (slightly leaning negative) considering 1) the contribution is a bit limited because the framework is only applicable for hierarchical label space 2) modeling ILF to label space with a graphical model is not too novel 3), but the paper still made reasonable contribution in this limited scope.

-------------

Post-rebuttal comment:

I thank authors' response and the clarification on the ""comparable"" assumption in their paper. I also agree on other reviewer's points for the contributions of the paper and increased my score. "	2
"Strength: interesting new problem setting; comprehensive theoretical analysis; the proposed method has good performance on three datasets.

Weakness: some potential missing references; paper presentation may be further improved; some more discussions on the questions I raised may be helpful."	3
"This paper studied an interesting problem in weakly supervised learning and proposed a novel method based on the indirect labeling functions and label relation graph.
The proposed method was validated theoretically and empirically.
Overall, this paper provided a data-centric approach for improving machine learning systems, which might be useful to the community.
However, there are still a few concerns mentioned above, and the writing and organization can be further improved.
Hence I recommend acceptance of this paper.
If the author can address my concerns well, I would like to raise my rating.
"	3
The paper targets an interesting problem and the provided results seem to support the efficacy of the model. However, the paper's experimental section needs to be improved upon to understand the importance of the results. The paper can be improved upon by providing a more thoughtful analysis of why the model works and where can such models be used.	2
"While the novelty in terms of individual components of the proposed model rather is limited, the overall method seems to provide good improvement compared to SOTA on real-world datasets. 

Additional comments:
- Patient information includes  demographics information and the first visit. This is not well justified. The first visit does not necessarily represent a global/static view of patient state. Clarification on this point would be needed.
-  PRIVATE dataset has a slightly smaller cohort (6321 from Table 3) contrary to what mentioned in the paper while MIMIC-III has 6412 patients.
I was expecting that the performance for the inpatient dataset (MIMIC-III) would be much better compared to the outpatient dataset (PRIVATE).  Inpatient is a well controlled environment compared to outpatient. For example, mortality prediction in inpatient is highly accurate compared to outptient models. This is not the case in the experimental results of the paper. This needs clarifications
- last line of page 8: We observe see  that -> we observe that
-  Figure 5, the arrows (up and down) needs to be explained (negative vs. positive interaction) in the figure legend."	2
"This work is does not present any fundamentally new methods, but rather tries to show the value of using largely pre-existing methods to incorporate additional information to the task of predicting HbA1c lab test results.  That is, in my view, a perfectly fine kind of paper for ICLR.  But given the above question about how BEHRT (a baseline) fares with the addition of medication data, I am reluctant to recommend acceptance for this work because that is a key piece of evidence regarding whether the complexity of KALP is really necessary.  I would reconsider if this was added to the paper.  
"	2
A paper with an interesting idea but with some problems	2
Interesting practical work and great industrial success, but limited academic contributions.	2
The paper is good in terms of writing and conducting experiments. However, the contributions are very limited. This paper requires more in-depth analysis about the model performance, ablation studies, run-time/real-time comparison, etc. A huge drop in the AUC metric after only a few iterations in Figure 2 also raise a concern about the performance.	2
Though the method proposed in this paper is intuitive and sound in general, the lack of rigorous algorithm analysis and significant lack in experiments in terms of setting and baselines may require the paper to be more polished before publication.	3
Modeling the long sequence is a hard problem in recommendation applications. This paper proposes the IMN model to learn the user preference from a long sequence and performs better than baselines. The technical significance and novelty of the proposed model are good. The reviewer tends to accept the paper. 	3
The paper is solid in both theory and experiments. The argument of this paper that the weight decay coefficient is responsible for learning powerful representations, is valuable and intriguing. To summarize, I recommend acceptance of the paper. 	3
Overall, I appreciate the incremental theoretical analysis the authors supply to try to understand non-contrastive self-supervised techniques.  I believe their work is sound and thorough, but would welcome more novel insights in this direction.  I would therefore recommend a weak rejection.	2
"I think the paper itself can benefit from clearer writing and some emphasis on how their analysis scales up to explain other non-contrastive SSL methods.

However, if I am to evaluate it in a broader context, I think in its current state it lacks novelty to be seen as a standalone piece of work. "	2
The paper seems overall technically correct. It presents a nice analysis that illustrates how the representation learning happens in a toy data setting and proposes a slightly more simple algorithm than in previous work. This is great, though not a major step forward (as ssl methods are designed to do this, the results are expected IMHO). Further, the experimental analyses indicate no significant improvements over previous work.	3
The idea and technical approach are well motivated and justified. It connects and generalizes two important prior ideas and offers an elegant solution. The results are convincing as the experiments show consistent advantages over a range of tasks and base architectures. The work also has good potential in practical applications due to its convenience as a drop-in replacement for standard convolutions. 	3
"In summary, this work presents a new dynamic convolution that can provide a powerful representation for image classification and object detection. The paper is well written and properly structured, and the extensive experiments prove the effectiveness of the proposed method.
"	4
This paper is well written, and the results about the accuracy improvements look promising. A major drawback is the actual inference speed when compared with the competitors, but it would not be a matter because one can constrain the model to the target speed in a particular circumstance. I recommend the authors study more the way of speedup the model in practice for practitioners. 	2
Overall speaking, the paper has novelty and very comprehensive experiments to support it. I am prone to accept this submission. But the weakness of this paper is too experimental-oriented without thorough analysis on the results. My first concern is critical, since I want to understand what happens when different dimension is dynamically adapted. If possible, I hope the author to add some visualization or other analysis on the dynamic coefficient from the statistical view to help readers have a better understand of the proposed dynamic mechanism. 	4
A simple and effective algorithm supported by geometry. But the author did not seriously discuss the drawback revealed by the relevant paper. This part should be improved.	3
The key ideas behind the paper has appeared in a series of papers elsewhere, and these arguably provide a deeper understanding of the problem than what is discussed here. That makes it hard to recommend acceptance. That being said, the paper is well-written and the empirical results are interesting, hence the borderline/reject score.	2
"The paper is a marvelous piece: well put together, professionally presented, relevant and interesting topic, excessive experiments and analysis, and, mostly, well written.

DISCLAIMER: the mathematics in this paper I am not too familiar with-- there appeared to be consistent use in variables, each properly defined, and expressed in the body of text coherently. If another review can validate the math, it would be great (i.e., my review does not include deeply validating the math, for low-rank I have minimal experience with.

My primary concern is that the analysis is limited (not in figures, but in an explanation). Also, the story could be told better. Note that the two aforementioned concerns go hand in hand: with a better description of the image montages (i.e., what to look for, highlight the pros, also show some failure modes). Share thoughts on why results are better; same with failure modes (which are not currently highlighted).

Thus, the figures are sufficient, but the intuitive insight by breaking the figures and plots down is severally lacking (both in-text body, caption, and in the visual themselves). This would likely yield a better story, end-to-end, for it might bring the motivation full circle: as is, I am not seeing it go full circle, as all the best stories do.

With that, assuming the math is correct (per another reviewer validating), the pros slightly outweigh the cons, so I am neutral (or slightly above, since there is no middle chose :) "	3
The paper is not well-written especially experiments parts, and the message of our them are not clear in the line of overall paper take-away.Therefore, the contribution seems to be not clear beyond aggregating all existing DA methods and systematically deploying them. 	1
"The findings are interesting and may contribute to the community. However, considering the motivation and the unconvincing experiments, I vote for ""5: marginally below the acceptance threshold""."	3
"
This paper provides a useful benchmark for the TS-UDA task. Extensive experiments are included. But because of the concern of technology novelty and the unconvincing evidence of some findings, I would like to reject it.

--- after discussion---

The author addressed part of my question. But I still think this paper is below the expectation. I would like to arise the rank to 5.
"	1
"While the novelty is low, the experiments as detailed and forceful, and the community may find them useful
"	3
While the authors' efforts in systematic experimental evaluation of existing visual domain adaptation algorithms and time series-unsupervised domain adaptation algorithms is commendable, there is no theoretical understanding at even the most basic level. hence the conclusions drawn from the experiments may be specific to datasets and baseline architectures used. The conclusions do not generalize.	2
Although I find the task very interesting and with a wide potential of application, I have a few concerns about the clarity of the paper.	3
In summary, the proposed idea lacks novelty and is similar to prior work [1]. Experiments are only performed on simple environments without proper comparison with all the baselines. There are also no ablation studies to understand the design of the method better. Thus, I vote to reject this paper.	2
The paper is well written and the proposed idea is well conveyed. However, there is no ablation study of the design choices, and overall experiments are not thorough. I do not recommend acceptance at this moment.	2
Overall, the idea of learning different behavior styles from expert demonstrations is interesting. However, I would recommend the authors to address my questions in the rebuttal.	2
This paper introduces a simple yet effective way for model performance inference. It outperforms existing gradient-based approaches in almost all experiments. I recommend accepting this paper.	3
The paper proposes an interesting and sound idea but currently falls short of properly contextualizing their work within existing work and consequently comparing against recent work experimentally as well.	3
"
The problem of automatically discovering efficient neural architectures is the foundation of work on AutoAI. From that perspective this is a nice and important piece of work that seems to have theoretical foundation and good practical performance. "	3
The paper makes a contribution of being the first work to employ transformer to model particle based simulations where the model explicitly accounts for particle, particle interaction as well as material properties during the simulation. The authors have performed a rigorous comparison with state of the art models and the achieved improvements and the explanations rendered for the improvements are sensible. Also, authors demonstrate also through qualitative and quantitative comparisons, the generalizability of the SiT model in various multi-material settings which are also convincing. 	2
SiT provides a novel Transformer-based architecture for predicting particle dynamics that outperforms previous graph-convolutional and Transformer-based architectures.	3
"
Overall, I am very supportive of the topic this paper addresses and generally also of using transformers for solving physical systems. Using transformers for solving physics is an interesting research direction. However, in its current form I am not enthusiastic about this paper which is due to the following main reason: 

(1) While the paper is generally well-written, several sentences are left to vague and unclear. In the introduction several concepts and terms are used without carefully introducing them. This makes understanding the text difficult and blurs what the contribution is. Also, some sentences almost read like a collection of buzzwords which makes is very difficult to comprehend what the actual meaning is. Consequently, while the experiments and results are technically sound, I got the impression more care should be taken in carefully explaining and formalizing the concepts that are aimed to be discussed in the paper. 

(2) With 12 pages the Appendix provides a significant amount of additional material, some of which can be considered quite relevant for the main contribution of the paper (e.g. Table 4). This indicates that the contribution of this paper may better be discussed in a format that has less restrictive page constraints. There are reasons why papers have page limits and I have the impression that for this work the authors are trying to circumvent them by providing a lot of material in the Appendix. 

(3) The paper does not discuss any limitations of the introduced method. 

Therefore, I don't think the paper is ready for publication. "	2
"The intent of the paper is good, and the results could bring interesting insights about whether attention inspired architectures could bring to the simulation domain. However the empirical evaluation makes it very hard to know which improvements are a result of the specific technical additions added to the model (extra scalar attention for the explicit messages and abstract tokens), and which improvements are just due to other small, but possibly very relevant, differences (including different input encoders/decoders, different featurizations, different outputs, etc.) between SiT and the baselines. My recommendation would be to perform the same investigations by adding the extra attention to the existing baselines one by one (or at least to the most promising ones) while keeping other parameters identical and test if the form or proposed attention helps universally. Similarly, the idea of adding extra abstract nodes for material types, and its impact on generalization, could be applied to existing baselines too and tested and studied orthogonally to the attention contribution. This would make the paper conclusions much more insightful and useful than in their current form.

EDIT POST REBUTTAL: I want to thank the authors for the additional experiments and extensive replies. While many interesting ideas are introduced in the paper, the way the comparisons with baselines is performed makes it very hard to really understand which aspects of the proposed architectures is making the model perform better than baselines on some settings. 

Since the authors are proposing a new neural network layer, it would be more useful if, rather than compare to different baselines each with their original hyper-parameters from their papers, etc, the authors took the approach of integrating the different parts of the newly proposed layer into existing architectures (possibly including non-simulation settings), and try to understand better that way how the new layer may help in a more apples-to-apples comparison."	3
"The main reasons for the recommendation are as follows:
* The manuscript does not demonstrate generalization from one dataset to another which is the main practical application of cell-type classification using single-cell data. 
* The claims of additional biological interpretation are not sufficiently elaborated
* Given the subject specific nature of the submission, it might not be of broad interest to the ICLR audience."	1
"In its current form, the proposed approach does not seem groundbreakingly novel, nor significantly improve over existing methods, nor offer extra benefits.
I think some of the design choices are debatable and I would love to hear what the authors have to say to the different points I raised.
Without further justification, though, I think the contributions are too minor to justify an acceptance at ICLR."	2
I think this work is a straightforward application of well-known methods to the single-cell RNA seq data. The results do not support the proposed method is superior to alternatives. I am therefore inclined to reject this paper. 	1
"I found the paper difficult to understand, classification results not convincing, and interpretability claims greatly overstated. Unfortunately I believe this is a clear reject.

Disclaimer: I have heard about attention and capsules but do not know how they work. However, I think this paper can be assessed even if one treats the model as a black box. Also, I do work with single-cell RNA-seq data all the time."	2
" Overall, I do think that, unless there is something I am missing, the limitations I described somewhat limit what we can take away for realistic settings, and so doesn’t quite settle the questions claimed so completely as suggested. I would recommend weak
rejection. "	2
 I’m not sure there is enough new stuff in this paper to warrant publication at ICLR. 	2
The paper is generally well written, and with some novelty. But the provided results cannot convince me that the proposed method is not just a trick. I will rasie my points if the authors can provide more results to convince me.	4
The paper studies an interesting topic. However, there are several issues such as minor improvements over previous methods, practical value, and also writing problems. Overall, this is an interesting paper, but the quality is not very high. 	2
"I have a number of concerns about the paper. In short, (i) limited technical novelty (ii) the theory does not seem to apply in practice (iii) the experimental evaluation is limited, and uses only small scale experiments (iv) room for improvement in terms of presentation e.g., many of the results are included only in the appendix. 

As a result, I do not believe that this submission, in its current form, is of interest to the ICLR community. "	2
"The idea of trying to use recent singular model theory to improve variational inference is interesting.
However, this work, in its current form, leaves too many open questions about the relation between theory and their experiments.
Furthermore, it is unclear what the take-home message is."	2
I have doubts about the contribution of this paper and its significance to the literature.	2
The token reorganization is new. The results are fine. Addressing these raised issues above will make this submission more convincing.  	4
Token processing is promising as it potentially benefits all the ViTs. The proposed method improves ViTs from the efficiency and accuracy perspectives. This will bring a wide range of impacts on the ViTs development. This reviewer recommends acceptance.	4
I recommend accepting this paper for its technical novelty, the model acceleration, the performance gain (when it is trained with a long schedule), and the potential of improving more with a better attentive score.	4
In spite of some experiments missing, regarding the significance of the novolty of this paper, I think it deserves a positive score. If the concerns can be well addressed, I would like to lift the rating.	4
"My major concern for this paper is the lack of novelty and lack of experimental details. In the end, this is an empirical work providing no novelty. Those details should be extremely clear.

Novelty:
- The introduction claims the paper proposes 2 pruning methods; However, in the method section, there are only references to two pruning strategies: iterative and one shot. These two strategies are well-known and utilized in the pruning literature; Iterative pruning was initially presented at Han et al. Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization, and Huffman Coding. 
For one-shot, also referred as single-shot pruning, Lee et al. SNIP: SINGLE-SHOTNETWORKPRUNING BASED ONCONNECTIONSENSITIVITY, ICLR 2019
The paper does not refer to any approach in the literature. More importantly, pruning is not only about strategy but also the saliency metric used to remove parameters. 




On the experimental side:

- For instance, the paper describes two modalities of pruning (iterative vs one-shot) but does not describe the type of metric used to measure the saliency of each neuron / parameter. Algorithmic details here would be beneficial. There is a reference to the type of norm but seems like those are not details. Would be also good to show numbers for importance based methods (see for instance Molchanov et al. Importance Estimation for Neural Network Pruning, CVPR2019)
- Network architectures: The algorithm uses the results from FastAi; What are the details of these architectures? Would be good to add some additional architectures to have more conclusive results.
- Tables seem to be too large (for instance the training time). Adding better tables / graphs would leave space to have these details in place. 

- The paper focuses on the lottery hypothesis; However, the ultimate goal is to prune a network as much as possible while maintaining accuracy. Therefore, I would expect comparisons to other approaches not based on the lottery ticket: importance-based pruning; magnitude, prune while training, zero shot pruning.....



- How are models with less than 2% accuracy selected? I guess that is an empirical selection, right?


Clarity:
- Would be more readable if the paper explains the lottery hypothesis and related pruning works and how those approaches can not be applied to tabular data; In the current form, seems like it is just plug and play: The paper states: We focus on applying this hypothesis to tabular neural networks. Why is this challenging? Why is this different to applying to other data / networks? What has been done in that space in terms of pruning / compression?"	1
Overall, even if it offers some interesting perspectives, I believe that this paper is immature and not ready yet for publication. 	2
The experiments and the methods are clear in this paper, occasionally with some problems which I have mentioned above. From my perspective of view, the main problem here is motivation. I would give a score of 5 currently, but I am also willing to change my score if the authors provide useful responses. 	2
The paper studies a fairly important problem, but I do not think that the proposed methods are validated well experimentally. Much more additional experiments will be needed to clarify the benefit of the proposed method.	2
Overall, I feel that this is a good paper with good results and clear presentation. I am happy to increase my score if my concerns can be answered.	3
According to the concerns in the **Main Review**, the author currently tends to reject the paper. The viewer also believes addressing the issues will make the paper stronger.	2
Overall, I think it’s a worthwhile paper if the empirical result is correct. The main weakness lies in the structure and the presentation of the paper which make it 1) hard to see the main contributions (is the main claim that hierarchical > flat or with the combination of the proposed components, MIP, OCMP, FIP, etc it works better than other hierarchical approaches?) 2) hard to draw insight as its comparison against prior best models is limited (e.g., does HACR also converge faster compared with other hierarchical approaches in Fig 4?). I would be happy to raise the scores if the authors could clarify more on the above two points as well as answering my questions.	2
The paper presents an effective algorithm for instruction following within the ALFRED benchmark but the means and insights seem overfit to the metrics. I am on the fence regarding acceptance and look forward to a response from the authors.	2
The paper is nicely written, it presents interesting and valuable ideas, and the experiments are comprehensive. I am happy to accept the paper if the authors can clarify my questions.	3
The paper makes a nice contribution to the community.	3
The paper proposes a framework for hyper graph neural networks, which is an interesting problem.  Though rigorous in its treatment and experiments, further analysis on what makes the work better then the baselines would help in better appreciating the significance of the contributions.	3
"Overall, this is a solid paper which proposes one simple, yet novel and efficient idea to generalize propagation maps on hypergraphs. Solid experimental results are provided in the paper.

I like this paper and I would like to see it accepted. I will raise my score if the weaknesses are properly addressed

-----

I am happy with the feedback provided to my concerns and those of other reviewers so I will raise my score"	3
Overall the paper is clear and of good quality in which claims are well supported by theoretical analysis. However, the novelty is incremental and experimental results are marginally significant. Positioning with missing prior work and evaluation on other hypergraph tasks would improve the paper.	2
In summary, the paper provides a solution to a practically useful problem but the novelty is limited and the presentation is not clear enough to understand.	2
Considering the weak points, I recommend rejecting this paper. There are many missing parts in methods and they need more experiments to justify their ideas. 	2
I have provided the review for the paper in the previous sections. Although, the paper considers an interesting problem setting, more experimental validation would be required to judge its claims. Further, the novelty is limited to application of an already existing ideas. I'm happy to take the discussion with authors in the post-review phase to hear their thoughts. At the moment, the paper is simply not in acceptable state. 	3
Generally speaking, although most of the results are correct, the main contribution of this work seems very similar to existing works and the empirical evaluation is not solid enough. Due to these reasons, this submission is below the acceptance threshold.	2
The novelty of the contribution seems questionable.	2
The theoretical contribution of this work is limited and they have ignored many existing defenses and attacks. Besides this paper contains too many obvious mistakes, typos, and the writing/template clearly does not follow the ICLR guideline.	1
"For now, I recommend weak rejection.

I think it is very interesting work that yields novel insights. However the main concerns I have is the limited scope and practical usefulness of the proposed theory / method (seems that it’s limited to a fixed # of epochs and performance isn’t better compared to log-scale Bayes opt methods). I will consider raising my score if my questions are satisfyingly addressed.
"	3
Superficially this paper looks like a rigorous analysis, but it makes so many dubious assumptions and approximations to ultimately obtaining some rather unconvincing functional forms with enough free parameters to fit almost anything.  This kind of analysis requires far more care, attention to detail and empirical validation than the current paper provides.	2
Overall, I believe the primary strength of the paper lies in the extensive experimental study to corroborate their claim. The major theorems need some restructuring to remove the confusions that may arise (e.g., see (1) and (2) in my review) from the current state. Hence, my scores are currently on the borderline. I am happy to increase my scores after discussing with the authors and other reviewers in the discussion period.	3
"The simplifications of the paper are too big to make any of the claims relevant for real networks. 

Main concern is that their functional form only applies to linear regression with SGD.

 The functional form that they obtain has very limited predictive power and can be seen as a taylor approximation in sqrt{eta/B}. "	2
"Although most of the high-level motivations and ideas of the paper have been studied in example/prompt selection and constraint-based decoding (e.g., PICAD), the authors are able to refine them and generalize them to more diverse domains and tasks by using GPT-3/CodeX without fine-tuning with a relatively small overhead to the output process. 

Also, results show that the proposed method still underperforms supervised methods, while both TST and supervised methods use the full training set. Given the large size of GPT-3/CodeX, it would be better if the authors further discuss the practical merits of their approach (compared to few-shot learning with median-size language models has more meaningful practical benefits at this time if they are able to achieve competitive performance by fine-tuning <300-shot (a small number) examples.). 

"	3
Overall, the methods proposed by this paper are novel and address major drawbacks of previous methods, leading to significant improvement of code generation performance across three real-world languages. The methods are also relatively straightforward to reproduce and built upon by future works.	3
"Overall, the paper gives some interesting ideas, but it does not demonstrate these ideas lead to actual wins and doesn’t discuss its limitations. Combining the two contributions TST and CSD does not lead to state-of-the-art results, while removing some of the advantages of few-shot learning. I believe this work is not generally flawed, but in this state is below the bar of ICLR.
"	2
"Pros:

. A new approach to code generation using pre-trained models.

. Evaluated on three languages (SQL, Vega-Lite, and SMCalFlow)

Cons:

. It is unclear if the proposed approach can be applied to other general-purpose programming languages such as Java, Python.

. Some technical details are missing."	3
"The theoretical results provided in this paper are kind of interesting but I think they are more about sequential training rather than continual learning. The experiments part is a bit weak to support the claims. 
As I'm not familiar with the NTK related work, I didn't check through the proofs of those theorems. "	3
Although the paper's significance is a little limited by the relationship with previous work such as [1] and the restricted scenario, the results (especially the self-knowledge transfer case) are interesting. It could be a starting point for more general and informative analyses of deep continual learning. 	3
"To conclude, I like the aim at theoretically approaching continual learning but it seems there is still a consequent gap to make the theoretical setting proposed fit an actual continual learning setting.
I still will grade this paper above the acceptance threshold, because I think the paper can be useful for the community. "	2
This is a well-presented theoretical study that applies previously developed techniques to analytically describe transfer/continual learning in neural networks. The results are to the best of my knowledge novel. The theory reveals interesting rich phenomena that occurs in actual neural network simulations.	3
The paper presents some interesting theoretical connections, but does not provide sufficiently compelling empirical results showcasing utility for applications in ML.	4
"This paper introduces some very interesting ideas about the use of EBMs and mean-field VI in the context of player valuation for cooperative games. Their new perspective is connected to Shapley/Banzhaf/probabilistic values, it permits approximate optimization (the gradients require sampling), and they define a new value (the variational index) that performs quite well in their experiments.

I had a couple questions and comments about the writing, but I expect these will be easy to address."	4
A well written and novel work on a variational framework for player valuations in cooperative games. While approachability to practitioners could be improved, I am in favor of accepting this work.	4
Overall, I think that this paper was nicely written and I like that it unifies the Shapley value and Banzhaf index, but I don’t yet see why the proposed approach is necessarily better than using either of these two existing criteria, so I’m leaning towards rejection.	2
Given the above technique issues, I think this paper is not ready for publication.	2
The paper provides a first good attempt at offline rl with pessimism, and should be accepted.	4
In general, the paper is well written and main points are neatly addressed. With a model-based approach using concentration properties of MLE, this work contributes to the literature of pessimistic offline RL and shows various concrete examples the framework can deal with. Some weakness include the over-advertisement of tackling infinite policy class, and lack of discussion on feasibility of implementation. 	3
This paper provides rich theoritical results for the important offline RL problem but it's a bit lacking a strong exciting point. 	2
While I like the direction this paper is going, I have concerns regarding the novelty of this paper and whether the experiments show the full power of differentiable neural architecture search (DARTS). Therefore I currently lean towards the rejection side. Please see my main review for more details.	2
"- Discuss additional references
- Provide some additional discussion how can the performance be improved
"	4
This paper can be regarded as an extensive experimental investigations, which find out that standard DARTS developed in SL are actually applicable to RL. Although the findings are interesting, it could be fit better as a workshop paper.	2
This indeed and interesting work / study and has a lot of merit. However Authors are strongly encouraged to address the concerns. 	2
This paper introduces a new penalization framework for the fairness MAB problem. The authors propose a UCB-type algorithm with theoretical guarantees. From the technical perspective, my main concerns are: 1) relatively simple algorithm design; 2) some issues of the regret analysis. Besides, the motivation for introducing such a new framework and its potential real-world applications are unclear in the current version.	3
I recommend the rejection of this paper due to the lack of a reasonable fairness guarantee.  See the detailed comment.	1
A solid analysis of a novel and intuitive algorithm for the MAB problem under a type of fairness constraints, however the comparisons with related work are rather weak. 	3
The paper has positive contribution to the community on studying the penalty-based fairness condition in MAB, but due to the weakness pointed out, its contribution is limited and is not easy to understand. Therefore, I think it is not a case of clear acceptance, so I give the marginal accept recommendation.	2
Overall I felt this paper was below the acceptance threshold primarily because I felt the experiment domains rather limited and disappointing that the only baseline was random noise (and that the method seems to do only marginally better than random noise). I also felt that prior methods could work in continuous action spaces if very slightly modified, so I felt this paper could be much stronger if it experimented with and compared to those variants.	2
"Although this work is looking at an important problem related to the robustness issue of MARL, its technical contribution fail short in term of the theoretical analysis, novelty, and evaluations. 
"	2
The problem under research is interesting, but the proposed method can access parameters of all agents’ policies, can modify observations of all agents, and depends on pre-defined target states. These assumption hurts the novelty and applicability of the paper.	2
Overall, I think the paper still requires substantial work to make sufficient contributions. Particularly, the authors need to address some aforementioned issues, such as marginal novelty and weak numerical results, to make the paper technically solid and sound. Hopefully my comments can help out.	2
The authors propose an interesting and novel algorithm for machine learning, but it's unclear how significant it will be.	2
The paper provides certain interesting result, however the novelty of the paper and advantage of the proposed algorithm need further justification. 	2
"I give authors credit for the algorithmic development and to a certain degree analyses. They are over-selling the work by using 'novel' too often (in particular in connection with the formulation). The role of \beta in Theorem 1 makes me worried about the significance of the result. 

In short, this is nice work but not impressive. Addressing some of the weakness would definitely improve the paper. 

Other remarks:
Remark 1: w^t stay in a bounded region: This is often not the case in practice. Gradient clipping is often needed to circumvent this. The reLU issue is addressed in an ad-hoc manner. 
Assumption 3: On the first read, it felt like such a \hat{v}_aster^t always exists. Then I realized that it is \hat{v}_aster^t(\epsilon). The authors should probably emphasize this dependency. 

Minor comments: 
The ""Loss Functions"" subsection should be take out. It's trivial. Definitions 2 and 3 should be removed as it is safe to assume that a reader not knowing these concepts would not encounter this paper (even less being able to understand it). 
New Algorithmic Framework
Period missing at the end of (7)
It is weird to write H (matrix) \cdot \eta (scalar) \cdot v (vector). I suggest \eta H v
The assumptions in Theorem 2 don't have to be repeated. One can simply state ""under the same assumption as in Theorem 1 plus additional assumptions ...""
"	2
"Despite being a well-written and well-motivated paper, I have to recommend
rejection due to a critical mathematical error. However, if this error
can be resolved in a revision, then I would be open to increasing
my final review score.

EDIT: The authors have uploaded a revised version of their manuscript that has addressed my main concerns. I now 
recommend a score of 6 for the paper. "	3
This work studies an interesting problem. The proposed method is reasonable for solving the problem. The novelty is not significant. In addition, experiments could be improved. 	2
"This paper studies an interesting and practical problem of learning better representations for tail and isolated nodes. Making use of a distilling approach, the combined information of both structures and attributes can be learned to generalize in nodes with less or even no structural information. Some weakness of the paper includes missing baselines, comparison to other GNNs, and deeper investigation of the label information.
-------------------------------
I appreciate the responses from the authors that addressed most of my concerns. I updated my rating."	3
Overall , I think the paper studies an important and interesting problem and presents a good solution which is theoretically sound. It however could try to cover more broader and crucial task especially w.r.t cold-start recommendations . 	3
"The paper provides an interesting and novel solution for the critical cold-start problem. However, several claims are not well-explained or well-supported.

The authors addressed most of my concerns, I would like to update my score."	4
The proposed method shows promising results. My major concern is about the empirical and theoretical analysis (see concerns above). Hopefully, the authors can address my concern in the rebuttal period.	4
NASI applies approximations for neural architecture search at initialization based on NTK. However, some of them are not convincing to me (see the main review above). If the authors can provide detailed explanations about that, I would consider raising my score. 	3
"This work proposes to search for good candidate neural architectures at initialization (NASI) so that we can completely avoid model training during the search. The theoretical analysis is also provided to analyze the optimization via NTK tools.

The authors replied my previous major concern with clarity, so I raised my score to 6. "	3
In general, this paper is of good quality and exceed the bar of being accepted by ICLR. Although I have some minor concerns about the performance, I think the paper has great contributions to the NAS community and should be recommended.	3
"The paper takes a relevant direction in research trying to make inference more scalable for BNN (and other models) by combining MCMC and VI in a stochastic gradient MCMC framework. The paper, however, does not provide theoretical guarantees of convergence while empirical evaluation metrics were not chosen optimally (only univariate metrics which are also not optimal for SGMCMC methods as shownin [1]). Consequently, I would encourage the authors to address the critical points and resubmit the paper in the future. 

[1] Nemeth, Christopher, and Paul Fearnhead. ""Stochastic gradient Markov chain Monte Carlo."" Journal of the American Statistical Association 116.533 (2021): 433-450."	3
This paper proposes a hybrid SGMCMC algorithm, but the underlying theory is not fully developed and the performance of the algorithm is not fully explored. 	2
This is a clearly written paper with a tightly scoped contribution. I advocate for acceptance, and looking forward to follow up work.	3
"The paper presented an interesting approach to incorporate independent structures into posterior inference. However, there are still some ambiguities in motivation and theoretical soundness. To better support the claims, some metrics or experiments regarding uncertainty quantification should be added or discussed since the proposed methods also seem to underestimate the variance.  

---
I have read the author's responses. It address some of my concerns. However, for my last concern regarding the uncertainty (which is one of the reasons we consider MCMC over variational inference), the author did not mention it at all. Since this algorithm also underestimates the uncertainty due to minimizing the exclusive KL, I recommend the author to demonstrate that it can provide reasonable uncertainty quantification. Therefore, I will keep my original evaluations. "	3
The paper proposes an interesting family of new contrastive learning objectives, and some empirical comparison of the objectives. However, I think the paper needs a more clear and accurate statement of the contribution, better justified theoretical results, and better empirical evaluations. 	2
"The paper is well-motivated and clearly written. I enjoy reading the paper, and I believe the results are technically sound as I do not find major flaws in the proof. Although mathematically the new framework simply replaces mutual information with f-mutual information, I think it produces several interesting observations and draws connections with the literature. Due to computational efficiency, the authors are not able to provide confidence intervals of the objective in their numerical experiments, which raises the concerns whether the new framework is indeed better in some applications.
"	3
"1) Novelty:
- The main limitation of the paper is that the proposed idea is not novel. The variational lower bound of mutual information (MI) based on f-divergence was well discussed in previous works (e.g., [1, 2, 3]) so it is straightforward if we replace the InfoNCE bound with the new bound.

2) About the method:
- The main advantage of InfoNCE compared to other lower bounds of MI is its low variance which allows stable learning to achieve good representations. The low variance comes from the fact that InfoNCE uses multiple negative samples per positive sample and normalizes over them. On the downside, InfoNCE is biased and requires a large batch size. SimCLR has this drawback but it was overcome by MoCo. Therefore, I expect the authors to theoretically and empirically analyze the advantages and disadvantages of their proposed method compared to InfoNCE in terms of bias-variance trade-off. They should also compare their method with MoCo, which has been shown to perform better than SimCLR.

3) About experiments:
- The experiment results do not show the clear advantage of the new method compared to the existing baselines based on InfoNCE when using a linear classifier. The difference in performance in Table 3 is small which can be attributed to randomness in running. I think the authors should at least provide the standard deviation of their results to highlight the significance.

- The results in Table 3 in the paper are still very far from the results reported by SimCLR (Table 8 in the SimCLR paper). For example, SimCLR reports 95.3% accuracy on CIFAR10 with a linear classifier but in Table 3, the authors only show 89.7% accuracy. I think the authors should check their experimental settings again to make their results more comparable.

[1] On Variational Bounds of Mutual Information, Poole et al., ICML 2019

[2] Mutual Information Neural Estimation, Beghazi et al., ICML 2018

[3] Learning Deep Representations by Mutual Information Estimation and Maximization, Hjelm et al., ICLR 2019
"	3
Theory 5 provides a way to tell whether an f divergence could be a good candidate, which attracts me a lot. But at the same time, the description of the main algorithm is not complete, some details are missing and the notations in the figure is not well-explained.	3
"This work proposes a simple yet interesting algorithm which meta-trains just a single vector and duplicates it as initialization for the all classification head weights for various classes in MAML. It improves the performance of the MAML, however I have some serious concerns regarding the importance, the validity of the motivation and the fairness of the results for this study. 

 
 "	2
Finally, I think this is a thorough study of a very useful algorithm and can improve many papers that are based on this approach. It is a good improvement on MAML that is based on a very detailed systematic study. As a result, I vote for acceptance.	4
I think it is interesting to research and rethink the possible issues in the existing baselines or benchmarks. I have a few concerns about the significance of this current version of the paper because of the poor performance or the arbitrary design of experiments. I would like to see the comments of other reviewers and the discussion with the authors to make a final decision. 	3
I found the paper clear and easy to read. All the experiments are detailed as well as the thought process behind them. I would appreciate more information on the full training process to improve reproducibility.	3
"Overall I think this paper considers a very interesting setting. Permutation invariant MARL captures many real-world applications but is less studied rigorously, and this paper fills the gap. 

I do think the presentation can be improved and some clarification needs to be made (see the ""weakness"" part).  "	3
Overall, I felt that while the paper provides some new insights and perspectives on policy sharing in MARL, it needs additional efforts to be accepted in top conferences.  	2
This paper contributes a theoretical analysis of policy sharing in fully-observable multi-agent RL, and a practical algorithm for training agents in communication-limited settings. The underlying assumptions and limits of the theoretical analysis have been clarified by the authors. With this, the paper makes a decent overall contribution that is also clear in context of other literature in Dec-(PO)MDP and Markov Game models.	3
This work makes multiple innovations, as aforementioned in my summary and pros. However, the first two concerns imply that the advantage of the proposed algorithm does not seem to be well supported by experiments, and the third concern seems to imply lack of practical examples for your proposed homogeneous Markov game. Hence, I tend to reject. **Once you figure out the first three concerns, I will change my score to accept.**	3
Comparison with existing attacks and defenses is insufficient and adaptive attacks are not considered.  Theoretical analysis relies on strong assumptions. 	2
Discretization is widely used for reducing communication burden in federated learning. This paper is the first to consider its effect on robustness, which is an interesting idea. However, some of the claims made in the paper are not grounded. The evaluation of the algorithm is also limited. In some setting, it's not clear the advantage of the discretized gradient over continuous gradient; in other settings, the evaluation is limited to a specific dataset and hence it's not clear whether or not the findings are generalizable. Also, the privacy inference attack setting is not evaluated. So I think the current version of the paper does not meet the standard of ICLR.	2
The idea of discretization mechanism for the purpose of robustness is novel and interesting to me, but I’m worried about the quality about the paper: (1) the paper is not well-written; (2) the robust convergence guarantee may be vacuous; (3) the experiments could be more complete; (4) the security claim is not well-supported. 	3
I do not recommend acceptance because of (1) lack of novelty; (2) writing quality; (3) robustness results are hard to interpret.	1
This paper has good overall presentation of the technical content and sound theoretical analysis regarding the proposed method as well as related work. From my perspective, the proposed method is novel. However, there are some details regarding the problem definitions and experiment settings that need to be further verified.	3
This paper introduces a new reconstruction-based loss for avoiding representation collapse and theory to justify their choice. In experiments on both graph- and node-level tasks, they show competitive performance with other SOTA methods. While the goal of providing some insights into SSL-based approaches is laudable, their theory is limited in that it doesn't provide any unique information that is graph specific and thus fails to provide insights into graph representation learning. When compared with similar SOTA methods, there is limited gain in performance; thus, it would be useful to evaluate other aspects of the algorithm and make stronger connections to the theory.	3
The idea of the proposed method is novel and interesting to me, but the assumption of latent graph about the generation process of graph is a bit strong. 	3
This paper studies a self-supervised learning framework to perform representation learning for graph neural networks. The authors provide theoretical discussion about latent graph prediction and related domains. Even though many theoretical analyses and derivations are provided, the resulting algorithm is too close to existing methods, and the latent graph prediction perspective makes it more difficult to understand the proposed method. It has strengths with interesting theoretical analyses, but the impact and the novelty of the resulting algorithms are limited.	3
"The idea is interesting and I like it.
I highly acknowledge the discussion section.
However, the model design choices are too broad (hyperparameters, augmentation, and adaptation functions) and necessary ablation studies are missing.
If analysis on important design factors can be provided, the paper would be more thorough."	4
It is indeed interesting and of practical value to distinguish any anomaly condition using sequential data with simpler network architecture. However, there are some places where this paper needs to clarify and provide more details to make it technically sound.	3
I recommend the paper be rejected primarily due to its exposition not being clear and technical aspects of the paper being unclear or wrong.	2
"·	To summarize, this paper proposed a pessimistic contrastive learning framework for sequential outlier detection. The proposed method is technically sound. However, the novelty is limited on multi-conceptual pooling and no further empirical exploration on this component is provided. Also, many selections for the model seems arbitrary, which needs more justifications to support the motivation; and the dataset selection seems conflict to the model assumption. The most important part is the author includes the acknowledgements in the draft, which implicitly reveal the author identity and is against the submission policy. Therefore, I recommend rejection to this work."	2
The idea is interesting but the paper lacks depth in empirical analysis.	3
The contribution is interesting and claims are supported. Unfortunately, the analysis part is a bit lacking as I highlighted in the main review section.	3
"The overall design of MobileViT block is neat, simple and effective. The MobileViT model is a competitive example of combing existing CNN blocks and new transformer-based block to achieve high accuracy and low-latency on mobile CPUs.

The paper quality can be improved by disentangle the improvement from the claimed contributions (i.e. MobileViT architecture, multi-scale training) from the rest (e.g. label smoothing, EMA, and any other training recipe under the hood). This is important for the main results in Figure 6 and 7.

 "	3
"Overall, I think this paper is well-written and -organised. The promising performance impresses me a lot. 

However, this model stack the mobilenet and transformer models, and thus, the novelty and theory may not be enough for the ICLR community's publication. Besides, according to the reviewer's previous experience, the proposed framework can also work well in most lightweight backbones. The reviewer suggests that the authors should conduct extended experiments to verify this framework is a CNN-agnostic strategy. It can provide general insight to the researcher. Finally, this model's mobile device application is limited (though the authors mention this point in the submission).

Overall, the reviewer votes for the borderline in the first round. The reviewer is also happy to change the initial score according to the quality of the rebuttal content."	2
Overall, I vote for acceptance. The well-structured paper proposes a useful molecule graph generation approach, accompanied by quite a comprehensive set of experiments.	3
For now, I recommend the rejection of this paper (weak reject). The novelty of the proposed method is significant, but there are some unclear parts in the description of the method, and the results do not sufficiently prove the superiority of the proposed approach (see the comments above).	3
The paper utilized many novel approaches in graph representation and reinforcement learning to resolve the timely challenge about SARS-CoV-2, but the experimental results of the proposed model fail to show sufficiently powerful performance compared to existing algorithms except for better synthetic accessibility and docking scores. More innovative designs may need to be introduced to the present model to get a persuasive improvement.	2
"I have been reviewing this paper for a few times, and I appreciate the effort that the authors have put to update the paper. However, the current draft can not truly reflect the recent advancement in the field, and it may mislead the future. Once the author have address my concerns 1, 2, I'm open to accept this paper.
"	2
A good paper for the NAS community; hits a sweet spot for the current bottleneck of the reproducible NAS and the scientific research of NAS. I would strongly recommend accepting this work for ICLR.	4
Overall, the paper is very well written. It is clearly motivated and supported by lots of ablation studies. The dataset and search space might not be SOTA, but it does provide a promising practice for future NAS research. There are a few questions that I would like the authors to clarify, otherwise, I would recommend acceptance of this paper.	4
Overall a well-written paper with no significant novelties. The presented benchmark does not seem to be a useful addition to the vast amount of already existing benchmarks.	2
The paper makes solid theoretical and empirical contributions for obtaining new implicit architectures with certified guarantees on their robustness. However, I am not sure whether the  proposed method advances the state-of-the-art in certified robustness unlike the L_oo nets. Further, many key details were not clear to me including comparison with the classical Knaster-Tarski theorem and whether the explicit networks are certified with complete verifiers or IBP.	3
"This paper is interesting and can be meaningful for further research as it derives the IBP computation on DEQ for certified defense. But the empirical results are limited so far.

=========Post-rebuttal updates=========

Thanks for the response from the authors. The authors have improved their experimental results where the baselines match better with [Shi et al.’21] (currently this update is not visible to the public). 

This paper does present interesting findings in terms of verifying DEQ and can be potential interesting for the area of DEQ, and this is the first work for verifying DEQ which is different from explicit networks.

But the contribution is kind of insufficient. The paper fails to demonstrate the benefit of using DEQ itself. If it is unclear why we need DEQ, it will be minor to study IBP bounds for DEQ, especially  IBP-MonDEQ fails to really outperform baselines with explicit models (very marginal gap). I think it may help if the authors can introduce some scenaraios in the experiments that are particularly suitable for applying DEQ, to demonstrate that DEQ is really needed. 
"	3
IBP-MonDEQ is an interesting and non-trivial extension of IBP to deep equilibrium layers that allows for relatively effective robust training of DEQ networks. The theoretical analysis seems comprehensive, and the experiments show that the proposed algorithm works in practice. I believe the paper would further improve by extending the experimental section and addressing its limitations (large runtime overhead).	3
A theoretical paper that develops a new class of certifiably robust deep equilibrium models with guaranteed unique fixed points, a worthy open problem. The model is largely theoretical (i.e. basic networks such as fully connected or convolutional layers), and as such will not achieve state of the art predictive performance. Nevertheless, the empirical evaluation serves its purpose at demonstrating the utility of the model.	3
The paper proposes to tackle challenging problems and provide a reasonably workable framework with several benchmarks. However, the core contributions from either the technical side or domain science side are not substantial and significant. Multiple pieces of work need more clear justification and explanation, and the metrics need to be improved with a more physically reasonable definition. I also expect to see a better comparison with the current baselines. Probably, the work is still ongoing so I recommend to reject it in the current form but expect to see more improvements in the near future. 	2
"Overall, I think the submission is of high quality and reaches the bar of ICLR.
It would be nice if the authors could address the concerns listed above."	3
The problem is specific but new to the general machine learning audience, and the proposed VAE model with equivariant score matching decoder is interesting for me. However, I still hold some technical confusion (weakness 1), and I think some important metrics from the highly related topic are missed in the material generation task, which is the most important empirical part of this paper. So currently I vote for a weak rejection.	3
The proposed approach is a novel and relevant contribution to the domain of materials generation.	3
This work demonstrate that lottery tickets can not only be unstructured but also be structural, which makes it a novel and interesting work. There are minor issues in experiments.	3
The paper is in a good shape that displays the first set of positive results bridging the gap between the lottery ticket hypothesis and practical accelerations on real-world hardware. Algorithm details, as well as results, are sufficient and convincing.	3
"Important baseline results are missing. Experiments are potentially flawed. They claimed what is not really realized in the paper.

"	2
The contribution of this paper is solid. I hope the authors could clarify the weak points.	3
This paper proposes a new few-shot learning procedure and shows its effectiveness in terms of prediction accuracy. However, the advantage of the proposal should be discussed compared with previous works; the authors should conduct a more thorough evaluation. Accordingly, I vote for rejecting this paper.	2
This paper exploits the fisher matrix to measure the relevance between different tasks which is an interesting attempt. In addition, the experimental validate the effectiveness of the proposed method.	3
The overall idea of using task affinity score for few-shot learning is impressive. However, I think this submission is incomplete and lacks a lot of technical details, which makes the paper not easy to be understood. Meanwhile, the paper also has not conducted any ablation study to show the real empirical improvement of the proposed strategies and modules. Moreover, although the method proposed by the author does not introduce additional model parameters, I am quite concerned that the computation cost of the fine-tuning source tasks during the task affinity score calculation.	3
I think this paper is a useful contribution, and although I don't think the issues I raise are completely minor, they shouldn't be too hard to address by the authors.	3
I recommend the paper be accepted as is. The paper's writing and presentation are very clear and polished, and the results are both sound and significant. I expect this paper to stimulate the cooperative MARL community.	4
I think the theoretical contributions of the paper are great and would benefit the community. I was hoping to see much bigger domain in the simulation part. The writing is currently very dense.	3
The main theorem is wrong. 	3
The paper presents a new detection-based defense against model extraction attacks that performs better than the previous (directly comparable) defense called PRADA. However, the defense is expensive in terms of computation and storage usage. 	3
The paper is targeting a challenging problem. However, the central concept of hardness of samples as defined in the paper appears flawed. The paper has several limitations when in comes to experimental evaluation.	2
"- The proposed approach 'HODA' extends the state of defenses against model stealing attacks by exploiting an interesting property of how an attacker's distribution contrasts a benign distribution. The experimental results indicate that indeed this property can be exploited to some extent to reject malicious queries.
- However, I am primarily concerned about (a) certain assumptions (being able to associate each example with the same user, distribution comparison) and (b) how this compares to simply performing OOD detection. In particular with (b), if the key idea to exploit that attacker's distribution is semantically different from the victim's, I would have appreciate a better comparison with OOD detection, or present some analysis/insights of the proposed 'hardness' justifying the choice."	2
In general, I think this paper has merit.	3
"1. Discussion of related works can be significantly improved.
2. Example application of the framework to improve upon/subsume prior results.

In summary, I think this paper provides some novel insight in the problem of reward learning, but there can be substantial improvements to be made to make the paper significantly stronger. I would highly suggest the authors make the additional effort, and I know it's gonna be a lot of work. But it will potentially make it a spotlight/oral paper instead of a borderline."	3
This is a strong theoretical work; it makes a fundamental contribution to reinforcement learning literature. The reward functions are atomic to RL -- understanding the theoretical limits on how much information can be extracted from various data sources used for reward learning is important. 	3
The paper is very hard to follow and I may not be able to assess the paper properly. 	2
Better fits journal publication or book chapter?	2
This paper focuses on semi-supervised node classification with label noise. Although the paper has some contributions over existing works, I think the contributions are not significant enough, so I lean towards a weak reject.	2
I give a borderline evaluation for this paper. My main complaint is the experiment results are not strong enough. More convincing results or a strong reason why PI-GNN is a good candidate in similar tasks should make me improve the evaluation.	3
"1. The presentation is overall good and easy to follow, but some descriptions or details are not clear.
2. The novelty is limited. Essentially, the proposed pair interactions belong to contrastive learning.
3. The experiments show better performance.
"	3
"Although this work has several strengths (important problem, simple but effective method, and extensive experiments), it is not ready for publications by following weaknesses:
- Related work on graph structure learning is not thoroughly surveyed and not compared. Technical novelty on the model might be overclaimed.
- Scalability is not discussed, even though it has the potential of huge complexity.
- Experimental settings and their demonstration are not consistent without justifications.
"	2
I think the paper presented an interesting idea for learning latent motion representation. The results are also very encouraging. It would be great if there are more in-depth analysis about the learned representation in terms of interpretability and disentanglement. Some writing improvement would also help the readers to understand the details better.	3
This work proposes a relatively novel method that aims to animate still image via latent space navigation. Experiments show the state-of-the-art performance compared with previous works. This work, nevertheless, can further be improved by providing more supporting experiments.	3
"Overall, this is an interesting paper with significant results. I lean towards accepting this work if the issues that were raised are resolved.
"	2
On balance I'm somewhat positive about the paper.  However, I have found some considerable issues as described above. 	2
"This work is quite interesting as it shows how it is possible to achieve compelling results in image animation without requiring any explicit intermediate structured representation for the motion of the main subject. The work builds on recent findings on the expressive power of the latent spaces of deep generative models for image manipulation and repurpose these for the task of animating images.
As a side effect of this drastically different approach, the method simplifies quite significantly the network architecture and the training regime while achieving equal or better results on standard benchmarks and user validation wrt the sota. The method presentation could be improved but the paper does not present major flaws in terms of methodology and experimental validation. For this reason I’m currently suggesting acceptance and rate  this paper at 8.  "	3
This paper explores only one of many differentiable approaches to learning binary variables. I think a more comprehensive comparison against alternative differentiable approaches, from simple REINFORCE to horseshoe priors, would provide more insight and ultimately a convincing conclusion on why the chosen approach seems the most worthwhile. I do not understand the relevance of the PINF section; perhaps it could be its own standalone paper instead. There are also some clarity issues.	2
Overall, I had a bit of a hard time understanding just how significant the improvements were when using this method. Is the differentiable method more robust to hyperparameters? In principle, with careful tuning, it seems like the non-differentiable version should also be to recover the same equations, given that they both use the same model for approximating the dynamics. In addition to significance, the clarity of the paper could also be improved.	2
"The only reason I feel the paper is marginally below the acceptance threshold is that -
* The authors haven't compared their method against one of the most popular methods - PDE-NET 2.0
* Lack of extensive experiments (see above) to demonstrate that their approach can identify a variety of equations."	3
"Pros:
+ The problem of differentiable model discovery on noisy data seems important, and the proposed method appears promising and easy to implement

Cons:
- Optimization landscape appears insensible

**Post-rebuttal update**: while the authors clarified on the experiments and related work, my core concern on the optimization landscape remains.  Therefore I am keeping my score unchanged."	2
"As I mentioned above, I think the paper is interesting and thus recommend the acceptance of this paper.

***
Post-rebuttal: I appreciate the authors’ clarifications on my questions. I'd like to keep my score."	3
I think this is a nice paper but to fully motivate the approach, it should compare against another obvious baseline. I would like the authors to motivate why the setup I described above is not considered in the experiments. 	2
I would lean towards acceptance of the paper. Using symbolic regression to approximate closed-form ODEs is novel and very interesting, and the results presented in the paper are compelling. However, I would want to see more comparisons to other methods, as well as results on more datasets to gain a better understanding of when this method will be useful and outperform existing methods. I am hoping that the authors will be able to to answer my concerns by presenting more results. 	4
"The work is derivative in methodology, but provide enough insight to be interesting. I have some questions on the Neural ODE comparison what would need some clarification. If this concern is addressed it would make it easier to support acceptance.
"	3
Overall the paper proposes a new strategy to optimize calibration via tuning \gamma in focal loss. Writing in this paper is a little concerning. The idea is a little incremental but could be potential beneficial to the community. 	4
The calibration problem is interesting and practical in real-world applications.  This paper presents an in-depth analysis of the calibration problem and proposes an effective method to improve it. However, the proposed method has not been proved to generalize well in different datasets. The authors may answer this question by a theoretical analysis or experiments in large-scale datasets. Another suggestion is to improve the figures of this paper. If the authors can address my questions, I am willing to raise my score.	3
"All in all, the idea is interesting but I find the paper in its current form to be somewhat below the standard for ICLR acceptance, especially the write-up itself. 
"	3
The authors do a good job in motivating their method and empirically showing why it is needed and why it should work. In mu opinion, the results are very hard to read and the paper has a problem with formatting/visibility or tables and figures. Additionally, If I have understood everything correctly, I think the formulation of equation 4 may be improved, eliminating a hyperparameter (see last bullet point in Cons section). Overall, these are the main points influencing my score. 	4
"As mentioned above, the major limitation of the paper is its limitation in explaining to which extent the world model is affected by the robot model and by the state of objects in the scene. Ideally, a natural evolution of previous approaches would be to provide a world model completely decoupled from the robot model and mainly influenced by the state of objects in the scene. 

Even after reading the paper carefully, it's still unclear to me if the world model proposed by the authors is mainly used to propagate forward the information of the state of objects in the scene. Considering that the authors rely on an analytical model of the robot, this is an important requirement and I would like to be convinced that either this is already a property of the proposed world model or that this is not the case for good/understandable/sound reasons. 

"	2
The paper addresses an interesting problem, presents good results, needs some clarifications.	3
"This paper provides an interesting approach to zero-shot learning but falls short of providing enough technical details about the implementation. The complementarity to existing work is not discussed enough.  For these reasons, I advocate for a reject.

EDIT AFTER DISCUSSION PERIOD: The authors have addressed the majority of my concerns.  Reading their answers and the discussions with other reviewers, I updated several scores upward and my final score to ""marginally above"".
"	4
Overall, this paper does present a novel solution to the problem of robust transfer learning on robots in a sample efficient fashion. It is a good example of how to successfully leverage information we have about the problem a priori (robot dynamics) but still learn anything else we don't know (object dynamics) from data. This is proposed as an alternative to the otherwise brittle methods which try to learn everything from data, end-to-end. While there are minor details to be elaborated on and fixed, the idea itself and the evidence provided here are sufficient for publication and are a useful contribution to the field.	3
The novelty is limited, and the impact of the proposed method could be rather minor. Some necessary details should be provided.	3
The idea is clear and the empirical evaluation is strong. Since that the reviewer has some concerns on the provided theoretical analysis, it would be safe to decide after reading the feedback from the authors. 	4
"This paper develops a self-supervised learning framework to extract node features supervised graph. It is an interesting problem. Theoretical analysis is also provided. Extensive experiments on large-scale data validated the effectiveness of the proposed method.
There are some typos, e.g., ""These have to be predicted using the textual information in order to best match the a priori given graph topology. This is achieved this by using the state-of-the-art XR-Transformer (Zhang et al., 2021a) method for
solving the XMC problem."""	3
Overall, while the proposed method is novel, there is some serious concern with the practicality of the RHOLS method because it assumes an already trained model on a hold-out set to train another model on training set. 	3
Overall, my vote for the paper is a (weak) reject. I enjoy reading the paper and think it has some good points and the discussions are meaningful. On the other hand, I think the technical contribution of the work is limited.	2
"It is a good work with good technical contributions on an important problem. 
Authors need to rectify / address the outlined concerns above. "	3
"The paper is well-written and easy to read, and the proposed criterion is neat. However, implementing this method in practice for the purpose of speeding up training would require: (1) training another model, and (2) reserving a subset of the labeled data as holdout. I think both requirements constitute serious limitations that defeat the original purpose. 

In addition, the experiments were conducted on small datasets only (at least ImageNet should have been included). In those experiments, the holdout data was of a comparable size to the training set and the improvement in speed is only around a factor of 2, which is not significant given the above concerns. "	3
The main issue of the proposed model is the lack of novelty. Although introducing torsion is helpful when representing 3D molecules, the proposed model doesn't have novel architecture when embedding it as compared to existing models. The proposed framework also just follows the idea of existing work. Thus I think this work is below the acceptance threshold of ICLR.	3
"This paper is generally good, in terms of story-telling, method description, and technical novelty.
There are three main concerns: motivation/example, model description, and related work. This work can be further polished after fixing these points.

-----

After checking the replies from the authors and polished-up version of the paper, most concerns on the story have been addressed. I would like to raise the score to 8."	4
While I am not sure that the method can be applied in other contexts, I think the idea is valuable, and the experiments are convincing, and I am prone to accept the paper.	3
"The work presents a novel and clear approach to the problem of fair learning under shifts. There are some open issues on the problem setup and experimental results, which I would like authors to respond to. In total, the work is a technically strong contribution to the nascent literature on the problem.

---
From the newly made changes and clarifications in the response, my concerns are addressed. I highly encourage authors to move the results on new dataset, discussion on related work and continuous demographic attributes to the main text."	3
The contributions of the paper are clear. As the paper is missing some related works, the baseline comparisons can be further extended to assess other factors as suggested. 	3
"I have found the main idea of the paper interesting, however, I have some doubts about their proposed method and the way that they evaluated their approach and compared it with other methods (see previous section). 
Also, I suggest the authors to focus only on group fairness as it is not clear how their approach can be used for individual fairness methods. "	4
"I think the paper touches upon an important practical challenge not only in fair ML, but ML models used for decision-making in general. The analysis is natural, straightforward, and technically sound; the most technical aspect is the computation of the upper confidence bound which is standard, so I think there is no particular novelty in its methodological contribution. 

That said, I think the conceptual contribution of the paper is still quite new and useful. In the light of the novel application, I thus think the algorithm is quite novel -- nevertheless, it would be helpful if the authors provided a clearer comparison to the most related works and explained how their approach is novel. 

Finally, I found the evaluation section convincing and well-executed. I would appreciate additional experiments on -- other widely used in fair ML -- datasets (even synthetic experiments).
"	3
This paper has different pros and cons. I vote for weak accept because the empirical results seem convincing. Adding comparison with CG-VAE is important and I hope the authors can address this concern during rebuttal.	3
"I think the paper reaches the bar of ICLR for the reasons listed above.
However, several concerns remain to be addressed."	3
Interesting proposal, needs more validation. 	3
"I am tending to reject this paper because of the following reason.
While the proposed method is very reasonable if inspired by SMILES, its novelty within the literature of graph-based models is less clear, and thus, I am not very enthusiastic about the proposed method. I would increase the score if I were convinced of the contribution of this work to the literature of graph-based models more clearly. So, I would like the authors to clarify which aspect of the proposed graph generation method is important and why. In addition, I would like to ask the authors to discuss why they don't take the VAE+BO approach to molecular optimization, which can implicitly constrain the output to be similar to the training molecules.

---

Nov 21. Updated the score to 5, given the first response by the authors. But still not positive to accept this paper because of the inconsistency in the experiment on molecular optimization.

--

Nov 24. Since all of my concerns have been addressed in the current version, I would recommend to accept this paper."	3
On the methodology side, the proposed method is a stack of ordinary convolutional layer and auxiliary output layers and thus falls short of strong motivation, novelty and insights. On the presentation side, it is more a technical report and lacks the differentiating elements from existing works. The experimental evaluation is weak, because of the less common error metrics, naïve baselines, and unconvincing result analysis. 	2
"My primary concerns are missing literature (and therefore baselines), and unintutive evaluation metrics.
"	2
Overall, this paper studies an important problem, but the novelty is limited. The experiments are also insufficient. 	2
"Overall, while the paper’s topic is interesting - given that multi-dimensional time-series prediction is an active area of research interest in the AI community, as well as focuses on predicting subscriber usage based on two real-world datasets (of which one is publicly available),  the paper’s methodology has limited novelty. Also, the literature review is not thorough, and the paper is not well-supported in terms of clearer description of results beyond the models’ accuracies as described in the main review. 

This reviewer believes that this paper would be better suitable for publication in a more applied venue - as the contribution to technical novelty, particularly the methodology proposed and the application of the study is very limited.  Additionally, the results should be incorporated with additional details on computation time and metrics beyond accuracy, which is integral for comparison of (deep) ML architectures with more traditional baselines, particularly as the paper aims to make the conventional CNN architecture deeper by adding additional auxiliary layers. 
"	2
"Probably the wrong venue.
Lack of formalism in the presentation creates confusion between data and model dimensionality."	2
Improves anytime planning with a learned component in low simulation budget settings. Could be improved with a bit more detail on the learning-planning loop and additional baselines. Overall, seems like a solid paper to me. 	3
Overall a strong paper, with at this point unfortunately slightly too many places where technical details are not entirely clear or only become clear much later than they should be. Based on the strength of the rest of the paper, I do expect that the authors should be able to clear things up (either in the paper or in comments to my review explaining why I'm wrong) without too much trouble, but I do think it is important to do this before it can be published!	3
"This paper proposes to improve AlphaZero and MuZero with the principle of policy improvement by using Gumbel. This paper presents empirical results as well as a theoretical proof. While the method looks interesting especially for using only 2 simulations, some comments are concerned as described above. 
"	4
This paper is likely to be of theoretical and practical interest to many in the ICLR community. My recommendation is not yet higher because I am not convinced of the framing around “planning with Gumbel” and because of the concerns surrounding experiments that I mentioned above.	3
"The proposed task is very interesting and important. Attacking explanations of deep neural networks have already been explored in the image domain [1,2] and graph domain (GNNs)[3]. It's interesting to have the investigation on the NLP task. 
"	2
"Novel paper investigating attribution (explanation) robustness in NLP models. Some sections read confusing. I recommend ""weak accept""."	3
While the technical methodology seems to be valid and confirmed with the experiments, there are some drawbacks such as not reporting all the results, limited discussion, and partly applying some methods from vision domain to text processing, also reported in the weaknesses field of the main review. Even though, I think the contribution is timely and valid, however, there should be several updates on the manuscript before it is ready for publication.	3
"The paper is organized nicely and I have no difficulty following the proof. I think the paper provides a valuable contribution for the learning of quasimetrics and would be useful for many real world applications.

I consider this paper to be marginally above the acceptance threshold."	2
The paper has some nice positive contributions, but I have concerns about the overall theoretical foundation of the work.	2
The paper examines an interesting problem. They propose an interesting program to solve the problem, with some analysis to show a class of algorithms can not be used to learn quasimetrics. It is also well written. Hence I recommend acceptance.	3
Overall an interesting paper with solid theoretic and experimental results. I am not an expert in this area though, hence I cannot say how novel the approach is. 	3
I think this is a very interesting idea, but unfortunately I'm not quite ready to recommend acceptance. My reluctance is largely due to two factors (1) a lack of connection to the broader literature, and (2) the need for more comprehensive experimental evidence exploring the performance of the proposed method. After reading the authors' responses, I have strengthened my score. As I detail below, I do not believe that the current paper sufficiently engages with prior art (especially in the experiments, but also in the main text), and as a result it is difficult to properly discern the relative contribution of this work.	3
"Overall liked the idea but the novelty can be limited. Yet it improves the quality of estimator substantially. Hence, I recommend acceptance.

"	3
I think this is an interesting, novel idea with potential for empirical benefits. However, I am wondering about the choice of response functions, both empirically (how does misspecification affect the procedure?) and theoretically (why is the theory independent of this aspect of the procedure?). I am intrigued by the idea but will adjust my score based off of the author responses. 	4
I think this is a very strong paper and recommend it for acceptance. Methods to learn continuous treatment effects will be of broad interest across causal inference researchers and practitioners, and the authors back up a clever idea with asymptotic theory and empirical validations. I enjoyed reading it and I think many others would too!	4
The paper is well-written and presents a new algorithm that uses encoder to adapt for new clients. The paper lacks few baselines and explanations but can be beneficial for FL community.	2
"Overall, the problem being studied in this paper is interesting and well-motivated. The paper is clearly written. The novelty perspective of this paper can be highlighted better. It seems like a lot of the aspect like hyper network are proposed by other approaches.  Some other major concerns are in terms of the experiments, there are results are not explained or claims without number supported. Please see main review for details. 		 
"	2
Given the major novelty concern, I do not think the paper is appropriate for acceptance in its current shape. The authors should try to make clear what is the novel part of the paper, how challenging it is to get those results, and add more literature review in related work.	1
see the comment above. 	2
The paper is ok. I think there is enough theoretical justification for why the method could work, though I think empirical evidence is necessary to still show that. The theory is a bit disconnected and seems more like a synthesis of many different works; I wonder if it can be presented more succinctly? In any case, I found the writing good enough to follow. My biggest concern is the generalization of the evaluation, since it seems quite short at the moment and very CIFAR10 specific.	3
While the paper seems solid in terms of correctness, I have a less positive impression due to, in my opinion, limited significance. This has to do with the simplification of data augmentation with respect to real world necessary for the theoretical analysis, as well as with the lack of strong motivation for the proposed algorithm.	2
"I think the paper is of interest, especially the theoretical analysis. It is well written and provides rigorous mathematical analysis to data augmentation and proofs for all the theorems, lemmas and corollaries mentioned.

But I believe the main application of the proposed method is in large scale dataset settings, while in small or medium-size datasets augmenting the full dataset is not an issue, but that wasn’t presented (see my concerns in the previous section).
Also, the improvement when increasing subsampling size is marginal, especially with diverse datasets.
This problem and the lack of experiments with large and diverse datasets prevents me from giving a higher recommendation of acceptance."	4
"This paper proposed a data efficient augmentation framework with extensive theoretical analysis. The presentation and the proposed method are sound. However, the lack of experimental results on large datasets such as ImageNet makes the results of the paper much less convincing.
"	3
This is an interesting work that tried to unify concept-based representation learning and disentanglement learning and proposed corresponding metrics that helped to find some important conclusions. Answering/commenting on the points, that I posted under the main review, can significantly improve the quality of this work.	3
It's a good idea to assess whether unsupervised and semi-supervised DGL algorithms can learn high-quality concept-based representations, and in doing so the authors developed two metrics that they suggest can be used to compare concept-based representations both with and without access to ground truth concepts. However, the results from their study do not seem very surprising or impactful, and I was not convinced of the rationale behind these metrics (for the reasons described above). For that reason, my rating is that this work is marginally below the acceptance threshold.	2
The authors propose metrics which address a very important problem in the relevant literatures (in particular, underlying factors which may be correlated), but the communication of the ideas and experimental results are not currently clear enough for me to accept the paper as is.	4
This is a well-argued paper backed up by extensive experimental results. To my knowledge, the method represents a novel contribution. I would recommend it for acceptance.	3
**Recommendation:** reject.  I have some questions about the novelty of the empirical results and the methodology.  Happy to change my mind if I missed something.	2
"The authors propose a novel, impactful and theoretically sound method for conformal training and convincingly demonstrate its efficacy.
This reviewer has some minor concerns regarding clarity that can be readily addressed. Therefore, I recommend acceptance."	3
The paper proposes a differentiable scheme to learn conformal predictors. This is clearly an interesting approach that could lead to improve conformal predictors, which are gaining increased attention in ML (but existing as a consolidated approach since about 15 years). The technical part of the paper is densely presented, making it difficult to really follow. It is also unclear to which extent the validity guarantees coming with plug-in conformal approaches are actually preserved. However, although no theoretical guarantee of that is fully discussed, the differentiable approximation seems close enough to preserve at least a reasonable empirical validity. The additional loss-based formulation and its various adaptations look quite interesting, but it is here even less obvious what are their relation to class-wise conformal methods (i.e., Mondrian ones), and what are the impacts of choosing different losses over validities (particularly class-wise ones). 	3
"‌‌ The paper provides an optimal transport (OT) based algorithm for improving existing summary networks for learning from set-structured data. The paper is original, and  technically sound, however, the novelty is a little bit limited. The proposed approach doesn't improve beyond the pre-existing summary networks architecture. Empirical results demonstrate that the proposed framework improves upon the existing summary network approaches as well as metric-based few-shot classification and generative modeling applications. The paper lacks theoretical analysis for the proposed approach, and the error bars are not reported for the empirical results.
"	2
In summary, I think this is a well motivated and elegant proposal that has been shown to be effective in a variety of experimental settings. I found the authors' claims to be well supported and I cannot find technical fault with their work that would support rejection, so I will recommend acceptance. The lack of supplementary code reduces my confidence in the review as I can only assess the claims as they are presented. The main issue with the work is its presentation which may be easily improved within the rebuttal.	3
The paper seems sound but I have concerns on the novelty: I am willing to raise my score if the authors clarify their contribution w.r.t [1] and [2].	2
This paper is well-motivated and definitely effective for real-world meta-learning applications, however, the authors should discuss the literature of representation learning using optimal transport.	2
Overall, the paper has a good quality with a clear motivation, it tries to answer a very important question about the Lottery Ticket Hypothesis. Authors' findings about the relationships between principle method iterative magnitude pruning and the renormalization group may lead to new insight on the universality of winning tickets. Such insight is important and can inspire a considerable potential for future collaboration between IMP and RG. 	3
This paper takes a bold step in connecting a theoretical framework from physics to a set of important empirical observations in machine learning (LTH). It presents preliminary evidence that this connection could be useful, but I think more support is needed to show that the machinery of RG is crucial and predictive.	3
In general, I think this paper is well written and proposed an interesting theory, however, the argument and proof are not very convincing to me.	3
While trying to obtain a prior for which tasks of a given winning ticket can be transferrable using the theory from statistical physics seems interesting, the paper doesn't provide enough experimental results to show how to use such an explanation to improve iterative magnitude pruning or determine the best architecture that can be transferred for different tasks. The work is more like working in the progress report, and more results can help strengthen the work.	2
"It seems the method cannot handle exponentially large state spaces. While the authors discuss the scalability issue, it is still difficult for readers like me to understand how to a bounded but exponentially large space. I would like to the authors provide more details. 

I would like to see the proposed method applied in more sophisticated games (e.g., Pong and Mujoco games) in which the states cannot be numerated. 

The utility of the explanation is unclear. It would be very helpful if the authors could share their survey questions. That could better help us understand the utility of the explanation. "	2
The authors present a new way (SSX) to create a certain style of explanation, but the style itself is not novel. SSX is not shown to be better than existing methods. Furthermore, its computational complexity is concerning, and the authors do not fully address how to apply their method to larger domains. 	2
My overall impression of the paper is positive, but I have several concerns. My most critical concern is that given the theory and numerical experiments in the current manuscript, I am not sure if the proposed approach delivers human-understandable interpretations of RL policies in applications beyond grid games arising in robotics, flight control, multi-armed bandit, etc. In other words, the proposed method seems to work only for the grid games, and it is unclear how a human user should interpret meta-states and/or strategic states in applications beyond grid games. 	3
"This paper studies an important problem from an interesting perspective. It explains a policy by predicting certain steps of future moves. In summary, I identify the following limitations from this work:

1. The readability and clarity of this work can be further improved. Due to some unclear terminologies, the proposed technique is not clearly described.

2. The technical contribution may be a little bit thin, and some design choices are not clearly justified. 

3. The paper misses a potential baseline that also explains DRL policies through action prediction. 

4. It's not clear the problem space of the proposed technique or, in other words, what types of RL tasks are suitable for the proposed technique."	2
"While the paper does provide some interesting results about the privacy threat of deep ensemble models, it lacks in the discussion of ""deep ensemble models"" as a whole -- which has been discussed as a defense in prior work. Further, some of the claims can be further substantiated as highlighted in my review. Finally, it would be good to clarify if the finding of the change in confidence distribution (for deep ensembles) of train and test samples has been observed in prior work, or is a new contribution in this work."	2
"Generally speaking, this is an interesting paper because the ``accuraacy and privacy"" trade off is under-resaearched. However, the experimental setting is relatively resrective. It is interesting to see whether the conclusion still holds for more advanced and generic ensemble settings."	2
A fairly simple observation but nicely analyzed	2
Although the paper is well-written and the experiments are well conducted, I think the novelties and contributions of the paper is not enough for acceptance: the authors just find out the level of correct agreement among models in the deep ensembles improves MI attacks in a particular deep ensemble setting, no algorithms nor theoretical insights targeted for the setting is provided. 	1
My main concerns with this paper are (1) there are not sufficient details to understand how the model works in terms of the supervision needed and the losses computed, (2) it is not clear what the motivations and contributions are (the authors only claim to show improvement in overall accuracy on very simple tasks), (3) the experimental results are not sufficient. The tasks appear to be quite simple (even though ResNets perform badly this appears to be due to lack of data, but the paper does not discuss specifically learning in low data regimes); the authors could have compared to stronger baselines; some of the comparisons may be unfair (see above) and the authors only show classification accuracy.	3
"I appreciate the merits in this work as mentioned above. However, I will give a temporary weak reject due to its limited novelty.

I have read the reviews and the authors' response. The authors acknowledge some general issues and point out  differences with earlier works, though I do not think is significant enough. Syncing with other reviewers, I decide to keep my initial rating."	1
A technically solid paper that lacks a bit of novelty and falls shorts in evalaution.	2
This is an important line of research, in my view, so the work is of potential value. However, I have major concerns about the extent to which the architecture relies on hand-designed features and rules. As well as limiting the value of the work, in its present state, I believe this issue also invalidates some of the evaluation (on CLEVR-Hans). But I am open to having my mind changed by the rebuttal.	2
Overall, I think this paper advances the research in this area, although more careful ablations are required to ensure the empirical results are indeed promising. 	3
"Incorporating inductive biases into normalizing flow architectures is a very relevant and important problem. Some of the techniques presented in the paper seem interesting, but I really struggled to understand the main methodology part in the way it was presented. The paper claims to outperform the state-of-the-art normalizing flows. For the reasons stated above, I do not agree with this. The paper would benefit from further experiments on more common datasets and from a comparison with more recent state-of-the-art flow models.
"	3
"**Update since author response**:
I thank the authors for their very thoughtful response.  I greatly appreciated the new experiments and more discussion on why the structured layers are simple and have fixed parameters for essentially simple inductive biases.  Also, I appreciate the new NSF experiments.  Overall, I felt these responses answered most of my major concerns.   I have updated my score accordingly.  I would encourage the authors to incorporate some of this discussion in the paper if accepted.

----
Overall, I think the idea of the paper seems reasonable (i.e., to compile known domain structures into flows). However, the compilation technique using conditional CDFs is relatively well-known as the Rosenblatt transform, and it is unclear why a practitioner would not just want to use a more state-of-the-art flow instead of spending time to construct a probabilistic model.  The empirical results are unconvincing that a SOTA flow (even with similar number parameters) wouldn't outperform the current approach.
"	3
I'm recommending acceptance because this paper makes a novel, yet simple and generic, way to impose inductive biases on normalising flows. It is validated by experiments in various domains.	3
Several concerns for this work: (1) Key experimental setting information is missing in the evaluation section. (2) Many state-of-the-art relational learning methods are missing. (3) For fair evaluation, it is better to present the hyperparameter tuning for the new method.	2
In summary, I appreciate that the authors make a great effort to unify several important relational neural networks. However, the paper suffers from its weak justification (on the claimed unification), lack of scalability, insufficient experiment, and ambiguous writing style in general. Therefore, I do not vote for its acceptance.	3
The theoretical analysis is not accompanied by an adequate empirical investigation. 	2
"Overall the paper is well organised with good theory. 
However, the main claims made in the paper need rigorous empirical evaluation.
Positioning with missing prior work and empirical evaluation on real-world hypergraph reasoning tasks would greatly improve the quality of the paper."	3
The paper's main contributions and results, especially on the expressiveness side, are simple and already well-studied in the literature, and thus are not insightful or novel enough to warrant publication.	1
This paper explores a useful area of improving the performance of the teacher network in self-distillation augmented NAS and shows that this can empirically improve NAS performance. Unfortunately, I currently recommend rejection of this paper since it does not have sufficient empirical results to demonstrate the robustness of the proposed improvement and the robustness of their method on different datasets and as a one-shot method. The paper's presentation is also unclear and some of the results may be stated incorrectly so it is unclear if all the conclusions are well justified.	3
The solution is somehow trivial (e.g., diversity-based sampling strategy). The experiments are not sufficient. 	2
"My major concerns on this paper includes:
1. The comparison between some recent weight-sharing NAS methods, e.g. AlphaNet, is missing.
2. The diversity-based sampling method may impose additional restraints of the design of search space of network architectures."	2
This is a solid paper in terms of contribution to the language modeling line of work. It will steer the community in the right direction of doing multi-task learning as a way to meta-learn new task or adapt to unseen tasks. 	2
"The authors demonstrate that massive multi-task learning with prompting can improve generalizability of large language models for zero-shot inference on unseen tasks. The authors develop a very useful resource for the community aggregating prompts across several tasks and datasets. The claim of ""true"" zero-shot generalization needs to be further supported by two more experiments as highlighted in the main review, namely, (i) analyzing the overlap in text from the MTL setup with that of the unseen tasks, and (ii) ablating paraphrase identification tasks to find the impact on NLI tasks.
"	3
"This paper presents a new method for NLP zero-shot learning using supervised multi-task prompted training. They proposed to use encoder-decoder model which is already pretrained using masked language modeling to attain better zero-shot performance with smaller model compared to large-scale autoregressive language model (GPT3). However, there are some shortcomings,

1- The zero-shot evaluations are based on some selected tasks, including NLI, story completion, word sense disambiguation, and coreference resolution. In this setting, other tasks such as paraphrasing, QA, sentiment analysis, summarization, topic classification are not considered as zero-shot tasks, and only used for training, which makes an incomplete evaluation for zero-shot learning.

2- The author mentioned the benefits of encoder-decoder pretraining objective (target-only generation) to auto-regressive objective (input generation) as a key factor. However, there are no comparison with auto-regressive model with the same pretraining and zero-shot task evaluation. The only comparison is on the BigBench dataset, where Transformer-based language models are pretrained with different dataset, and also performance comparison on NLI, story completion, word sense disambiguation, and coreference resolution.

3- The paper is centered around understanding the multi-task pretraining, using explicit supervised learning of model. However, the evaluated model (T0) is using pretrained weights of T5 for training, which implicitly learned multi-tasking during unsupervised pretraining of T5 model. This makes the evaluation biased and an unfair comparison to GPT3 model."	1
"The authors ask two very interesting research questions with regards to prompting and finetuning of large LMs. A wide array of experiments answers these questions, and I think this will lead to better usage of LMs in the future. I did point out that there are some baselines that the authors missed, but I don’t think that that’s a big deal. I recommend that this paper be accepted, and feel that it will be very valuable to the NLP community. 
"	3
Overall, I vote for marginal acceptance. This is an interesting novel task (audio-visual dereverberation), and the proposed synthetic and real data are quite interesting and useful for the community. But the paper's results show that visual input provides marginal performance gain over the audio-only baseline. The audio-only baseline outperforms some benchmarks in terms of PESQ, WER for speech recognition, and EER for speaker verification on synthetic data, and WER and EER for real data, but I have concerns about the subjective listening quality of the demos, compared to other dereverberation algorithms I have listened to. Thus, I am concerned that this isn't a significant improvement over audio-only prior work, at least in terms of subjective listening quality.	4
I believe the manuscript in its current form offers a very limited contribution to the research community, and the contribution of visual information in dereverberation is minimal. 	2
This paper proposes a multi-modal learning framework that is applied to speech dereverberation. The idea is interesting and somewhat novel, but the experimental validation is not thorough enough to support the key idea of reverb-visual matching nor to provide sufficient evidences that visual stream does convey information about room acoustics.	2
Overall, I enjoyed this paper!  I think some of the reporting could be done better, but this is a minor complaint.	4
"The reviewer leans towards recommending the submission for rejection. The ultimate objective of an RL system is maximizing the returns and it is unclear from the empirical evaluation to which extent the proposed method improves the performance of the agent on unseen tasks. However, the reviewer has limited familiarity with the related work on causality and it is possible that they missed something. Addressing the outlined concerns might improve the overall score.
"	3
"It's not clear how much the interventional prediction aspect of this work actually helps (see Figure 6), even though this seems to be the main proposed contribution. It is also not clear what advantage that the proposed method has over TMCL, other than the ability to qualitatively visualize distinct cluster's in the z's. But if this does not hurt performance or generalization (Fig 4 and 5), what would be the reason that the interventional prediction is crucial?

== After Rebuttal ==
The quantitative results seem to still indicate a marginal improvement over the baselines, but perhaps what is really required is to identify environments that the community cares about that would more drastically distinguish the proposed method from those that do not use the proposed intervention prediction method. However, given the current experiments, the authors have adequately addressed my concerns and therefore I have raised my score to a 6."	3
Overall, I like the idea of the paper and believe it has significant contributions to the RL community. 	3
"I think the paper is well-motivated, presents and interesting and novel method and demonstrates that method's improved performance on the problem being targeted, and hence deserves acceptance. I think the visualisations and ablations are useful qualitative evidence for the benefit of the method, but additional visualisations, or more quantitative investigation of the clustering hypothesis would be beneficial. Also, the explanation of the method is sometimes difficult to understand. The method is only an incremental improvement on previous work, which is necessary to perform but limits it's impact and novelty.

EDIT: After seeing the revisions and additional experiments, I have raised my confidence from 3 to 4."	3
Generally, this paper proposes a simple and practical extension of the ILA algorithm. The experiments on various benchmarks are strong. However, this paper lacks the support of theory and some views might not be well supported.	2
"The proposed method is simple, and provides significant improvements. However, I have the following concerns:

- More clarity on some of the baselines and evaluation used is required (see above).

- There is possibility of overfitting on hyperparameters due to using a single dataset for evaluations. Providing additional results will help resolve this issue.

- The generality of the proposed method is not sufficiently demonstrated (only one attack, only one dataset).

Please see full comments above, and address if possible.


After rebuttal:
=====

After the rebuttal, results on 2 new datasets, as well as results on defended models, and single-step attacks are provided, comparing AugILA with other ILA variants, which partially addresses my concerns regarding these points. Additionally, some concerns regarding the use of baselines have been resolved by providing responses to my comments.

The additional results shows that the proposed method consistently and in the majority of the cases, improves over ILA and ILA++, and I am convinced that the proposed approach improves over ILA/ILA++.
However, unfortunately no comparison is provided with other SOTA methods beyond ILA-based approaches, which limits the paper's contributions.
More specifically, my comments regarding evaluation on new datasets, single-step attacks, and defended models are not fully addressed due to the fact that the provided results are only comparisons to ILA-based approaches (ILA, ILA++), and baselines such as VMI-CT-FGSM and I-FGSM + LinBP + SGM + ILA, which were among the baseline presented in Table 1 have been missed. 
Especially, because as reported in Table 1, VMI-CT-FGSM and I-FGSM + LinBP + SGM + ILA have better success rates than of the ILA baseline.
This leaves uncertainty on how the proposed method would perform on other datasets, single-step attacks, and defended models, compared to non-ILA approaches. In my opinion, this is the main drawback of the new results.

In summary, although I believe the paper has been improved after the rebuttal, due to the limitations mentioned above, I keep my score.
Nevertheless, I believe by strengthening the baseline comparisons, this work has the potential to become a valuable contribution for the community.




"	2
In summary, I find the experiments satisfying, although the additional experiment show can make the claims more convincing.	3
The paper is well-written and the proposed method does improve ILA. However, due to the insufficient experiments, especially evaluation on the defended models and target attacks and the sensitivity of the selected layers, this paper is not ready for publish. 	3
The problem investigated in this paper is. But I think the contribution of this paper is not clear and the writing of this paper should be improved. I recommend the authors to take a thorough review of the field of neural process and think about how effectively adapt the it to address the modalities missing problems.	2
It may not be so fit for iclr.	2
The article should conduct more thorough literature research, method design, and improvement of writing.	2
"The paper introduces an interesting approach to a relevant problem and reports
promising empirical results. However, the paper makes strong claims in its
comparison to previous work, some of which are not sufficiently well supported.
Further, for a fair comparison, the experiments would benefit from more
supervised and semi-supervised baselines. Therefore, I tend towards rejecting
the paper in its current form."	3
The approach is interesting; the main novelty is being able to train T, kind of like a learning early stopping. However, T also increases compute cost, which may not be a completely fair comparison to fixed-cost discrete-time approaches. Another minor concern is the lack of control for compute cost due to the training of T.	3
The paper is well-crafted and the method presented is an interesting addition to the few-shot meta-learning literature, with possibly some limitations that are not well discussed. Furthermore, the paper misses exploiting what probably is the main advantage of the method (meta-learning task-specific, or conditional, $T$'s). These two issues currently limit my score to a weak accept.	3
Overall a very strong paper with novel and significant contributions without major flaws.	4
"I recommend weak accept. I think the novel derivation of memory-efficient forward-mode differentiation is a significant contribution in the few-shot learning context. I would consider raising the score if my questions about adapting T are addressed.

=== Post rebuttal ===

The authors have addressed all my concerns. I increased the score to 8 (accept)."	3
I liked the contribution and narrative of this paper, and my opinion about the result is rather positive. 	3
"This manuscript proposes new swap schemes for parallel tempering. The methodology is supported by theoretical analysis, detailed discussion of methodological details and simulations. It will be of interest to the ICLR community.


EDIT: Having read the review of ""mbau"" and the corresponding discussion, I agree with the reviewer that the DEO* scheme may not converge to the target distribution of interest. I also agree the current argument in Section C.2 needs to be made more rigorous. Based on this point, I am inclined to revise my initial review score for the paper."	3
The proposed method does reduce expected round trip time but fails to converge. The practical method is not well understood and experiments need to be amended.	4
This is a very interesting paper. Since the treatment is rather nontrivial and I feel it deserves appreciation, I’d like to see just a little more about how different theoretical and empirical ingredients of this work mesh together.	3
Although quite technical, the paper's results are very relevant to the field of quantum machine learning, and proved in a quite general setting. I anticipate this to have a big impact on the design of variational quantum algorithms.	4
"When the paper provides some attempts to combine random matrix to VQC training, the theoretical findings are not very correlated to their empirical findings. The current version is not very ready to get published and several claims and description need to be modified. 


**Additional Comments**

1. The information on optimization settings (optimizers and initialization) is not provided in
the paper.

2. Since the quantum generative model refers to a large model family, the ‘critical points’
for each concrete one are not the same. The paper does not show what specific models are
used. Is it a quantum Bayesian network? Quantum hidden Markov model? Or anything
else?

3. Many important related papers are not cited, e.g, for the expressiveness of QNN, which has to be credited from

“The effect of data encoding on the expressive power of variational quantum machine learning models,” Physical Review A, 2021 - APS

***

### Post-Author Response 

I think most of my comments have been carefully covered by the authors. Thank you for your efforts during the rebuttal period. 

With the current version, some of the writing, reproducible discussion, and presentation have been largely improved. 

I therefore increase my score to 6; one remaining place that would still improve is the current VQA experiments is limited to a single empirical case. 

Some of the variational circuit works have been studied in a classical-quantum format. The author could consider to add some connection to enhance potential impact of the works. 

"	3
A sharp formulation and sound theoretical analysis of a well motivated and important claim, that quantum variational states which do not depend on specifics of the Hamiltonian under study, require exponentially many parameters to produce accurate ground state energies.	4
The authors proposed an OOD detection approach the relies on synthetic outlier samples. The proposed approach is shown to be effective but the overall novelty is limited. 	2
The manuscript proposes a reasonable baseline for open-set object detection.	3
"Overall, this paper could clear deliver the idea to the readers with well-written text and designed figures. The idea is somewhat novelty by the related work. The unknown-aware training object is similar to the dynamic focal loss with the virtual outlier. I agree that the motivation about sampling virtual outliers in the feature space instead of pixel space. Therefore, I vote this paper marginally above the acceptance threshold.

Given that I am not very familiar with OOD, I would like to check other reviewers' opinions.

——————————After Rebutal ——————————

The author has addressed most of my concerns. In such a situation, I would like to raise my scores."	3
Overall, this paper is technically and experimentally sound, which conduct comprehensive experiments to verify the proposed method. The reasons to accept outweigh the reasons to reject (e.g., a few unclear details).	3
Overall, I tend to vote for rejection since (i) the paper’s theoretical findings are rather trivial and (ii) the paper does not provide good insight on why the termination rule works, or on what kind of problem can it performs better. But I think the proposed termination rule follows my intuition and the paper could be improved if the authors provide more detailed analyses on that.	2
"The paper presents an interesting new termination criterion for MCTS.
The work should include comparisons to other selection rules, and misses references to some of these.
Without these comparisons, it is unclear how substantive the contributions are, and I do not recommend acceptance. 
The language of the paper must also be improved."	2
"While the paper has some interesting and well-evaluated ideas, and definitely should be publishable at some point, in its current form I feel that it is not yet ready; there are too many little (but sometimes important) errors.

---

**After response from authors:** I am satisfied with how the authors have addressed the issues and updated my score accordingly."	3
"Very interesting connection between DNN(s) and PDE(s), generalizing neural ODE(s) but 
the practical algorithms presented don't make a strong case for the method.
"	3
I think that the framework provided by the authors is not novel enough, and the authors haven't empirically verified their methodology with relevant previous work. Further, the paper in the current state is not very well written.	2
While the work is promising, it is not rigorous nor clearly written enough to justify acceptance as there are shortcomings both in terms of the expositions and regarding the substance of the work.	2
This paper is technically significant as it gave a model-agnostic PDE theory that encompasses several DNN models. I also think this paper is empirically significant as they support the theory well. On the other hand, I could not confirm that some parts of the proofs are correct (see【Correctness】 section). Since they are likely to be minor issues, I scored the correctness rating at 3. However, if these questions cannot be resolved, I may decrease the rating of correctness.	3
The paper is strong in theoretical presentation and experiments are sufficient. However, the readability of the paper is an issue and requires some modifications. 	3
In my opinion, this is an important theoretical result for cooperative MARL with good empirical support from experiments.	3
"Overall, although the theoretical analysis adapts well-known tools from the safe-learning literature, I think that the proposed algorithms represent an advancement for the cooperative multi-agent setting and the experimental results are able to highlight the advantages of the proposed approach. 
"	3
I'm leaning towards accepting the paper as the proposed methods seem novel and have nice theoretical guarantee. But I'm not very confident as I believe that the paper would merit a stronger empirical section.	3
"Nice way of dealing with the time domain by using a hypernetwork, but experiments are not so convincing that
the approach is better than the standard.  "	2
The paper is in a popular direction, proposes a reasonable improvement, but the presentation and methodology of the study could be significantly improved.	2
I think the problem is significant and the model is novel. While the paper needs some revision and polish, I would recommend borderline acceptance.	3
I think the paper is well composed, interesting, and convincing. Its methodology is well founded and is clearly communicated, and the empirical results strongly support method. I think it also opens many important directions for subsequent research. For these reasons, barring something major that I have overlooked, I recommend acceptance.	3
The concept of the torus representation is novel and its practical value is empirically convincing. However, there is no assessment in terms of theory. The connection to quantum physics is not well explained. 	4
This is an innovative method that looks very promising given the limitations of the evaluation on artificial data.  The paper is not perfect, and could improve in being more critical of the approach proposed, as well as reproducibility in terms of describing the evaluation procedure.  But the results clearly show an advantage over previous approaches on a variety of artificial datasets, which is technically impressive even if the practical impact of the method on real data is still unknown.	4
Based on my concerns, I think this paper requires some significant improvements and I rate it as rejected for now.	2
I think this work presents a simple method that shows significant improvements over previous methods in an extensive and scientifically rigorous evaluation, which makes this work a solid contribution to an important problem (anomaly detection on multivariate, tabular data). For this reason, I am leaning towards accepting this work, though I have some remaining questions and concerns (see above).	2
"Pros:
+ Good rule of hyperparameters which helps reproduce this method.
+ Complete set of experiments and ablation study for the architecture choices.
+ Good stability study of the sliding window k
+ Not sure how authors find out the effectiveness of tanh and two normalization schemes, but they seem to perform well.

Cons:
- No comparison to long-lasted baselines such as RRCF and KNN in the ODDS.
- I think the claim should be changed to ""a default rule of hyperparameter selections performs well"".

I would put the marginal acceptance for now but I will be happy to raise my score if the baselines are provided in the ODDS."	3
"This is a paper presenting an interesting application of contrastive learning for anomaly detection on tabular data, with extensive experiments showing the strong empirical performance of the method. Ablation studies and sensitivity analyses are provided, as is the code for reproducibility. The main issue with this paper is the similarity to the somewhat contemporaneous ICML 2021 paper listed above, which should be discussed and compared to. On balance the relative novelty and strength of the experiments lean me towards acceptance. 

**Post Response Update:**
The authors have included a discussion of the related ICML 2021 paper and highlighted the differences, as well as added additional comparisons to baselines that support the superiority of the proposed method. I think the paper is a clear accept now."	3
"Strength
- The experimental results are quite promising and extensive.
- The proposed method is simple and easy to reproduce.

Weakness
- The intuitions of the proposed method is limited. It is actually my biggest concern on this paper. If this can be improved during the rebuttal period, I will increase my scores. Please see my detailed comments above.
- Some sensitive analyses are somewhat weird (e.g., k = 1).
- Some additional baselines are needed to complete the experiments.

-----------------------------------

Thank you for the detailed responses.
I carefully read the responses.
Some concerns (e.g., assumptions, consecutive features, and additional baselines) are resolved. Thank you.
However, there are still some remaining questions.

1. Intuition for negative pairing.
- It is still not clear why the authors select those pairs as the negative pairs?
- In figure 1, b_i^3 and a_i^6 are highly overlapped but still negative pair. 
- It would be good if the authors can explain about this clearer with proper intuitions.

2. k=1
- If k = 1, we do contrastive learning with the feature (with dimension = 1) and the feature (with dimension = d-1). 
- If the entire feature dimensions are large, the differences would be larger.
- I am not sure whether this is the right way to do contrastive learning between these large different features.

Although other reviewers are leaning to acceptance, I will stay with my original scores (5) until those above concerns are resolved.

Thank you."	2
"Pros
*  Novel and interesting idea to use the two agents Markov game to tackle c-MDP.

Cons
* Some issues with the Markov game framework and no analysis for them.
* Confusing notations and problematic formulas.
* Experimental results are mixed."	3
Overall, I like the idea in the paper. However, I won't be able to support the acceptance. My major concern is theoretical support and clarity. Hopefully, the authors can supplement some progress during the rebuttal. 	2
"Although the paper has some interesting ideas, it needs significantly more work in strengthening the supporting claims (weak experiments as well as no code), placing the work in context to the related work, as well as writing and presentation. I think the paper in its current state will be marginally useful to the community, and as such vote for rejection. 
"	3
"Though DESTA framework is new and interesting, its merits are neither presented theoretically nor empirically. In addition, although there is no theoretical results supporting the authors' claims, reproducibility is rather low.
Hence, I vote for rejection."	2
"From the perspective of learning intrinsic rewards and preserving the optimality, instead of hand-craft intrinsic rewards, it is a worthwhile investigation direction.

However, there are some concerns about the method and experiments needed to be addressed. So I give a borderline reject currently. If all these concerns are addressed well by the authors, I am willing to increase the score."	3
"While the results look good, the text and equations are unclear. My current recommendation is to reject the paper, because the presentation quality falls short of the expected standard. However, there are interesting aspects to the proposed method, and the results look promising. If the presentation is improved significantly, I would be open to recommending a weak accept. I would strongly recommend to the authors to condense the presentation of the method in the main text and spend more time on evaluating the method. With more evaluation and clarification of how the method functions in practice, and if the further results are also good, I would consider raising my recommendation to accept.

EDIT: the resubmission addresses a significant portion of my readability concerns, I'm raising my score to a weak accept."	3
This is a solid paper that proposed a promising approach to MARL. Many details in the paper require extensive examination, care, and previous knowledge. The paper is fairly notation-heavy, but it makes a fair effort to explain concepts also in plain English. The results are promising and I believe it can provide a valid contribution to ICLR. 	4
The reviewer finds the idea novel and the method solid. However, the empirical evaluation, the discussion of related works, and the presentation of the paper can be further improved.	3
The idea is interesting, but there are a few points to be addressed.	3
I really like the idea of considering the additional level of uncertainty and I find it novel. However, I found the methodological session difficult to follow and I think that could be improved by providing an intuition on the prior on $g$ and the connected posterior. At the practical level it is not clear to me how the proposed approach could improve the performance of existing algorithms. 	3
There are some clear errors in the paper, and I am not convinced that the empirical performance justifies this methodology to be accepted at ICLR. 	3
I think UA-L2O is a novel and promising method that might lead to further research in quantifying optimizer's uncertainty.  	3
See above for detailed comments.	2
"
The paper has provided sufficient and substantial evidence of the observations made of the 3-way connections between - 
learned or hand-crafted image representations (autoencoders and perceptual models), the distribution of
natural images, and human perception.

This paper may initiate good discussions/work along the domains overlapping visual perception and ML/DL scientists .
In terms of technical know-how, although the paper takes a small step, but does not really contribute to a big
gain in analytics and new formulations/algorithms.

Although there are some unanswered questions and issues, the paper appears quite interesting.
Minor improvements would be necessary for it to get accepted."	3
I think it is more a review paper and there are not important enough contributions. 	1
A theoretical explanation of the link between natural image statistics, autoencoders and perceptual distances, with some seemingly important implications. This is not exactly my field so I may be missing relevant background.	4
I vote to reject the paper, due to the lack of clarity. To improve the clarity of the paper, significant rewriting seems to be needed.	4
"I feel the draft is not in a status for publication yet. The authors may need to polish it to have a clear story line. Also it needs to be self-contained.


########Update after the authors' rebuttal###########

I would like to thank the authors' explanations on the notations as well as the assumption concerns. Overall I feel this draft has potential to be a good one. As stated in the authors feedback, in its current status more work is needed to make it easy to read. (For example, the algorithm descriptions are still hidden in the appendix.) As such I am keeping my ratings unchanged.  "	2
"The paper proposes a well-motivated and (seemingly) theoretically justified way to regularise representation learning in VAEs.
The results also support the main claim of the paper.
Therefore I believe this paper is beneficial to the community."	3
"Given the points raised above, I cannot recommend acceptance. My main concerns are with (i) the lack of motivation for both the ""interventional consistency"" and the ""explicit causal latent block"", as well as (ii) the (sometimes significant) lack of clarity. The paper did not make a convincing case that ""interventional consistency"" is a valuable tool for representation learning.   "	2
"Though the paper presented interesting ideas, it unfortunately falls short of a systematic study to make a practically useful methodology, or of a compelling hypothesis test that shed light on underlying scientific questions.  
"	3
The proposed methodology is supported strongly by the main set of results. However, there are some missing explanations and justifications required for statements in the analysis performed that need to be clarified. 	4
I believe this paper, albeit short on technical novelty, does provide a sufficiently novel and important empirical evaluation and details analysis of Dynamic Sparse Training (DST) ensembling methods, and could be accepted - assuming the authors can address the issues I have detailed in my review. In particular this paper has demonstrated, I believe for the first time, that DST methods such as RiGL may not match dense training when standalone, but can surpass the generalization of a dense solution as an ensemble - while still being (theoretically) more efficient to train than the dense model. This result will drive further research into DST methods, as well as encourage researchers to explore the practicality of DST training for making DNN research more computationally accessible.	2
"The work address an important problem ""Making Ensemble training and inference More Efficient"" using dynamic sparse training algorithms. Results demonstrate that sparse ensembles (1) reduce training and inference cost and (2) increase the robustness (and sometimes accuracy) over the dense models. Given that sparse models provide same accuracy using less parameters, (1) seems not surprising, however (2) is quite interesting and possibly novel. Authors introduce two algorithms for finding sparse networks however both algorithms seem to do the same thing at inference (contrary to how they are explained); this part and some other points need further clarification. Overall, I believe this work has great potential, however current version lacks few important baselines and written poorly. Therefore my initial rating is weak accept. "	2
"This is an interesting paper with some interesting insights, but the paper tries to do too much in too small a space. This leads me to have too many open questions for a good rating. The paper would likely have been better if it honed in on a single aspect that they present, and did so in a clearer fashion with more bases covered.
The conclusions on page 3 are drawn much too fast, and are unclear. The authors have to convince the reader a lot harder to make the claims they make.
On the one hand the paper's main method only seemingly work more efficiently with very dedicated hardware implementations, but several important aspects of efficient hardware implementations of training, such as data movement, are seemingly ignored. 

I would be willing to increase my rating if the authors addressed my above questions, rewrote section 2 significantly with a more convincing analysis, and added a better complexity analysis of the overhead of their method in terms of data movement.

*** post-rebuttal ***

Given the author's excellent response to my questions, and the questions of the other authors, I am increasing my review to a 5. For a 6 or higher I definitely need the efficiency conundrum resolved, as for now I just don't see how to resolve the dichotomy between being either a generally applicable method on common-day and general hardware, or a method that is really aimed at hyper-efficient dedicated implementations."	3
Overall a well written paper that is easy to follow (except a few parts highlighted above). The proposed approach is somewhat novel, it combines stochastic rounding with logarithmic quantization and stochastic pruning. The main strength are that most parts are well motivated and the strong 4 bit training results. The main weaknesses are around SMP and some other parts that are unclear. Overall the contributions slightly outweigh the weaknesses of the paper.	3
The paper is interesting, tackles an important problem, and presents promising results. Therefore, I do not recommend a clear reject. However, there are unfortunately many issues I though were unclear and hence raised in my detailed review. Therefore, I cannot recommend acceptance of the paper if these issues are not addressed. Hence, for now I am rating the paper as a weak reject.	3
This paper proposed several techniques for effective 4-bit training. Some parts of the proposed method still require high-precision, but it might be a good starting point for full 4-bit training in the near future.	3
"This is a paper that clearly defines a problem in Federated Learning and proposes to extend existing tools from continual learning to this setting. The problem is well motivated, and clearly formulated. Convergence guarantees are interesting but it would be better to have the real bounds for the practical algorithms used, bounds that account for the error estimation of $f_t$. This would allow to see how practical the algorithm is in real settings. Experiments are consistent with claims and show the potential of this framework.

My score is based on above comments, but I think several of these can be addressed during the rebuttal. "	3
While the topic is interesting, the paper has limited novelty, unrealistic assumptions, and hardly interpretable results.	2
"This paper proposes a novel framework and considers the important problem. The theoretical results are obtained only for convex and strongly convex cases. Some assumptions are questionable and analysis can be generalized. Experiments are done only for deep learning models and cannot support the theory. 

I think that novelty of the paper is significant, but the paper has some issues. Overall, the paper is marginally above the acceptance threshold. "	4
"My main concern is about the novelty brought by the paper: 

* As the first main contribution, authors list a novel CFL framework. I assume that authors refer to Eq. CFL as t
he ""CFL framework"". However, it seems that (CFL) is just a special case of the general FL objective (1). 
Maybe authors mean by CFL framework the data access model, i.e., online gathering of Local datasets over time? 
If (CFL) framework does not target an online data access model, I don't see the fundamental difference 
between (CFL) and total variation type models that have already been proposed, e.g., in 

A. Jung, ""Networked Exponential Families for Big Data Over Networks,"" in IEEE Access, vol. 8, pp. 202897-202909, 2020, doi: 10.1109/ACCESS.2020.3033817.

SarcheshmehPour, Y., Tian, Y., Zhang, L., and Jung, A., “Networked Federated Multi-Task Learning”, <i>arXiv e-prints</i>, 2021. https://arxiv.org/abs/2105.12769


D. Hallac, J. Leskovec, and S. Boyd, Network Lasso: Clustering and Optimization in Large Graphs, Proceedings SIGKDD, pages 387-396, 2015.


These papers consider local models assigned to nodes of a graph or networks. These nodes could represent different time periods of the time-evolving datasets.

* As the second main contribution authors list a convergence analysis of a gradient method for (CFL). However, it is not clear how the resulting convergence rates are better than what can be obtained from existing analysis techniques for (online) SGD of strongly convex functions, see e.g., 

[1]Rakhlin, A., Shamir, O., and Sridharan, K., “Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization”, <i>arXiv e-prints</i>, 2011. https://arxiv.org/abs/1109.5647

The convergence rates depicted in Table I need more discussion. It is not clear to me how Table I shows that the proposed convergence rates are substantially 
better than rates of known FL methods. 


* As the third main contribution authors mention approximation techniques. However, it is unclear what precise approximation 
techniques are proposed for solving (CFL). The relevant Section 3.2 only introduces a measure (information loss) for the approximation quality but 
does not discuss how precisely these approximations will be constructed for (CFL). 

* The main theoretical results of the paper are bounds on the number of iterations/rounds required to achieve prescribed accuracy. These bounds involve several parameters used by Assumption 1 - 4. However, i do not see how the current numerical experiments verify these bounds on the required rounds for achieving prescribed accuracy. 

The clarity of presentation and quality of language needs improvement: 

* what is the output of Algorithm 1 ? Is Algorithm 1 meant to be run in an online fashion ? Otherwise the time horizon T should be listed as input parameter. How precisely is S_{t} constructed? The summation in step 6 seems to have flipped a \tau for a t in the subscript of \omega 

* ""...St is a subset of clients sampled from all clients set..."" How precisely is this set sampled ? 

* what is a ""cross-device setup""?

* ""We give the formal definition of CFL framework in Algorithm 1.""  pls avoid the term ""framework"" when you actually mean a specific algorithm (your new algorithm 1) 

* ""...the previous local object functions cannot be properly approximated."" ps provide more justification for this ""non-approximhatability"" 

* Definition 3.1 talks about approximating loss functions but Eq. (3) is about gradients 

* ""To analyze Equation (2) and Algorithm 1 in-depth, we propose the Gradient Noise Model..."" It seems a bit unusual to use a gradient noise model for analysing an  algorithm. The gradient noise model might be prescribed by the application setup (data access model) 

* should the LHS of (4) and (5) involve the gradient of local loss functions instead of the global loss function ? 

* ""...and in some conditions, μ-convex."" 

* Assumption 2 and 3 involve conditional expectations given parameter vector \omega. Pls provide more details about the proability distribution of the random variable \omega. 

* There should be more separation between the cells of the last row in Table I. 

* ""We prove this comment..."" I would rather say to prove a theorem/lemma or claim. 

* ""..can be bounded by an arbitrary non-negative value..."" So we can choose this value arbitrarily small ? 

* ""... we analyze the theoretical performance of Algorithm 1 ..."" 

* ""..., despite that the term induced by information loss concurrently trades off the optimization....""  Pls refer to the term using \eqref{}

* what is a ""...regular linear convergence term...""?

* ""...the model instead reaches the neighborhood of..."" Do you mean ""iterates"" by ""model""?

* what is a ""...smoother optimization phase.""?

* ""We consider to federated learn ...""

* ""...this (most) challenging time-varying scenario mimics the realistic client data sampling scheme (from some underlying distributions). "" in what precise sense is this the most challenging scenario ? 

* ""Note that the trade-off between Hessian estimation and computational overhead constrains the practical feasible of such approach."" Unclear 

* ""... as a method to approximate diagonal Hessian matrix, is slightly preferable than Fisher Information Matrix, though the latter one involves less computation.""

* what is ""...challenging non-overlapping time-varying heterogeneous data"" ? 

* ""...we are the first to provide such theoretical guarantees ..."" what precisely do you mean by ""such guarantees""?"	2
The problem seems quite similar to the standard SVMs and a new distributed solver for the problem looks not significantly new.	2
Overall, the methodology seems to have limited novelty and limited potential for improvement over baselines. Important details seems to be missing in the presentation. Simulations seem to be weak.	2
Overall, I believe the authors address a generic and important problem. The proposed methods are novel and are supported with acceptable theoretical and empirical evidence. I believe the issues I raised above are only minor, but important to address, if possible.  	3
"Generally, this is a solid work for accelerating ODM. The authors not only propose a novel partition strategy leading to excellent performance, but also detail the intuition behind, i.e., utilizing the stratified sampling to preserve data distribution. The theoretical analyses are broken down into many small steps and easy to follow.
The experiments are conducted on data sets obtained from public repositories and necessary links to the readers are given. Considering all these, I vote for acceptance.
"	3
In summary, I think this is a borderline paper. It has some good properties, but it is limited by using batch normalization. It also introduces additional costs at the training time.	3
The training efficiency remains unclear. Although the stochastic pruning scheme can explore the channel space more sufficiently, it is also possible that more training time are needed for such exploration. More details should be provided here.	3
There is a severe idea overlap between this work and previous work. Meanwhile, the empirical performance is not strong.	2
The paper has room for improvements, but it is a nice new attempt to the pruning of networks.	3
The authors introduce an interesting, new framework for IRL that can learn a reward function in an offline manner. Furthermore, they draw insights from anomaly detection problem and improve upon the LogReg-IRL. They demonstrate, empirically, that their method can learn a better reward function and achieve higher performance on two OpenAI Gym tasks. However, the experimental section for this work is very sparse; if the authors can address the concerns listed above, in particular adding BC as a baseline, I would be willing to increase my rating.	3
Lacking comparisons to relevant prior methods and evaluations on diverse tasks	3
Because of the concerns discussed in the Main Review tab, I am leaning towards rejecting the paper for now. 	3
"## Overall thoughts
The paper is well-written, and I had no difficulty understanding it for the most part.  I'm a bit concerned by the experimental methodology and (more importantly) the motivation for using Gamma distribution.  If the authors can better address these issues, I am willing to adjust my review accordingly."	2
"This paper describes a novel diffusion model based on Gamma noise instead of Gaussian noise.  The approach is shown to produce good empirical results compared to the Gaussian case.  But there are questions about the quality of the writing, the motivation for the use of Gamma diffusion, and the formulation, all of which the authors may want to address.  It would also be very useful to see more extensive experimental work on larger, more diverse data sets.

Post-Author Rebuttal:  While most of my concerns have been addressed by the authors, I remain somewhat concerned that the difference between Gamma and Gaussian noise processes ceases to be substantial after several steps of the additive diffusion process.  The authors have agreed that the most significant difference will be at the beginning of the forward process (ie the end of the reverse, generative process), which makes sense.  In the end, I am not sure how much impact this work will have in the longer term -- will people begin choosing to use Gamma noise instead of Gaussian noise in practice?  While I can comfortably raise my score to 6 from 5, I still believe this is a borderline paper.

"	3
A sound idea with some strong experimental results. However, the presentation and the discussion can be improved.	3
Even though the idea is interesting, this paper should be further improved from the mathematical formulations and the experiments. If so, I tend to increase my score.	3
After carefully reading the paper, I believe while this paper made novel contributions in reducing the precision of 32-bit component of BNNs, it couldn't show its advantages over existing works such as ReActNet. More specifically, ReActNet-A achieves a better accuracy and also requires a lower number of OPs, making the advantage of BoolNet limited to the implementation results. However, the implementation results on a custom hardware designed for a specific network doesn't establish a fair comparison. My suggestion is to report hardware performance of the networks on a commercial accelerator instead for a fair comparison and more reliable results.	3
"As this reviewer mentioned, a more concrete analysis for the motivation should be added and comparisons with previous BNN architectures are required. The current manuscript seems to be lack novelty in this reviewer’s opinion. But, this reviewer is ready to listen to other reviewers’ opinions. 
"	2
This paper is the first work to build fully 1bit neural networks. I like this work and recommend acceptance for it.	4
As this reviewer mentioned, the motivation to use only binary network is good research direction, However, the novelty of this paper is not obvious and the BoolNet needs to be evaluated on other benchmarks. I recommend this paper to be rejected. 	2
The take-home message of the main theoretical result is very interesting, and could open a new way to think about many other optimization problems with explicit orthogonality contrainsts. However, I have only skimmed the proofs and have not carefully checked them.	4
Due to the aforementioned reasons, I think that the paper is not yet ready for acceptance. However, I feel that the paper has a lot of potential and I hope that the authors keep working on the issues which have been raised in this report.	2
The work is well-presented, and tackles a challenge of unknown true subspace dimension via overparametrization and implicit bias of optimization method. The work combines ideas from previous work on DPCP and implicit bias for low-rank matrix recovery. The overall result is novel, while the approaches seem to be a combination of previous methods.	2
Overall, I think the paper's idea is nice and looks sound; however, I am not an expert in this area. 	3
"Overall, I think the contribution would be important to the RL community. My main concerns with the paper is that the presented approach claims to be helpful for exploration but the empirical results are not demonstrating this: One specific way to demonstrate this would be to show and emphasize that HyperDQN is able to achieve better returns on Atari games that are classified to be hard-exploration games. This would be a more powerful argument for HyperDQN. 
"	2
"This paper is a well written one with extensive experimental results. 

The novelty of this method is rather limited as the method is exactly the same as the one proposed in a prior work. 

The empirical evaluation results appear to be less convincing to me because the results on Atari 2600 are only shown for intermediate model trained after 20M frames and the full training scores (inferred from the learning curves in Fig 12) appear to be much inferior than another variant of randomized exploration model which is the noisy net. 
"	2
Overall, the paper introduces a novel approach for randomized exploration in deep RL, is well written, and shows strong promise in terms of performance compared to other state-of-the-art methods. Additionally, it includes valuable theoretical and experimental discussions that provide further support for their contributions. I thus vote for an accept.	3
The paper proposes a computationally lighter method *HperDQN* for exploration in deep reinforcement learning and shows its superior performance in experiments. However, the formulation of *HyperDQN* is conceptually not novel and its performance compared to other methods is also questionable.	2
Overall, I find the paper's attempt to shed light on the theory of MAML interesting. I have two main concerns on the test set/validation set separation and comparison with another result in the literature. I am open to raising my score based on the authors' responses.	3
The submission formalizes the MAML inner-learning rate theoretically, and it can benefit meta-learning community to some extent. However, I'm not fully convinced if the finding is practically important, or something new (in terms of intuition) as well.	3
I don't think those results well pass the bar of ICLR conference. According to the above, I would suggest a weak reject (barely below the bar). PS: I am not sure if result one (find the better alpha) could be applied in practical tasks. My confidence is mostly on the theoretical part of the paper	2
The theoretical contributions of this work are somewhat interesting but are also of dubious practical relevance.	3
Overall this work is well-motivated and appears to provide new insight about the inner loop adaptation of MAML and the significance of the meta-initialization learned by MAML, so I would consider recommending acceptance. However, as stated, I am not familiar with related theoretical works so have selected my confidence score in accordance with that. 	3
Writing is too hard to follow.	2
Overall, I like the goal and the ideas behind the paper. Unfortunately, the paper is extremely unclear in its presentation: I could not put individual sections into context, I do not know what the goal of the paper is, and I don't understand the results. Therefore, I cannot recommend this paper for acceptance.	2
I believe designing novel architectures inspired by the human brain is an interesting and important research direction. However, in its current state, this paper was not able to convince me that the GTNN architecture was one worth pursuing. To be honest, it wasn't clear to me what were the actual contributions of the paper, and how the experiment results are backing up the claims made in this paper. For those reasons, I recommend rejecting this paper for ICLR.	2
"The results are interesting; however, it is not clear to me how one can extend the results beyond the Gaussian mixture model considered in the paper. Furthermore, it is not clear how to get efficiently computable bounds from the statements of Theorems 1-3. The bounds are needed for model complexity penalization. Also, the following questions are needed to be addressed: 

- Theoretical bounds are based on the KL divergence between the pseudo-labeled and true distributions, which is hard to measure/estimate unless the additional information is specified. Any computable upper bound?
- The loss function used for the generalization error computation is designed in such a way that even for a high misclassification rate on later stages the loss function would converge. (See Eq.~ 5 for the details). Could you advocate using this bound?
- No lower bound is provided. Any bound known from the literature?
- Figure 1: the upper bound on the generalization error, which should be in between 0 and 1, is about 14 at convergence. Why is it so? A small number of objects? A similar concern regarding Figure 6.
- When one is talking about the generalization error, one derives the bound for a class of functions rather than for a single function. However, in the paper"	2
The analysis of the paper seems interesting and correct. It is novel to look at the generalization error at each iterative update. However, I do have concerns about the significance of the work. It is not clear to me and it was not made clear by the authors how we can make use of this analysis and conclusions to train better models for example.	2
The paper presents new generalization bounds for semi-supervised learning under iterative pseudo-labeling. But the results are distant from settings of practical interest.	2
This paper provides a novel information-theoretic generalization bound for semi-supervised learning, which involves both disintegrated mutual information (characterizing the input output sensitivity) and the disintegrated KL-divergence (distribution mismatch for pseudo-label) conditioned on the previous outputs. To my knowledge, the bGMM results are quite novel and interesting. 	3
marginally above the acceptance threshold, given that the weakness shared above. Overall it was well motivated problem, and the authors proposed approaches and show compelling results. As pointed out in the above for its weakness, the paper would be strong by providing these insights and intuitions. 	3
Overall, I found the paper is a heuristic work on personalized federated learning with little justification and little motivation. The scope of its application is unclear. The empirical results do not seem to be sufficient to support the claimed contributions. 	1
The authors do provide a fine solution to solving a real problem of generalizing knowledge across clients by kernel factorization. Extensive experiments are provided as well. However, if we consider each part of the techniques, like model parameter kernel factorization, grouping users based on similarity, I may expect more novel contributions.	3
Overall, I think the paper studies an interesting problem. However, there some issues in the paper that need to be addressed.	3
"This paper studies the novel and interesting scenarios in federated learning, i.e. label heterogeneity and domain heterogeneity, and the experimental results show that the proposed method outperforms other federated learning methods on single- and multi-domain datasets. The main logical structure of the paper is relatively clear. However, there are also some shortcomings that need to be improved, e.g. the proposed assumption is not well justified and the experimental setting should be detailed. Overall, this manuscript is not mature enough.
"	2
"I strongly recommend accepting this paper. The paper provides a tight construction that shows the optimal number of parameters in a ReLU network required to memorize arbitrary $N$ data points (comment 1). It also carefully discusses bit complexity and shows that the construction is also optimal in terms of the number of bits required (comment 2). For the rest of the comments (3-8), I raise some minor criticisms/questions/suggestions, but I believe the authors will clarify my concerns.
"	4
Overall, I found the paper to be very well written and an excellent contribution! I recommend that it be accepted.	3
The provided results are significant and correct, and the proof technique seems to be useful for other domains of theoretical analyses on neural networks.	4
The authors try to pack lots of information into this paper, but the key details to support the usefulness of the proposed method are not fully explained. Without such details, it's hard for readers to appreciate its intellectual merits, and it's equally hard for readers to implement it in real applications. Therefore, this paper in its current form, doesn't seem to meet the bar of ICLR, and requires major improvements.	2
Overall I am unconvinced by the technical novelty of the work and its implications. I do not necessarily buy the argument that existing feature selection techniques fail on RGB images. Additionally, I do not see the point of the experiments where a subset of the data is selected to train the classifier without sacrificing the accuracy -- each training technically requires the full dataset, even though the classifier gets to see only a subset, and training a new classifier would require the entire dataset to select a different sample.	2
This paper present an interesting method and promising results for subsample selection used in deep learning, while lacking theoretical justification for the proposed method.  	2
The paper writing is clear, and the proposed method is clearly advantageous over other baselines in terms of generalization and efficiency. Extensive experiments have been conducted to support the claim. I would vote for an acceptance of the paper.	3
The paper provides an interesting and well-motivated modification of DDPMs for conditional generation, but needs stronger baselines to help assess the performance of the proposed modification.	3
Please see the weakness and questions section.	3
The paper achieves good empirical performance but needs more polishing around the motivation and some experimental details. I will be happy to increase my rating if my queries are clarified.	2
This work has some questions, which should be addressed to reach the acceptance threshold.	2
The paper resolves two trade-offs with clarifying the relationship among them, and does not explain the meaningfulness of a dynamic ε	3
The paper provides some interesting experimental results on using dynamic $\epsilon$ in learned index structures, but the theoretical justification does not seem to explain the improved empirical performance adequately.	2
This paper is well-written and studies on the theoretical analysis of the database field, which focuses on an important and popular problem. 	2
"The paper lacks a motivation and justification for the proposed model design. The theoretical claim might not be correct. The experimental results are not convincing enough and larger scaled experiments on OGB are needed.

---
# After rebuttal: 
I would thank the authors' effort in addressing my concerns. However the concerns like the technical contribution and experimental improvements are not fully resolved, but I do appreciate the authors' effort in making the code accessible which improves the reproducibility in the community. I suggest the authors double check the code provided and make sure it is consistent as well. "	2
"Given the above content, I think this paper is an okay submission, and I will give boardline rejection. I’m willing to increase the score if the authors (1) intuitively and empirically motivate the layer-by-layer variable dependency in the decoder, (2) analyze the effectiveness of VI and how your method prevents mode collapse, (3) conduct comparative experiments with suggested baselines, and (4) analyze how your model is sensitive to the community number.
 
"	2
"Overall, although the paper proposes some interesting ideas by explicitly modeling community structure, latent positions, and random node factors, the paper lacks a discussion and comparison with state-of-the-art non-VAE-based baselines and uses relatively small datasets in the experiments, thus is less convincing in performance. Considering the abstract states ""The experimental results on real-world graphs demonstrate that our proposed model achieves the state-of-the-art performances on link prediction and community detection tasks"", I cannot recommend an accept given that stronger baselines clearly exist."	2
Overall, the methodology of this work is solid, which is a reasonable combination of several solid probabilistic models. The learning method is easy to implement. Although the novelty of the whole method is not very strong, the experimental results show its feasibility to some degree.  	2
"This paper provides an interesting attempt for accelerating the computation of nonlinear eigenvalue problems relevant to the Schr\""odinger equation. However, the contribution of this paper is incremental and may not be sufficiently novel for ICLR. Therefore I am inclined to rejecting the submission.  "	3
I think both the theoretical and empirical contribution of the paper is limited. I consider this paper to be marginally below the acceptance threshold.	2
See above.	2
In summary, I recommend its publication in ICLR after my comments are addressed.	4
This paper has good motivation, a good solution for the problem, and extensive and supportive experiments. The major concerns are low efficiency, convergence difficulty, and non-significant contributions.	2
I consider the weakness to outweigh the strength. This work does provide an interesting idea, which is selecting auxiliary loss functions automatically from a huge search space. The empirical results suggest that the average performance of applying the chosen auxiliary task is higher than baselines. However, the risk of using this method seems high---at the first step of pruning the search space, there remains a high chance that good candidates are never tested, thus the pruning result can be based on the evaluation of bad combinations of operators and inputs. Moreover, the experimental result also suggests that this risk exists.	3
The paper ignores at least 2 simple but important prior works, and do not outperform the results in those baselines. The contribution is greatly undermined by the fact that simple RL with image augmentation can outperform the complicated bilevel optimization and auxiliary loss in AARL. Some experiment plots may contain factual errors. 	2
The authors combine different AutoML techniques to automatically derive the best auxiliary loss function for RL. Although AARL is an interesting approach, the claim in the paper are not supported by empirical findings. 	2
I think the paper is very interesting, intuitive, and a natural extension of the Flooding idea. The empirical section can be improved though. 	3
This paper presents an interesting loss function that provides more robustness and also performs slightly better in regular settings. I found the paper interesting and potentially have a strong impact. I think the experimental design and theoretical sections could be strengthened more and highlight more benefits dealing with noisy labels and dataset biases. My overall rating is “weak accept”.	4
I think the proposed iFlood is simple but effective. The theoretical analysis is straightforward. However, there are still some problems, and the experimental results are not good. So, I recommend to ''marginally below the acceptance threshold''.	4
I think the idea of iFlood is neat and simple. Theoretical analysis is good enough. However, the effectiveness of iFlood can not be guaranteed in experiments. So I think this paper is marginally below the acceptance threshold.	3
Overall, the motivation for building such equivalency is not very clearly presented. The proposed convolution operations seem to be ``general'' for undirected graphs but not specifically related to the hypergraphs (except for the involved Laplacian is derived from the hypergraph). Also, it would be better if the authors could include existing GNN methods for undirected graphs for comparison to demonstrate the superiority of their proposed framework. 	2
Overall I am very positive about this paper.  AS mentioned above, I find the techniques and the evaluation to be solid with very little in terms of weakness.	3
Though I am not familiar with the hypergraph field, this paper seems to provide some useful theory and techniques for hypergraph learning. I resort to other reviewers to evaluate the novelty and significance of the theory.	3
Overall the paper is clear and of good quality with claims being well supported by theoretical analysis. However, the novelty is incremental and experimental results are marginally significant. Positioning with important prior work and evaluation with missing baselines would significantly improve the paper.	2
The paper introduces some new nice idea, but it seems overstating some of the contributions. The experimental analysis is extensive but on very small data set of low-resolution, while actual problems encountered to scale this method up to more 'real-case' resolutions are not discussed appropriately.  	3
"This is a very interesting study, the method and theoretical analysis seem sold, but the motivation and comparative experiments should be improved.

"	3
The way of representing and analyzing the results is a bit overclaimed without serious supporting experiments. However, their method demonstrated state-of-the-art performances compared with recent approaches. 	2
The novelty of the paper is incremental and further explanation and experiment are required to demonstrate its effectiveness.	2
The two main parts that are unclear to me are the need for *relative* metrics and what the metrics are actually capturing (supposedly similarity between human- and machine-generated texts). The experiment and analysis do not provide enough information to support the claim that the proposed metrics are objective/universal and have guiding ability.	2
"Overall, I admit that OMUG is a sound method to identify some aspects of texts by humans and models. But I have some questions and concerns that I want to listen to the author’s responses.

"	2
Overall, the current state of the paper needs to be improved to match the authors’ claim on the universal/objective applicability of the proposed metric. I see no strengths from the paper empirically although I particularly favor the simplicity of the proposed method. The major weakness of the paper is the lack of sufficient documentation of the practicality of the proposed metric via rigorous and extensive experiments involving various text generation tasks and more datasets. Thus, I am rejecting the paper based on these grounds and will wait for clarifications from the authors.	1
I read the second half of this paper quickly since IMHO there are major flaws with the work. The paper makes many claims about how widely applicable the proposed metric is, and lacks evidence for nearly all of them. 	1
The evaluation is flawed as the proposed natural language evaluation metric is not compared to human judgements. Also, Chinese poetry generation is not a suitable task to evaluate on given its subjectivity.	1
The problem is not very motivated. 	1
"This paper proposes a new RL-based framework together with a new GNN architecture for improving network resilience. The new frameworks can generalize to unseen graphs. However, the empirical performance of the framework is not convincing on the tasks considered in this paper. 


=============================after response=============================

The updated results in the revision resolve some of my main concerns. I decide to raise my recommendation score from 5 to 6."	3
The novelty of the proposed method is unclear and the experiments need major improvement.	2
"Overall, the paper tries to address an interesting problem of tackling the output discrepancy issues when distilling unconditional GAN models. The experiment part provides several ablation studies. However, I have several main concerns regarding to: 1). more thorough comparison with the same architecture network trained from scratched and more discussions; 2). try the proposed method on other types of images to validate whether inheriting style modules and latent-direction-based distilling loss still help or not. 


==============After Rebuttal===========================

Thanks the authors for the clarification. I agree with other authors about the limitations of the proposed method, especially about how to extend to other unconditional GANs and other datasets. Thus, I will still give a borderline rating. "	3
"Given C and D comments, I can not accept the paper “as is”. Critical points are missing and conclusions can not be drawn without them. I hope that the authors will be able to address my comments. I thank them for their work and highly encourage them to continue this line of work and re-submit. Best.
"	2
"Overall, although the topic covered in this paper is relevant to the ICLR community, the reviewer does not find the paper ready for acceptance. In particular, the contributions seem rather narrow and the empirical validation is not compelling. For details, see the main review.

**Post-rebuttal update:** The reviewer thanks the authors for the feedback which addressed some of their concerns. However, some the reviewer's concerns wrt the stated contributions remain. More precisely, claiming the initialization strategy (already adopted in pruning methods) as contribution appears a bit stretched, and the compression approach being tailored specifically to StyleGANv2 makes the contribution rather narrow. There might be value in the analysis and lessons learnt wrt compressing StyleGANv2, but considering the limited results and their unknown generalizability to other models and more challenging datasets, the reviewer considers that the paper is not ready for acceptance."	2
"Please see the content in the main review. I tend to recommend ICLR 2022 to reject this paper. If my concerns can be well solved, I will increase my score.


--------------------------- After Rebuttal --------------------------- 

Thanks for your detailed feedback.

I totally agree with the authors that the discrepancy problem is different between classification and unconditional generation. The discussion in the introduction and results in Figure 2 and Table 1 can well demonstrate their claims.

The results in Q4 should be added in the paper.

However, as other reviewers said, the contribution is limited. The proposed compression method is only verified in one model, StyleGANv2. Thus, I encourage the authors to explore more modules to improve the influence."	2
"I think it is a good candidate that could bring contributions to our community. Although the topic is a bit narrow to unconditional GAN, the experiments are thorough. Most of their claimed are supported by their experiments.

--------------------------- After Rebuttal ---------------------------


Thank you for the feedback. It is good to see more experiments such as the new results on LSUN church dataset. The authors have addressed my questions. However, I agree with other reviewers that the contribution is a bit narrow (or limited). I think this paper is really borderline, and I have no objection for acceptance/rejection.
"	3
The paper is in general well-written, but the reviewer still has several critical concerns over the justification of the claimed property due to lack of enough details. Authors are encouraged to provide more discussion in the main context to justify how the proposed components lead to the improvement compared to the standard approaches. More comparative results to other baseline algorithms are necessary to demonstrate the claimed significance of the proposed algorithm.	3
The paper asks a very important question: establishing a.s. guarantees instead of w.h.p. ones for safety. The paper is well-written. However, the safety definitions introduced in this paper do not indicate the stability of the system in any sense, thus I found these safety requirements relatively weak. Further, though the authors compare the results with the regret bounds, Theorem 3's result does not directly reflect the regret, so I hope to see more comments on the connections between Theorem 3 and regret. Lastly, the paper establishes a.s. guarantees only in the asymptotic sense, instead of for finite samples. Since the least square estimation used in the current literature also converges a.s. asymptotically, I am not sure how much progress this paper provides compared with the literature. I think it would be much more interesting to establish a.s. guarantees in non-asymptotic cases (some random example that comes to me right now is  Regret <o(T) a.s. for any finite T, instead of w.h.p., but any other a.s. guarantees would be great, especially in constraint satisfaction, which is also an important requirement for safety). 	3
My overall impression of the paper is positive. While the switching between policies seems a bit ad-hoc, the performance and convergence guarantees are solid -- correctly formulated and true. The paper addresses a real problem of current interest and provides a solid theoretical foundation for the proposed method. The numerical simulations are non-trivial and convincing. I have some minor concerns, but they can likely be addressed in the author response.	3
"Update: After the edits the authors did, together with the edits they promise to do, the paper can be accepted.
_________________________________________________________
Original Summary: Regretfully, the manuscript cannot be accepted due to different issues. First, it aims to re-establish existing results. Moreover, there are frequent incorrect and/or incomplete claims in the manuscript. Unfortunately, lack of novelty, lack of connections to existing work, incomplete literature review, unsupported and/or wrong statements, and subpar presentation, prevent the manuscript from being accepted.
"	3
This paper has promising experimental results, but is not ready for publication at this stage. It could become a stronger paper after major revision by motivating and explaning their approach more carefully.	2
"An interesting idea with a fairly thorough discussion. However, some issues about clarity in relation to the technical parts of the paper.

Experimental results illustrate some promise, but it is very unclear if the reported performance is practicable as they are based on oracle tuning of hyper-parameters."	3
The authors propose a potentially interesting density measure for density-based clustering methods. Yet, the proposed measure is not sufficiently discussed in the scope of similar works for spectral clustering and the experimental evaluation is using class-label information for hyperparameter tuning, which is not eligible for clustering analyses. Hence, I reject the paper as is and suggest that the authors extend the experimental evaluation and provide a comparison to the related spectral clustering approaches for the next submission.	2
The proposed idea is interesting and suggests an alternative way for specifying density functions for density-based clustering. However, there exist several unresolved issues regarding theoretical justification and, mainly, experimental validation.	3
This paper is a valuable pioneering work on unsupervised 3D scene segmentation, with a full variational training pipeline built up for irregular 3D point cloud data. 	4
Overall, I think the problem is interesting, and the proposed framework is technical sound, but the results presented on the simplistic datasets cannot sufficiently verify the effectiveness of the method.	2
The idea of unsupervised point cloud segmentation is cool since point clouds are hard to annotate, and they are easy to capture or collect. However, the dataset and evaluations are not quite reasonable to me, as described above. Thus I give a borderline reject for now. 	3
To the best of my knowledge, the paper is the first to tackle 3D object-centric scene decomposition (unsupervised object discovery). It is well written and reasonably extends SPAIR to 3D scenes with new challenges only in 3D cases. However, the baselines are not carefully chosen, and the scalability of the proposed SPAIR3D is questionable. Overall, I think the paper is slightly above the acceptance threshold.	3
The idea to decompose a scene in an unsupervised way is novel. However, the proposed method is mainly based on SPAIR and the only technical contribution seems to be Chamfer Mixture loss. It is unclear what is the advantage of this loss design over traditional chamfer distance and how it can help scene decomposition. Moreover, I did not see further analysis on different proposed components of the method and inspiring conclusion for the community. 	2
"The paper proposes a new method with careful theoretical justification. Experimental results support the effectiveness. The paper is well-organized. The paper is beneficial to the readers.
"	4
Overall I think this is an OK paper. The VT representation of an edge map seems novel to me. I like the simple idea which works pretty well on several datasets. The implementation is straightforward without bells and whistles. However, I still have some concerns about the details including the support image, DT performance, etc.	3
The paper takes a first-principles approach to thin boundary estimation. The novelty outweighs the concerns mentioned. The paper is worth known to the community.	3
"This is definitely interesting work and I think efficiently modeling hyper-edges is important. However, currently, the paper is very dense to read and needs to be improved. I would also like to see an experiment on inferring relations that exists between more than a pair of entities. 

=====Update 11/26=======

Thank you for the rebuttal. I am keeping my score to weak accept. "	3
The paper is strong overall in terms of novelty of the methods and good main experimental results.  However, I would really like the authors to improve on the presentation of this paper.  In general, I lean towards accepting this paper, but in order for me to improve the score I need to see more details on their new methods are efficient in terms of time and memory and how the synthetic dataset is generated.	3
In summary, SpaLoc is an incremental variant of the NLM model. The contribution is minor and limited to the specific NLM model. Therefore, I recommend rejection.	2
"It’s an incremental work, focused on scaling an existing method by converting a dense representation into a sparse representation. 

Even though the theory is for generic arity, the experiments involve only binary relations, and hence many complex GNN based methods can be used in addition to R-GCN. For example, Recurrent Relational Networks (RRN)  (Palm etal 2018) that work very well for multi-hop reasoning and scale well across the size of the graph.

**Running time:**  For the sparse (I,V) representation, how are the three basic operations (Reduce, expand, permute) performed using fast tensor operations on GPU? In my understanding, one downside of sparsifying the tensors is: indexing now needs to be handled separately, and can’t be done using fast GPU tensor operations. All the three basic operations of NLM may require dictionary lookups done on CPU, instead of GPU. Hence, even though sparse tensors for very high arity may now fit in the GPU memory, performing basic NLM operations may require frequent interaction with CPU, and hence dramatically increasing the training / inference time.  Can you provide a comparison of the running times of different baselines with that of SpaLoc?

Experimental details are missing. For example, what is the maximum arity used in the intermediate layers for KG experiments? How many layers for both tasks?
Many baselines have not been cited or mentioned, except the acronym in the table (e.g. TACT in table 2). 

**Clarification Questions:**

1. *Subgraph sampling:* The description for both sampling techniques (neighbor expansion and path sampler) focuses on 2 arity edges.
Specifically, in neighbor expansion, it appears that node set is expanded based on only binary relations? Are relations with higher arity used when expanding the node set? In path sampler, do you consider paths via edges of higher arity?

2. On page 6, under **Subgraph Sampling**:  ""*During inference, usually our goal is to just infer the relationship between one pair of entities:*"" In its original form, NLM would predict the relationship between all pairs of nodes as it works with dense representations. In SpaLoc, don’t we get all the non-zero entries, in the tensor of the desired arity, all at once in the forward pass?

3. *Section 2.3:*  How do you select hyperparameter \epsilon = 5%?

4. *Ablation study:* there is no ablation on ‘sparsification loss’. How important is it?


**Typos**

1. Page 2, Line 6: whether a predicate is true **for** some tuple of entities.

2. Section 2, 2nd line: why is V a  tuple?

3. Figure 2, Last row in the output of Permute and NN should be (3,4) and not (4,3)

4. References to Figures say Fig1b, Fig2b, but there are no parts a and b of the figures. For example, in the 1st line of section 2.2.

5. Section 2.2, page 4, in the 3rd line of the paragraph describing ‘EXPAND’, what is gF?

6. Section 2.2, page 4, equation in the description of ‘REDUCE’: if output of f is a vector, then shouldn’t it be a max pool (max across each dimension), instead of just max?

7. Page 4, last word: superscript should be (i,r) instead of (i,k)

8. Page 6, Information Sufficiency: You have defined S = (V_S, E_S), but you refer to S as H_S subsequently. Also, in the hyper-edge path, the subscript on the last node of the 2nd edge should be r_2 and not r_1.

9. Section 3.4: Ablation Study: it should be mentioned that all ablations are on synthetic family tree reasoning tasks.

"	2
"I recommend rejecting the paper: It is currently not apparent why one would want to use many entities during training, when it’s possible to train with few entities and extrapolate to many at test time. Furthermore, the good results reported in section 3.3 seem irrelevant, since the experiment actually performs a different task from ""relational rule induction"".

The paper contributes to a specific line of research, by extending the NLM [Dong et al., 2019] to larger input sizes. In general, the paper would significantly benefit from more justifications and explanations for the proposed methods. More details on the experiments are also essential for the reader to understand the results. The technical contribution consists of a combination of different existing methods or variations thereof, and the information sufficiency value which appears to be new. 

---

I thank the authors for the rebuttal and the additional ablation study and details on the hyperparameter tau. I keep my score at 3, because in my opinion the need of training with larger input graphs is insufficiently covered. In particular, the datasets reported in Table 2 are based on *sub-sampled* graphs, as described in Appendix F of [Teru et al., 2020]. This means that it is possible to train NLM on the KG datasets considered in this work, by simply applying the same sub-sampling strategy with fewer roots. The new results in Table 3 also do not support this point, since it looks like that training with the complete KG (instead of subgraphs v1, v2..) does not improve the performance."	3
The paper contains some intriguing ideas, but I cannot recommend acceptance due to the lack of clarity and justifications.	2
The proposed method has the advantage of being intuitive and simple, but it is suspicious whether the experiments to support its effectiveness are reliable. Therefore, my initial rating is 5, and I expect the authors to address my concerns through a rebuttal.	3
"Overall, I vote for accepting, even with some concerns unsolved.

The pipeline of Pixab-CAM is simple and easy to adopt. This paper also provide comprehensive analysis of its motivation in section 3.2. The adversarial patch is novel and contributes to this filed."	3
"In this paper, the author proposes a new variant of class activation map (CAM), i.e., Pixab-CAM with a novel evaluation metric to show the effectiveness of the proposed Pixab-CAM.

However, several drawbacks including writing, novelty and fair comparison suggests me to reject this paper."	2
The paper presents an interesting and novel approach combining several techniques to improve DNN-based open-set recognition with distance-based classifier. Paper is well-written and the methodological choices are properly justified. More detailed ablation study and the analysis of the model efficiency would still improve the presentation and verify the claims better. This is an important and practical problem and can bring new insight for the field. I recommend the paper to be accepted.	3
While the empirical results are favorable, justification and analysis of including L_bg,k could be more detailed.	2
The paper is well-written and easy to read. However, the novelty is limited and the experiments are not sufficient.	2
In general, the novelty is kind of limited but the experimental results are quite good. There are some issues that must be addressed regarding the setting class centers. Also, more comparison to the related methods and some ablations studies are needed in experiments. A borderline paper closer to the acceptance region in my opinion. 	2
"Overall, I really like this paper. I would like to see the authors address the items in my wishlist (specifically iterms #1-3), but the paper stands pretty well on its own.

I also think that diffusion models and 3D point clouds is a nice and timely area of research, and this paper will likely be a nice reference point for others trying to combine DDPM's with 3D point cloud applications."	4
Good performance. Novel approach. Comprehensive paper. Contributions are well justified. some concern on the reliability of the reported numbers due to subsampling the test set.	4
"I'm leaning to reject this paper (Borderline Reject) *at this time* due to the following reasons:

1. Some claims in the manuscript lacks references. 
2. The results of the Completion 3D benchmark are not consistent with the online results.

**Update (11/26/2021)**

The authors address all of my concerns in the rebuttal. Therefore, I agree to accept the paper."	3
"The paper proposes a sequential decision making idea to do transfer learning using and keeping two networks: pre-trained source and from-scratch target. The results that are presented suggest Auto-Transfer offers large gains, but the scope of the experimental section is a bit limited.

--------------

After reading the detailed reply by the authors, I decided to raise my score: 5 --> 6.
"	3
While the authors address an important problem with an interesting technique, a more principled evaluation would be necessary for this work to be complete. The task should be better contextualized within the existing literature, and the proposed method evaluated more throughly. 	3
See main review.	3
"I am currently indecisive, leaning towards rejection. The paper presents a nice narrative about the adversarial multi-armed bandit approach, highlighting the benefits of the routing of representations in combination with the dynamic selection of operators to combine and transform features. However, experiments seem to hint at a different reality, where fixing these operators is indeed more beneficial.  Based on this, I consider that authors need to present stronger evidence that supports the value of the approach, as it is currently presented in the paper, or they need to update the narrative of the paper to account for the empirical findings.
 
Please, consider my questions for rebuttal.

Post-rebuttal review:

After the evidence provided by the authors, I have decided to increase the valuation to a 6: marginally accepted. The new tables are able to defend the value of the main idea, but still, there are some doubts about the potential of ""full"" vs. ""route"" in different problems/datasets. This is the main reason for not assigning a higher score.
"	3
This manuscript provides a sharper utility analysis up to some clarity in the proofs. There's also some reservation on the significance of the techniques presented in the proofs.	2
It is not sure that there is the function satisfying assumption 4 and assumption 5,so the problem setting may be not reasonable. And the authors do not give the loss function task in detail in experimental part. There is no lower bound so we do not know if this upper bound is optimal. And the authors should give more experiments about different parameter. 	3
The results given by this paper are clear, the theoretical analysis seems solid, the organization and the presentation are fine. But the proof sketches should be more clear, the notations, the typos, and the grammatical errors should be checked. Although there are some minor issues, I recommend this paper to be accepted because of the superior theoretical results and the novel technologies.	4
The presentation of results is clear. Extending existing results to the DP case and getting the first O(1/n) utility bound is significant. The method is mostly based on Klochkov & Zhivotovskiy 2021, thus not very novel.	3
Overall I like the idea of the paper and the presentation, however I have one concern about the Assumption 3 and the possibility that the main result (upper bound) in the paper might be trivial in some cases, and hope the author could address that in the rebuttal period.	3
"The application to situations where one knows there are some correspondences (but not the correspondences themselves, for privacy reasons) is sufficiently of general interest. The analysis seems correct and an important contribution. This is further supported with a novel algorithm that is reasonably sufficiently demonstrated on synthetic and real data.
In general it seems close to a model paper."	4
I think the results are technically interesting and the paper has good motivation, I vote to accept this paper. 	3
I think the paper has made some interesting and important contributions to the problem of matrix recovery with missing entries & without correspondences, while there are still a lot of rooms for improvements. Since this is the first work (to my knowledge) that attempts to solve the very challenging problem of MRUC with missing entries, I would like to encourage its publication, hence, for the moment I recommend for weak accept, with the belief that my concerns will be sufficiently addressed in their revised version.	3
In short, the authors propose convolutional model for self-attention, which is lack of novelty. The method also has flaws, and the experiments are not very convincing.	2
Overall, the proposed method is not good enough for publishing in top tier conferences such as ICLR. The contribution and novelty of the proposed multi-scale self-attention method are marginal. The experimental results cannot fully support the proposed approach.	2
This paper falls short of showing the benefit of the proposed method. It is unclear whether the new architecture is indeed better than existing alternatives both theoretically and empirically.	2
"I think the novelty is limited and some explanation is not clear.
"	2
My recommendation is to accept the paper. It builds upon much work about exploiting inductive biases to make a more efficient transformer. While not revolutionary, it is a well-written paper with good analysis and experiments. The idea is has good intuition behind it, and the fact that it is a drop-in replacement for self-attention encoders makes it attractive for ML practicioners, too.	2
To sum up, the idea in this paper is incremental and more empirical evidences are needed to demonstrate the effectiveness of the proposed PoNet architecture.	2
"The paper is good but not enough. It empirically improves the accuracy of classification and language modeling. But the significance and the originality are not high, and they do not bring us much new knowledge. Their comparisons to baselines are not sufficient, e.g. they can be benefited from reporting GPU/TPU time. Since it does not meet the high standard of ICLR, I tend to reject this paper.

---------------Post Rebuttal-------------

The authors have clarified my concerns about performance, I decide to raise the score from 5 to 6.  I still recommend authors to provide code, report error bars, speedup on GLUE. Moreover, the combination of StructBERT and MLM can not be a significant contribution. Finally, the challenge of combining these pooling methods and pertaining objectives should be discussed and well-evidenced."	2
The authors introduce a pooling-based module to replace the self-attention layer in the Transformer in order for fast training speed and low memory usage. Extensive experiments show the effectiveness of their methods. However, some details of their models should be further clarified in the paper (such as the number of parameters). 	2
I have to reject this paper in the current version. The proposed idea is interesting but without supporting proofs or empirical experiments, it is very hard for ICLR community to justify its correctness and judge its applicability to real-world problems.	2
Interesting, though provoking paper that proposes a novel paradigm for causal modeling in general systems that are often unrepresentable in current causal graphical frameworks.	4
While the proposed idea of meta-SCM is thought-worthy, the current shape of the paper is far below the quality bars of scholarly conferences such as ICLR. The authors need to add empirical validation of the proposed idea and overhaul the writing of the paper. They also need to show that their proposed meta-SCM has enough novel methodology contributions.	2
Overall, I think this was a good paper with some nice theoretical developments and an easily understandable and interpretable model.	3
"- A more comprehensive discussion on Assumptions are needed.
- More interpretations about the theoretical results need to be provided.
- Some claims in the abstract need to be modified.
- A better treatment of hyper-parameter selection is required.
- Some typos need to be fixed."	3
My general feeling is that there is theoretical contribution but the experiments could be improved.	2
To conclude, I find the paper interesting and easy to follow, the technical contributions are fine but obviously need further refinement. The literature review is insufficient, and some SOTA methods, such as LSTM, GPR, etc., are missing. The section of experimental results can be improved. 	3
Although the paper introduces some interesting insights, due to many similar works in this direction, I do not think the contribution of the paper is significant. 	2
"The idea of this paper is simple, and the proposed method is straightforward and seems very easy to implement: Given all available options of existing normalization techniques, one may handily combine different techniques for estimating the mean and the variance. In practice, there might not be apparent drawbacks of trying such a heuristic strategy. On the other hand, the strategy still lacks more solid technical supports and insights to justify why it works and how to apply it for different scenarios. It seems that, for most cases, it would be much easier for the user to adopt a simpler strategy like ""*if memory is sufficient, use a large batch size + batch normalization; otherwise, use a small batch size + group normalization*"". The experimental results look fair, but the improvements are not very significant. Therefore, whereas this paper presents a new strategy to carry out normalization for training deep networks, its contributions are marginally significant and require further investigations. 
"	2
"In the current form, I consider this work to be below the bar to 
be accepted due to the problems and limitations mentioned above. 
I am willing to increase my score if the mentioned improvements
are performed and presented in the paper. "	3
Given the current weakness of experiments and theory part, I tend to weak reject this stage.	3
Given the novelty of the proposed method and extensive experiments, I like this paper and would recommend acceptance. 	4
Please see main review. 	2
I liked this paper. The proposed approach is simple and generic (more sparse reward domains should be investigated for their final claim to be substantiated) yet practical. The experimental results are comprehensive and I think it would be a great value to RL practitioners.	3
"
*Summary Of The Review

I am quite worried about the fact that the algorithm is looking quite simple.
But the results are looking good.
"	2
This paper proposed a Transformer-like model for few-shot font generation. However, its technical novelty is limited, evaluation is not sufficient, and the discussion is not thorough. The authors can improve this paper by including more recent works as baselines and comparing them using more evaluation metrics.	1
I like the design of the glyph attention part of the model. However, I am concerned about the use of a query and a content set with unclear relationship between them. Also, seeing the baseline comparison proposed above will give more evidence about how challenging is the problem and how well the method performs. 	2
As analyzed in my above comments, I think the quality of this paper is slightly below the bar of ICLR. I will be happy to read the author's responses and discuss with other reviewers to make my final decision.	2
Overall, the paper’s motivation is clear and reasonable. However, there are still some issues on modeling and experiments that need to be clarified. Also, the overall writing is not clear.	3
This paper proposed a novel and working solution to a tough problem of backdoor attacks detection with binary or multiple labels. The solution is well motivated and theoretically justified. Although the convergence rate might be a potential drawback of this approach, it still makes solid contributions to the community.	3
"My main concern is the it lacks discussion and comparison with works that works
under the same setting. Its analysis is not sufficient to understand the
applicable scenarios and its key benefit."	2
"See above.

*[Update]* Having read the rebuttal (and looking at the additions to the revised paper), I would like to thank the authors for showcasing an honest effort to answer/address many of the comments/clarifications I initially had. I have updated my score to reflect the same."	2
"I will vote for accepting this paper. The proposed ET statistic is novel and technically convincing. Extensive experiments have been conducted to evaluate proposed BA detection algorithm.
"	4
"Overall the method is novel and useful. However, the chosen baselines are quite weak. The explanation of the inference part needs correction and clarifications.

UPDATE: I have increased my recommendation from 5 to 6, given the clarifications and extra experiments provided in the rebuttal. "	3
In my opinion, this work is original and well-presented. The paper managed to discuss the proposed method in detail and the experimental evaluation was well-constructed. I think this is overall an impactful work and is significant to the probabilistic ML community.	3
My main concerns are about the motivation of introducing NPs for multi-task learning, the lack of important baselines, and the tightness of the bound.	2
The advantages of NPs for multi-task learning should be further interpreted.	3
"In my opinion, this work is slightly below the acceptance threshold. But I'm happy to change my score if the problems are addressed well.

Advantage:
It formulates the addition of a pessimistic penalty or an optimistic bonus when learning Q-value estimations as a reward shifting, which is may of independent interest.
Targeting different settings in RL, it designs different algorithms using reward shifting.

Disadvantage:
I'm not sure whether the intuition that changing the initialization of the Q-value network is the main reason why the algorithm with reward shifting outperforms other baselines.
If we see reward shifting as an extra term added for Q-values, there are some other ways instead of adding constant to do that, while there are no baselines about that is involved.

"	2
"The paper provides helpful insights to better understand the effectiveness of reward shifting in deep reinforcement learning. Major claims are well supported by empirical experiments in various domains, including offline and online RL. My main concerns are about the clarity of the paper, which I believe needs to be further improved, and additional baselines (please see the above Concerns for details). Therefore, I vote for a weak reject for the current submission. I am willing to adjust my scores should my concerns be addressed in the rebuttal period.  


=== Updated ===

My major concerns were addressed during the rebuttal period, so increased my score accordingly.      
"	3
I like the topic this paper studies. However, the main argument in the paper is intuitive and seems well-known to the community. Beyond some empirical validation, I do not think it gives enough insight on how different initialization affects the learning dynamics and final learned policy. Also, there are important questions left, such as: (1). How to select the optimal bias $b$ to facilitate the learning process and (2). Does the scaling factor matter, empirically? Though the topic is interesting, given the current status of the paper, I would recommend a reject and I believe it would be a valuable paper if the authors study more deeply on the underlying causes of reward shifting.	2
Overall, I am still having some concerns about the claim of the paper. These concerns are pretty important and thus I encourage the author to engage in the discussion period and clarify these if there is any misunderstanding. I am happy to re-evaluate if the author convinces me. Based on my current evaluation, I don't recommend acceptance.	2
While the paper studies an interesting problem, the reported performance gains are marginal, especially given the complexity of the model. I lean towards rejection.	2
"This is a very good submission that touches upon a kind of neglected and less fancy area of data augmentation for time series - authors propose a very neat solution that improves upon the state of the art and demonstrates good performance across datasets and various ablation studies.
I do not think that being too picky on things that do not matter for the sake of finding some minor negative things that do not really affect the scientific merit of the paper.
Having said that, the although am working in contrastive learning and self supervised learning, time series data augmentation is not my cup of tea."	3
"Main concerns about the paper are its technical correctness and the experimental results, including the ablation analysis. This work seems to be not ready for publication yet.

"	2
The authors propose an interesting solution to the challenge of seeking data augmentations on time-series data. Downstream task performance seems promising compared to benchmark models. In general the paper is well-written, but some clarifying questions remain (see my earlier remarks). Also, some of the (ablation) analyses can be made more complete. 	3
"Overall, I like the motivation and some technology in this work, but my major concern is the insufficient experiments. Thus I stand in "" marginally below the acceptance threshold"" now and waiting for the author's response."	3
The paper's analysis seems reasonable on some given models. However, it is not clear in the paper whether this finding is still valid for larger models, or whether it can handle inputs with different sizes. I am willing to raise my score if the authors can address my concerns in their response.	3
The paper clearly presents the problem and methods. The experiments can be improved to further support the conclusions. Therefore, I rate to 'marginally above the acceptance threshold' now.	3
"It is an interesting point for research in ViTs. The motivation is not sufficiently clear and solid. Some of the claims are not precise as stated above. In the experiments some important details and comparisons are missing, while a variety of experiments have been included including self-supervised learning. 

Overall, in the current form this submission is not strong enough for acceptance. I pretty much vote a rejection rate. However, more information from the authors is needed to make a final recommendation. 


******* Post-rebuttal Update *********

The authors have well resolved all of my concerns with additional interpretation and experiments.
Overall, this work is novel in taking the normalization aspect for resolving the limitations of existing ViTs in exploring local context, and showing consistent gain for SOTA ViTs, based on interesting observations and solid design. Therefore, I recommend acceptance."	2
"My recommendation is 5: marginally below the acceptance threshold. I like the idea of the paper, and overall the execution looks promising. However, I think that some important details and experiments are needed before the paper is published. If the authors address most of my questions/ comments, and if no obvious red flags appear, I would be very happy to update my recommendation.

--post rebuttal

Thank you for the clarifications and the new experiments. I will raise my score to 6. However, I recommend better tuning of AdamW for the GLUE experiments, because the baselines are rather weak, i.e. not all tasks match the results in Table 1 of the BERT paper (https://arxiv.org/pdf/1810.04805.pdf). I would also suggest that the authors add the new experiments in the appendix and fix the typos in the blue text, e.g. missing space before citation and missing closing parenthesis. Also, in the appendix when you discuss the interpretability of the equation from P_1, I think you mean ""asihn"" instead of ""sihn"".

Finally, I would like to note that the paper seems acceptable only after extensive experimental details were made available in the Appendix and the blue text. It's unfortunate these details were missing originally, but I am glad they are available now :)."	3
The overall framework and concept are sound. This will be a good paper if the experiments can support the arguments. However, the current experiments have left out too many details and results for validating the effectiveness of the proposed method.	3
In summary, this paper needs an overhaul of its writing and presentation. The proposed method is not well-justified and the important technical details are missing. The experiments are lacking and cannot support the claims made by the paper. That said, I recommend rejection.	2
"Overall, this paper addresses an important problem (better understanding what learned optimizers are doing) and does so by proposing  a new technique: using symbolic regression to fit interpretable equations to pre-trained learned optimizers.

I think the paper could be greatly improved by:
- Adding many more details about the experiments and methods. In particular, the ""complexity"" regularization of the symbolic rules seems like an important parameter. It's worth showing results as this parameter is swept across different values, as (presumably) there is a simplicity-vs-performance tradeoff that occurs.
- Show us the learned symbolic equations for the optimizers in section 4.3! The whole point is to have interpretable equations that we can stare at. Showing that these have good performance is one thing, but it would be much more interesting to tie that performance to particular parts of the symbolic equations.

[[ UPDATE AFTER REBUTTAL ]]
After reviewing the author response and updated manuscript, I have chosen to increase my score."	3
"I lean to accept this paper since it is nice and appealing to distill the complicated numerical L2O model into a symbolic optimizer.
However, I think the paper would be improved dramatically by showing the generalization performance of the symbolic L2O model as well as the performance difference between the numerical L2O model and the distilled symbolic one."	3
"The central contribution of the paper (the RoundTourMix distillation procedure) seems ad-hoc and many of its decisions are not well-enough justified. The experimental validation has some flaws that need to be addressed to make it a convincing contribution.

Finally, this is honestly not a great fit (topically) for ICLR and would be better suited in a games (e.g. AAAI AIIDE, IEEE CoG) conference or workshop. In its current form, the contribution is too ""heuristic-y"" for publication at ICLR."	2
I believe this paper offers sufficient contributions to the field to potentially be accepted. Although it by no means solves all problems in this domain, it does help toward bridging the gap between black-box deep RL policies and more interpretable, symbolic RL policies. For me, the paper's primary contributions are the use of an original distillation procedure to translate black-box policies into symbolic ones, which offer the benefit of being more human interpretable and generalising better to tasks outside the training set. So this paper does have promise. But what prevents me from fully supporting it just yet are issues with the empirical evaluation. I would, in particular, like to ensure that the PPO and A2C baselines are accurately generated and that the results are robust to the choice of random seeds before I can fully support the paper's acceptance.	4
I believe the authors tackle an interesting and relevant problem in RL, combining both deep learning and symbolic approaches to good effect. To the best of my knowledge, there are solid contributions in this work, and I would recommend accepting this paper.	4
"While the paper is overall clear and easy to follow, it makes many strong assumptions that are tailor-made for simple 2D games. It is unclear how the symbolic rule framework can scale to more complex domains. I am unconvinced that the distilled rules are more robust or causal than the PPO counterpart, especially when trained with robust representation learning techniques. 

I give my rating due to the limited usefulness and problematic assumptions of the proposed approach in realistic tasks. "	2
I recommend a weak acceptance for the paper, due to the novelty of their algorithm and the interesting empirical results presented, where the final policy is very interpretable. My main concerns are with the wider applicability of the method.	3
In conclusion, the paper presents a novel idea to perform adversarial attacks for audio-based systems, but the experiment setup and results need improvement. The amount of work required is perhaps more than what could be finished based on the time constraint. 	3
"Overall the paper is well-organized and the proposed method is interesting.
If the authors can add necessary evaluation, such as the attack effectiveness comparison with other attacks against the state of the art defenses; the adversarial transferability analysis, and the adversarial evaluation against different ASR systems, I would like the increase my score."	3
Although the idea proposed in the paper is interesting and could have a great potential in the way that adversarial examples are generated for ASR systems. However, the current version of the paper displays only a limited experimental setup of the proposed algorithm that does not help to show the significance of the algorithm. I am willing to increase my score if my above concerns are addressed. 	3
"The idea is solid.
Experiment results are good.
Writing is straightforward and easy to follow.
All in all, I recommend to accept the paper."	4
"I think this is a good paper with merits to be included as part of the conference proceedings. 
I believe that the paper is well-written, the claims are well-supported, and the experiments seem correct. 


I believe a few things can be improved (as mentioned above)
The reason I don't give a higher score is because I believe that the methodology does not seem to be too novel and the results are marginally better, except for one or two datasets. 

"	2
"A simple and clean solution for influence-based data relabeling.
"	3
The paper does an end to end study of using influence functions for relabeling instances to achieve a lower test-loss. The technical novelty is slightly limited as the formulations are extensions of current influence functions, however the paper is supported by strong empirical studies (including limitations about RDIA in deep models).	4
"The proposed method is very similar to the one proposed in [Ref1], while the current paper misses this important prior work.
Although the ways the amount of relabeling is optimized are different (using gradient descent or by one step update), I believe the close connection between [Ref1] and the proposed method needs to be described appropriately in the paper, e.g., through detailed discussions and experimental comparisons.

* [Ref1] Training Set Debugging Using Trusted Items, AAAI 2018.

The strong aspect of this paper over [Ref1] is the experimental results where the results on DNNs (on MNIST and CIFAR10) are reported, while [Ref1] considered only kernel-based models.

---
## After Discussion with Authors

I conducted an experiment on breast cancer dataset by myself (see below).
There, I confirmed that RDIA does better than the One-Step GD update of [Ref1].
I therefore decided to increase my score with a strong expectation to the authors for **referring [Ref1] appropriately in the main body of the paper**.
As I mentioned in my original review, this is not the first study considering relabeling based on the influence function-like technique.
Recalling that influence function is one specific example of implicit gradient, [Ref1] is the first work in this direction to my knowledge (even if [Ref1] does not describe the fact explicitly).
As I demonstrated in my code, the simplest version of [Ref1] with only one-step update (and without any human intervention) does good job.
**I strongly expect the authors to pay certain respect to [Ref1] and do not underestimate their technical contribution**."	2
Given the points listed, I give the current rating here. It would be helpful if the questions listed can be addressed.	2
"Although the proposed method is novel by embedding a transition matrix to the target model, there are several concerns: 
1) the experiments with more kinds of AT baselines, 

2) the decrease of performance on clean samples, 

3) the applicability of the proposed method for the dataset with large class number, 

4) the comparison fairness.

Thus, I think this paper is not enough for publication until these concerns are solved."	3
"
Overall, from my point of view, I think the novelty of the methodology and experiments evaluation of the paper in current form is not enough. I suggest rejection. 
"	2
Overall, I think the paper's merits outweigh its drawbacks. It provides some important questions and findings to multi-task learning. Therefore I would recommend accept the paper. 	3
I carefully review this paper. I would vote for marginally below the acceptance threshold and make the final decision after seeing the rebuttal from authors .	3
This paper proposes a simple method to improve multilingual finetuning of pretrained language models. The proposed solution and its effectiveness indicated by the experiments results has potential for enough technical significance. However, the current version of the paper has room for improvement in terms of experiment completeness in order to draw conclusion on the practical value of the proposed method.	2
Overall, this paper presents a simple method with strong empirical improvements, yet the intuition and source of improvement can be somewhat confusing.	2
Overall a good quality submission with novel and interesting ideas. It would be great for the authors to address the weaknesses mentioned above.	3
"Overall, I vote for marginally accepting. I like the idea of unsupervised continual learning and handling it by the proposed LUMP method. My major concern is about the limited novelty of the proposed method -- adopted from mixup in supervised learning, and some additional experiments on class-incremental or task-agnostic settings(see weakness above). Hopefully the authors can address my concern in the rebuttal period.

[After rebuttal]
The authors addressed most of my concerns, so I would like to raise my score.
"	3
The paper shows interesting results that confirm the potential of self-supervised learning methods in the context of continual learning. The technical novelty may look incremental but the experimental conclusions are very interesting for the community. The paper is clear and well written.	3
"This paper attempts to rethink the standard continual learning in a new point of view by considering unsupervised representation learning methods. This purpose is interesting, but the major difference between the SCL (supervised continual learning) and UCL (unsupervised continual learning) seems to be just adding a unsupervised representation loss at the backbone and freezing it in the second stage of predicting head finetuning.
Moreover, some parts of the empirical results are not clearly presented or explained, an improved version of experimental results could be helpful for better validating the contributions."	3
I am not very familiar with DDPMs but as far as I can tell, the numerical technique introduced makes sense and leads to improved results. I, therefore, recommend acceptance. 	2
The novelty of the methodology presented in this paper qualifies this work to be accepted to the conference conditionally to improve the clarity of Section 3. 	3
The idea of separating the gradient and transfer part is (to the best of my knowledge, I am not an expert though) novel, and I can see many applications besides the ones proposed in this paper. The shown results might not be exactly state-of-the-art in terms of generating images, but show clear advantages over traditional numerical methods. Therefore I recommend accept. 	3
I personally find this submission interesting and significant, yet believe that clarity needs to be improved to enable readers take the most of the paper's insights.	3
"See above
===========================
After the rebuttal: Thanks the authors for the detailed response. 
Though I found the proposed idea of using RL for AMR is interesting, the technical contribution is still limited. 
Therefore I like to keep my original rating."	3
"The paper is well-written and two tricks for RL are impressive (using Nmax to take care of varying dimension of state/action space and use of surrogate reward for training). However, I have some major reservations: 1) Article review and position the paper in the large context of AMR (the adjoint-based AMR and goal-oriented mesh refinement in the PDE-optimal control community can achieve all goals proposed in the paper), 2) Discussion on test cases and practical aspect of the algorithm (other BCs, parameter should be explored, geometries are too simple for mesh generation whose main backlog is dealing with complex problems, scalability for complicated PDEs like Navier Stokes over complex geometries, etc.), 3) Discussion on the solution space and mathematical definitions (this is rather lighter comment but I think authors should clarify some concepts better in the manuscript), 4) Discussion on the algorithm (what about other constraints of AMR that traditional algorithms can handle, de-refinement, etc.). 
Overall, when it comes to RL for AMR I think it is novel and authors use some nice tricks to handle the challenges. But the overall use of RL for AMR over traditional methods is not demonstrated and there remain major concerns regarding the pragmatism of the method.  "	3
The paper proposed a promising direction for future research, but it would be more convincing if the authors tested their method on practical scales with more solid validations.	4
Overall, given the concerns above, I do not feel like this work is ready for acceptance at this point. This is a work that rests mostly on its empirical results, and yet these are not completely robust at this point, given the issues described previously. As such, I am classifying as marginally below acceptance for now. Nevertheless, I am interested in reading the authors responses to the points raised above.	2
Formulating AMR as an MDP sounds reasonable, and structuring meshing policy using the graph neural network is also straightforward. Formulating a specific component (step) of an iterative algorithm using a learning-based module (i.e., RL) and GNN to learn a policy that can generalize over varying sizes and actions is a pervasive idea. Thus, the technical novelties of the proposed method are limited. The only novelty would be employing these ideas to finite element analysis.	2
"While the novelty is limited, the authors are able to achieve good results with a much smallerset data size. Potentially, with more data, this method can achieve much more superior performance than CLIP. However, I think the insights conveyed from this paper are limited so I will choose borderline accept.


Post-rebuttal update:
- I am very happy with the additional results, which address most of my concerns on fair comparison with CLIP. I will remain the original rating due to the limited ""novelty"". But I would give a 7 if there is a 7 because I think this paper should be accepted."	3
This paper provides three simple yet effective additions to CLIP and show its strong performance with much less pre-training data. Overall, I feel the paper is clearly written, with extensive experimental results to support its claim.	3
"**After rebuttal: I have read the authors' rebuttal and it resolves my concerns. I will raise my score to between 6 and 8.**
"	3
"This paper shows a strong empirical contribution; DeCLIP shows better results than CLIP with a smaller dataset size. However, I think the weakness of this paper slightly overweighs the strengths.

In particular, I am not fully convinced that DeCLIP is data-efficient and CLIP is data-inefficient. This argument should be supported by empirical comparisons, e.g., the reproduced CLIP results should be reported in Figure 1, Table 2, and 3. I listed why I think the DeCLIP results and the CLIP results are not directly comparable in the weakness section.

All technical components of DeCLIP are hard to say novel; self-supervision uses SimSiam and masked language model, multi-view supervision uses the EDA augmentation, nearest neighbor supervision can be viewed as a self-distillation method. However, the combination of known techniques can be novel if the empirical contribution is significant. I think this paper has a limited borderline novelty, but the empirical contribution can be a strength. For example, if this paper outperforms ""ViT-L/14@336px"" with much fewer data points, I think the empirical contribution exceeds its weakness. (disclaimer: I do not request ViT-L/14@336px results. I know that it is very expensive and unachievable by ordinary infrastructures) As of now, I think the empirical contribution may need more verifications, especially for the vanilla CLIP results.

Overall, I recommend ""marginally below the acceptance threshold"" for my initial recommendation.

-----------------------------
Post rebuttal comment.

After reading the responses (for raised concerns by all reviewers) and the revised paper, I think the authors address my concerns very well in general. I would like to encourage the authors includes the results in A3, A4 (not completed yet), A5, A6 in the paper. Also, I encourage the authors to add zero-shot results (Q7) in the final revision, which was not possible at the submission time (I missed it).

After the revision, the arguments are generally well-supported (revised my score for correctness has been updated to 4 from 3). Now, I recommend ""6: marginally above the acceptance threshold"" for this paper."	2
This is a good paper with thorough experimental analysis. It is significantly improved from the previous version and I would recommend accepting it. 	3
"The paper presents an interesting idea for self-supervised pre-training of the encoder stage of the SMT architecture. The conceptual approach is not limited to visual navigation but applicable to a variety of (3D) reasoning tasks.

The paper lacks clarity in the details of the approach, see above weaknesses W1-W4. With that I’m concerned that the proposed masked-zone prediction task (which is the core contribution of the paper) is a good choice for handling the pre-training. While the approach achieves SoTA on the visual navigation task there are only limited learnings but numerous uncertainties for the reader why this is so.

I would be willing to increase my rating, if the above weaknesses are addressed and it is shown that the proposed masked-zone pre-training is superior to alternative encoder pre-training approaches..
"	2
Overall I think this is an excellent paper that presents an idea that is both simple, practical and effective. The method is well validated on two challenging benchmark environments, MP3D and Gibson, and compared against numerous state of the art approaches and achieves great performance. In the response, i would like the authors to address the points in “weaknesses” above.	3
I like the work the authors have done with the article. I don't think we have a model of great originality, but rather of an incremental nature. However, the ideas are clearly stated, and the experiments are well designed and executed. I need the authors to defend their position on my criticisms of the novelty of their ideas, and to comment on those parts for which I have not found experimental evidence. Overall: I see here a borderline submission.	2
"The manuscript claimed to provide an analysis for Adam but actually didn't and only analyzed a much simplified (non-stochastic) algorithm that is not being used anywhere. The proofs contain many errors and some are vital.

I'd like to thank the authors for a detailed explanation and revising the manuscript.
After the authors' response and revision, I think the proof of lemma 14 is now correct. However, I still think that the analysis of the algorithm described in eq (2) is not very meaningful, as it looks to me more like inventing an algorithm that is not used anywhere and deriving some (however novel or difficult) analysis for it."	2
"1. The authors provided new theoretical results for implicit bias of momentum methods.

2. The learning rate of GDM/SGDM is smaller than that of GD/SGD, which allows narrower choices.

3. No numerical results."	3
The paper found interesting result that in my opinion are significant. However the paper has two major issues that should be considered before acceptance.	3
Overall, I think the paper is interesting. The authors push forward the understanding of implicit bias by analyzing the effect of momentum. The tools in this paper might be used to analyze more complex models. Therefore, I recommend for acceptance. 	4
I vote for a weak accept currently considering the novelty of the proposed methods. But the concerns above need to be well addressed. 	4
This work presents zero-order optimization methods for DARTS, which is very novel in the area. The paper is well-organized, with sufficient analysis and sound technical contribution. I think it is a good work.	4
Overall, it is a good paper that opens a new research direction of differentiable NAS family. The quality in terms of visualization, theoretical proof and empirical evaluation are also good. So, I recommend an 'accept' for this paper. The concern preventing me from giving higher rating is that all the optimization methods are directly obtained from previous works.	2
"This paper clearly tackles an important problem to a currently relevant class of algorithms in the community. However, the proposed solution is completely tailored to a particular type of reinforcement learning problem: episodic, goal reaching problems, where the observed reward signal is zero in every transition but the final one. This drastically reduces the applicability of the proposed method. Some important components/assumptions of the overall approach are also not properly discussed/evaluated. Moreover, this is a paper that touches on one of the most fundamental aspects of deep RL solutions, but it justifies itself only on a small set of domains that are not necessarily the domains in which deep RL algorithms (and their replay buffers) were originally proposed and evaluated. This raises more questions about the real feasibility of the propose approach. The text could also be improved for precision.

"	2
The authors provide a clear explanation of their method, code and implementation details. Although the experimental setup is a bit limited,  the results look good and the proposed approach has the potential to improve future work in this space. Moreover, many additional experiments are proposed to shine light upon the proposed method.	3
Although the paper is well written, the current version does not convince me to recommend acceptance.	2
"Overall I believe the paper is good enough to be accepted. While a few additional experiments would be beneficial to the understanding of the method's performance, the main blocker towards me giving a higher score is the limitation of the method to environments where the terminal state holds most of the reward. I think the method is still interesting, but showing it can be applied in environments without rewarding terminal states and that it still improve performance in these environments would be beneficial.

EDIT: Having seen the additional experiments and clarifications the authors have added, showing that TER could be used in situations where reward is concentrated in states rather than just in terminal states, I raised my score from 6 to 8."	3
This paper addresses a relevant issue, however the main conclusions they draw didn't exactly make me jump on my seat, and while the proposed method may have some originality, the experimental results leave quite a question open about their significance given the considered sparsity/performance tradeoff point the authors consider. I would recommend the authors to revisit their claims under the light of the existing literature dealing with the relationship between sparsity and generalization ability, and to present their experimental results at a higher accuracy target.	2
I am inclined to recommend acceptance, owing to the paper’s novel angle and solid experiment evaluations. I have no major critique.	3
"This is an interesting work with solid execution. More experimental clarification and analysis would be welcome.
"	3
My biggest reservation currently lies in lacking comparison with SOTA efficient adversarial training methods. The value of this work cannot be solely established without that comparison. Overall, I’m currently rating as borderline and would decide my final rating based on the authors’ rebuttal.	3
The paper is well-written and is technically sound and the proposed problem makes sense.	2
"In general, considering the significance of the researched problem, this paper might be accepted by the ICLR2022. However, some points should be clarified and strengthened in the revision.


-----POST REBUTTAL-----

The authors have addressed my concerns. Thus, I increase my score from 6 to 8."	3
"Open-set single domain generalization is an interesting and important problem setting. However, the proposed method is not approapriately designed for this problem setting. In addition, the experiments and discussion lack an important related work, which is open-set classification (or out-of-distribution detection). I vote for ""weak reject.""
"	3
"This paper proposes to study a new and interesting transfer learning setting called open-set domain generalization and develops a new method (CrossMatch) for this problem. However, CrossMatch is mainly built on several previous methods, e.g., the multi-binary classifier (Liu et al., 2019; Saito & Saenko, 2021), adversarial data augmentation (Volpi et al., 2018; Zhao et al., 2020a), making the overall novelty incremental for ICLR. Thus, I tend to give a ""borderline reject"" score."	3
See main review.	2
"In general the paper is interesting; however, the novelty and contribution is limited. Most of tools are from the existing work, and neither insights nor empirical results are strong enough to make this paper stands out.
Adding the discussion related to the generalization bound (as pointed in the provided literature) and extra experiments would increase the value of the current draft."	2
"I think this paper provides some empirical explanation on why dropout is useful to achieve a flatter minima. However,  I feel it is more interesting to rigorously explain this phenomenon given that many works already prove that SGD can reach flatter minima. In addition, the novelty of this paper is not that high based on my understanding. Therefore, I am slightly negative about this paper, but I am open to adjust my score based on the authors' feedback and other reviewers' comments.
"	2
Complete missing of theoretical analysis. 	2
"Overall, this paper studies an important and interesting problem. The paper is well-written and the experiments are extensive. However, the assumption used by the proposed REM is not realistic, and the authors missed an existing blackbox method that could entirely destroy the motivation of this work. Furthermore, some critical information, such as runtime, is missing from the current experiments. Therefore, I give a vote for rejection before the above concerns can be addressed.  
"	3
Overall, the paper has shown that the proposed method can successfully reduce the test performance of the model (adversarially) trained by generating robust-error-minimization noise. However, the novelty is a bit limited and more justification of the data privacy protection is necessary. Therefore, I would like to recommend rejection. 	2
While the novelty of the method is weaker, I do find the study of error minimizing noise to be important. The proposed method is sensible, and the experiments are quite thorough to convince me of the effectiveness of the proposed method. Overall, I like the submission, and would recommend weak accept.	2
Existing works have demonstrated adversarial training (AT) can remove the effect of both data poisoning using targeted adversarial attacks and unlearnable examples. This paper demonstrated an effective method that can prevent the data from being used for AT. Despite the potential limitation on the perturbation radius difference between AT and the unlearnable, this work reveals a weakness of adversarial training. 	3
The paper is well-written with a somewhat interesting idea, but ultimately, it seems unlikely that such an architecture provides advantages over strong baselines, which are not really explored. 	2
The method introduced in this paper is intuitive and seems promising empirically. However, some important aspects are not carefully considered in the evaluation. Therefore, it is hard to justify the benefit and contribution of the proposed method.	3
Overall, the idea of this paper is interesting and reasonable, while the proposed LPSC can be potential for dense prediction tasks. However, there are several limitations of this method, while the experimental results are not very convincing at this stage. For now, I will vote for a marginally below.	3
Overall, I think there are some interesting ideas emerging from this work, but in its present state it's a bit borderline. As outlined above if the problems with presentation could be addressed, and further information given on the computational complexity with more analysis/discussion of the results then my concerns would likely be assuaged. 	3
"Overall I think this paper is well written, and the proof seems solid. Given Hu et al. [2021], the results of this paper are not that surprising. But the proof technique is different. I still have some concerns. So currently, I would like to suggest a rejection, but I am open to discussion and willing to change my score.

"	3
This paper is a good paper with sufficient contribution. Some minor places could be stated more clearly.	3
The paper proves a bound on the excess risk in a nonparametric regression setting, where the estimator is a deep ReLU network trained by GD with L2 regularization, and the regression function belongs to the RKHS induced by the Neural Tangent Kernel (NTK) related to a deep network. This is an incremental contribution w.r.t. Hu et al. 2021. Some technical parts necessary for the proof come from cited works. In this respect this work is also quite incremental. However, I believe that contribution points in a relevant direction of understanding neural nets from nonparametric point of view. I'd be happy to raise the score if authors clarify some details.	2
While the contribution of the paper seems a bit incremental and I am not a hundred percent confident about the technical quality of the paper, I think the paper works on an important topic and has merit to be accepted. 	2
Overall, I think that the topic the paper tackles is important and timely. The paper proposes a compelling combination of existing techniques to increase various aspects of efficiency. That said, unfortunately, in its current version the paper is not so clear to follow (in motivation and implementation). In terms of technical novelty, it seems that the added components over LoRA might need a bit more work for their performance gains to be more convincing.	2
"This paper addresses an important problem and shows some promising results to improve the efficiency of NLP models. However, the presentation of the paper can be improved and there should be more justification of the motivation and novelty of the method. In particular, it seems that the main difference between this method and LoRA is the sparsity of pertained models, which is achieved mostly with existing techniques, yet it seems that LoRA (and/or similar method) can also be applied on top of sparse pre-trained models.
"	2
"Overall, I think the paper present an easily understandable method to an important and interesting question. But it reads to me many details are hidden or omitted so the efficacy of the proposed claim cannot be justified properly. I didn't list all of my concerns but at least the above mentioned five points should be addressed in order for me to increase the ratings.


Post rebuttal:

After author's reply, I still cannot confirm there is significant technical contribution. In addition, despite authors posted some additional experiment as requested, the numbers cannot match the original paper so I am not convinced the empirical result as well. In sum, there is no enough argument that the proposed method should work or any explanation why it should work. On the contrary, the experiments suggested that initilization plays important roles which might imply the claimed contribution might not be the real meat. Empirical evaluation also don't support the claim so I decide to remain my original ratings. "	1
Results with initial success but rejected as further evaluation is needed. 	3
Some core method design is not well-justified and the technical contributions are limited. There are some valuable empirical contributions though. 	2
Overall a nice paper that makes some hefty claims and as such, I would like to see more results, have some questions answered and discuss with other reviewers before I can recommend acceptance. Showing results hold for other contrastive methods would also go a great deal in showing the validity of them and increase the impact of this work but it might be outside its scope.	4
"Even though the idea of the paper to look further into the disentanglement properties of contrastive methods is interesting, I cannot recommend accepting it in its current form due to a lack of comparison with existing methods, impreciseness in the language/results, and too few explanations about the observed results.

**Update**: After the rebuttal, I increased my score by one level from 3 to 5 - yet, there are still open issues that prevent me from recommending acceptance."	1
"I am not very familiar with the field of disentanglement study, so I am not able to judge very well the significance of this work. However I like the simple and clear idea (to study disentanglement properties of contrastive methods) and the comprehensive experimental results. I especially like the concept of group disentanglement and the improvement of benchmarks on a real world dataset. Therefore I am recommending an accept.
"	3
"To summarize, the empirical observation that BYOL features are better disentangled than other methods is interesting. However, there are two important limitations: i) there is very little analysis trying to explain why this happens, and as no applications were shown, better understanding and analysis should be the main product of this research but they are lacking. ii) the experiments are incomplete - the group disentanglement is only shown for one dataset, our replication did not observe it for SmallNORB, such selection of results might make readers reach the wrong conclusions about the properties of the method. I suggest more complete experiments and more analysis.

###################

Our discussion with the authors and the poor experimental design reduced our confidence in this paper - we change our recommendation to rejection."	2
The derivations of new bounds that utilize the proposed decision boundary variability measures are strong. The claims are supported by experimental results where appropriate. This paper would be even stronger with more detail/discussion about existing work.	4
Overall, I think the paper introduces a number of interesting potential ideas for explaining generalization in neural networks.  But my initial read leads me to a number of questions and concerns before I can more confidently back this paper for acceptance.	3
"There is a valuable empirical analysis for the prediction variability. However, the more realistic cases should be considered.
Additionally, to validate the claim of the proposed, assumptions 2--3 should be checked in experiments.     
"	3
For the reasons listed above, I think substantial experiments are required to better understand the phenomenon and confirm the empirical findings, rigorous definitions and clarifications are needed in the theoretical sections, and thereby my current rating.	3
I believe this work studies an interesting and promising research direction and proposes a convincing framework to tackle this problem with solid technical contributions. Yet, I am mainly concerned with some missing baselines, relevant works, and ablations.	3
The paper presents a novel and interesting approach to synthesizing programmatic policies that uses a differentiable approximation of a domain-specific language for policies. This differentiable DSL allows for the use of policy gradient methods to search over the space of programs. The main weakness of the method is the computational complexity required to compute the action of the policy during training. The method iterates through all programs one can synthesize in the DSL up to a given depth. The computational complexity of the policy isn't even mentioned in the paper and it can be a show-stopper depending on the DSL and on the size of the program that needs to be synthesized. 	3
"In summary, I like the high level idea and motivations of this paper. The method is also seemingly novel and performs well. But there are some issues with detail clarity, experiments and analysis, and a comparison to similar baselines. I am currently voting for rejection, but will happily change my score and/or be responsive to authors during the review process as these concerns are addressed.


UPDATE: Score has been revised after discussion."	3
The authors did not do a sufficient job motivating why the problem or model were interesting or important.	3
The paper aims to tackle an important problem. However, the introduced model definition (Eq. 2) is incorrect / not helpful in this regard. Thus, unfortunately, the conclusions drawn are very questionable. Moreover, notations, related work, and experiments needs improvements.	3
My main concern is about the problem setting. I think the definition of *robustness*, *adversarial attack* and *defense* about CO in this paper are unreasonable. By modifying the original graph in CO tasks, the problem becomes a different problem and the solution lose its meaning to the original problem.	1
This paper considers an interesting problem, but the adopted settings need better justifications, and the proposed methods are unfortunately not novel. 	2
"I recommend rejection because the method itself is incremental over prior work and experimental results are only marginally better.
Furthermore, I think paper would benefit from another revision as some things should be written more clearly."	2
Overall, I think the current version is a borderline paper. The proposed methods are new and the improvements shown in the paper are not that marginal compared to most of the related recent papers. However, the paper lacks comprehensive experiments to convincingly support the claims. If more consistent experimental results can be provided during rebuttal, I would vote for acceptance.	3
Due to the concerns pointed above, I do not think the current manuscript is suitable for publication. However, I welcome the authors to clear my concerns in the rebuttal period. I will do a re-evaluation then.	2
"The proposed two tactics are backed up by intuitive but useful insights and shown effective for relaxation-based training. Therefore, the work would be inspirational to the community.
However, the current experimental evaluation is not comprehensive enough and lacks several important details, so it is a bit difficult to tell whether the approach is generally effective or not. Also, the presentation can be improved.
Given these concerns, I think the paper is marginally below the threshold. If more experimental details are provided during the discussion phase, I am happy to re-evaluate this paper."	3
The paper employs a methodically sound approach on a practically important problem. However, important details of the experiments as well as reasonable baselines are missing.	3
There is an challenging and important problem here and a promising method. However, the experiments are rather limited in scope. The modeling contribution is interesting but it is unclear if it generalizes to other combinatorial optimization problems. 	2
Overall, this is a nice paper that applies an interesting learning based approach (inspired by previous works) to the order fulfillment problem, with encouraging results. However, I think the paper needs to improve in multiple aspects to address the mentioned weaknesses: positioning, motivation and empirical evaluation. Even then, I am not sure if ICLR is the most suitable venue as I consider this a relatively straightforward application to the order fulfillment problem, admitting that it does require suitable modifications to the graph neural network architecture (but nothing groundbreaking in my opinion). Whereas ICLR welcomes applications as well, I think in that case it should consider a problem which is more established and thus has stronger baselines from other papers, such that it really convinces on the need/benefits of using ML.	2
I think the idea of converting an order fulfillment problem to a tripartite graph, which is then fed to a GAT, is interesting. However it is hard to know if the model could generalize well to other synthetic datasets or real world problems, since the authors only used one synthetic training dataset in their experiments, and the generative process is unknown.	3
"I vote for a borderline acceptation after discussing with the authors. I recognize the authors' efforts in pushing the boundary of HRL although I still have some concerns on the complexity of the proposed methods and the practical computation cost.

"	3
This paper presents an interesting and novel idea at the intersection of deep HRL, novelty based exploration and reachability. The experiments are sound but could require further clarification and expansion of scope. The clarity of the paper can also be improved to more directly address the need and important of stability regularization. 	3
Overall, I vote for a weak accept. The ideas in the paper are interesting, and the experimental evaluation is thorough and demonstrates the benefits of the proposed algorithm. However, the work could benefit from a more detailed description of how the relevant measures are estimated, as well minor changes to the experimental procedure.	3
This paper is really well executed. It builds on top of an already complicated architecture adding more than one new component to that architecture, but it does so while providing proper intuitions for each one of these new components and, more importantly, actually doing ablation studies that quantify the impact of each component. To me, Section 5.4 is the highlight of the paper. I also appreciated Section 5.5, which shows how the paper is also concerned with stability over different parameters introduced by the proposed metric. I think the paper would benefit from further clarifying some parts of the text, but otherwise this is a good paper.	3
"I could not find any positive aspects for accepting this paper to the premier conference in the field of representation learning. Both the methodology and documentation need to be improved for acceptance.

[Post-rebuttal comments]
After reading through the paper and author rebuttals, one of the reasons for misunderstanding among us is a gap in the target time duration of ""microscopic"" and ""macroscopic"". My understanding is
- ""Microscopic"" user behaviors correspond to short sequences of individual actions during single sessions, which last at most a day.
- From this viewpoint, the proposed BoI representation removes ""microscopic"" information that will be useful for the next user action.
- Monthly and yearly user behaviors might be ""microscopic"" since a month or a year is shorter than their lifetime.
- However, a month is sufficiently long to capture whole the interests of users, which implies that monthly behaviors are ""macroscopic"".

I understand that the authors try to model ""users"", not ""user behaviors"", as implied by the downstream tasks. I also understand that ""microscopic"" (by my definition) sequence modeling is not required and only ""macroscopic"" (again by my definition) user behaviors are only of interest.

I have just revised my evaluations according to the above discussion."	2
This work has some merits but it suffers from several issues that need to be addressed. Thus, my recommendation is a weak rejection.	3
"This work proposes a universal Lifelong User Representation Model to learn different aspects of user preferences in extremely long sequences and easily generalize to other downstream tasks by the self-supervised method.

However, the experimental design of this work needs to be improved, and more explicability needs to be provided about crucial modules and abnormal experimental phenomena, which have more enlightening significance for the community."	3
The paper provides a very simple, yet seemingly effective approach. They do not provide statistically significant results, as they only evaluated with 3 seeds, and this is evident in the results. Moreover, they do not compare with all possible baselines on the topic. Importantly, the paper reproduces false claims, abusing the term bimanual manipulation and the term safety, showing that the authors clearly are not in touch with the topic of robotic manipulation. If the authors change their narrative and fix their motivation, provide sufficiently more seeds to make their results reliable, and also compare with appropriate baseline, I might consider increasing my score,	3
This paper proposes a novel inductive bias (the attention network and regularization) for efficient bimanual manipulation, which outperforms the simple MLP and multi-agent baselines. The paper is easy to follow and experiments are well explained. However, there are certain limitations in observation spaces and applications. In addition, analysis of some experiments is not convincing (e.g. generalization, synergistic interactions) and it needs additional experiments to support the claims of the paper. I would like to see the authors' responses and adjust my recommendation accordingly.	3
Overall the paper introduces an interesting and simple solution to a complex bimanual set up. However, there are some details around the claims and contributions that appear not clear from the current state of the paper. The provided evaluation resorts to a very small number of seed runs (3) which makes it hard to asses the significance of the reported results. Therefore, I cannot yet recommend this work for acceptance. I would happily reconsider my score if the authors can clarify the detailed above issues and provide a more thorough evaluation in support of their solution.	3
"The paper presents a well formed idea and method, but has some points to be clarified that would improve the manuscript.
"	2
"I tend to give a rejection for now. This work is well motivated and has a natural goal. The formulation of gradient matching brings something new to this area. However, there are some flaws like claims with minor issues (**C2**) and missing details (**C1, C4**). Some claims needs to be updated and some details need further explanations. Furthermore, I have some questions about the reliability (**C3**) which may affect my recommendation as well. If all of these issues are solved, I'll raise my score.

--------
*After rebuttal: my main concerns are well addressed, hence I recommend to accept this work. Besides, the manuscript can be improved by providing more discussion and analysis. See the response to authors."	3
One major weakness of this work is that the main idea of using cosine similarity of gradients as guidance for policy search is not well justified in the experiment section. Except for that, based on the experimental results that (1) the proposed method reduces the policy search time by a large margin while maintaining good performance compared to other SoTA augmentation methods, and that (2) the proposed method is applicable to multi-layer augmentation, overall the reviewer considers it a good paper. 	3
Nice motivation and an interesting approach. Results on standard benchmarks are strong compared to other DAS methods. Experiments can be further expanded to different datasets to show the effectiveness of the proposed method. Can also add more comparisons to the previous methods in Fig. 3/4/5.	3
"Overall, I believe the proposed method is interesting: it builds interesting policies in an intuitive way and requires less per-dataset adaptions than previous work.

I do believe, though, that the results simply are not strong enough for publication and a few comparisons in the paper are unfair to baselines. It might be possible to improve upon this method in some ways though, for example with Batch Augmentation (""Augment Your Batch:.."" by Hoffer et al.), as this is closer to \theta’s objective."	3
"Although the proposed way of computing distances between sequences is learnable, flexible, and light-weighted, the experiments on few-shot action recognition could not support the supremacy of TAP over existing methods. The paper also lacks necessary ablation study on the data augmentation used in TAP experiments. Overall, I think the quality of the current draft is marginally lower than the acceptance threshold.

====== Post-rebuttal ======
The authors have addressed my concerns and questions in the rebuttal. I'm raising my rating to 6."	2
"The paper is well written, addresses an interesting problem and proposes a novel approach for end-to-end temporal alignment. However the evaluation of the method, while comprehensive in some ways, has some holes. This may be a concern since the very nature of the method, temporal alignment, is not evaluated.

# Update after rebuttal

I believe the authors did a good job with their rebuttal. All my concerns have been well addressed and the paper is know more polished. I believe the quality of this work is good enough for publication."	3
"The concept is interesting, and potential applications are many.
However the proposed concept has not been well studied.
"	3
This paper focuses on one meaningful topic, targets the challenge, and proposed an interesting solution to solve it. The presentation is easy to follow, the model is good (maybe the pseudo self-classification is not very novel, but still ok to me), and the experiments are sound to me. So I suggest accepting it. 	3
Overall, the proposed approach showed quite promising results on several different time series classification applications. The problem formulation and techniques are mostly adopted from existing work, which reduce the technical novelty. The authors need better justification on the performance improvement achieved by proposed approach, and highlight their contributions in domain generalization methodology. Please refer to the main review for more suggestions on improvement.	2
"I could be swayed to change my ratings fro ""correctness"" and ""empirical novelty"" but in general there are too many unanswered questions for me to vote to accept this paper. "	3
Interesting and novel idea and good mathematical analysis. May need more experiments to demonstrate the claims in the paper.	2
"The overall presentation is good. The main concerns are in above. 
The models (CaiT-S and DeiT-S) are fairly small. Can the authors provide more experiments on big models to further verify the effectiveness of the proposed method?"	3
"
The paper address the feature collapse problem of ViT from spectral prospective, decomposing attention to low-pass and high-pass components. The therotical part is nice, while the experiments does not show adequate evidence that the proposed method would actually help scale depths of ViTs (e.g. beyond 24 layers) as title indicates. 
"	3
The problem the paper seeks to solve is relevant and interesting. However, there is insufficient empirical or logical arguments put forth to justify why this approach is good or better than previous approaches. Empirical results are not convincing, the novelty is mainly in the use of a new dataset and literature survey is not thorough.	1
"While the paper presents an interesting application, the method is a very baseline method with little adaption to the prior architecture. The evaluation is missing a benchmark to make this paper competitive and establish the application problem,

It is also a very application problem and may not be the best fit for ICLR but instead a robotics or computer vision conference as it adapts an ML method to a generalised problem."	1
Based on the weakness of this paper mentioned before, I think the quality of this paper is quite low and cannot be accepted.	1
The proposed method lacks novelty or contribution. The experiments are not complete or convincing. The results do not seem promising. Therefore I recommend rejecting this paper.  	1
I found the paper deals with a practically very important and motivation of the paper is good. However, the novelty of the paper is very limited, and the experiment is not enough for showing the advantage of the proposed method over prior works.	1
The idea of ensemble historical states is novel. The theoretical analysis is meaningful. The experiments show this method has good potential.	3
The paper lacks clarity which makes it specifically hard to judge the contributions of the paper. In addition to this, the paper experimental evaluations requires more work.	4
The paper is well written with theoretical propositions and extensive experimental results.	3
"This paper proposes an adversarial training scheme called SEAT, where the final robust classifier is united by averaging states of every history models through the adversarial training process. Although the idea of self-weight-ensembled method is intriguing, I am not that convinced by the current experimental settings and found it is lack of several competing baselines and performance on larger dataset.
"	2
"Long time series forecasting using periodical information is interesting. The novelty of the paper is limited as the dual task already exists in bi-directional RNN based models for time series. It misses ablation study of using only the last segmentation instead of the long time series. 
"	2
See above	2
This paper introduces a SCFORMER to address the problem that the complexity of the Transformer increases quadratically with the length of time series. The essential idea is to segment the input into small segments and calculate attention among the segments rather than each data point. To further improve the performance, this paper introduces an interesting dual task, which uses the current time series as input and will predict the historical time series. However, this the novelty of the segment correlation attention within the SCFORMER is limited and the time complexity is incorrect. Besides, the experimental results in table 5 do not sufficiently support the claim that the dual task could help model to achieve more robust results.	2
The technical contribution of this paper is marginal and the experimental analysis can be improved.	2
To summarize, I think this paper does not contain many new concept from my view. According to the experiments, I think the results are good, however some experiments are not that solid. Since I don't work in this area, I rate this paper as borderline reject, and would like to participate the discussion in rebuttal.	2
In general, I think finding methods to encourage specialization in models (while still achieving high accuracies) is a very important direction to pursue. The method presented of how to train specialized models (using anchors) is intuitive, and might be useful if demonstrated that it works. Unfortunately, it’s not possible to assert this from the limited baselines/experiments presented.	3
"The use of specialized models on sub-regions can be very interesting for shallow models but for complex models such as deep networks, it is not convincing that the models specialized in sub-regions can bring any change.

How the authors found their results different than predicting a sample using only the subregions models by maintaining a mapping for a test sample to a sub-region model.


The results show the only marginal differences."	2
In summary, I think the idea for encouraging specialization in the model ensemble is rational and novel. But I am conerned about the novelty of the specific technique adopted in this paper as well as the insufficient experiments.	2
Overall, writing is clear, and the empirical results are shown to outperform GAIL and some other baselines. However, some SOTA algorithms (which I believe is relevant to this work) are missed, and it is unclear that the mode collapsing issue also happens for those relevant works or not. Although I agree that there is an algorithmic novelty for this work, I believe more elaborated results should be given with the aforementioned references. 	3
I believe the paper proposes an interesting approach to imitation learning, however the technical issues described in the weaknesses section makes me currently tend towards a weak reject. If the authors were to successfully clarify these, I would consider raising my score. 	3
Despite some important issues with method presentation and justification, I think the paper presents a solid method and solid evidence of its utility. The paper can definitely be improved, but as it stands now, I think it's a good contribution.	3
"The contributions seem rather incremental and the claims are not well-supported.
I still gave a relatively high recommendation, because the results are fine and the overall ideas of 1) tackling covariance shift of BC by using an RL objective to match the expert state distribution, and 2) estimating the expert state distribution once instead of iteratively based on NCE with adapted noise to improve stability, are reasonable."	2
"My main concern with this paper is its scope, novelty and impact.
From modeling and formulation perspective, I do not see much novelty. There is not a novel modeling paradigm which be of interest of general machine learning crowd (as a reader I did not learn a new ml method). Of course, the paper proposes a novel method by combining some existing machine learning tools (like graphical models, sequence-to-sequence modeling). In my opinion, this novelty could have been justified if the method was applied to some high impact problem. Unfortunately, it is not clear to me if the speech duration modification is such a problem."	2
The paper provides a novel contribution by bringing neural generative models to the task of time-scale modification of speech signals. The work is evaluated in terms of objective metrics and shown to outperform some baselines. A (somewhat limited) listening test suggests this model attains high-quality samples though there may be some limitations in interpreting these results. The work is well written, but some of the claims need to be revised, and the contributions better situated against prior work.	3
The paper brings the state-of-the-art ML research to an important but less active filed of study: speech signal modification. The proposed approach is reasonable and evaluations are solid. However, the authors haven't done a great job of explaining the drawbacks of previous approaches, in practice, and the critical need for unsupervised adaptation. Hence, the overall paper reads as a solution (transformer-based models) looking for a new application. The comparison baselines also reflect this discrepancy. A better comparison would be also related approaches in pitch and style modification.	3
The reviewer vote for rejecting the paper; several issues on evaluation, correctness, and clarity are visible.	2
The paper investigates an extremely important problem: class-imbalance, and shows that part of the poor performance on classes with less samples stems from poor transfer of invariance to these classes despite good invariance building up for larger classes. Their proposed solution is thus very well motivated, and is a very nifty idea to model nuisance parameters. All in all, this is a solid applications paper in my opinion, and I think some minor changes in writing to focus more on fixing class-imbalance would make it a good paper for the computer vision community.	3
"I'm sorry if my review reads negative, but I really like the paper!  I think the question it asks is insightful, fundamental and important. 
Yet, I am not fully convinced by how this question is then investigated (see my detailed review below).
I find the ""GIS"" part not so interesting, although it indeed remedies the symptoms and gives ""bold numbers"". I would be much more interested in *why* this happens. And if, for example, lighting plays a role, then why dont the first layers (which presumably are shared between classes) to some degree deal with this?
If the authors can somehow convince me, either with some new insights, or by new argumentation, I would be very happy to upgrade my score.
"	3
"Overall, I enjoy reading the paper as it is well-motivated and well-written. The experiments designed in section 3 are quite interesting. However, the proposed method in section 4 is not well-described and justified; the experimental comparison in section 5 is not sufficient. I thus give a score ""5"" for now. More accurately, my score is between ""3"" and ""5""."	2
"===================
Justification:
This paper provides a new perspective to understand the long-tailed problem of DNN models, yet, the technical details and the solutions of the paper are not satisfactory"	2
It is a promising paper, I have couple minor questions.	3
"Overall I think the proposed test is intuitive, but I am a little confused about the experiments and theory as described above. I would feel more confident to judge the paper if my confusion is resolved.
"	2
This paper presents a nice new federated analytics technique for computing correlations. I have some concerns about the security of the protocol, but it's possible I'm misunderstanding something.	3
"The problem is important. However, technical concerns remain and the paper does not seem to be well-developed and is hard to read. 


Revised comments: Upon reading the authors' comments, I understand their perspective on proofs (not one that I completely agree with) and I thank them for pointing out the places where proofs may be found. However, I am still not convinced that the writing is transparent and clear, or that the issue of differential privacy may be entirely waved off. That being said, I will revise my rating and downgrade my confidence/expertise score, so if the paper is appealing to other reviewers, I will not stand in the way of its acceptance. 
"	3
Overall, I think this is nice paper, with good baselines and clear results, however, its presentation right now is quite lacking and hence I feel it may need some work before being ready for publication.	3
I think this paper is mainly held back by two factors. First, critical details are missing in the main text which makes the proposed method hard to understand. Second, I have some questions regarding the main empirical results that may undermine the empirical significance of this work. Therefore, I don't think this paper is ready to publish yet.	3
This paper proposes a novel idea of learning representation with a clustering+prediction loss, which makes the representation generalize to novel observations from the same task distribution. The experimental results are convincing, although a little marginal in many environments. The proposed method has some potential to help representation learning in RL and zero-shot generalization.	4
In general, the paper is interesting. The proposed method is novel and while being simple, it works effectively and outperforms the baselines. However, I want the authors to discuss in more detail about why their method works well in practice while in theory it can fail.	3
"This paper presents thorough analysis of using a 3rd person camera vs. a hand camera and finds that hand cameras generalize better. I believe this work presents a useful contribution.


### Post Rebuttal Update

I thank the authors for their response. I continue to think this is a good paper that should be accepted for publication."	4
"The paper is a good case study. The ideas presented in the paper are not something that's unknown but this paper does a good investigation on the choice of perspectives.  


"	2
"The paper is well written, the contribution is clearly presented and the experimental results are thoroughly executed. An evaluation comparing the presented results with the proprioception-only case could further improve the analysis.
"	3
The paper tackles a single problem and studies it extensively. The problem in my opinion is significant and the paper would likely have an impact. Therefore I vote for clear acceptance. 	4
The paper is pretty well-written with extensive results and figures. The impact of the paper can be largely improved if the large-scale experiments can be conducted and a better accuracy than the current state-of-the-art methods can be achieved.	3
"MLP-Mixers and ViTs are more difficult to train than CNNs. This study shows that SAM is very effective for training MLP-Mixers and ViTs, and attempts to understand why. These could all be important contributions to computer vision. I am convinced that SAM works well for ViTs and MLP-Mixers. But this paper is insufficiently rigorous, leaving me unconvinced of why it works. I don't think its suitable for publication in its current state, but it certainly could be after revisions.
"	2
"In overall, I vote for acceptance for this paper but I think it can be further improved. 
In the experiments, ResNet-SAM results are promising, the claim could be stronger if other vision transformers can be evaluated as ViT has been widely argued for its inefficient architecture design.  



 "	3
Overall, this paper is well writen. The overview of SAM and the explanation of some observations are clearly presented. Besides, the authors demonstrate the effectivenss of SAM in various tasks by showing a large amount of experimental results. The experimental part is comprehensive and convincing. The major drawback of this paper is that the SAM is an existing idea, even though the authors use it to overcome the issue brought by heavy pre-training and data augmentations. Therefore, this paper is like proving the effectiveness of an existing idea by doing many experimental validations. 	2
"I recommend this paper for acceptance. The proposed idea is simple and novel, the paper is well written and the empirical evaluation is well executed. 

---
**Post-rebuttal update**

I thank the reviewers for the rebuttal and I keep my rating of 8. 
Congratulations for the nice work!"	3
"[Recommendation]
Despite strong experimental results and analysis, at this point, I believe the technical novelties are not significantly different from the work by Ni et al. Thus, I believe the work is marginally below the acceptance threshold. If the above comments are addressed, I’m willing to increase the score.

-----------------------------------------
[Post-rebuttal]
I thank the authors for the response, along with clarifications and updates in the manuscript. 
As the authors have addressed most of my concerns, I'm happy to increase the score accordingly. "	3
Overall, the paper proposes a simple extension to standard data/task - augmentation methods for meta-learning but justifies it with rigorous theory. The theoretical results are non-trivial extensions/combinations of existing work. The effectiveness of the approach is evident from the extensive empirical evaluation. The contributions are strong, albeit limited to the meta-learning research community.	3
"This paper proposes a novel task-augmentation method of MLTI and shows the effectiveness of proposed MLTI through the extensive simulation results and theoretical analysis. The proposed MLTI can be applied to both optimization-based and metric-based few-shot learning methods. Adding some experimental results would make the readers to better understand the proposed method.
However, I believe the idea of this paper is valuable for few-shot learning field, and I recommend to accept this paper.
"	3
"Overall, the approach is described well enough to understand the approach, and is emperically shown to result in decent performance gains in the low task data settings for which it is intended.  The theoretical sections corroborate this, but I found them hard to follow.
"	3
"Overall, the paper tackles an important problem, namely explaining generalization via an experimental study and using a novel idea of ""image functions"".  The results indicate promise, however the fact that the experiments do not provide a comprehensive view such as showing the statistics for all or many layers and having more points to establish the correlation in Figure 7 make the paper a little premature.  Also the description of the details is somewhat unclear, and more importantly there isn't a good discussion of how this approach relates to previous work.  It is claimed that the evidence shows algorithmic stability but this isn't justified directly, and the reader also isn't clear whether the results show variance reduction or some other idea that leads to generalization."	2
The idea could be potentially interesting, but I found the writing and presentation is a bit confusing.	2
The empirical findings reported in the paper are not insightful and the paper makes claims that are heuristic and not justified. 	1
"Overall, I found this paper extremely vague and confusing, and struggled to follow the message the authors try to convey. 
In the current state, I do not think this paper reaches the standards of a venue such as ICLR."	1
It is difficult to pinpoint the novelty of the proposed compression algorithm as it stitches various ideas from various areas (ensemble, multi-head, FGSM) that have already been proposed. 	2
I am on borderline as there are specific concerns outlined above regarding contribution of multi-head this simple idea, presentation of the ensemble-aware loss, results when bpp is high, etc. But I also believe deep codes is a promising direction and we shall encourage more work in this field and may go with a lower bar. If the authors can address my questions, I will lean to accept.	2
In general, this paper solves a classic problem with deep learning, which is interesting. While there are several issues (see the main review section), which prevent the reviewer from scoring it high with the current status.	2
Overall, the paper was interesting because of the in-depth analysis and insights. However, I have a few concerns as stated in the main review. I would like to hear the answers to (3), (5), (6), (9) from the authors. 	2
"Correctness: The techniques use for proving the results are rather classical in primal-dual methods.

Novelty: I gave a 2 precisely because this is a rather clear extension of the original EOT what make use of classical analysis tools to further describe the performance of the EOT algorithms. There is no new tools, but rather the use of old tools on a problem that looks very similar mathematically.

Empirical: The numeric section is very small, with toy examples of low dimensions, etc. It show it works but really does not provide any additional insights or interesting applications. 

I think this paper provides sufficient results for publication."	2
"I have recommended “Accept”. 
I think the theoretical contribution is novel and valuable. I followed the proofs and did not identify any concerns with the arguments. 

The experimental component of the paper could be improved, and some additional discussion of the main results would be appreciated.  
"	4
Overall, the paper provides some novel theoretical results for the PAM algorithm that is used for solving EOT. The paper is well-written and its theoretical results are strong. However, my main concern is the scope of the contributions. I am personally not familiar with EOT applications, but if other reviewers believe that EOT is a sufficiently interesting problem, I will be willing to vote for acceptance.  	3
This work is well-written but not particularly technically innovative, and is missing a thoroughly convincing use-case.	2
"The authors propose an interesting architecture for encoding RGB inputs and predicting actions after the anticipation period. The method has the capacity to encode edge information of graphs in the transformer model, but the type of data/task does not make use of this advantage. Other approaches (AVT) provide very similar solutions by using a temporal transformer, instead of a recurrent model, to capture the temporal dependencies. The GNN analogy seems weak and distracts the reader from the main approach (recurrent transformer). It can be fixed by simply removing the GNN and edge related sections, this will save space for additional ablations and quantitative results on other datasets.

The claim that this approach surpasses the SOTA approaches is weakly supported by quantitative results. The slight overall increase in performance cannot be attributed to explicit edge learning (as can be seen in the results). It is unclear which component(s) of the proposed architecture is the cause; it could be the added squeeze and excitation block, or bypassing the bottleneck representation (by not using a [CLS] token). The paper is not ready for publication in its current state; however, additional ablation studies, experiments on additional datasets, and removing unnecessary sections can significantly increase the paper's chances for publication.
"	3
I recommend to accept this paper as the technical contributions are quite strong. However, its appeal to the wider audience in ICLR is a minor concern.	2
The overall idea of using message passing for constructing graph representation for video action anticipation. But the proposed edge learning method has a minor influence on the model performance. Therefore some of the arguments made by the paper are not fully supported.	3
All in all I think the paper tackles an interesting problem, but its presentation lacks clarity, which makes the paper hard to follow at times and difficult to compare to current art. Also, the experimental validation is lacking in some aspects, which should be fixed to make the paper a solid contribution.	2
Given minimal modifications from existing work, this work is missing comprehensive experiments. Experiments on real robots as mentioned as part of future work would strengthen the paper significantly.	2
The idea of combining  a differentiable physics engine for online system identification with a universal controller(UC) is interesting but the paper could improve its quality by adding more details on how the two modules work for continuous control tasks (e.g. regarding latency).	3
The paper proposes a nice framework that combines differentiable physics with UCOSI. However, employing differentiable physics for system identification has only limited novelty. The experiments are a little bit weak with only 2 simulation tasks and no real-world environments. I think the paper's contribution doesn't pass the bar for acceptance.	2
The paper presents an interesting idea, but more experiments on different systems and tasks, and possibly a more diverse set of baselines, could significantly strengthen the submission.	2
"Finding are unclear
The paper is difficult to follow
Writing can be improved
Experiments are inconclusive.
"	3
The paper is well-written, but poorly motivated. I simply do not see what we learn from the paper (in the intersection of 'previously unknown' and 'relevant'). In addition, while the experiments are well-designed, the datasets seem cherry-picked, and the models out-dated. 	1
I believe regardless  of how good some of transfer learning models perform, we need to elaborate on situations those models collapse. Therefore I think having similar papers truly exhibit weakness and strengths of these models is necessary. 	2
This paper presents some interesting observations, but the methods or findings are only marginally significant or novel.	2
In summary, I think that the setting considered this paper is quite interesting, but it would be better to perform more experiments to evaluate the performance of the proposed method.	3
In overall, the paper has some interesting results to characterize the cases that the problem of imitation learning becomes feasible. However, it seems that these are some sufficient conditions and it does not determine the boundaries of feasible cases. Moreover, in the case of the RL problem, the advantage of using the proposed algorithm is not studied theoretically.	3
I do not believe the paper is currently ready for acceptance. I believe the theoretical contributions are very limited. The proposed algorithm is potentially interesting, but I have concerns about the relevance of the proposed problem setting and how it compares to some simpler baselines. The paper would benefit from more thorough analysis (both theoretical and empirical) of the proposed algorithm as well as additional discussion about the problem setting.	2
Overall, this is a technically sound paper. Both the theoretical and empirical results in this paper are expected. But it is good to have a paper to confirm the related theory. Furthermore, considering that the authors did not manage to give a theorem on when expert data improves learning efficiency, I think it is fair to evaluate their contribution as marginally above the acceptance threshold but I would not fight for the acceptance of this paper at this stage.	2
"I can consider raise my score if the author can address my question in the ""main review""."	2
I think this is a borderline paper that addressed an important question with reasonably good performance while lacking necessary elaboration and justification on their novelty, as commented in my 'Main Review'. I am willing to upgrade if my concerns can be addressed during the rebuttal period.	3
It would be good to compare with the ECML paper approach mentioned above for scalability and performance. 	3
The algorithm under study is not described completely. Important hyper-parameters are missing. Because of this, the final results are circumspect.	2
The paper certainly takes a step forward with regards to understanding what components of RvS algorithmic frameworks matter to obtain satisfactory results - but, the paper falls short because (a) the paper doesn't present results with all types of datasets in the D4RL MuJoCo tasks (and this is standard in the offline RL literature), (b) the comparison to decision transformer isn't fully sketched out because the decision transformer framework is more general in that it offers the ability to generate a variety of behaviors at inference time by changing the conditioning variable during inference.	2
"In general, I like the new perspective this paper presented. In particular, the finding ""find that more complex design choices are generally not necessary"" provides a different way of thinking offline RL. Therefore, I prefer to accept at the moment."	3
The paper gives a good overview of different variants of behavior cloning. The contributions to the discussion may be helpful for future research. However, my feeling is that there is little reliable insight provided. Therefore, I think it is also possible that the benefit is small.	1
"The paper makes the incorrect claim that the presented technique is novel and it crucially fails at finding and citing any instances of related work which are very easy to find with simple search terms like “gradient-based optimization of hyperparameters”.

I believe that this can be an honest case of an independent rediscovery of the technique by the authors combined with a lack of experience in finding related work and writing the paper in a more grounded way (e.g., avoiding vague statements). I believe that there is value in the experiments presented by the authors and I would like to encourage the authors to keep working in this problem space and improving their manuscript to get it eventually published after addressing the crucial shortcomings and finding a new angle to present their contributions.
"	1
In summary, I'd love to see improvement in this paper. Based on my understanding of the field, I strongly recommend authors analyze the pro and cons of other algorithms and investigate in-depth the issues of not having a truly adaptive learning rate. with adaptive learning rate in developing optimization methods, it is often necessary to include theoretical analysis. And, theoretical analysis and empirical study can work together to understand the proposed method better. 	2
The paper is ready for acceptance.	3
This work is practically contained in existing papers, which are ignored by the authors. Strong reject.	1
In summary, I think the proposed approach is technically sound and a nice extension of ETC, but I don't agree that it provides significant technical contributions. Experimental results are strong, but as the experimental settings are different from the original setting, I do not think the paper should claim DOCHOPPER achieves state-of-the-art results on three datasets. 	2
Overall, the paper proposes an interesting approach to answer questions over long documents efficiently. However, there are some issues with the execution, presented results, and analyses making it difficult to fully accept the main claims in the paper. 	2
Trivial method, contributing little scientific knowledge. 	2
"The research problem of this paper is the inefficient computation time of existing QA models. This paper proposes a novel machine reading comprehension model for a long document. This paper provides evaluation results of their model on four types of datasets, and some experimental results show that DocHopper outperforms other baselines models. However, some evaluations results (results on ShARC and HybridQA) are insufficient to show the validity of their idea since these datasets are not designed for long-document QA. Also, it is unclear which experimental setting the authors used to compute the runtime of their model and baselines.

Since my concerns are from the unclear description of this paper, I'm willing to increase the overall recommendation score if the authors provide a more clear description."	2
To summarize, this paper addresses the HPO problem by transforming the hyper-parameters to trainable parameters and including them in the training process of the network. This approach can speed up the optimization time and allow the hyper-parameter optimization to happen in one single training session. Overall, the idea is interesting but the experiments are not convincing.	2
As mentioned in Main Review, the contribution of the paper is not enough. The novelty is not strong, since it is not the first model. The theorems are trivial and meaningless to describe the reparameterization, since the specific mapping is given. The experiments are not comprehensive enough considering the dataset and baselines. Moreover, the method itself is not explained well. 	1
In summary, the proposed method is ok but this paper ignores important previous works, which makes it hard to appreciate the contribution.	2
This paper proposes ideas that are already existing in the literature, therefore it should be rejected. 	1
This paper works on combining the pre-trained models from self-supervised learning, and the proposed method is well-motivated and reasonable, though not technically novel enough. However, experimental evaluations are not convincing enough and lacks comparisons in many situations.	2
The main concern comes from the fine-tune manner in the testing phase, which may cause cheating for the model from the validation data. The key contribution needs to be further clarified as the applicable scenario for this method seems to be limited.	2
"1) As mentioned in the appendix, the ensembling method seemingly does not work on the linear regression evaluation but only boosts the performance on the K-NN classification. How to explain that phenomenon? Why not use the linear probing or finetune protocol to evaluate the representation? Also, I suspect the superior performance is highly dependent on the self-supervised methods, e.g., Barlow Twins，MoCo, SimcCLR, etc. Since the feature extractor is the ultimate goal in self-supervised learning, it would be more convincing if authors can propose an approach to train end-to-end in a self-supervised manner. 

2) Does the proposed method only boost K-NN classification performance? If so, I think the contribution of this work is not enough because K-NN classification is only a very limited downstream task for self-supervised learning. In addition, the proposed method need to be trained based on a pretrained model, and be fine-tuned during test stage. Compared with the cost of time, the improvement is trivial. "	2
"The overall approach of using self-supervised ensembling may potentially be interesting for the community. However, some details of the proposed method needs to be addressed and more experiments need to be done to explore potential weakness/strengths of the proposed method. I am happy to review the rating if these concerns are addressed.
"	2
The motivation of the proposed method is clear, but more comprehensive experiments are needed to verify the effectiveness of Hyperdistill.	3
Overall, I vote for accepting. I believe that the paper is well-written, well-motivated and the claims made in the paper are well justified empirically. Moreover, the content is relevant to the ICLR community.  	2
"The paper is clearly written, the theoretical justification for the paper is well-presented and intuitive, and experiments confirm the value of the proposed method.

My only criticism is the comparative lack of care with which the distilled dataset is analyzed; this stands out particularly in contrast to how the  rest of the technical difficulties are addressed."	3
I think the topic is novel and impactful and the idea is novel and interesting. Experiments show the effectiveness of the method. Hence, I recommend acceptance.	3
Based on the weaknesses stated above, the handcrafted prediction function is the key but there may be better alternatives. Additionally, other subgraph-based GNN can be easily ported over to solve the SED problem.	2
Considering the strong empirical performance and the theoretical guarantee, I recommend an acceptance. My only concern is the possible limited technical innovation in Eq. (6), which desires more in-depth analysis and discussion. Besides, it already appears in previous papers and a discussion is lacking.	3
I think there is still room to improve the paper. The model seems reasonable, and the experiments show promising results under specific settings. However, the technical novelty, strategies of generating training data, and the ability to generalize the model to large graphs are not well discussed.	2
In general, the idea is novel and interesting. The authors also provide proof of the properties of the proposed metric. Basically, SUBGRAPH search and matching is an important problem. The work well validated the effectiveness to use GNN for this problem. I weakly champion the acceptance. 	3
"The authors provide a good insight on how the adversarial imitation learning algorithms fail. The sensitivity to minor differences between expert and agent samples does impede the learning of agent because it may be pushed to learn how to imitate the details instead of other more important features, and thus less robust to the noise of data. Given that, the authors introduce auto-encoder to handle this issue, and the experiments confirms the claim by showing its better performance over other baselines.

However, I still have some concerns regarding the writing, mostly the lack of important details. The experiments may need to conduct in another round with a more recent and powerful RL agent, which should be easily available now. I also hope to see if the authors can evaluate their approach in more complex and/or real-world environments. The theoretical understanding of the proposed approach is also worth further investigating."	3
The method is intuitive and performs strongly across in an extensive imitation learning evaluation. Their analyses are also convincing that their. I, therefore, recommend acceptance.  	3
A new auto-encoder based reward function was presented. Theoretical and experimental justifications were insufficient.	2
The paper is an interesting approach to IRL, using minimax games and autoencoding to enhance training.  While the approach appears novel (IRL is not my main area), the experimental results, while generally positive, are a little mixed in terms of which approach is preferred (especially VAE vs non-VAE). 	3
The main theorem seems interesting but can be problematic in the way it is interpreted. The experiments are designed in a way that does not support the theoretical claims. Since authors corrected the statement which fixes the condition of n and (especially) h, I raise the score to 5.	2
"===============================
General comment
===============================

The transition between the general result of Theorem connecting the values of the neurons and the prediction accuracy on the one hand, and the result of Theorems 7.1 and 7.2 could be made more clear. It would be better to explain in plain english somewhere that you are interested in (1) deriving a bound on the prediction accuracy based on the value of the neurons and (2) show how that bound can be controled from the initialization. As an example, instead of section 4, which is useless and does not clearly introduce section 7, I would put a short paragraph, in the general introduction explaining the two aforementioned steps. In a few lines, I think that would clarify the whole paper. 

===============================
Detailed comments 
===============================

Abstract:
- I would be careful (remove or expand) with the sentence “This state of affairs is in contrast with the steady progress on the theoretical underpinnings of traditional dense and convolutional neural networks ”. It is not clear to me that a lot of progress has been made on the understanding of traditional convolutional neural networks

Introdution:
 
- Perhaps provide a word of explanation on the “Weisfeiler-Leman hierarchy” 

Section 2:

- Your definition of the SBM is not very clear. I would say something like “In the setting of the SBM, the graph is built as follows. Two communities C_0 and C_1 are first created. , For any two vertices v_i and v_j, those vertices are then connected with probability q they belong to different communities and p if the belong to the same community”
- When you introduce b_0 and b_1 you should introduce b as well. In particular, the notations of section 3 (or at least some of those notations such as b) should already appear in section 2 when you introduce the model of the GNN. 
- In section 2, page 3, when you discuss the balanced loss, the connection between the average of the derivative of the loss with respect to each of the b_0 and b_1 weights and the averages of Z over the vertices labeled as 0 and 1 is not clear.
- In your definition of Z, g0 and g1 are not defined. Are g_0 and g_1 defined as in section 3? then those definitions should appear earlier
- You initially mention parameters W^{(0)}, W^{(1)} and b as the trainable parameters but then only discuss coordinate descent for b0 and b1 which are not defined. This is not clear
- In Theorem 2.2, when you describe the accuracy, as correctly predicting the label of a single sample, I would recall the definition of lambda. In fact, lambda should be defined in the statement of Theorem 2.2
- The sentence “bigger λ generates bigger gap” is unclear. What do you mean? remove or expand
- generally speaking, the definition of g_0 and g_1 is not very clear. I would start by saying that g_j is defined from f_j and b_j. Then you can say g_0(x) denote the logit so that g_0(x) = log(p_0) + c  and  g_1(x) = log(p_1) +c  where p_0 and p_1 denote the probabilities of the vertex being classified in either of the two classes C_0 or C_1
- Don’t you make your life overly complicated by taking the softmax while you only have two communities ? Why not take the sigmoid with a single output p_0=p, assuming p_1 = 1-p ?

Section 4 : 

- it does not really make sense to have a whole section to describe the paper structure. Put section 4 as the last paragraph of section 3, or add it as a subsection in 5

Section 5

- It would be good to have some additional intuition on conditions G_1 to H_2
- What if p=q in Corollary 5.3. Is that the reason why you take a>b in assumption 2.1. You should state that clearly. Why is that a problem ?
- The third remark is not very clear to me. I understand that we don’t want mu_0 and mu_1 to have the same sign as this would mean that the network is unable to discriminate between the classes but the output to the network is encoded in Delta not in mu so it is not clear to me that a small loss necessarily implies a small difference mu_0 - mu_1

Section 7

- In the statement of Theorem 7.1 I would explain the condition on mu_0 - mu_1 by adding a sentence of the form : “provided that the difference mu_0-mu_1 is large enough”, …
- The distinction between Theorem 7.2 and Corollary 5.3 is not clear. In particular, I want to make sure I understand the following: What do Theorem 5.1 and Corollary 5.3 assume in terms of the training ? Do they assume a 0 loss. This should be stated somewhere. The ambiguity also comes from the fact that you say that Theorem 7.2 refines Corollary 5.3. in what sense is Theorem 7.2 a refinement of Corollary 5.3? From what I understand Theorem 7.2 is just the application of corollary 5.3 to the dynamics of the neurons. 
-  The sum in the second probability in the statement of lemma 7.3 is unclear. I guess you mean the sum over all neurons whose initialization satisfies G1 and G2 ?
- Proof of Theorem 2.2. Why can you assume that P(\ell(x) = 0) is 1/2 ? doesn’t that depend on the probabilities p and q ?

Section 8
- Figure 3 is not very intuitive. Why not add axes labels? Also why not indicate on the figure that n increases with the rows and h with the columns

A couple of additional typos and comments 

- page 3, right above Theorem 2.2: “It is easy to see that Z > 1 if prediction is wrong “ —> “if the prediction is wrong”
- page 3, right above Theorem 2.2: “Then we update other parameters” —> “Then we update the other parameters”
- In the statement of corollary 5.3, you should replace the “i-the” neuron in each line with ”ith neuron”
- section 7: “In this section, we prove our main result that with high probability ” should be replaced with something like “In this section, we prove our main result which states that …” or “according to which ..”
- Section 8, first paragraph: “is able to recovery” —> “to recover”

"	3
In this work, the authors provide the first provable guarantees for the performance of GCNs for semi-supervised community detection tasks, which is highly significant. The presentation and notations used in the paper requires quite a bit of work to make them easier to follow for readers. The experiments were unclear to me as well as lack discussion or analysis. My recommendation was split between 5: marginally below the acceptance threshold and 6: marginally above the acceptance threshold and ultimately I decided to go with the former.	4
The main result of the paper is interesting, novel, and can motivate follow-up work. The presentation, on the other hand, should be improved.	4
"I am absolutely no expert in the NAS area, so I can only provide an assessment of a reader that is not at all familiar with the related work. However, I find it that this paper tackles a very important problem of general cross-task vision architecture search and makes important steps in this direction by providing an evaluation test-bed and methodology that experimentally validates the claims. In particular, Tab. 2 confirms that the proposed method finds several well-performing network architectures for semantic segmentation, while ‘Cls+seg’ entry in Tab. 3 works very well for both semantic segmentation and ImageNet classification. I see this as evidence that the proposed methodology is indeed capable of finding versatile neural network architectures. 

Based on what I can conclude based on the manuscript I see this paper as a very solid contribution, and I think this paper should be accepted. I hope, however, that the other reviewers will be more familiar with NAS and can provide deeper insights (and spot potential issues that were missed by an “untrained eye,” in case they are present). 

** After rebuttal **

Thanks for answering my questions in the rebuttal! All my doubts were clarified. I have read other reviews and didn't spot any major weaknesses. I am retaining my rating (accept). "	3
"The idea and the experiments of the paper are good, but some components need more explanation. Also, comparison with existing work needs to be more specific.
"	4
The NAS benchmark (NAS-Bench-MR) contains valuable large-scale experimental results and be used to benefit other NAS research. The proposed NCP method is sound but kind of naive. The writing of this paper needs to be further revised for both method's details and organizations. Considering all these conditions, I would suggest accepting this paper to ICLR but would not champion it. 	3
The ideas presented in the paper seems incremental and the paper is not well written to be accepted at ICLR.	2
I believe that the paper is interesting (although the approach is extremely simple) and could provide insights into how architectures and engineered biases can be beneficial for behavioral cloning. Unfortunately, the presented results are not statistically significant. Additionally, I believe that the paper could do better in substantiating the claims about why this approach should perform better. This could either be done through loss function visualizations, mathematical arguments (bottlenecks and compression might be applicable), or other quantitative reasoning. I am happy to change my view if these improvements are provided.	3
Overall this work addresses a well known problem (copycat problem) and builds towards training better imitation learning policies with temporal information. I enjoyed that the method is simple and effective on the NoCrash benchmark, but I find the justification and analysis a bit incomplete in its current state to be recommended for acceptance.	4
"The authors proposed a combined approach to address the ""copycat"" problem in imitation learning when learning with a history of observations. This approach is new, the problem is interesting but the paper lacks satisfactory investigation of the problem itself and how the proposed method can solve the problem."	2
The paper is well organized and technically sound. The results should be interesting to the community of theoretical deep learning.	3
This paper uses the novel analysis tool to study the approximation bounds for 2-layers ReLU networks and is technically solid. However, the presentation of the paper is a little bit hard to follow and I've spent about 15 hours checking technical details of the main paper and supplyments... 	3
This paper extends prior work on infinite-width NN approximability for bounded Radon semi-norm to finite-width NN approximability on bounded sets, which is a more reasonable setting and strengthens the classical Barron norm bounds.	3
By the introduction of R, U-norms the authors show that their approximation bound is tighter than bounds in the previous papers, and meaningful in more instances such as finite-width neural networks. The novelty of this paper is high. Overall,  I think the results in this paper are significant. 	3
"The reviewer  found the paper very well written, and tackling an important problem. I like the ablations done in the paper, and the focus on ""constructing datasets"" for learning task-induced representations."	2
The contribution and novelty are both very limited.	2
This paper proposes a framework called Task-induced representation learning (TARP), with the claim that having task supervision during representation learning is a lot more beneficial than unsupervised, task-free representation learning. Moreover, TARP is able to use supervision from a set of multiple, perhaps diverse tasks so that the representations can benefit from different kinds of guidance and thereby encode features that are useful for downstream tasks. Through a set of experiments on DMC, VizDoom, CARLA etc. the authors show that TARP representations outperform usual unsupervised learning approaches. While the paper is well written and the discussion is interesting, it is well known that task induced supervision will induce more relevance compared to unconstrained, unsupervised learning, and I feel the paper is not strong enough because it does not compare to more relevant baselines such as DREAMER, Task Informed Abstractions.  	2
In summary, I think the paper is interesting, but importantly, I did not find many of the findings surprising enough to recommend for acceptance. There are additionally two main issues with the paper in its current form. First, the contributions section in the introduction should, in my opinion, be re-written (see disadvantages section above). Second, experimentally there are results on only a single training-testing transfer pair for each of three environments. I would expect a more thorough investigation of multiple of these pairs to assess the robustness of methods that make use of task information. 	1
Currently, I think the paper is a good one for ICLR. I incline to accept it. I am also open for further discussions and adjusting my scores if needed.	3
The paper is well written and the presented architecture is intuitively appealing and supported by reasonable theoretical results. The experimental results are interesting but their significance is unclear since I am not convinced that the LSTM baselines are strong enough, based on prior work.	4
"The proposed method is clearly presented and thoroughly demonstrated in terms of prediction performance across many tasks. However, there is surprisingly little investigation as to whether it learns representations at multiple timescales or how important these are to its success, and there is some uncertainty as to the sensitivity of results with respect to the hyperparameter $\Delta t$. My score reflects these weaknesses but could be improved if they are sufficiently addressed or rebutted.

UPDATE: Following the authors' responses to this review and others, I have improved my evaluation of the technical significance and my overall recommendation. "	3
"Overall, I recommend accept conditional on satisfactory explanations to some of my questions -- it's an interesting, relatively principled derivation of an architectural gadget for long range sequence modeling, building on the recurrent network modeling approach rather than the popular attention based approach. I like the idea of deriving the architectural updates from ODEs which have the desired properties you want. I thought the theorems were not that strong, but fine in terms of giving a little motivation for the architecture. Overall the experiments were good, though they tended to only be on rather small datasets. 


UPDATE:

Thanks for the clarifications, I updated my correctness score and still vote to accept."	3
Overall, I like this paper. I would like more empirical results on data distillations.	3
This paper tries to reveal the underlying reasons of performance gap between CA and DE models, and a KD method is further proposed to improve the performance of dual-encoder model, while the analysis on the gap cannot guide the design of new student matching architecture or new distillation method.	3
They have theoretically analyzed that the DE models are expressive enough and the gap should not exist between DE and CA models. They found the gap is because DE models are less generalizable and proposed an effective distillations algorithm to further reduce the gap. Taking these advantages into consideration, I think it is good paper to be accepted.	3
This is a well written paper, however the contribution is not significant, due to multiple recent works already having covered this idea pretty well.  The theory presented is also neither informative nor surprising.  Some of the empirical analysis is certainly interesting, but I don't think it is surprising enough to warrant publication by itself.	2
The entity-monitoring problem as evaluated in the paper does not pose the challenges outlined in the intro. I am not convinced that the proposed method is a better alternative for associating/clustering objects compared to using pre-trained representations on data from the corresponding domains.  	2
The general idea of replicating traditional algorithm designs in a deep learning architecture is sensible the lack of detail in the method description and unclear experimental design and evaluation means that the paper is not in a publishable state at this stage.	2
To summaries, the current submission does not situate the method well in the literature because some closely related works are not covered in the paper. The evaluation is also not convincing in terms of how the method can handle real-world challenges and lacks evaluation of real-world data. I therefore vote for a rejection.	2
"### Verdict

This paper developed an extension of the Euclidean graph-based NNS from Prokhorenkova & Shekhovtsov (2020) to hyperbolic space. Although the problem is relevant, the paper as is lacks considerably, both in terms of theoretical and empirical results. Namely, it does not address the sparse case theoretically, and the theory that is developed borrows heavily from Prokhorenkova & Shekhovtsov (2020); the necessary analytic constructions/proof techniques necessary to bridge the gap are fairly straightforward, and cast considerable shade on claims of theoretical novelty. Moreover, empirical results lack considerably. First, non-toy results were given only for the sparse setting, not the dense setting (which was the only setting for which theory was provided). The uniformity assumption and assumption of small dimension was not addressed in the application section. These assumptions were critical for a proper theoretical analysis, and should in some way be handled when the theory is applied. Only a single baseline was given for Section 5.1, and claims of hyperbolic graph-based NNS efficiency (over the Euclidean counterpart) were made by testing on 1 dataset with 1 graph construction. A considerably more thorough evaluation is required. Trials/mean/standard deviation were not reported, despite this being crucial for graph-based NNS. Only a single graph construction was considered throughout the entirety of the empirical evaluation, despite the fact that the tested setting (hyperbolic graph-based NNS) does not have a previously established state-of-the-art graph construction. All of this taken into account, I must recommend a reject rating for this paper.

### Updated Rating

In their revised paper, the authors have assuage some of my concerns as they have been laid out above. As a result, I have updated my rating to a 5/10. Given that the changes were fairly considerable, I hesitate to recommend acceptance until they have been carefully evaluated; moreover, I believe more can be done to address the concerns reviewers have, specifically surrounding empirical results."	2
Overall the insights of the paper are important but their validity in the practical domain should be better explained in the paper.	3
Interesting and potentially impactful work, but I would like to see more comment on the validity of the assumptions and improved empirical evaluation before raising my score.	3
This paper is an interesting work to analyze best-first-search graph-based NNS algorithms in the hyperbolic space.	3
I think this is a solid and interesting paper.	3
"I opt for a rejection, since I struggle to see a clear contribution w.r.t. the literature. Other attacks that do not need any optimization have comparable efficacy to the ones scored by BMA and MAMA, but without all the effort that is needed for training the meta learner. Also, the table is reporting misleading results in the delta column, since it is computed against the original evaluation, and not w.r.t. the strongest attack against a particular defense.
Also, these techniques only adapt themselves on adversarial trained models, limiting the scope of the meta learner efficacy."	2
"In the perspective of proposing an adversarial attack algorithm, the author has conducted detailed experiments to support the method. However, the contributions of this paper are the improvements to the existing framework.

Hence, I support a marginal acceptance of this paper."	2
"The paper provides only small improvements in performance. But since this is at minimal computational cost, this is not necessarily an issue. Given the importance of effective adversarial example generation in properly assessing the risk in usage of ML solutions, any contribution to improving the state of the art is welcome.

The authors draw a distinction to a similar Learning to Learn approach from 2020, establishing that though their approach is not therefore strongly novel, is distinct and has merit in its own right.

Hence I would recommend this paper for publication.
"	2
The paper proposes a novel approach to tuning an LSTM-based music generation model using a learned reward function in addition to music theory-based rewards. Adversarial inverse RL is used to that end. This addition leads to an improvement in the generated music. However, the quality of the generated music does not hold up to other recent work for polyphonic music generation. There are a few other minor issues in the paper listed above. Thus I rate this paper marginally below acceptance threshold.	3
The main contribution of this paper is an adaptation of inverse reinforcement learning techniques to the music generation domain. But the discussion of the music domain is confusing (from both a musicological and technical modeling perspective) and so much of the potential value of this adaptation is lost. The empirical results are also underwhelming.	1
The paper provides a novel approach to generating music and opens up numerous avenues for follow-up research. Impactful concepts and impressive results in this paper, alongside the general lack of publications on this topic, will make this paper a relevant contribution to the venue. The text, however, needs substantial edits to improve clarity and incorporate details of the proposed model – which, I hope, the authors will be able to do during the rebuttal period. I, therefore, tentatively recommend accepting the paper.	3
"Considering the advantages and disadvantages of this paper, the reviewer recommends weak accept. If the authors can verify their method on ImageNet, the reviewer will raise the rating.

After reading the authors' rebuttal as well as the other reviews, I lower my rating because I don't see the experimental results on ImageNet as my expectation. "	3
Based on the novelty, I tend to reject the paper. 	1
"The proposed training framework which jointly targets PU classification and conditional generation is novel and sound.
But the novelty is limited and the lack some key experiments to show the improvements.
So I tend to reject the paper."	2
"This paper proposes CNI-CGAN to jointly exploit PU classification and conditional generation, so as to address the label sparsity problem. I feel that the high-level motivation is reasonable, but I still have some concerns:
1. I think the motivation of this paper need more explanations. For the label scarcity problem mentioned in this paper, I feel that some recent semi-supervised learning methods are also applicable. For example, Semi-supervised learning under class distribution mismatch (AAAI 20), Safe deep semisupervised learning for unseen-class unlabeled data (ICML 20) also tackle the out-of-distribution data in semi-supervised learning. Besides, they are naturally deep model which do not need any extensions. In this sense, my question is, why adopting Multi-PU learning (needs extension to deep model) rather than these semi-supervised methods?
2. The authors wrote ""In classification, a common setting to utilize unlabeled data is semi-supervised learning (Miyato et al., 2018; Sun et al., 2019; Berthelot et al.,2019), which usually assumes that the unlabeled and labeled data come from the same distribution, ignoring their distributional mismatch. In contrast, Positive and Unlabeled (PU) Learning (Bekker & Davis, 2020; Kiryo et al., 2017) is an elegant way of handling this under-studied problem, where a model has the only access to positive samples and unlabeled data."" Firstly, as mentioned above, there are some recent semi-supervised methods which can well handle the distribution mismatch. Moreover, the authors claim that PU learning can elegantly handle this problem, which I cannot  fully agree. Note that in the setting of classical PU learning (e.g., uPU and nnPU), they also assume P data and U data come from the same underlying distribution, therefore I feel that the nnPU method deployed in this paper cannot well handle the  distribution mismatch problem.
3. For estimating the transition matrix \tilde{C}, usually it is a not an easy task in label noise learning. Some assumptions are usually needed, such as anchor point assumption (see Are Anchor Points Really Indispensable in Label-Noise Learning? NeurIPS 19), etc. However, I do not see such pre-set assumptions in this paper, so I doubt about the estimation quality for this matrix.
4. For experiments, many recent PU learning works are not compared. Besides, another important work ""On Positive-Unlabeled Classification in GAN"" (CVPR 20) should also be discussed and compared. Therefore, I think the empirical study should be enhanced."	3
This paper introduces and proves the representation bottleneck, which is a common  phenomenon in the DNNs. This is well-written and easy to understand. There seems to be some shortcomings in the experiments and conclusion, but the paper provides good insights to others.	3
This paper discovers and theoretically explains an interesting representation bottleneck in existing DNNs, and relieves the bottleneck by proposing two novel losses. As the discovered bottleneck reflects the common difficulty to learn middle-order interactions, this work may provide a new direction to train DNNs. 	4
"Theory is excellent but there is space for improvement in the experiments. 

----------------------
After rebuttal: experiments are extensive and insightful enough. "	4
Overall, the paper makes an important evaluation study to further the understanding of the representation capacity of DNNs. The novelty is in using the multi-order interaction proposed in Zhang et al. (2020) to understand the complexity of interactions in DNNs and the proposal of different loss functions to encode/penalize interactions of specific complexities. Hence, I am leaning towards a positive score for the paper.	3
The paper is lagging in experimental results with the recent baseline. Also, the contribution is limited compared to SupSup.	2
Overall, the contribution over related work seems limited to me, but I am open to feedback from the authors if my initial perception is not correct.	3
"Please find below some of my concerns:
1. First of all, please state clearly in the introduction in which aspects your paper is different from [Wortsman et al., 2020]
2. In page 3, you mention for the first time the term: GN-setting. Where is the meaning of 'GN'?
3. Please clarify the following statement (section 3): ""It then generates the mask M by setting the top s fraction of scores in S to 1 and the rest to 0"". How is this fraction chosen: heuristically, or do you use any criteria?
4. Regarding the experimental settings:
- State clearly, for each dataset, how do you define the tasks. How many basis tasks do you have for each dataset? How comes that for PermutedMNIST you have 250 learned tasks and the maximum could be 784 (Table 1)?
- What is the relationship between number of masks and number of basis tasks: you have one mask per one basis task?
5. Plots 2,3 and 4 are not clear
6. Figures 3 and 4: min(jMj; 25) basis tasks. How did you choose 25? Why do you use this criterion for basis tasks?
7. Please use a standard evaluation methodology in order to be able to interpret your results
8. Compare your approach against (at least) the following related methods: Packnet, Piggyback, [Wortsman et al., 2020], Ternary Feature Masks (TFM) [Masana et al., 2021]

M. Masana, T. Tuytelaars, and J. van de Weijer. Ternary Feature Masks: zero-forgetting for task-incremental learning. Proc. of CVPRW 2021, Workshop on Continual Learning"	2
While the proposed idea seems novel and the results seem to indicate good efficiency in terms of model footprint, the results show performance degradation to the baseline, and I feel the submission has rooms for improvement for acceptance due to the narrow focus on comparison to SupSup, as pointed in the main review. In overall, I do not think the paper reaches the acceptance threshold.	3
No theoretical advances, but some clever thinking and solid results. Happy to recommend acceptance.	2
"The considered task, i.e. generative models as a data source for representation learning, is **very interesting for the community** and has recently received increasing attention. The experiments in the paper appear sound and provide **interesting insights**. My major concern is about **the contribution regarding the latent space transformations**, as a very similar strategy was already proposed by Yang et al. Nonetheless, the findings of this work are interesting, so currently I am leaning towards accepting the paper.

**Post-rebuttal**:
The authors successfully addressed my main concern about the paper by Yang et al. so I update my rating from 6 to 8."	2
"The paper is a step forward towards training only using synthetic data, a long-standing goal that has been of interest since the introduction of deep generative models. 
Moreover, it introduces and evaluates a novel way of combining generative data and contrastive learning that is shown to be very promising.
With the rapid advances in representation learning, this submission providing a strong and comprehensive empirical study is likely to be valuable to the community."	3
"The direction that the authors pursue in this paper is challenging and novel. However, the results are mainly negative.

For negative results, to be useful to the community, imho either the results should be suprising or should offer some insight. I have given a reject rating mainly because of the empirical results.

Please see the weaknesses section above. If atleast the first two bullets points can be addressed convinvingly by the authors, I can reconsider my rating.

**Update after Rebuttal**:

I thank the authors for their additional experiments and discussion. I've upgraded my rating from 3 (Reject) to 6 (Lean Accept).
The rationale for my rating change:

* The authors confirmed that the SimCLR baseline is lower than the reported results, due to the low resolution (128x128) of the generated model and reduced training time.
* The models (baseline and generative) are trained for 20 epochs using cyclic learning rates so I think the results might be transferrable to full training.
* The authors have shown more promsing results on pretrain LSUN + transfer on Stanford Car classification.
* ImageNet100 results as far as I see are provided only in Section 4.1.2 and 4.3 as a diagnostic tool, so the comparisons are fair.

The authors have taken a first-step towards usage of generative models for unsupervised representation learning. Even though, the results are somewhat underwhelming on ImageNet itself, this work can serve as a promising and strong baseline for future work in this topic.

**For the final version**
If the paper gets accepted, I encourage the authors:
* To report results for full training (100 Epochs), at least with the ""Real baseline""  and their best setting for generated data
* To open-source their code.
* To explicitly state that the latent transform is applied only once, as compated to pixel-level transformations, which are applied twice. This might be a trivial detail but can be extremely important to reproduce the authors results."	3
The proposed idea is reasonable but didn't conduct a comprehensive study comparing related works. 	2
The paper is well motivated. The experimental design is fine, but it's unclear how the evaluation metric, the likelihood, is computed without marginalizing the hidden vectors. The presentation is fine.	2
This paper provides an insightful exploration of how one can use an LVM as a speech generative model.  Although not completely achieved here, I feel that this paper shows some intriguing progress towards that goal, and towards speech models with semantically meaningful latent states.  I think many researchers in this area will find interest in all the engineering that was put to work in this paper, which might also be useful outside of this particular problem.  On the downside, this paper doesn't feel like it addresses any deep scientific questions (although it touches on some near the end), and it mostly reads like a todo list to get clockwork VAE to work with speech.	2
The proposed algorithm is interesting and well-supported by experiments. My concerns are that a more intuitive description should be added to make the motivation, rationality, and insight of algorithm design easy to follow.	3
"Overall, while the paper presents some nice ideas, is technically novel, well-organized and easy to follow, I have some concerns primarily about the relationship to prior work that make me lean towards rejection at this stage. 

########
Edit after rebuttal:
During the discussion with the authors, they have addressed some of my concerns by running additional ablations (removing the clustering component), comparisons to transfer learning methods, and experimenting with deeper backbones. They also have commented on the scalability of their approach, and discussed pros and cons w.r.t efficiency at training time vs deployment for transfer learning vs meta-learning methods. Overall, my view is that test-time efficiency is important for certain applications, which makes meta-learning methods a good candidate, and the particular variant proposed here seems especially suited at the important problem of handling heterogeneous data and outperforms previous MAML-based meta-learning methods that were designed for this. Based on these, I updated my score from a 5 to a 6 to reflect that the paper is slightly above the publication bar in my opinion.

To strengthen the work further, I recommend comparing against recent methods developed for heterogeneous data on common benchmarks. Some examples are the following:
- Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes. Requeima et al. NeurIPS 2019.
- Selecting Relevant Features from a Multi-domain Representation for Few-shot Classification. Dvornik et al. ECCV 2020.
- A Universal Representation Transformer Layer for Few-Shot Image Classification. Liu et al. ICLR 2021.
- Learning a Universal Template for Few-shot Dataset Generalization. Triantafillou et al. ICML 2021.
- Universal Representation Learning from Multiple Domains for Few-shot Classification. Li et al.
- Memory Efficient Meta-Learning with Large Images. Bronskill et al. NeurIPS 2021."	3
Given the above points 1, 2 and 3 I think the paper is marginally below the acceptance threshold	2
"Overall, it is a well-written paper with interesting and, at times, conflicting ideas. I deeply appreciate the amount of work invested by the authors. However, I am not convinced about the approach’s utility.

---post rebuttal update

Thank you for the clarification on the 'shortcut tunnel assumption. The assumption results in estimating the path cluster assignments through feature cluster assignments. This means that the information related to the path cluster assignments is already present (linear or non-linear relationship) in the feature cluster assignments. Thus, I am not entirely convinced that the model is getting any additional information to justify the improvement in the results because of the addition of the path cluster assignments.

The additional experiments using the heavier backbone strengthen the result.  I agree that on the curated 'meta-data set the proposed approach has an edge over prior methods. The authors have incorporated these results in the updated draft.
"	2
Object detecton accuracy also highly depends on network depth. Although the present experiments well support the effectiveness of the proposed method in terms of efficiency, further experiments comparing methods with various Neck networks sharing depth will provide another perspective demonstrating the advantages of the proposed method.	4
This paper gives another insight into the balance of backbone and neck in the particular problem of object detection. It proposes a framework called giraffeDet, which proves very effective from the experimental results. The main contribution, in my opinion, is about the exploitation on the connections in skip-layer and cross-scale. But this contribution is minor. Also FLOPs is an important metric in efficiency, but the more straightforward inference time will add onto the claim about the light backbone + heavy neck.  	2
"Overall the paper provides an interesting alternative to object detection architectures. However the current experiments do not directly support this claim (mixed change of network architecture and the detection head), and it is unclear if the proposed architecture is compatible with existing techniques (e.g., DCN). The proposed method is also at risk of low run time. 

My current rating is a weak reject. In the rebuttal, if the authors show solely replacing the backbone (E.g., replacing ResNeXt-101-DCN-FPN (ideally, the largest EfficientDet or Swin-L) with the proposed network) can improve a state-of-the-art detector (e.g., CascadeRCNN, GFLV2 etc.) with acceptable time cost, I will increase my rating."	3
In summary, the paper should be rejected as the contribution on physics-inspired networks is marginal and the claims of the paper are not backed-up by experiments. Just adding a model and comparing the learning curves is not sufficient to isolate the impact of the proposed model learning approach.	1
Modeling the Hamiltonian is a promising direction but I do not feel a proper comparison to related approaches has been made here, such as to MBPO or to Dreamer's asymptotic performance.	2
Despite the technical soundness, the proposed model seems to outperform existing models in the experiments. However, due to the concerns on technical soundness, I think this paper is not ready for ICLR yet. Thus I recommend rejection.  	1
"The paper does a good job in sharing new insights of policy optimization, and in connecting the policy optimization with classification. However, the two main claimed contributions are somewhat weak.
Rethinking policy improvement (Proposition 1 and 2) is not the original idea of this paper. Reinterpreting PPO by hinge loss is also not the idea of this paper. At the same time, the improvement analysis is based on some very strong assumptions. Therefore, I vote for rejection for this paper."	2
"Pros: novel idea and interesting analysis of reinforcement learning as classification
Cons: restrictive assumptions of the theoretical analysis; the experimental analysis could be given more space given that it actually exists in the appendix. 
Starting from 5, but happy to update the scores to acceptance if there's a convincing answer on the cons during the rebuttal. "	3
The paper mostly achieves its goal, which is to provide a theoretical justification for PPO-clip. I currently lean towards acceptance, although the paper suffers from some presentation issues and imperfect justification of some algorithmic/theoretical design choices.	3
I found this paper to be valuable to the deep-RL community as it presents an interesting generalization to a commonly used algorithm with theoretical justifications. 	3
I'm leaning towards an accept due to the use of the novel idea of soft-ball projection and the demonstration that the proposed method works on a number of different standard datasets.	3
Paper seems to lack enough theoretical novelty at this stage. My current vote is Weak Reject, but I also like to see other comments and feedbacks.	2
The proposed method is novel and interesting to me. The empirical results seem promising. Overall, I think the paper has a decent empirical contribution, and it also has the potential to offer a unified theoretical explanation for adversarial training. Unless I misunderstand some key parts of the paper, I recommend accepting. 	3
The paper makes a theoretical contribution by extending the work on WDR for AT to other AT approaches. It shows promising results of their unifying framework on common example problems. Therefore, taking into account the weaknesses listed above I vote for a score marginally above the acceptance threshold. However, I am willing to increase my score if the weaknesses listed above are addressed, especially a thorough comparison to the WRM approach by Sinha et al (2017) (~500 citations, oral paper at ICML 2018). 	3
In general, this paper presents some solid contributions to distributional robustness including both theoretical and experimental results. This paper is acceptable to the conference, but some weaknesses should be fixed, including the theoretical justifications and more comprehensive comparisons.	3
While the combination of text and financial stock data via BERT is an interesting task, the paper presently has multiple serious issues in novelty, clarity, and experimental rigor.	2
This paper proposes a new and powerful method to solve an intersting problem. However, the technical contribution is limited and the presentation quality needs to be greatly improved to meet the bar of ICLR.	1
The presentation of the paper is below standard, it has no contributions, and the experimental setup is not convincing.	1
Highly relevant new dataset with recent baselines run on it to get an idea of SOTA performance. However detailed analysis of the baselines on the dataset is lacking	1
"Overall, I think miniF2F is an important benchmark that could help the community advance the research of theorem proving. I recommend accepting this paper.

===========================
I would like to maintain my old score of this paper after reading the authors' responses and other reviewers' comments."	4
The main value of this work is in the set of formal Olympiad problem statements, which have not appeared in other datasets. There is little technical novelty in the ML algorithms and analysis of their performance, especially in comparison to [Han et al. 2021], on which this paper heavily relies.	2
The paper makes a solid step forward on creating cross-system Olympiad-level formal math benchmark and should have profound benefit on the community with its continual development.	3
"# Post-rebuttal update

The authors have clarified most of my questions, and I am raising my score to weak accept. My original conclusion remains similar, but improved clarity makes it a stronger submission. Recap:

Why accept: 
* The empirical phenomenon of mitosis is interesting and shown to be robustly present in some cases, but absent in others. 
* The paper is well-written and clear (given recent updates and replies to my questions, and a few more clarifications that I expect to see in the final revision).

Why weak: 
* Exact conditions for mitosis are still not well understood. In the rebuttal the authors have updated the condition set to 100% training set fit + ""optimal"" [what is optimal?] weight decay), which is good, but I still don't think this is precise enough, and most notably the paper does not contain enough systematic experimental evidence to confirm it (e.g. mitosis measurements in wide FC networks, sweeping the weight decay from 0 to X, and observing that the rate of mitosis increases respectively).
* Exact implications of mitosis are also not well understood / demonstrated (e.g. what is the relationship between mitosis and generalization? Can a network demonstrate improved generalization with increasing width without mitosis? Can a network without improving generalization with width exhibit mitosis? etc).

# Original Review


The paper presents an interesting empirical finding, but in my opinion does not explain very well why mitosis occurs, or what are the implications of it, which is why I am leaning to reject it at this time. However, many of my concerns could be addressed in the rebuttal, so I might change my score."	2
The mechanism of representation mitosis seems to be interesting and is worth exploring. The paper provides satisfactory explanations and ample demonstrations to help understanding the mechanism. However, some of the explanations are at best heuristic in nature and would be much stronger if they could be supported by rigorous theory. I am inclined to go with a weak reject for now.	3
This paper identifies a potential interesting phenomenon in deep neural networks. However, I think the empirical evidences need to be strengthened to support the claim well.	2
"I think this is a nice paper that demonstrates an interesting empirical effect and proposes a promising connection to the behavior of overparameterized deep neural networks. However, in its current form, I think the submission is marginally below the acceptance threshold. In particular, I am bothered by the fact that the effect appears only after training ""using state-of-the-art procedures"" and is absent for complicated datasets (the example in the paper being ImageNet).

Overall, I think this paper makes a nice set of observations and performs careful scientific experiments. With a little more clarity in terms of how general this effect is, I would be happy to accept it to ICLR.

### After Author Responses
I appreciate the comments of the authors. I still am concerned about how general the mitosis effect is, since it doesn't occur in simpler models. However, with the additional clarity, I have increased my score to be at the acceptance level."	1
The model capacity can be a big concern and the technical depth is somehow limited. 	2
Overall, I think this paper is marginally below the acceptance threshold.  I like the idea of the ensemble with random gates in deep neural networks. Although this method can beat other ensemble methods, it is less attractive than basic adversarial training since the latter takes less training time and is flexible to be tuned with different perturbation strengths. If my concerns are addressed, I would like to raise the score.	3
The idea is quite interesting and the results are good. However, the current version lacks the theoretical analysis and other key empirical comparisons as mentioned above.	2
Although applying control gates is an interesting idea, I have some concerns about the RGN method. I have listed my major concerns in the main review.	3
Well written. Good empirical Analysis. But in some places the paper lacks the justifications and proper reasoning. Experiment/results section need some better explanations.	3
In general, the observation that abnormal points have a large discrepancy between the prior association and the series association is very interesting. A novel Transformer based model is proposed to model this discrepancy. Experiments demonstrate the effectiveness of the proposed method. However, it is unclear about the convergence property for the proposed min-max strategy. Besides, there are many ambiguous details that hinder the readability of the paper.	3
"Overall the paper has goods results and seems sufficiently interesting and novel.
"	3
"Updated after the discussion
- Novel idea of using the degree of self-attention as a metric of anomalousness.
- New approach of positional encoding based on Gaussian kernels
- Comprehensive empirical comparison with alternative methods

----- Original summary -----
- Novel idea of using the degree of self-attention as a metric of anomalousness.
- Unclear and unjustified descriptions.
- Lack of the critical baseline (major issue in this domain). "	3
"* Although this paper introduced the federated kernel regression framework which might be useful for developing new algorithms for model agnostic federated learning, however, the application of the proposed method is limited.
* Although the theoretical analysis part is rigorous, the analysis of the experimental section is not clear enough or even missing."	3
Overall, model agnostic FL area is lack of theory results, so this work has its own novelty. However, the setting is somewhat toy, so I am afraid it may not be very helpful towards understanding model agnostic FL in practice (e.g., multiple clients and complicated models that do not have closed form solution.) 	4
The paper presents an interesting theoretical analysis of co-distillation for two different kernel regression models. The empirical evaluation confirms the theoretical findings. The constrained setting on the one hand allows for strong theoretical results, but on the other limits the practical insights that can be drawn from them. Here, a broader empirical analysis would have improved the significance of the contributions.	3
This paper presents interesting thoughts that were substantiated via a theoretical exercise on a simplified setting. But unfortunately, in both theory & practice, this is still pretty much a work in progress. Given the huge gap between the theoretic and real-world setup, it is not clear whether one can apply the insight derived from the theoretic setting to another distant real-world setting -- please see my points in 1-5 above. Thus, while this could be the beginning of an interesting theory, more work is needed to complete the idea. As a matter of fact, the current experiment implies the proposed distillation is not working. In addition, I believe the authors have not considered the compute/communication cost of this distillation scheme.	2
"The main issue with the paper is that the degree of novelty seems limited, as it seems that in practice the proposed method is just Feudal RL optimized via ES instead of gradient-based methods. Also evaluation is insufficient.
"	2
Although the proposed methods are novel, it seems that the experimental results need to be improved. 	3
Based on the above review, the novelty and the significance are questionable. The statements should be improved for clarity. The experimentation should be improved to support the claim of the paper. 	2
This paper proposes a straightforward application of evolution strategy on hierarchical learning. The novelty is limited. It misses an important prior work [Hierarchical Reinforcement Learning for Quadruped Locomotion, Jain et al. 2019], which also uses ES (ARS) on hierarchical learning for locomotion. The prior work seems to achieve better results than this paper. In addition, comparing to HIRO, it is hard to conclude that the proposed algorithm outperforms the state-of-the-art.	1
Based on the innovation, clear model description and solid experimental results, I recommend for accept. 	4
"The paper is well written, easy to follow, and presents a very useful explainer-agostic method to correct the OOD bias of subgraph important scores by GNN explainers. Small questions are made to make sure
1. Even this explainer-agnostic method can correct the OOD biases, the original GNN explainers didn't take into account those biases. Does this cause any problem?
2. What the surrogate graphs are like? They are recovered original graphs containing the given subgraphs?
"	4
In summary, I like the idea but I believe the paper does not make a good job of conveying the message to the readers.	4
From my perspective, this paper is interesting in some aspects. The causal view on the OOD problem in DNNs is a new idea.	3
Because I think the main idea of the paper is really interesting, I will give 6-7 grade. I am more than willing to increase my score if more large scale experiments are added and writing is improved.	3
I have been working on related areas and I have read this paper carefully.	2
The algorithm seems to work well, but I am skeptical about the alleged problem it solves and that it really addresses it. Writing is the main issue.	3
"Overall I think that this paper could be a nice contribution. I think that the experimental section can be improved, and some important references are missing. Upon addressing my concerns I am willing to reevaluate the paper.
"	3
"The paper is well written and the proposed solution is simple but there are still concerns about the wide applicability of the method, and also lacks comparisons with very related baselines.

"	2
The studied problem is important and beneficial for many graph-learning tasks. The proposed method is rationale and technically sound. More details about the experimental settings could be clarified in the evaluation section.	3
This paper is well written. It proposes a novel end2end algorithm DIR towards causality-based graph representation learning. The algorithm is based on an intuitive principle that is theoretically justified by the authors in this paper. Meanwhile, the extensive experiments also confirm the effectiveness of DIR.	3
The authors address an important problem and provide comprehensive experimental details, including qualitative analysis, quantitative results and thorough ablations. However, further experiments on more challenging domains would strengthen the paper. Also, the writing can be improved.	3
I will be weakly recommending rejection since it is unclear to what extent the method works because of the integration of HRL and adversarial environment design, and to what extent the same results could be achieved through the simpler direct combination of the existing approaches in both fields.  In addition, it seems that the baselines could be made stronger, and I am not convinced by the empirical results.	2
The proposed algorithm presents an interesting and novel way to adversarially generate a curriculum for training low-level RL policies. The algorithm seems general to some extent and performed well in two tasks. However, the writing needs improvement (a lack of details, missing conclusion, etc.). There should also be a discussion on the limit of the current algorithm, which relies on the performance of a specific plan-planning method that provides the true structure. I am willing to raise my score if these problems are addressed in the revision.	3
Despite its rather incremental nature, I like the overall paper. Mainly the questions w.r.t. experimental details and the smaller scale evaluation tasks prevent me from putting it above the acceptance threshold. If some of my issues/questions are addressed, I am happy to improve my score.	2
see main review	3
"Overall I think this paper presents promising preliminary results on zero-shot neural architecture search for object detection. However, the novelty seems to be limited (comparing with Zen-NAS) and the experiments don't seem to be thorough enough. These downsides prevent me from recommending acceptance at the current stage. However, I'll be glad to raise my score if my concerns are addressed properly during the author response period.

====

Post-rebuttal update: I decided to raise the score to 6 based on the empirical results presented in this paper. I still think the novelty comparing with Zen-NAS is not that large and the challenges of the current approach (e.g. cannot deal with complex connections in the FPN, basically can only search for relatively simple backbones) prevents me from assigning an even higher score to this paper."	2
"I appreciate the contribution of this paper, but I prefer to reject this paper for the above weaknesses. If the authors solve my concerns, I would like to raise my rate.

==Final Decision==\
The author's feedback has resolved my concerns except the ranking. Though the author did not resolve ranking concern, I decide to raise score to 6. I hope the authors would provide the results about how the multi-scale weight ratio affect the correlation for more heads in the final paper."	4
The explanation of the maximization part or two-player game part is not super clear to me. 	2
The paper is an extension of MCR. It appears a bit complex. 	3
While the paper proposes an interesting and promising method to learn an encoder-decoder with a novel two-player minimax game, the paper is not yet well enough written and lacks some experimental results that allow for comparison with state of the art methods.	3
While the proposed method has potential and there are some positive aspects of the work, the manuscript is not ready for publication due to deficiencies in several aspects, including: motivation of the need for the general approach, justification of the specific choices in the approach, other clarity issues, relationship to BERT and transformer-based language models, and certain aspects of the experiments.	2
While there are strengths in terms of the model novelty, there is some weakness too. The paper can be further improved from various angles such as strengthening the experiments section by comprising with some closely relevant works including conducting some downstream application analysis such as document classification performance.	2
The paper mixes OT and LDA in a rather original way. Nevertheless, this idea is not new and has been already proposed in other papers	3
"The idea of learning latent topics for text documents using bag-of-words embeddings is not quite new given that a few previous papers describe similar methods. However, it's still empirically useful to introduce concrete optimization methods to learn such a model. This paper presents extensive experiments on news and web pages to show the superiority of the proposed method, compared to other state-of-the-art methods. 

"	2
My main concern comes from the deduction from equation (10) to equation (11). The authors may add more explanation to show the equivalence.	2
"This paper introduces a mathematical formulation to recover the OT map that maps two distributions for doing generative modeling and (unpaired) image restoration tasks. The main weaknesses of the paper are the presentation and experimental validating the approach.  It's hard to understand what is different with respect to previous work also doing OT map estimation. What's particularly interesting about this mathematical formulation that enables it to work on the original dimension?  Experiments are only on small images which implies that the claim of working on high-dimensions and large-scale datasets is a little too strong.

--
After more discussion with the authors and after considering the updated manuscript I'm raising my score (marginally above the acceptance threshold)."	3
Overall the paper is well-presented, with novel theoretical and empirical results. I would like to recommend accepting this paper. 	3
The idea of this paper is very clear, the theoretical analysis is rigorous, and the experimental results can fully illustrate the performance and effectiveness of the method. But there are a few problems, The c-transform $u^{c}(x)$ on page3 should be $u^{c}(y)$, The computational complexity of the algorithm is not reflected in the text, what is Q-embedding? what is the relationship between it and the generated mapping?	3
"I would make a recommendation for accepting this paper.

Overall, I think the idea of the paper is interesting and also leads to improved results on benchmark datasets. The main concern is that there could have been more discussion  on why this method works for these real world datasets by connecting them to the synthetic setups presented in the paper."	4
The paper provides an intriguing idea to improve group distributional robustness, but at its heart it is just an *ad-hoc algorithm* that *seems to work*. It is unclear what really is being optimized since there is no clear loss function that is minimized. The results also seem a little unconvincing since some of the real-world datasets are altered and not used in their original form, e.g., CivilComments, which raises questions regarding if the approach even works at all, irrespective of the issues regarding lack of theory behind the algorithm.	2
Overall, I think this is an interesting and solid paper.	3
This paper provides a practical algorithm to the broad field of distributional shifts, domain adaptation, and fairness.	4
"I am certain that the proposed method can indeed be useful and of interest for many in the field, and the method appears to be largely novel. However, I am concerned that it will not be very impactful, the way it is delivered now. To be clear, I don't believe that the mathematical approach is necessarily complicated. However, the provided explanations are over-complicated, and the approach is insufficiently motivated/contextualized.

To address this concern, I suggest that, for instance, redundant definitions (as Def. 4?) are moved to the Appendix (if at all needed). Instead, there should be some paragraphs in Section 2 that puts concepts like submodularity in context. Can you map it to a concept in Bayesian DL?

Regarding the baselines, I think it is important to compare the method to more recent advances in order to fully grasp its capabilities. As is shown in the Related Work, the field has evolved a lot since deep ensembles were first proposed. Question to the authors: how come more baselines were not considered?"	3
This paper is with interesting idea and solid execution. I recommend a weak accept before the questions are answered.	3
This paper has done a great job in making Deep Ensemble Bayesian theoretically, despite being short of empirical verification. So, currently,  I recommend a weak acceptance for it.	3
The paper proposed a circle-based method for rule learning, which is an interesting view. However, some claims (e.g., shorter rules are better) are not convincing, and the motivation of such a design is not clear. There is no substantial difference between the rule learned by this method and the previous works, and the search space of rules is even smaller (e.g., compared to NLIL[3]). I recommend the authors clarify these issues. 	3
In summary, although the idea of using cycle representation is interesting. The proposed method is unnecessarily complicated. And the current draft lacks too many justifications to the components in the proposed method. At this point, I would recommend rejection but I'm happy to raise my score if the authors address the aforementioned concerns.	2
"The approach put forward in this paper is not convincing since it's convoluted (W1), neither well-argued for and nor convincing (W2), partly unclear (W3), there is no analysis or theoretical insight (W4), and practical benefits and costs ultimately remain unclear (W5).
"	2
The paper proposes to treat rule learning as cycle learning in knowledge graphs. The approach is very interesting and novel with solid experiments showing its effectiveness. I would suggest accepting this paper. 	4
"Following the aforementioned consideration, l recommend to accept the paper ""6: marginally above the acceptance threshold"""	3
"The main novelty and contribution of the paper lies in using the Goldback's conjecture to design the OS-block for an efficient kernel size design. However, this key factors suffer from the above two points and the manuscript needs to be revised to address these issue. Authors can either theoretically show the benefit of prime decomposition or experimentally verify the efficacy of the prime decomposition of the even number ""e"" over other trivial decomposition of ""e"" such as (e1, e2) or (o1, o2). "	2
"I like the elegant decomposition to cover all RFs. Yet, much related work is missed, and clarity can be improved.

These issues seem fixable."	3
Possibly solid contribution, but high uncertainty on the originality of the contribution due to lack of references to the surrounding literature.	2
Overall this is a great paper, and I believe clearly meets the bar for ICLR.	4
"My rating of the paper is based on the following issues:

1. Weakly justified claims
2. Practical difficulties of the proposed formulation
3. Missing details
4. Literature review and positioning"	3
"Summary Of The Review
- Fairly good paper, but overall quality could be improved
- Significant contribution, but might be slightly oversell
- Convincing experimental validation but limited to only 3 experiments, with few additionnal results"	3
Novelty is limited and some motivations are not clear.	3
See the section above.	4
A decent paper with good amount of novelty for a computer vision conference where faces are particularly studied but of limited novelty and interest for the ICLR community. 	2
The experimental evaluation is unsatisfatory.	3
"Overall, the approach is sounds and experimentally performs well. However, the novelty and significance are somewhat limited. Further, exposition can be improved and claims (specifically claim 2) are (at least) imprecise and miss-characterize prior work. Finally, some ablations that would have been useful are missing and should be included. Given these concerns, overall, I view the paper as being slightly below bar for acceptance. I would be happy to revisit my assessment, and potentially increase the score, if authors can convincingly address the raised concerns in the rebuttal. 

Authors have addressed a number of my comments in the rebuttal, and promised to addressed others (claims) in the camera ready revision. As such, I am raising my score and believe the paper should be accepted.  "	2
The paper presents a clever combination of domain adversarial training for adapting the components of object detection. The experimental results obtain SOTA for some combinations explored. The weakest part is of the presentation is the introduction, and should be reviewed. Also,  although the set of references included have a good cover of the area, the concepts presented in the text are not connected to pre-existent concepts as it would be appropriate to be.	3
Considering the simplicity, novelty and effectiveness, this paper is of high quality and above the acceptance bar.	3
The approach highlights the shortcomings of the previous methods and systematically tries to resolve them. There are a few issues with the comparisons and clarification is needed for one of the claims.  Overall the writing and structure of the paper is good. 	2
"My biggest concern for this paper is the lack of novelty and depth of this paper. I still give a positive review because I got sick of papers that boast about their fancy design but in reality cannot outperform a simple network. 


==========================================

Based on other reviewers' comments and the authors' responses, I would like to keep my original score."	2
Since the idea of PointMLP is quiet simple and it can be considered as a refinement of PointNet++, I do not think that the novelty is very high, although the experiments show some interesting results.	2
I think this work introduces an interesting finding that small modifications in PointNet++ make dramatic changes in the results. But, more analyses would be needed to clearly see how each component in the proposed method (post-MLP, geometric affine, residual connections) affects the accuracy and inference time. 	3
Well written paper with impressive experimental results and clear description of design choices.	4
Given the impressive results obtained with diffusion models, the paper addresses an important and timely topic. The proposed approach significantly outperforms other methods in terms of quality achieved with distilled diffusion models at small number of evaluation steps, which, ultimately, is one of the main goals on this topic (even though the distillation process might be relatively expensive). The presentation is very clear and easy to follow and different possible design choices are well motivated and intuitive to understand. The different choices are evaluated in well designed experiments and comparisons are very convincing of the benefits of the presented approach and also include very recent works on the topic. The supplementary material also contains interesting material on different formulations and connections between different approaches. Thus, I recommend accepting this paper.	3
"The proposed method is novel and effective in reducing the sample time of diffusion models, and experiments on several datasets well support its effectiveness and better performance than prior fast sampling methods. Still, I have some concerns about the method regarding computational cost and generalization to larger datasets. Thus, my initial rating is a weak accept. I’m willing to increase the score if my concerns can be addressed. 
"	3
"The distillation idea in itself isn't entirely novel, as it was proposed by [4], too. However, I certainly acknowledge that this work shows how to do it in an elegant, scalable and high-performance manner using a progressive protocol. Overall, the paper addresses an important problem and shows promising quantitative results. Hence, I am leaning towards recommending acceptance. That said, some weaknesses remain. I would be willing to raise my score, if the weak points were addressed in a satisfactory manner.

[4] Luhmann and Luhmann, ""Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed"", 2021."	3
"The main idea of the paper is interesting and the experiments are quite
convincing.  However, I feel that the motivation behind this improvement is
quite superficial. I think that the authors should better motivate their study,
especially putting an emphasis on the limitations of classifier guidance in
score-based generative models. I will increase my score if the authors address
my concerns."	2
"Although the idea is new in the context of diffusion models and the experiments support the major claim to some extent, I think 1) the proposed method is of less practical significance and less flexibility, compared with the prior work on classifier guidance, 2) the experiments can be improved to better support the claim and to make the results more impressive. Thus, my initial recommendation is not accepting the paper.
"	2
The paper has some novelties but they are not well-motivated and not well backed up by the provided discussion and evidences.	2
In summary, I give positive comments to this work though some aspects of this method can be improved.	4
The paper analyzed an interesting representation drift problem of a popular continual learning method. However, the paper doesn’t explain the loss functions in detail. The reviewer couldn’t justify the correctness of the applied contrastive learning method.	3
The overall idea of this paper is simple but effective. The motivation is well illustrated and the results are convincing. It can be improve with more insightful discussion and analysis on the outcome of the feature distribution (similar to Fig1) to show the proposed method can indeed separate the representation of old and new data along with the training. 	3
Overall, the paper presents a strong analysis of a potential failure mode in continual learning, proposes a simple method as a fix, and compares thoroughly against baselines to validate their results. 	2
Overall, this paper proposes an interesting phenomenon in online continual learning. However, the relation between the solution and the insights are not strong. Besides, the empirical improvements are marginal. 	2
Concern with the motivation: the setup is not that different from online learning after all.	2
In summary, the original contribution is not very clear. The authors have ignored to discuss comparisons with a branch of data stream mining reseach that has provided similar conclusion. And the empirical evaluation is on simple datasets, and may need further evidential support from more complex datasets.	1
The paper addresses a real need, but more insight is needed.	2
This paper proposes a novel learning setting called anytime learning at macroscale (ALMA). The proposed idea is simple and technically sound. Extensive evaluations are conducted. However, the contribution is unclear, and the presentation needs improvement.	2
If this result is correct, it would be a breakthrough in machine learning. Yet, we need to make sure the proofs are correct. The current version is a bit difficult to read and check. Notations are not introduced well and the proofs are referring to other recent results. I recommend authors make a self-contained paper.  I also need the author's responses on my failed sanity check to understand the proof better. 	4
"Overall I think the results presented in this work are solid and interesting. There is some missing related work, but I think it is fine since the results in this paper have no significant overlap with the existing results. I did not check the proofs in this paper in detail.
"	3
Overall, despite some flaws in presentation, I think the paper poses a meaningful contribution which warrants acceptance.	3
In summary, I think though the paper proposes AL framework as an alternative to BP, it is actually a simple extension to a previous work, and does not proposes substantially new ideas. Some details are missing, and experiments are not extensive enough to cover state-of-the-art architectures.	2
I would recommend this paper to be accepted. While there are several issues, the empirical results are strong (particularly the LSTM reduction in epochs). I think is a lot more to explore with the dynamic layer accumulation and gradient isolation, too, that would be interesting to other researchers.	2
With some more clarification on how you ended up with this methodology and a clear algorithm for how to implement AL, the reviewer would be happy to accept the manuscript.	3
"(1) The proposed method can be optimized locally, and achieve competitive results. The proposed framework can be used for CNNs, RNNs, and transformers. The idea is interesting.

(2) More analyses about the motivation and the necessity of inverse transformation for Y to latent space are needed.

(3) The analyses and discussions about related works, such as multi-label classification and ladder networks, are missed.

(4) Some experiments are suggested to support the authors' opinions (e.g. (2) and (3)) if possible.



"	3
I feel the authors have done a good job highlighting the motivation of such a dataset, steps of creation of the dataset from Visual Genome, paying attention to the generalizability of the approach, discussing the major applications of the dataset in detail. The github link also provides holistic understanding of the work. This dataset will help the research in the field of CV. Once they overcome and address the current weaknesses of the dataset, it will become even better dataset asset. 	3
This work introduces a good way to study the effects of distribution shifts. Specifically, this work proposes a framework called Metashift, which contains systematic annotation about the differences between different shifts. However, the major concern is about the constructed tasks in the experiment. More explanation/discussion can be included to eliminate the question.	3
I am inclined to accept this paper because of (i) the simplicity of the approach to generate the datasets and (ii) the usefulness to the community. However, I would be more confident with acceptance if the authors would address the weaknesses of the paper (se above). 	3
"
After reading the paper for the first time, I figured a sure ""accept."" However, I then noticed they always used it for ""smile,"" besides when ""not smiles"" (i.e., any attribute but ""smiles"" were transferred. Not that this should take everything away from the work. Nonetheless, I believe the authors are not transparent enough in this aspect of the paper-- do we need K models for K attributes? The authors attempted to show ""black hair"" and ""bangs"" but it was unclear what was important in the results.

They showed many image montages throughout: it is good to highlight the 'take-home message.' Too many times was I zooming in trying to understand the differences between samples. A better explanation of the variations is needed.

I cannot help but see this work as extremely high in potential: a potential that far surpasses the quality of the current version. More insight, edge cases, and failure modes would be a plus. More attribute types would be best. Which attributes work, which doesn't, and how does this relate to the motivation (i.e., global vs. local). 

If this was FG I would suggest ""accept,"" but as the paper shows plenty, but does not explain enough. It's only being used on smiles for most of the time, with no emphasis on this, just seemed suspect (not that it was intentional, but just was a letdown when I finished the first pass, was excited about the paper, and then realized this which was a total let down).."	3
Overall I tend to reject the paper. Please refer to the Main Review.	2
I think this paper has limited novelty and insufficient evaluations. The detailed comments are listed above.	2
"This submission proposes a novel test-drivern directional latent mapping network with a new semantic direction consistency constraint for semantic-level facial attribute transfer. By treating the facial attribute transfer problem as an attribute semantic feature projection problem, a semantic directional decomposition strategy is proposed. The reported experimental results seem good, especially comparing to the SOTA like StyleCLIP.

I have some concerns about the experiment settings and I feel some more discussions are needed. On one hand, only a few attributes are considered in the current submission, more experiments with various attribute transfer are expected; on the other hand, it is expected to know whether this method can still work when more than one attributes need to be transferred."	2
"Overall, the connection between perturbed samples and cycle monotone maps is interesting. However, the paper lacks clarity and intuition about why this is good or useful.  From the experimental side, the paper does not compare to any non-OT domain adaptation methods and is challenging to apply in real-world scenarios because either an OT problem would need to be solved for each new target sample (that wasn't in the original target dataset) or a neural OT would need to be used (whose results were less significant).
"	3
"As a summary the ideas from this paper are interesting but it is clearly an unfinished work. I encourage the authors to polish their paper and improve their experimental results to eventually resubmit their work. 
"	2
Using adversarial attacks for domain adaptation is an interesting idea and has been explored before (https://arxiv.org/pdf/1810.00740.pdf). This paper proposes generating adversarial attacks and using OT to label the target domain. However, I think the current version is not well written: the theory can be greatly simplified and the experimental settings are not clear. Therefore I would recommend rejection.  	3
Although this paper studies an interesting scenario, I am convinced neither by the motivation behind the proposed approach, nor by the experimental results.	2
In general, the proposed method is somewhat novel. However, the method is not very well justified and some parts of the method are unclear. 	2
Overall, this work introduces a neat idea, that is taking the interpolation of two anchor samples to augment the minority class. But overall the paper is lacking in depth and the experiment results are not very convincing. 	3
"Class-imbalanced domain adaptation is an interesting and important problem in the UDA field. The strategy of interpolated adversarial samples (IAS) is new and interesting, however, the overall novelty of the proposed method sounds not much high for a top-tier conference. In addition, the experiments do not fully verify the effectiveness of PAT especially for IAS, thus I vote for ""weak reject."""	3
The novelty is limited and some statements should be explained further so that the advantage of the proposed method could be more obvious.	3
This is a neat and well-thought-out idea. However, as I've expressed in the main review, some of the claims need further justification and the paper clarity needs to be significantly improved. I am willing to update my score if the authors are able to provide a convincing response!	3
The contribution of this work is rather incremental and there is not enough discussion with related work, the experiments are not convincing either. 	1
The paper addresses in important challenge in lifelong learning, i.e., to be able to generalise across a variety of classes that are learned sequentially through different tasks, meeting both new classes and drift, when the distribution of one class changes. The method appears sound and has a strong theoretical background. However, I feel that the paper could be improved significantly in clarity, readability, and presentation of results. It's not clear to me what are the main claims of the paper, and while some results appear to be good, the final set of results (fig 4) are difficult to interpret. In short, this paper has a strong potential, but requires a better presentation and a better illustration of the results and baselines for the last experiments in Fig. 4.	4
The submission is a step forward in implementing approaches that can learn in a continual manner. The analytic demonstration that the loss is an upper bound is a cherry on top. However, the submission can be greatly improved if the authors focus more on the consequences of what is in fact novel in their approach (i.e. the GMM on top of the autoencoder latent), providing more experiments and a deeper discussion of the failure modes.	3
This work is well motivated. The proposed idea on reducing computational cost is neat, and it works effectively as experimentally demonstrated. The proposed wide & deep predictors for NAS helps to efficiently find the high quality architecture for the proposed network, achieving promising performance. Meanwhile, the potential issues are i) the idea of MAKPCon is mainly based on existing well-known techniques and does not seem to bring significant theoretical insight and ii) the novelty and contribution of the Wide & Deep neural predictor part can be better clarified.	2
The paper has relatively weak technical contributions and lacks latency comparisons with state-of-the-art solutions. Therefore, I would like to see the authors' response before the final decision.	2
Overall, I appreciate the paper's efforts in making lightweight point cloud processing models. This could be very useful in many applications. However, the method is weak in terms of the final performance (parameters vs accuracy) and is overly complicated. Some important details are also missing (see weakness). Hence, I would recommend rejection for the current version of the paper.	2
"I have two concerns,

1. The introduction to the proposed MAKPConv is unclear.
2. The experiments are not enough.

As a result, currently, I vote for borderline reject. But I might change my score if the authors can address my concern or other reviewers can give more positive reviews."	2
"(+) The systematic study of training deep GNNs of this work is interesting.

(-) Some claims are not well-justified. Some related works are not discussed properly. Alations need to be done for the proposed methods. The performance of DGMLP is not very strong."	2
"The theme of the paper is “Hyper-Parameter Selection and a Modified GNN for Semi-Supervised Node Classification”.  Perhaps that’s a better title.  

Overall the paper builds on the many other GNN works for this problem, incorporates the best known practices, and provides a flexible approach to achieving the state of the art results when compared to other algorithms.  However, the presentation could be significantly strengthened as outlined in the Main Review.
"	3
"This paper formally define the concepts of embedding propagation and transformation depths, and distinguish their impacts on the deep GNNs. Based on the empirical studies, some well-known guidelines are proposed to design deep GNNs. 

However, I cannot find much novelty from this paper, which seems to ensemble the existing knowledges. Some of important baselines are missing to validate the effectiveness of the proposed methods. "	1
The experiments, which compare various GNNs, provide useful information. The proposed DGMLP contains interesting ideas. However, experimental results are mixed and do not clearly support all claims made in this manuscript. The proposed smoothing level metrics do not measure smoothing across nodes.	2
This paper shares some insights on the proposed FP-DETR, but it lacks comprehensive experiments to justify the claims. After reading the rebuttal and extra ablation studies, most of my questions have been answered. I believe this paper has some merit and worth accepting for a poster. Hence, I have raised my score.  	2
Although the idea is borrowed from the NLP field, applying it to the CV field may be helpful for the community. It provides a new perspective towards the mitigate the discrepancy between pre-training and finetuning. At the current stage, I prefer to accept this work.	2
The paper proposed a novel architecture to empower a transformer-based object detector with smoother pre-training and fine-tuning paradigm. The paper is well-written and provides interesting discussion on its intuition and new perspective on positional embeddings in this context. However, pre-training helps improve the model is not a novel theory by itself. The insufficient experimental comparison decreases the strength of the paper. I am looking forward to seeing the rebuttal and willing to change my rating accordingly.	2
This submission provides a novel way of pre-training DETR's encoder part on image classification task first, and then finetuning DETR on object detection task. However, there is one novelty not fully verified. Besides, the method may have some problem in scalability. Thus, I would wait for authors' response before making the decision. 	2
Although It is worthwhile to study the scenario of training models across different hospitals based on federated learning, there is not enough theoretical contribution to either the federated learning field or the curriculum learning field and the organization of this paper should be improved.	3
see the comment above.	2
My initial score is reject mainly due to the lack of clarity and unconvincing experimental results. As pointed out in the weakness section, the assumptions, implementation details, effectiveness, and limitations of the teacher and the scheduler are not fully presented in the current manuscript. Although experimental results seem to show the effectiveness of the proposed approach for model/data poisoning attacks, I'm not fully convinced by the results due to the lack of their detail and the absence of existing robust FL methods. That said I would like to update my score once all these points are resolved.	2
Overall, I think the paper is a useful extension of some earlier works on uncertainty quantification with an eye towards statistical efficiency. I tentatively recommend it for acceptance with some reservations.	2
"This paper considers interesting and important problem, and the proposed approach is broadly evaluated; but as mentioned in the main review, I lean to reject though I'm willing to adjust my understanding and score.



==== POST-REBUTTAL ==== 

Thanks for the additional discussion and experiments; I think the response mostly addressed my concerns so raise my score to 6. To my understanding, the key message of this paper is that finetune a base predictor with respect to a desired length metric (along with a coverage constraint) if we want to improve the efficiency (in the same length metric) of the final conformal predictor given fixed sample size; this is likely to be true and is more convincing given the additional experiments. As one minor note, [R1] is applicable to both classification and regression as demonstrated in their experiments. "	2
In general, the paper is well-written and the idea is appealing. As mentioned in the main review, it would be good to understand more about the solutions that the proposed method finds and what (if anything) they might be sacrificing (e.g., conditional coverage). It would also be good to compare to somewhat more recent baselines (e.g., CHR for regression, RAPS for classification).	3
Interesting paper. Some minor issues w.r.t. comparison with existing work. 	4
It seems that the theoretical part of the paper does not support the motivation well. If this problem can be solved, I'll raise my rating.	2
This paper provides a solid contribution to the area of min-max optimization, with their theory supported by experiments in training GANs.	4
This manuscript proposed an interesting one-sided MVI assumption for nonconvex-nonconcave minimax problems, and developed some algorithms with polynomial-time complexity to first-order stationary points. The theory requires certain unrealistic assumptions (e.g., large minibatch, bounded iterate), and overall is not surprising. Empirical results are poorly evaluated and are not convincing. 	2
This paper presented some interesting theoretical analysis of Adaptive optimization algorithms, but part of its theory and experiments are not convincing enough, and the overall writing needs to be improved.	2
This paper extends the previous direct S2ST Translatotron model with Translatotron 2 model, which significantly improves translation quality (BLEU), the naturalness (MOS) and robustness (UDR) of predicted speech compared to Translatotron,  through a few modifications. The paper also proposed an approach to train targets in source speaker’s voice and prohibits generation in a different speaker’s voice to mitigate spoofing audio artifacts.  Experimental results on S2ST datasets of different sizes (relative large and small) and multilingual S2ST demonstrate the significantly improved performance from Translatotron 2 over Translatotron, and also the effectiveness of the proposed approach to retrain the source speaker’s voice in the translated speech.	2
"Not enough novelty.
Some components not explained clearly,"	2
Contribution and novelty are moderately significant. Experiments firmly supports multiple claimed improvement to the previous model. Key technique are completely left out as reference to other work, making out-of-domain readers difficult to follow.	3
"The paper is well written.  The experiment results of  improvement in  translation quality , naturalness and robustness are convincing. The voice retention capability in the speaker turn setting of Translatotron 2 is appealing.  However, the overall  paper’s novelty is limited mainly because the paper brings in the best systems available in literature together to improve speech-to-speech translation systems performance. Thus, the paper might be more appropriate for a speech technology focused conference. 

"	3
"The paper describes an incremental improvement over a previous paper on the same topic.  However, the experimental details are not adequately described and the claimed benefits relative to a cascade system are not justified.

I remain somewhat sceptical of the practical utility of the overall framework.  Nevertheless, following the authors response to questions and updated paper, I have increased my score to ""marginally above"" the threshold.
"	2
"The evaluation is not satisfactory as it is very synthetic and seems to be tailored to benefit the presented approach. Therefore, I lean towards rejecting this paper. 

Update after the author's response: I am happy with the additional experiments and increasing my score to a 6. "	3
Weak reject. Though the approach is novel and significant and seems correct, the clarity of the description of the approach and clarity and correctness of its evaluation are lacking. I would be willing to raise my score if the authors provided a much more clear description of the algorithm and its evaluation such that it would be possible to reproduce the majority of the results based on the paper alone. I would be willing to raise my score further if the authors provided a more thorough evaluation of the scalability of the approach beyond just neural networks of different parameters (i.e., programs of different static and dynamic lengths, more or less branches, different input sizes and splits, and different neural network architectures).	4
This paper is well-motivated and targets an important problem. The proposed method is novel and interesting. However, I have some questions regarding the justification of the proposed method and the limitations on its applicability. Overall, I am slightly leaning towards acceptance, but would like the authors to address my questions before I can solidify my recommendation.	4
"An interesting paper proving regret bounds in a meta-learning HRL setting. However, in order to get any traction on the problem, the authors seem to need to make a large number of unprincipled assumptions (and introduce many novel definitions) such that the general interest / applicability of results is in questions.

A reframing of the results in which the authors present the key contributions separately would likely result in a much more impactful contribution."	2
Overall, I believe that the formulation and algorithms are quite novel, and the hierarchy formulation covers many real situations in HRL. The paper can be further improved by adding more explanation about assumptions and main theorems. 	3
While theoretical analysis is limited to the tabular setting, the paper presents interesting theoretical results for HRL in a meta-RL setting. To further clarify the contribution of the work, I would like to encourage the authors to deepen the discussion on the relation to the previous studies.	3
Although the paper seems to be theoretically solid and presents many interesting notions, the current version does not convince me to recommend acceptance.	3
The paper is overall convincing, even though lack of theoretical insights and of computational complexity analysis reduce a bit my final evaluation.	3
The paper still needs some work to merit a publication. However, I would be willing the accept the paper if the authors do a careful revision and address all the comments raised by the reviewers. 	3
Technically sounds paper, but lacking in motivation and presentation. 	3
I recommend rejecting this article. Although predicting NN performance from the network structure and dynamics is an intriguing topic, it is not one that this research proposes. The method, as well as the experiment, are more about preliminary and experimental exploration. Apart from the rolled graph representation of the convolutional layers, there is only a little theoretical enhancement.	3
This paper provides a principled way to study the effect of example selection/sampling on convergence of SGD	3
"This work revolves around a new assumption on the gradient variance that captures SGD, Random Reshulffing (RR), and Shuffle Once (SO), but it is not limited to these methods. As such, it is a bit more universal and it even allows the authors to obtain new rates for Shuffle Once in the nonconvex setting. Some parts of the theory are not clear to me, in particular, I do not feel like I understand Assumptions 4 and 5. I think some polishing of how the results are presented could help. On the other hand, I checked the proof of Proposition 2 and it seems quite novel to me. Mostly, the results look quite good and the theory is not a straightforward modification of prior work. It has some weaknesses though, in particular, it does not guarantee any benefit of RR or SO when comparing to plain gradient descent.  
What I really like about this work is the new variants of data selection proposed by the authors. In particular, they show some promising results on ImageNet by using their new data augmentation technique and study its guarantees, although based on rather exotic assumptions. The authors also get insight from their main assumption to construct a better permutation ordering based on greedy selection. I think that this direction is very promising and may lead to improved training strategies in a wide range of applications."	3
This paper presents new and intuitive ideas, and useful results, and is a step in the direction of unifying convergence analysis for different sampling schemes of SGD.	3
The paper presents new albeit simple result that helps bound SGD convergence based on example sequence. The authors also propose two new selection algorithms and demonstrate their efficacy using experiments on MNIST and CIFAR. 	3
Overall, due to the weakness in novelty and experiments of this paper, I recommend rejecting it in its current form. But I encourage the authors to improve the paper according to the comments and resubmit it in the future.	3
The paper is well written and the presenting idea seems interesting and novel.	3
The main positive aspects of this paper are: $i$) first EBM-based model for molecules; $ii$) a model that leverages the node permutation-invariant nature of graphs/molecules. On the other hand, the main negative aspects include $i$) limited evaluation setup; $ii$) no significant improvements over baselines (the empirical gains are questionable). 	3
This paper propose a novel method of energy-based model for molecule generation implemented by GNN. However, the modification of loss function is not justified properly and the improvement is too marginal. 	2
"Overall, I think the cons are points that could be addressed and do not hurt the main merit of the paper. Thus I recommend acceptance but I would recommend addressing the cons if possible.

"	2
Overall, this is a good paper but there are some issues that should be improved, in particular, reproducibility and table for readability. 	3
"This paper shows that a simple pretraining method, the PrefixLM, using two datasets from the dataset of ALIGN (Jia et al., 2021) and the Colossal Clean Crawled Corpus (C4) dataset (Raffel et al., 2019) (optionally, along with WIT (Srinivasan et al., 2021)) achieves new state-of-the-art in various vision-language benchmarks. However, this work is narrowed to achieve the state-of-the-art performances while failing to show 1) better performance of the PrefixLM than MLM, 2) relationships with the other multimodal pretraining losses (e.g., ITM) 3) in the controlled experiment in terms of pretraining dataset and model architecture. The weaknesses W1-3 hinder this paper's recommendation since the main argument of the PrefixLM is in a vague position. 
"	3
The paper proposes an interesting and novel variant of RM that enables it to get around the memory/compute constraints of the original method. The method appears valid and appears to work but the empirical evaluation is lacking likelihood evaluation, experimental details, comparison to recent approaches and does not demonstrate that the method can scale to relevant problems. Unless these issues can be rectified, I cannot argue for the paper's acceptance. 	3
Please see the detailed comments section.	3
The paper seems to contain interesting material, however the degree  of novelty seems to be a bit incremental.	3
"While the paper is promising, given the two major concerns above, I don't think the paper can be accepted in its current form, and recommend weak reject.

---------------------------

While the authors clarified certain points, after the discussions I still have concerns over the overall presentation. Although the method demonstrates promising results, I don't feel the theory is properly explaining the improved performance, and more analysis/investigation is needed to clarify the claimed hard sample mining perspective. I still recommend rejection.

The authors have repeatedly tried to force their conclusions on me in their response. I want to emphasize that I still don't find the presentation to be convincing, and I believe the paper in its current form hasn't sufficiently analyzed an important point it touched upon."	3
"This interesting paper suggests that fiber bundles are good models of many-to-one tasks in machine learning. The authors propose a deep neural network to model fiber bundles and argue that it outperforms conditional generative models. I find the premise of the paper natural and convincing, but the numerical results, evaluations of conditional generative models, and the empirical claims about the influence of topology lacking, hence my recommendation. Overall I think the paper has a lot of potential but the narrative and the comparisons should be much more solid, especially when it comes to existing conditional generative models and diagnosing the (topological) modes of failure. My initial recommendation is thus a rejection but I am more than willing to adjust it after the author response. (I would've chosen a 4 but it is not available anymore.)
"	2
"I think this is an important addition to the literature and has the possibility of being useful for future work about explainability and disentanglement.
"	4
"The idea of the submission is quite interesting and I really like its presentation in the paper. However, the theoretical setting seems not to be applicable to real word data. Furthermore, a study on the robustness with respect to the additional hyperparameters is missing. I therefore give a score of 5.

----

**Update**

I thank the authors for addressing my concerns in the update. It seems that the good performance is rather independent of particular hyperparameter choices and prior distributions, but mostly from the guiding idea of allowing the model to be a *local* (as opposed to *global*) trivialization, as indicated by the experiments with q=1. Yet, a convincing argument is missing. In general, I still think, that there should have been a theoretical study under which conditions and  assumptions on the data Bundle Networks can be successful. 

I raise my score to 6."	4
This paper proposes a new problem, i.e., the fiber learning problem of a ML task, as well as an effective solution to it. The proposal is novel and interesting to me. However, the motivation, the clearity, and the empirical support can be improved.  	3
"I marginally accept this paper mainly due to its novelty and better experimental results. 

After reading the response, I keep my score at 6."	3
This paper has proposed a new method to address the heterogeneously distributed data problem in federated learning. However, the algorithms has some impractical components and the effects over state-of-the-art is not convincing enough due to lack of some baselines. 	2
The level of originality is low given the prior works. A new formulation lacks deeper understanding and theoretical analysis is weak.	2
"My main comments are : 

* The level or rigour needs to be improved. As a case in point, already the title suggests that the notion of ""sparsity"" 
is crucial for the proposed approach. However, i was not able to find a rigorous definition of the quantity ""sparsity"". 

* In my opinion the novelty of the proposed approach is limited. First of all, the sparse FPL problem is a special 
case of the regularized FPL problem (3) using a specific choice for the regulariser (requiring the non-zero entries 
of the i-th weight to agree for all clients. The regularised FPL has been studied in a sparse model context recently e.g. in 

* M.Yamada, T. Koh, T. Iwata, J. Shawe-Taylor and S. Kaski, ""Localized Lasso for High-Dimensional Regression"", Proc. AISTATS 2017 
* A. Jung, “Networked Exponential Families For Big Data over Networks,” in IEEE Access, Oct. 2020. doi: 10.1109/ACCESS.2020.3033817.
* Y. Sarcheshmehpour, M. Leinonen, A. Jung, “Federated Learning from Big Data over Networks,” to be presented at IEEE Int. Conf. on Acoustics, Speech and Sig. Proc., 2021.


it might be useful to compare your approach with the (generalised linear model based) methods proposed in above papers. 

* The FedSpa algorithm seems to be a straightforward combination of existing techniques (stochastic gradient descent and masking search techniques 
for sparse deep net training). 

* The relevance and usefulness of Theorem 1 is unclear. How did you use Theorem 1 for applying FedSpa (e.g., choice of hyper-parameters) 
in the numerical experiments of Section 5? Im also not sure if (5) can be referred to as ""error bound"" as it actually 
bounds the gradient of the objective function along the trajectory of the algorithm. However, the numerical experiments do not 
use this gradient as the figure of merit but the resulting accuracy. How is (5) relevant for bounding the accuracy of FedSpa? 
"	3
"A very well written paper and nice experimental section.
The paper can be strengthened with motivation of the new definition and its (formal) relation to existing definitions.
"	2
The paper addresses an interesting problem of assuring DP with a novel framework using neighbouring information of data. The results derived for the decision theoretic and Bayesian attackers are useful and provides insights. A mechanism is proposed to achieve this new DP definition but the method seems to be hastily presented and descriptions of certain concepts are unclear. This is the part where the contribution could be improved. The usefulness of the proposed framework is validated through reasonable experiments.	3
The paper is novel and well written. I recommend its acceptance.	4
The topic of the paper is interesting, but overall the paper lacks novelty and contribution, and the experiments are not carefully designed and done, and with no comparison to other models	2
The evaluation of this paper is extremely limited and uncompelling.	1
Paper does not clearly formalize the problem, misses to draw concise relationships to established, relevant prior works, and does not report the empirical results in an interpretable form.	2
The paper studies a very interesting problem but in my opinion, it lacks scientific writing and reasoning in some instances. It can be further improved if it is written in a more organized way where the objectives are defined and the error and the structure of the model are clearly reported. Please refer to my main review where I have addressed these concerns one by one.	2
This paper has a clear motivation and introduces a knapsack algorithm for NAS, yet, validates the proposed method on limited devices and has weak novelty since the latency estimation is dependent on LUT.	2
Hardware-awareness of pruning, while touched upon by some works, still opens up large potential for inference latency reduction. This paper proposes an interesting direction in optimization where each layer is assigned importance score and measured latency of each layer. Then, the overall pruning problem is formulated as a knapsack problem. Overall performance of the pruned network seems reasonable. I liked reading the paper, and I would like the paper to be in the program.	3
The quality of this paper is good. The experiments are thorough and solid. However, I think the novelty of this paper is limited.	4
The performance is promising. However, some important details of the proposed method are missing, which makes it hard to follow. Moreover, experiments are not sufficient. More experiments are required to show the effectiveness of the proposed method.	3
"I believe this is an important paper to encourage future research into heterogeneity as a component for language emergence. The paper is well-written and the authors are able to scale up the Lewis game setting without observing a decline in desirable language properties. However, the paper in its current form makes claims that cannot be substantiated in the evidence, which I find to be a significant barrier for me to recommend acceptance.

UPDATE AFTER REBUTTAL:

After the discussion with the authors, the paper has toned down its claims and added clarity about what it does not claim: it does not present an overall better language emergence setup, and the goal is to primarily address the apparent decline in structure as population size goes up in machine learning in contrast to sociolinguistic studies. Thus, I am increasing my score for this paper."	4
The paper provides a rich set of experiments looking at the effect of population size and diversity of emergent languages, and so will be of interest to many working at the intersection of emergent communication in NLP and cognitive science.	3
I have a generally positive view of the work but, as a non-expert, I saw some weaknesses in the paper.      	4
"This manuscript is on the target. It is interesting and the research is well worth pursuing. However, it has crucial issues that need to be addressed to better appreciate its contributions and scope. In particular, the results do not lend much support to an issue that has already received attention in related (non-neural) research on language emergence.


# Recommendation update

In light of the authors' changes and discussion, I have updated my recommendation. The literature review is much more complete now, and I think weakening some of the statements has made the contribution much clearer. I am still concerned about the rather weak effects and correlations reported, but now the framing is much more in line with the content."	3
The major strength of the paper is the formulation of the problem and the insights it brings to this classic task of object detection in autonomous driving domain. The models are well designed, experiments are complete and results are convincing. Although more results/ analysis can be added, the paper is self-contained in the current stage.	4
This is a well-written paper with a significant contribution towards using historical data from the route to aid in object detection along repeated routes. 	4
"  Apart from a few minor concerns, such as comparisons to
  the use of ""traditional"" HD maps, I do not see any major
  flaws with the proposed paper. The proposed technique is
  orthogonal to improvements in architecture, data
  augmentation, etc., and it seems to work in a wide range
  of settings. The paper itself is well written and the
  experimental evaluation is thorough.
"	4
"Overall I would like to see this paper in ICLR. 

It is a nice idea, a well-written paper, and I would learn something from it. There are lots of results in the paper, but most of them are simply showing that the algorithm improves scores. I would really like to see a couple more experiments helping to show *why* the scores improve. I would also like to see some stronger baselines using point cloud data immediately preceding P_c as additional input."	3
I have worked in this field and published related to this work.	2
"The paper presents a novel approach for obtaining explanations from a black box measure. The method appears sound, however, the evaluation is lacking in certain aspects. The user study has some flaws and the quantitative experiments rely on a single metric. My recommendation is a borderline reject that can be improved if authors can better argue their evaluation approach.
"	2
"Overall, I vote for rejecting the paper. Although the proposed technique performs well in both human and functionally grounded evaluation experiments, many important design choices are not well motivated. Overall, I believe that the study needs some further refinements before it can be accepted to ICLR 2022. 


I have divided my detailed feedback into two categories: “major concerns” and “minor improvements”. I am willing to improve my current score in case the authors can address points raised in the major concerns section.

Major Concern

* What are the reasons that LIME and JSLIME are performing relatively similar in comparison to the proposed FBFull and FBDiag methods on MEPS dataset (Table 1)? Does that mean that the problem at hand can be solved with LIME and JSLIME formulation as well? If so, what are the benefits and limitations of the proposed explanation techniques in this paper? 

* How can the usefulness of the analogy-based explanations be argued for when the result of user studies show that users can get nearly similar accuracies using AbE or FBFull (Figure 3)? 

* Can authors provide explanations on the effect of each of five additive components in Equation 2? 

* What are the reasons for not performing the human and functionally grounded evaluations on the same set of techniques? In addition, how can this affect the generalized statements about which explanation techniques perform best across both evaluation experiments? (For example, LIME and JSLIME are missing in human studies in Figure 3 whereas PDash is missing in the functionally grounded evaluation in Table 1)

* Why lambdas and alphas are not tuned per example and what is the effect of this on the fidelity of “local” explanations (section 5.1 - AbE hyper-parameters)? 

Minor Improvement

* Can authors provide a more detailed explanation for the problems that hinder the extension or use the work of [Zheng et al., 2020; Plummer et al., 2020; Zhu et al., 2021] for the problem at hand? 

* I see a potential problem in the additive definition of w_{x_i, y_i} (section 4.1). In the current definition, the loss cannot differentiate between these two cases:  perturbations x_i s are close to x and many y_i points are further away from y and vice versa. This can be problematic since removing and adding terms to the explained pair of instances changes the Mahalanobis distance asymmetrically (see Example 1-3 in Figure 2). Can authors confirm this and provide an analysis on the possible effect this can have on the quality of explanations?
"	2
I'm tending to accept this paper as it is well written and provides interesting and novel approaches to explain similarity-based methods.	3
Unfortunately, I do not believe that this paper meets the standard for publication. While I like the proposed theory, I am really unconvinced by the experimental evaluations. In fact, the qualitative AbE examples raise more questions than they answer, and make me doubtful that the method is really working as intended. A more careful experimental investigation, perhaps with some revision to the theoretical approach based on the problems found, would make this paper a much better one. 	2
Concluding, the problem consider by the authors is novel and worth investigating, the approach seems to be adequate, the evaluation is limited but I think the paper is worth accepting.  	4
As a summary, the overall novelty of the proposed approach is rather incremental, the search space is too restrictive and limited to be practically useful for designing practical NF architectures (only searching for a binary choice between two architectures, not accounting for other more complex bijective transformations), and the experimental results are insufficient to illustrate the usefulness of the proposed approach for improving NF architectures.	3
To my best knowledge, this work is the first work aiming to design more efficient flow models via a DARTs-like NAS method. However, there are still many issues to be addressed. This work can be made significantly stronger if the aforementioned issues are well addressed.	2
It is unclear the essential technical contributions compared to existing NAS methods.	3
I like the idea of utilizing pre-trained models to transfer the knowledge for zero-shot semantic segmentation, and I think it would be the main trend for many other zero-shot settings. However, this paper only adopts a naive method to solve the problem, leaving some critical issues to the readers. So my initial rating is borderline.	2
"Experiments section is weak. Most of the tables contain much weaker and older models for comparison (even VGG16). The biggest concern is that ViT-L has 307M parameters while the largest backbone used in comparisons is ResNet-101 which has about 45M parameters. ViT-B has 86M which is still larger than ResNet-101 but will at least be a fairer comparison that using ViT-L for image decoder.  

However, the flexibility of the approach is powerful. Lseg can dynamically handle arbitrary label sets on the fly with varying length, content, and order.

Hence the recommended rating is “marginally above” the acceptance threshold.

=====

The reviewer thanks the authors for their response. Glad to see that LSeg still shows improvements when using ResNet101 as backbone. Updated the rating."	3
"The clarity of presentation needs improvement. More ablation study necessary.


==========================================

I have read the revised version and still think it is not the final version. For example

1 In Equation(2), what does y_ij means? An image has several labels therefore y_ij is a vector? In this formula it seems it is a scalar. 

2 In the revision paper, I still cannot find the training loss. softmax is not a loss objective. A objective function should be something like

                                                                                         min f(x)

3 In comparison, although they are both ResNet-101, but ResNet-101 from CLIP is abosolutely better than other ResNet-101.

===================================

I have read the response and now think the comparison is OK.
From Figure 2 I think every pixel has more than one label. But in Equation (2) it seems y_ij is a scalar."	3
This is a well presented paper that shows convincing progress in masked image modeling pretraining for vision transformers. It should be accepted for publication at ICLR.	3
Overall, I think this is a practical method with good experimental results. However, the proposed mask image modeling loss may not be clearly verified. As its current state, I would like to rate this paper as marginally below the acceptance threshold.	2
The paper is quite well written, the method is simple and gives better performance than DINO. Ablations and analysis are quite interesting. Nevertheless, the comparison between the different approaches does not seem to be complete enough. So, it is difficult to see if the iBoT approach brings a significant gain.	2
This paper has novel contributions with both theoretical and practical implications. I recommend acceptance.	3
The paper contains novel and inspiring empirical observations, which are justified by establishing a relationship between generalization and prediction disagreement of two models learned independently using SGD.  The paper is also well written.	4
I am recommending acceptance because this paper presents a surprising, simple experimental result and provides a satisfying partial explanation which connects test error disagreement to calibration. Ideally there would be more thorough experiments and more exploration of whether the experimental result is useful, but the paper is substantial enough that it is okay if such additions are left to future work. (Because of these qualms I would give the paper a score of 7 instead of 8 if it were possible to do so.)	3
This paper presents GDE, an empirical phenomena about the generalization of deep learning, with novel theoretical justifications via the connection to calibration, and thorough experiments.	3
"This is overall a solid paper that is theoretically motivated with strong experimental results. However, the scope of the project is somewhat limited and the proposed method is incremental in nature. Furthermore, the writing could be improved and I have some doubts/questions regarding certain experimental details. My starting grade is 6. After the authors' response, I will reconsider my evaluation.

"	2
"Overall, I see that the paper has theoretical strengths and presents an interesting application for the method.
However, the methodological contribution is small.
The experimental evaluation is or seems to be flawed, as it is questionable whether the comparison is fair. 
Grammatically, the paper should be reworked. (which does not impact my score, as I expect this to be fixed in the final paper.)

For these reasons, I suggest rejecting this work in its current form. 

Improving and extending the comparison to related work (e.g., Mena et al.) and making the comparison explicit would improve the paper. It is important to explicitly state the differences if there are any. 
The empirical evaluation should be more rigorous.

---

I appreciate the response and revision; however, I still have concerns as per my last response."	3
"I emphasize that I like the paper and my main concerns are on fair comparisons. I would be happy to increase my score if the authors include an additional Gurobi/SCIP baseline where they let the solvers run for sufficiently long to solve the problems to global optimality, and then compare against the relative optimality gap from the global solution. 
"	3
The authors presented Gumbel-Sinkhorn-TopK (GS-TopK) to improve Soft-TopK. But there are concerns on the meaningful improvement for the actual derived approximate optimal solutions. The presentation of the submission can be improved significantly. 	3
The proposed method, DeepTLF, is a very straightforward combination of GDBT and DNN, which seems to outperform all modern tabular neural nets, which I find very interesting. However, the technical novelty is quite limited, and considerable amount of experiment details are missing, therefore making it difficult to accurately evaluate the paper.	2
"Interesting and to my knowledge novel idea to use decision functions of tree ensembles as input features to neural networks. The experiments show that this method works, but are not 100% conclusive if it is superior in all aspects, in particular comparing to DNN (see my concerns above).
I rate the paper *""6: marginally above the acceptance threshold""* and am willing to increase my rating if my concerns are fully addressed."	3
This paper is clearly written, and the proposed method is simple, while seems to work. But some details of the experiment, especially the hyper-parameters, are missed. 	3
I think this is an interesting paper that has technical novelty, however there are issues with the experimental results that need to be addressed.	4
Overall I think the paper provides a nice and sound contribution. The results are convincing on the tabular data but I would like some discussion about other modalities where results are somewhat comparable across several metrics. In addition, I have a few points I'd like some clarity on.	3
Utilizing invariances for robust explanations is an interesting direction, but the paper relies heavily on the algorithms and results presented in [1],[2]. The results, though competitive, are not striking enough to warrant the usage of LINEX over existing methods (and also over other formulations for robust explanations such as [3],[4] in the above references). 	2
The paper lacks in novelty, contribution, and supporting theory of the proposed methodology. 	2
Overall, interesting work. However, for stronger recommendation of acceptance some of the points above should be addressed.	4
I think the proposed problem of spatiotemporal mode collapse was not covered and described clearly. This makes it difficult to understand where the contribution of the proposed method comes from and what it improves. Spatiotemporal slots idea reminds the attention modules on the temporal data and leads to a weighted fusion of temporal weights. It is very similar to self-attention ConvLSTMs. Mainly the introduced problem is not clear and also approach is limited in novelty. These are the points affected my decision.	2
Although this approach seems to be a good engineering effort to come up with an effective model that works reasonably well on very small-scale datasets with relatively low complexity, I believe the novelty of this method is very limited. Moreover, comparison to SOTA and evaluation on larger, complicated datasets are missing.	2
I believe the idea and the approach are quite novel and interesting, however, I do believe some extra experiments should be performed and some improvements over the paper presentation are needed.	3
"My main issue with the paper is that it claims to present a new exploration method, but it only compares to relatively simple exploration methods that only induce exploration via action repeat. The absence of more competitive baselines makes the significance of the paper relatively small. Moreover, some of the claims are overstated, some important methodological details are unclear, and the discussion of certain related works are missing.

After the author's response: The additional experiments and updated related work section have addressed my primary concerns. I strongly suggest adding the details about the method from the rebuttal (which I found useful) to the paper."	2
"A novel method is presented, which, in the model-free setting, improves upon current alternatives for temporally extended actions,
in the mod, and model-free SAC. I believe that the results will be of strong interest to the community. The core idea is a creative one that performs well, and will promote future work and discussion.
"	4
This paper is well-written and well-motivated. The proposed technique is interesting and the experimental section is strong. Overall, this is a good paper and I recommend acceptance.	4
"This paper needs significant improvement in the writing clarity, mathematical soundness, and experimental validity to reach publication status. For example, this paper did not provide its own version of the policy, valuation and uncertainty networks making it difficult to evaluate the validity of the approach and the theoretical analysis --- It requires the readers guessing and jumping across the literature to get the important components neural net constructs, e.g. the valuation, policy and uncertainty aware networks. 

"	2
"In summary, I think the current manuscript extrapolates empirical observations in one domain -Sokoban– with a few related heuristics. For showing something specific about them, other heuristics or specialized solvers should be used. I wouldn't be surprised if different algorithms solve fast different kind of problems.

I understand the intention of the contribution is about DRL-based heuristics exploiting the so-called left-tail. However, I don't want to made mistake I was pointing out. The question is whether that observation is a property of the DRL proposed here, or a fairly common situation that appears in other cases. Comparing with specialized solvers and other ML-based heuristics might help to answer this critical question. 

Therefore, I recommend to reject the paper for ICLR 2022.

I think this paper would be a more appropriate for SOCS, ICAPS or a general conference like AAAI, IJCAI or ECAI.

For the sake of clarity, I'll react to the contributions:
1. We study the interplay of the policy and value networks in A*-based deep RL. Our experiments show the surprising effectiveness of the policy network, further enhanced by the value network, as a guiding heuristic for A*.
	- This observation could be significant for solving Sokoban, but it's not. 
	- This observation could be significant for DRL-based heuristics, but it's not. 
	- Both possibilities would require comparison with other algorithms, including specialized solvers, unless other problems beyond Sokoban were considered.  
2. We identify heavy-tailed runtime distributions of PSPACE-hard planning problems and propose a series of distribution-independent statistics to quantify the heaviness of tails and effectiveness of random restart. For the first time, we show and extensively study heavy left tails from experiment data, introduce an abstract tree search model with critical nodes, and formally show how heavy left tails can arise during the search.
	- The so-called left-tail phenomena appears in many techniques using search. See other papers about Sokoban mentioned here. The amount of instances tested could be smaller, but the high diversity of running time is consistent with the findings here.
	- In any case, given the experiments, this observation would be valid only for Sokoban, and the reader must wonder if there is something special on the DRL-based heuristics.
3. The tails on the left of the runtime distribution are explained by the critical role of the policy network as a guiding heuristic. Polynomial runtime scaling can occur because the policy network helps avoid exploring exponential size sub-trees early on in the search. figure 3 visualizes the phenomena by revealing a small set of critical nodes in the early part of the search space.
	- This is also fairly common in search. I don't think this is a contribution. The same sentences ""Polynomial..."" Apply to the behaviour of a SAT solver.
4. We show the importance of using uncertainty aware networks in the planning domain and how it can add a controllable amount of randomness to a backtrack-style solver.
	- This holds. The difference is of 4% absolute.
5. We show how a restart strategy can improve the deep RL planner effectiveness. In particular, our experiments show how for a given search budget, there is an optimal restart strategy. For larger budgets, more frequent restarts are most effective.
	- This is an interesting observation. It's possible that DRL-based heuristics benefit more from restart in comparison with specialized solvers and generic domain-independent planners"	2
"This paper shows interesting experiment results. However, we need clarification of the experiment setup and empirical results.
The idea is interesting and the approach is promising to understand the behavior of search with DNN. This is empirical work, so it's hard to make definite statements on the correctness. However, I think experiments don't support the claims well. "	3
This paper is somewhat extreme along multiple dimensions. On the one hand, there are many more interesting ideas than is typical for a single paper. On the other hand, there are many more experimental details missing than is typical even for rejected papers. The writing also needs to be more clear, precise and formal; the experiments and analysis need to go deeper; and the scope of the abstract and introduction needs to be narrowed. Overall, I think this could turn into a very good paper (or several papers), but the changes required will likely be too substantial to accept it for this conference.	4
This paper shows that differentiable simulators can be leveraged to significantly speed up training even on very challenging high-dimensional tasks, making it an interesting empirical and conceptual contribution. 	2
"The paper presents an intuitive reinforcement method for leveraging derivative information from a differentiable simulator. The methods significantly outperforms prior methods in terms of both sample efficiency and wall-clock time. However, the proposed method is not novel as it is largely the same as that introduced in a prior work, and it is missing an important baseline comparison. Therefore I cannot recommend acceptance of the paper in its current state.
"	2
"Overall the paper introduces a novel differentiable environment suite, and aims to ameliorate a key issue in analytical policy gradients. However I believe there are deficiencies in both these things that I have listed above, so recommend a weak rejection for now.

Having said this I believe this paper has great promise, and think these issues are not major, and am happy to raise to an accept if they are sufficiently addressed.

--------------- POST REBUTTAL ------------------

The authors have addressed most of my concerns, including the sensitivity of their approach to architecture, and included new environments aligned with canonical continuous control problems. Furthermore they focused more on the simulator in their updates, which is where I believe most of the impact of this paper lies, and produced additional interesting analysis about loss landscape smoothness."	2
"The main convincing result from the paper are the empirical results showing that using a differentiable model speeds up learning by a large margin in terms of step count on challenging tasks. I have not seen such results using differentiable simulators before, so I think it warrants publishing. The work itself seems primarily an engineering contribution: the methodology and ideas are not particularly novel, but executed in a sensible way. The quality of the work was generally good. Some more related work could be discussed, and the writing could be improved to be more precise.

______________________________________________________________
**Update**

I have increased my score to 8, novelty & significance score to 3, correctness to 4, and confidence to 4.

The authors have done a good job of addressing the reviewer concerns, and have made several changes to the manuscript, including adding more references; running more ablation experiments, e.g. experiments with a deterministic policy; added 2 new environments, etc.

I also note that their approach by differentiating through the trajectory and adding a terminal value function is not exactly the same as previous methods, such as Dreamer or MAAC. These previous works sample start states from a replay buffer of real data, and perform short rollouts from these states. The current work, on the other hand, operates on-policy, and it samples the start states from the end of the previous short rollout (this process is repeated until the total length of the short rollouts exceeds the episode length, and then the episode is restarted).  The policy is updated after each rollout, resembling a more typical truncated backpropagation procedure, but while adding a terminal value function. I think such an approach is tailored to using a simulator, because with a model-based approach, the predictions may diverge from the real trajectory when the task horizon is too long. On the other hand, if using a replay buffer based approach, such as is used in Dreamer or MAAC, the simulations to generate the replay buffer may be a waste of computational resources, so the approach presented in this paper seems like a natural idea to use in the setting when one has a simulator, and is not a direct copy of the previous works. The authors have also added a discussion of previous model-based RL methods compared to their work into Appendix A.4.6."	3
Overall the key idea proposed in the paper is sound and reasonably well justified with experiments. That being said, I have some concerns regarding some of the design choices and is the approach robust to these design choices. Hence, I have given a borderline rating.	3
The authors do not present a new idea but a very ingenious solution for a recurring problem. Having little data to train the tasks does not impede training well if you have a little imagination. By generating data that cannot be used directly as positive examples (because of the limitations of generation methods), they use this data as negative examples for other tasks. Although I still doubt if this can be extended to other scenarios, for example, when tasks are more similar or different. I think the authors present surprising results.	2
The idea is interesting, but the novelty is limited, and some details and comparison experiments are missing.	2
"Negative Replay looks like a promising approach to continual learning with approximate generative models. The papers propose interesting results with several baselines.
However, the experiments are not designed in a convincing setup to evaluate negative replay. The evaluation of negative replay would benefit to be applied on several classical approaches before being applied on a specific/complex training procedure."	3
This paper proposed a novel architecture to linearly combine the aggregated outputs using different weight matrices for each node. The model is memory efficient since the memory consumption is $O(V)$ instead of $O(E)$. However, the proposed method did not solve the scalability problem of GNNs when applied to considerably large graphs. And it is unclear when the provided memory efficiency is necessary, and thus the proposed method is the first choice. In terms of performance, the improvements compared with PNA are also not very convincing. The significance of this paper might be limited if the efficiency/performance improvements are marginal. I would encourage the author to explore the theoretical understanding of the proposed architecture further. Currently, section 4 is well-written, but the conclusions are limited and may have some flaws. In general, I could not recommend the current manuscript for acceptance.	3
"I think this is a thorough piece of work, but I am concerned that the impact is limited for the following reasons:
- I am not convinced that the results presented reflect the current state of of O(V) scaling GNNs. You cite ""Training graph neural networks with 1000 layers"" which I believe scales O(V), and I think achieves better results on OGBN-ARXIV. 
- The ideas seem very related to existing work. This isn't to suggest that the ideas are identical, more that the contribution appears to be incremental.

That being said, I think the analysis is beneficial to the community so give it a weak accept."	3
The authors take a different tact with their proposed GNN, which is welcome. They explain it clearly, and the paper's writing and organization are solid. The results support their claim that they can achieve competitive accuracy, but the quantitative computation and memory results are somewhat underwhelming. In my view, the contribution of this paper is its approach and challenge to convention, which feels good enough to publish, even if it left me wanting more substantive.	3
I think the introduction and related works description clearly recap the background of both GCN algorithm and potential algorithm-accelerator co-design, and also the current dilemma in the GCN community, that is, the more accurate but less time/memory efficient anisotropy approaches VS. less accurate but more time/memory efficient vanilla GCN approaches. The proposed EGC can alleviate such a dilemma by achieving both higher accuracy and efficiency.	3
The paper identifies an interesting phenomenon and has good proposed fixes, but there is currently a lack of rigour/convincing arguments and seems to have some incorrect statements. There is a lack of references to some existing work with similar approaches. The method has somewhat limited significance as it is only demonstrated to be useful with DDPG.	3
"It is a paper on an empirical study of an important question in RL with satisfactory claims and fixes. In the latter part of the paper, some thorough discussions are missing, and the choices made around the combined fixes are not well motivated, for which I would like to hear from the authors.

*** Updated ***

The authors addressed all my concerns satisfactorily. They even addressed a last-minute question about a highly-related work that I didn't notice before. The authors clearly described the difference between that work and theirs. 

More elaborately, the authors promptly and elaborately discussed a comparison to this newly discovered work. They not only showed that they can discuss this work in their submission fairly, but they can also utilize it to improve their submission. Moreover, their new results showed that the existing work was inferior to their proposed solution. The authors' explanation also made sense that the existing approach normalized the pre-tanh action, which limited the action space too restrictively.

Hence, I increased my score from 6 to 10. Thank you!"	3
"Although I liked the empirical study, the analysis and the proposed findings may not generalize beyond the setup used by the paper (tanh nonlinearities) and lack of thorough evaluation on a large set of environments. Furthermore, there are other weakness such as missing results and fixes without much motivation with possibility of visualizing results on a cherry-picked environment. Overall, I feel the paper can be significantly improved by addressing some of these weaknesses. 


-------------------------------- Update after rebuttal -----------------------------------------------

Based on the discussion with the authors, I feel that the paper can be accepted at the conference. I'd like to update my score from a 5 (weak reject) --> 7 (accept) but the ICLR review scale only allows me to pick a score of 6 or 8. Since a very related work (highlighted by reviewer cHfn is not discussed), I decided to go with a score of 6."	3
I work mostly on the theoretical side of RL so I am not sure how significant the experimental results are in this paper.	2
"The authors have adapted the instrumental variable work of Angrist etal. to estimate a better state transition for a linear dynamical system when the noise is Gaussian.  However, the authors do not seem to be modeling the mechanism of data formation, and they are not estimating the effects of interventions.  The Gaussian noise assumption has not been justified. 

The authors are performing regression and the causal language is misleading.
"	2
In overall, some parts of the paper are not well-written. Moreover, the model of latent confounder (additive noise with special form in Figure 4) is not justified and the application of the proposed algorithms might be too restrictive. 	2
"This paper studies an interesting problem in imitation learning with the presence of unobserved confounders. The causal identification method with instrumental variables in Section 3 seems interesting. However, as demonstrated in the example in ""Main Review"", I am not sure if estimating the causal effect of observed state S on action A is sufficient for a successful imitation. In other words, I am not convinced that the proposed approach in this paper is technically sound."	2
Overall, I find the paper to be novel and interesting.  It could be improved, in my opinion, in terms of the suitability/justification of the IVR assumptions in the considered application domain as I mentioned in the weaknesses above.	3
My primary concern is the rigor of the experiments, and my secondary concern is that some claims are overstated. 	3
Authors proposed a heuristic algorithm, randomised adaptive Lasso, that is evaluated empirically on 4 analytic PDEs. In those specific cases authors show success of their algorithm in model selection problem. Having said that I still recommend to reject this submission because I think the empirical evaluation is limited to PDEs where we know the True coefficients and generalisation on more difficult problems in unknown. And finally, the limited theoretical justification behind the work has some strong assumptions which are not sufficiently discussed in the paper.	2
The problem of variable seclection consistency of model discovery in PDEs is studied by first pointing out that sparse regression may fail with violated IRC and then proposing a randomised adaptive Lasso to tackle it. To me, this work is interesting. However, the proposed rAdaLasso seems not quite novel, and its relevant theroetical analysis seems a bit weak. 	2
"In summary the method is a great contribution to PDE discovery methods, but requires a much more comprehensive experimental section to convincingly demonstrate its benefits under different settings and demonstrate insight into its behavior. Currently the paper feels premature for publication, but no doubt will become an excellent paper in future.
"	2
I recommend to accept it for its straightforward intuition, simple methods and strong experiment results.	2
"The paper presents a simple new method for improving GNNs. The proposed method is based on existing ideas used in other domains, but their application to GNNs is novel. Through extensive experiments, this method was shown to benefit a variety of GNN architectures across different datasets and tasks. The simplicity and generality of this method is likely to have a big impact on the field of GNNs, which prompts me to lean towards accepting the paper.
"	3
Overall, I like the simplicity of the proposal and the fact the paper tackles relevant issues in GNNs. Also, the paper shows good empirical results. However, my main concerns revolve around limited technical novelty, lack of experiments to validate some intuitions behind the method, and comparison against related methods. 	2
"The proposed idea is intuitive and sound, and the results are positive. However,  important baselines and detailed experiments needs to be added to support the claims and justify the contributions. The writing also needs to be improved.
"	2
The paper provides supporting evidence that end-task aware training can achieve a better performance while being more data-efficient. The paper is well-written and easy to follow. It provides an alternative to the pretraining + finetuning setup and their observations that end-task aware approaches are more data-efficient might make training large models more accessible to entities with slightly less computational resources.	3
The MTL f design to combine pre-training tasks and fine-tuning tasks in continued pre-training stage is well motivated. However the experimental setting is a bit limited and the results are also mixed.	2
"Strength: problem setting is practical and interesting; good performance and improved data-efficiency.
Weakness: lack of visualization and clear description for the method; comparison in computation cost and data usage is questionable."	3
I think the paper raises an interesting question, but then only addresses a different, but related question. The results should still be of interest to the community. I think the paper would be much stronger if the authors actually ran experiments that forego pre-training entirely and instead use their mixed-training approach.	2
I think the paper is above the threshold. If the authors can provide evidence that the proposed method outperforms (or has some relative advantage) compared to standard FVM solvers, I will be happy to raise the score.	3
The demonstration of this method is pretty good, but there are too many restrictions on the problem. If the form of the equation is fixed, a simple polynomial fitting may achieve a better fitting effect than the neural networks. 	2
The results in comparison to other models are compelling, particularly the model comparison on figures 6 and 7. However, the methods section of the paper needs more clarification and justification of the modelling choices in comparison to other papers. As the paper requires a significant re-write, I suggest a reject.	3
The paper has very thorough experimental results and good descriptions of the methods. However, the claims made in the paper may need a second look/ need some rewriting.	3
Overall, I think this is an interesting approach, but unfortunately I think the framing of the paper as a causal estimation paper, rather than a paper which uses counterfactuals a mechanism to improve invariance and performance makes the contributions of the paper slightly obscured. 	3
"Overall, I vote for accepting this paper considering its novelty on solving link prediction from a new perspective of counterfactual learning.
Nonetheless, my background on causality does not allow to check all of its correctness in such a short time. "	3
The authors propose a highly creative approach to improve link prediction accuracy by an innovative definition of treatment for a node pair in a counterfactual learning set up. The proposed approach yields highly impressive empirical link prediction accuracy.	4
"From the quality and novelty perspective, the proposed method is interesting and the experiment is thorough. However, my major concern is the causal model in the paper. I am not very convinced with the causal model with 'node similarity' as a treatment, and 'link' as outcome. The proposed approach is more like a data augmentation, where similar pairs are found with different community-relation to enrich the training set. It would be helpful to spend more effort to illustrate why such causal graph design makes sense.

In addition, the experiment results are exciting, but I didn't observe any submission in the open benchmark website. I highly recommend make a submission for a fair comparison, or explain why the submission has not done yet."	3
This paper has proposed an interesting idea (i.e., operator splitting) to improve the scalability of neural network verification, but this idea is not novel for its applications to neural networks (e.g., neural network training). The paper will better position itself if more detailed comparisons are made with such methods in the existing literature. 	2
The authors have proposed a novel method for the neural network verification problem. It is better than Bunel et al 2020. However, authors have not compared against recent state-of-the-art published baselines. Also, complete verification experiments are missing.	2
Very well written paper, with interesting and significant contributions, and which provide a great summary of the state of the field.	4
 This paper addresses an important problem. However, the unclear presentation, the lack of novelty justification, and missing experimental details should be improved to make it an acceptable paper in ICLR.	2
"DeepSplit is an improvement upon two closely-related solvers for the same convex relaxation: (Dvijotham et al. 2018) and (Bunel et al. 2020a).  The work focuses on solving the relaxation exactly. However, as shown in a variety of recent papers (Xu et al. 2021), [Active Set],  [Bunel et al. non-convex 2020], (Wang et al. 2021), heuristics and switching to tighter relaxations (possibly considering branching) might verify more properties in the same time (both in incomplete and complete verification). These developments have been somewhat ignored by the authors.

The authors should put their work in perspective with more recent and scalable optimizers for the same relaxation: (Xu et al. 2021), [Bunel et al. non-convex 2020]; and with works on tighter relaxations [Active Set]. Furthermore, as commonly done for solvers of the considered relaxation, complete verification experiments are needed to fully assess the quality of DeepSplit's speed-accuracy trade-offs."	2
The novelty is limited and the contributions are incremental. 	1
I think the research problem is interesting. However, I think the technical novelty is marginally significant, and the motivation of the proposed metric may not be clear.	2
This paper introduces uncertainty-aware pseudo-labeling techniques to PU learning. Although the idea is interesting, the technical novelty and the performance improvement seem marginal. In addition, according to the paper, it assumes cleanly-labeled validation data (we can access negative samples) are available, which is a strange assumption in PU learning.	2
Although the proposed estimation method is different from existing ones, my main concern is that the two assumptions are unrealistic. They actually run against the very setting of PU learning. Without the two assumptions, the proposed method would have problems. The paper should also compare with some earlier methods.  	2
I am confident to suggest the paper for acceptance since it introduces a novel and interesting way of using transformers to train local image features that outperform the state of the art. The method is easy to train since only image-level labels are required, however the loss is applied in the local feature level, which is an interesting novelty.  Detailed ablations motivate the design of the method. My main concern are inconsistencies in the mathematical formulation of the method, which I am sure can be resolved for the camera-ready version.	3
"I conservatively give a borderline before the discussion phase. The motivation and intuition of this paper are quite nice, but some parts of the details are questionable. I think the overall quality of this paper is about ok, but it doesn't hit. Some parts of FIRe are not absolutely novel. The authors need to defend these designs to convince the readers.

------------------------------------------------------------------------

The authors' response satisfies me. Hence, I have changed my final rating."	2
Summing up, although the paper does not have a very strong novelty, its several contributions together with the solid experimental results in a relevant application domain are sufficient for acceptance. I found two minor issues that I think the authors should fix, but the rest is excellently written and presented, which is also valuable.	3
"Authors describe an architecture for deep image retrieval. For this, they use mid-level features that they call super-features (SF). They construct thes SF by an iterative attention module. They build them as an ordered set in which each element focuses on a localized yet discriminant image pattern. Auhtors show that for training, only image labels suffice. Authors present a set of experiments on common landmark retrieval benchmarks to validate that their SF substantially outperform state-of-the-art methods when using the same number of features. They also show that this requires a significantly smaller memory footprint to match their performance.

Main contribution: A method called: Feature Integration-based Retrieval or FIRe for short. It can decomposed into 3 main parts:

1) an image representation based on mid-level features (super-features) and an iterative module to extract them.
2) a framework to learn such representations. It is based on a loss applied directly on SF yet only requiring image-level labels.
3) a set of extensive evaluations that show significant performance gains over the state of the art for landmark image retrieval. 

Some possible works that could be added to reference section:

Intellige: A User-Facing Model Explainer for Narrative Explanations
J Yang, D Negoescu, P Ahammad - arXiv preprint arXiv:2105.12941, 2021 - arxiv.org

Next generation 3D pharmacophore modeling
D Schaller, D Šribar, T Noonan, L Deng… - Wiley …, 2020 - Wiley Online Library

Automated cyberbullying detection in social media using an svm activated stacked convolution lstm network
TA Buan, R Ramachandra - Proceedings of the 2020 the 4th …, 2020 - dl.acm.org

AUTOMATIC MODULATION CLASSIFIER
HA Rasool - Iraqi Journal of Information & Communications …, 2020 - ijict.edu.iq

Next generation 3D pharmacophore modeling
D Šribar, T Noonan, L Deng… - WIREs …, 2020 - refubium.fu-berlin.de

Synergistic Target Combinations Against Obesity: Focus on MCHR1/H3R Modulation
DA Schaller - 2020 - refubium.fu-berlin.de
"	4
The paper is well written. The proposed method is well-backed by theory and the empirical results are convincing.	3
This paper is well motivated and clearly written with sufficient theoretical and experimental backups. Overall, I believe this is a good paper. 	2
The theory results in their current form are limited, and this can be addressed by expanding its scope to include classes of non-linear dynamics models; furthermore, the theory results are partial in their characterization since they fall short of characterizing the sub-optimality of the learnt policy as a function of properties of the offline dataset (measured in the form of coverage). That being said, the paper is definitely promising in terms of its empirical results. I'd be up for revisiting my reviews if these issues can be addressed.	1
Overall, the paper is a nice contribution, but I think some aspects of the presentation need to be changed before it is ready for publication.	2
To summarize, the paper proposes a fairly simple but reasonable idea that achieves some improvements. However, since the improvements are rather small and the complexity of the model increases substantially, it is unclear if this idea will receive a lot of attention.	3
This paper provides a solid foundation on the theoretical formulation and analysis of graph representation learning over bag of subgraphs. The contribution is technically sound and the solution is well supported by detailed analysis of design choices. Thus I recommend acceptance.	4
"This is a solid paper on improving the expressive power of graph neural networks. The proposal approach of equivariant subgraph representation and aggregation is novel, the theoretical analysis is solid, and the results are positive. 
"	4
"Since the idea is not original if Cotta et al. work is seen, and it has lack of experimental tests in initial submission as mentioned above, I would recommend rejection to this work.

But as the author mentioned that Cotta et al. work should be seen as contemporaneous work and there are a lot of additional test in second version, I changed my rate to 6."	2
This paper proposes a well-motivated and mathematically elegant family of distributions on convex polytopes, allowing to place positive probability not only on the interior of the polytope, but on its faces as well. While there is room for improving the experiments, I believe the presented experiments and theoretical developments -- which can easily enable future work -- should be accepted.	4
This paper is very solid. It builds some important theoretical foundations for the mixed random variables. It seems that this will be useful for the future research on this topic. 	4
This is a paper with mathematical rigor and connections to emergent communication, which is a current topic of interest for specific machine learning communities. Albeit the submission of this paper would better fit a probability and statistics venue, to the best of my knowledge, the definitions 1-3 and propositions 1,2 appear to be novel. 	3
"My main point is that the authors
1) do not provide information regarding the case of having more than one discrete outcomes with different alphabets; 
2) what is the algorithmic complexity with respect to the number of observations, the rate of continuous vs discrete outcomes;
3) they still work on the code and they do not provide it unless the paper is accepted"	3
Explores an important research direction but neglects existing literature.	2
Despite the idea of including textual explanations in the learning pipeline to improve performance is interesting and trending these days in the DL research community, the overall contribution of this paper is very modest. The paper's central claim is ambiguous due to a missing operative definition of explanation and not adequately supported by experiments. 	1
A well-written paper advocating the advantages of adding explanations to RL setups wrt to sort-one-out identifications. As a non expert of this field I have found the presentation clear and convincing. The arguments to support the claims are mostly empirical, but restricted to a specific simulation architecture.	2
The exposition is clear and the background and relevant literature are well-represented. The paper only lightly presents a theoretical analysis of the work, and the value of the work seems to be primarily based in the comprehensive set of experiments presented, which serve to show that training an agent with an auxiliary loss to generate explanations (alongside a scene reconstruction and policy optimization) would allow the agent to perform better on various odd-one-out settings, compared to had it not been trained to give explanations. Notwithstanding over-claimed statements (to-be-addressed) and some open discussion questions highlighted above, this paper seems like one that the ICLR would enjoy reading and referring to.	1
"I think this paper is more on the ""technical report"" side, with quite limited novelty. I also find it gives me the wrong impression that the investigation is more about ""scaling-up"", but instead it just does search in the ""small-to-base"" scale regime. It definitely teaches me something and presents some interesting data points, but to me the scientific merit is not enough to warrant a publication."	1
Paper contains an interesting empirical study on how iterative greedy search can be applied to the design parameters of ViT. Resulting models show speed-latency improvements over hand designed ViT models. The method of the search is not well explained (it seems to be the previously proposed method). Resulting models seem to be less generic and more engineered, not clear how presented results will scale beyond Tiny/Small/Base models. Presented study will be interesting for people designing ViT models, however its limitation is in the conclusions/findings specific only to ViT models (even with transfer to PiT). 	2
The proposed method is not novel, but the insights brought by the authors are interesting.	2
"Please provide a short summary justifying your recommendation of the paper
Although some parts of the paper are not clearly explained and the method is not very innovative, the analysis of the transferability of different scaling strategies on different hardware devices in this work is very positive and provides great inspiration to relevant fields. Therefore, it is recommended to accept this paper.
"	3
"I am not convinced that the core idea of the paper -- converting residual networks to an equivalent plain network -- addresses any fundamental issue.  The argument for speed of one vs the other is empirical, and may merely depend on what is optimized in the underlying software libraries.  To make a case here, the paper should provide analysis in terms of achievable parallel efficiency by an optimal implementation.  Similarly, the argument made for pruning RMNet seems to be one of convenience rather than fundamental difference -- is there not an equivalent (though perhaps not off-the-shelf) approach in terms of of pruning components of the original network?
"	2
In general, the paper has showed the strength in removing the residual connections on the classification tasks, it would be much more convincing to have more experiments on other popular tasks.  	2
"\+ Generalizes previous methods to other typical ResNet types

\+ Extensive Empirical evaluation

o/- Somewhat incremental, quite empirical

\- Some details missing"	3
The paper puts forward a novel method: RM Operation, which can equivalently remove residual connection across non-linear layer in ResNet-based architecture and shows great power in network pruning. I think RM operation is novel and can inspire future works on model design.	3
Novel and potentially useful setting to study but some concerns on execution related to benchmarks and how we should interpret the results.	3
The approach works, the components are well motivated. Despite this, in my opinion, the approach can be understood as a variation on more classical attribute-related few-shot learning tasks, solved by combining methods that work well on these. I would like to highlight that, despite the previous point, I feel the proposed idea has scientific merit, hence borderline score I have given currently.	2
This paper introduces an interesting topic and performs extensive experiments to verify the conclusion. However, the technical novelty is hindered since most of the contributions are not new. Besides, there are some unclear points and claims that need to be clarified and discussed. I'm looking forward to the authors' responses.	2
"
The question of addressing transferability at attribute level is interesting. A ""few-shot"" formulation to do so, as proposed in the article, is a possible direction.

However, the overall presentation of the problem is messy, superficial and lacks precision. The algorithm novelty is low and the experimental part relies on questionable protocols. 

The article should be reworked with a clearer focus on the scientific hypothesis that is tested and better justified experimentations.

Final decision:

My conclusion about acceptance has not really evolved: the problem of attribute transferability addressed in the paper is interesting, but the way it is studied and presented must be improved and better justified (see detailed comment of rebuttal after author's response). "	2
This paper considers a fundamental online problem in the presence of predictions with a nearly tight result for this setting.  The experimental validation is reasonable.  Some improvements could be made to the presentation, but it is currently clear.  	3
"Due to the weaknesses mentioned above, I do not believe the paper is ready for publication.

Update: Thank you for addressing my concerns. After reading the new version, I am happy to update my score."	3
Overall the paper is a nice addition to the new area of learning-augmented algorithms. The proposed algorithm is practical and almost optimal, though it would be nice to obtain the optimal robustness bound of $O(\log n / \log \log n)$. The paper seems to be a good fit for ICLR.	3
The key issue that needs to be addressed is the motivation for assuming that new facility locations can be predicted using a machine learning model. I feel this is quite unrealistic and significantly diminishes the value of the paper. It would be great if the authors could address this issue in the rebuttal phase. Otherwise, I think theoretically the paper is worth publishing, albeit at a more theoretical computer science (such as ESA, or APPROX ). 	3
This is a credible paper on online facility location with predictions. The problem considered is important and relevant, and the results obtained are tight in the sense that consistency and robustness results in the right ballpark. But, I have concerns about the error metric, which is the most important parameter here - with the current error metric, the very realistic possibility of having a single bad prediction is also going to relegate the guarantees of the algorithm to those of having no prediction at all. I would also have been happier with an algorithm that can use an online algorithm for the problem in a more black box manner - this would make the framework more generally applicable (and I suspect this is possible even with the current framework, or some modification of it, because I don't see the framework as conceptually using anything specific about Meyerson's algorithm, although I might be wrong on that last point).	3
Overall I think the paper is a weak accept, if the aforementioned concerns with the presentation of the lower bound construction can be resolved (if they are not resolved in the rebuttal period, I would rate at a weak reject). I think the algorithm and its analysis are fairly non-trivial but intuitive and enjoyable to read about, the problem being addressed is one of interest, and the lower bound/empirical results nicely complement the upper bounds. However, it is not clear to me if this paper stands out in particular amongst other papers in the learning-augmented online algorithms literature, and there are some concerns with the presentation of the lower bound I wish to see addressed before publication.	3
Overall this is an interesting new application of backdoor attacks with good empirical results. It is hard to tell why the main proposed defense outperforms the baseline attack and I hope that this will be addressed. 	3
"I think the overall contribution of this paper is limited and some implementation details are missing as mentioned above.
=======================
The rebuttal has addressed most of my concerns. Therefore, I would like to upgrade my initial recommendation."	2
As the first work that relates backdoor attacks against VOT, this paper reveals the vulnerability of VOT to backdoor attacks caused by outsourced training or using third-party pre-trained models and achieves valuable results. Although there are still some shortcomings in experimental analysis, this article is a good attempt on the security of the VOT and worthy of follow-up work.	3
"Despite being enthusiastic about this general area, I found it too difficult to follow the experiments. Not enough detail was given about:
- data sets used and how they were sampled to produce ""training"" and test data
- exactly what the experimental setting was. What problem, exactly, are these systems trying to solve? I think, but am not sure, that given many instances of objects with unique ids at ""training time"", they are simply trying to retrieve objects with the same ID as a query image at test time. However, not enough details are given about the details of this problem.
- Given the nature of the problem, it appears as though a variety of nearest-neighbor style baselines would be appropriate.



In my assessment about ""confidence"", I have said that I am fairly confident of my review. Ironically, I didn't understand the paper well, but this is the problem with the paper. It should be easier to understand what was done. Thus, I am confident in my assessment that the paper should have been more understandable. In particular, I am not someone coming from far outside the area; thus, the paper should have laid out the details more clearly so that someone with my background would understand it."	3
"Online self-supervised learning is a very interesting and realistic setting. The online EM algorithm that maintains a set of prototypes makes a lot of sense.

There are a number of papers that are very related, most notably Bottou and Bengio 1995 (online $k$-means) and Lathuilière et al. 2018 (Gaussian-uniform mixture). Not only one would expect them to be cited and discussed, but rather the method formulation should be based on the formulation of this prior work. This would improve positioning of this work, make the formulation more elegant/better motivated and dispense the need for certain hyperparameters. The related work section needs improvement in general.

Writing is good in general, but the formulation in unclear at several points, relying on Appendices. Pointers to prior work are also missing in the formulation, primarily (Ren et al., 2021).

The protocol of comparisons to non-online methods is unclear. The choice of competitors is not well justified: there are simpler methods that are easier to adapt to online. There may be more baselines to consider e.g. on pretraining. There are too many hyperparameters. The networks used are too small."	3
"In this paper, the author assumes a new scenario of nonstationary distribution. To solve the unsupervised and online setting, they design a novel online unsupervised prototypical networks (OUPN). By introducting the Gaussian mixture model and EM, the method could add or drop the cluster in an online way.

The idea is interesting and encouring, but the necessity is not well claimed. The difference between this method and life-long learning / incremental learning is encouraged. Also, the hyper-parameter of maximum number of K clusters should pay much more attention."	3
The proposed approach is original and clearly shows strong results for the task of unsupervised, online, non-iid instance classification. However, there are major issues with the presentation, and the unsupervised categorization claims made in the introduction are not supported by the experimental results. If the authors can address both concerns in the rebuttal, I'd be happy to recommend an acceptance.	3
Nice result, but in my opinion not innovative enough for ICLR.	2
The paper is novel, well written, and admits theoretically and practically great results. I recommend acceptence. 	3
Overall the paper discusses an important and interesting problem and the experiments performed on real datasets show the utitility of the method in practice. However the theory results are heavily based on existing coreset techniques for classical k-means and  I am not very sure if the paper has enough technical novelty for ICLR. 	2
Despite some of the comments above, I believe the conceptual contribution and the experiments presented in this paper are of enough interest and robustness to warrant its acceptance. I am rating it as marginally above the acceptance threshold for now, expecting the authors to improve and respond to the comments above.	3
The method provided by this paper is simple, powerful, and physically intuitive. However, there are significant issues with overselling, substantiation of claims, and clarity that mean I cannot recommend acceptance of this paper in its current form.	3
The paper proposes an interesting adaptation to the supervised approach of solving inverse problems by means of deep learning. Unfortunately, the included experiments are not representative enough to show whether the promised benefits of this approach hold true. In addition, there are several issues regarding the presentation of the material.	2
The basis for problem set up is unclear to me in general, therefore the contributions and results are completely compromised by that.	2
"Overall, the paper is empirically addressing a series of important questions about the (relatively) new class of architectures (ViT). The paper serves as both: (1) effectively a survey of recent papers and observations made in prior works and (2) empirical evaluation of these models. Empirical evaluations point to some interesting insights that are ultimately leveraged to build a new form of the model which is a (relatively simple) combination of the MSA and CNN. 

The main concern with the paper is density of the text and the general lack of motivation for the various experimental designs, as well as sometimes somewhat tenuous (and not well explained) connections between empirical results and high-level insights. Despite these issues with exposition, which I think would benefit from fixing, I think the paper does address an important topic and drawn insights can be broadly useful. For these reasons I am in favor of acceptance. "	4
I like this paper, and the insights it provides in terms of optimization landscapes of ViT vs CNN architectures. It also provides a pretty clear route towards better ViT or CNN architectures by combining the strengths of both. I wish though the experimental evaluation would focus more in ImageNet (currently mainly in appendix), and not CIFAR.	3
"- The visualized loss landscape in the paper is different from existing literature https://arxiv.org/pdf/2106.01548.pdf; where a sharpness aware optimizer is developed to flatten the loss landscape of ViTs.

- I am not entirely convinced that the findings in the paper fully explain how ViTs work. I guess, the paper draws analogy between MSA and spatial smoothing, and then builds most of its argument around that. An appropriate title reflecting that will be better, in my opinion.

- Some of the terminologies used in the paper confused me, e.g., multi-stage vs multi-scale. All deep architectures are multi-stage in my understanding. Most have multi-scale feature hierarchies. Vanilla ViTs have single-scale features throughout all stages.

- Small patches improve performance (counter to what is shown in the paper). See https://arxiv.org/pdf/2106.09681.pdf"	3
I overall think that this paper shows some interesting phenomena, even though many of the observations are post-hoc and don't seem to lead to immediate actionable design rules. However, follow-up work may still benefit from these insights.	2
The experiment needs to be improved. The paper performs empirical analysis only on one setup. It could make the paper better if the authors could perform analysis on other settings (e.g., asymmetric noise, etc). 	3
"This paper aims to provide a theoretical understanding of why self-supervised pre-trained features help neural networks improve resistance under noisy labels. While the theoretical claims look solid, the important, more general settings outside of symmetric synthetic noise are missing (please see ""Cons""). 
Hence, in the pre-rebuttal phase, my score is 5 and it can be re-evaluated once the authors will address my concerns 

Post rebuttal decision:
I am grateful to the authors for their responses to the reviewers' concerns during the rebuttal phase. Nevertheless, the paper presents an important study the experimental setup is not comprehensive enough to support the paper's concussions, Hence, I vote to reject the paper in its current form and maintain my current score, I encourage the authors to expand the experimental study by my and other reviews comments. 
"	3
The authors propose an interesting idea that SSL can help learning with noisy-labels but due to the theoretical, experimental and clarity concerns above, I think the paper is marginally below the acceptance threshold.	3
I do not recommend the paper for acceptance currently as I do not think it presents a coherent picture (of a very interesting topic) and also have some confusions about the presentation of the paper.	2
Overall, the paper proposes a novel defense to the backdoor attacks that could benefit the field. However, the current experiments are not convincing enough because some critical baselines are missing. There is no discussion on how general the proposed defense work when different training techniques are used in different phases. And there is no discussion on how a backdoor attack may work around the proposed defense, and how to further defend such workarounds like many other security papers did. Therefore, I give a weak reject.	3
Overall, I believe the method is useful, however the above questions are important to be addressed.	3
I think the proposed method is natural and simple, which makes the paper easy to be extended to different settings. I recommend weak acceptance since the experiment part did not fully convince me.	2
I like this paper and would support an accept.	3
While this paper provides novel idea, it lacks some comparison and justification in comparison to the existing literature. More work needs to be done to justify the modeling choice. Also, more experimental work needs to be done to justify the utility of the above model. In light of the above observations, I have made my recommendation of the paper. 	2
"The paper discussed a way to cluster a dataset with multiple chainable joining keys. The authors provided mathematical discussion and evaluation on a small synthetic dataset. However, the paper itself is poorly written, to a point that it's hard for Reviewer to understand how the authors arrived at the main idea and what its strength and weakness are. Clarity is the main weakness.
"	2
I recommend the paper be accepted. While it does need some serious work in clarity and maybe some additional empirical investigation, it does present a new technique that is attacking a meaningful problem.	3
Overall, while the technique proposed seems useful in improving computational efficiencies of existing VI approaches to LFI, the technique itself is a relatively uninspiring combination of existing techniques, of which a few aspects thereof seem either lacking in explanation or over-convoluted in construction. I would tend towards a reject, but welcome the authors to challenge this sentiment by addressing the points in my main review.	2
I recommend weak acceptance as I think the method described in this paper may be interesting for the SBI community. I also find the paper well written and experiments convincing.	3
In sum, this paper proposes a practical way for the posterior inference with unknown likelihood. The method is simple and natural, the writing is mostly clear. However, some sections and notations are not clear, and it is better to show the limitation of each component of the proposed method. 	2
This paper presents a simple and effective idea. It makes both SNLE and SNRE more computationally efficient while maintaining accuracy. Experiments demonstrate this clearly. Provided that my question above can be answered, I recommend this paper for acceptance. 	2
See the main review.	2
"The paper is great, but the experiments are very limited, which I believe it puts it below the bar.
*****
After the response:

I have decided to upgrade my recommendation. Upon a more careful look through the supplementary material, the paper does include some interesting ablation studies. I think it would good to provide some experiments on a dataset different to ImageNet. "	3
Overall, the paper explores an interesting direction of research. The proposed model architecture seems to be train time efficient and improves both robustness and performance of its counterpart model. There are some concerns that need to be addressed raised in the main review, which should further clarify the technical novelty and motivation for the work.	3
Overall I think the paper shows an interesting direction and present good results. However, the limited novelty prevents me from giving a higher rating.	2
While I find the idea interesting, and I like the direction the authors are going -- this work is still quite preliminary and needs more work.	1
This paper studies the problem of adversarial training and tries to leverage a fusion-based method against adversarial attacks. The integration of fusion and adversarial learning should be a very interesting topic to be studied with. However, I have concerns about the technical quality, the novelty of the manuscript, and the violation in page limitation. All of these lead me to recommend its rejection.	1
Due to the weaknesses mentioned, I recommend to reject this paper.	1
Overall, the main concerns of this work are the novelty and unclear experimental setups.	1
"Grammformer is a thoughtful combination of novel contributions and familiar insights in the ML4code field.
While the runtime performance is unclear, the proposed approach can still deeply impact the future of LMC (especially for generative tasks), therefore I recommend this paper for appearing at ICLR in order to get the exposure it deserves."	3
Interesting idea and well-written paper. However, without end-to-end evaluation on specific tasks, it is hard for me to judge the usefulness of the generated code sketches. 	3
This work brings in the several techniques (Grammar based program synthesis, Language models over partial programs, separate model for non-terminal selection, etc ) to solve the problem of code completion with sketches. I would rate the paper medium on novelty as many of these techniques have been already explored in the literature in different contexts. However, to me, the important contribution of this paper is the problem setting. In my opinion, Keeping placeholders in place of low-probability program fragments can improve the utility of code completion tools to a great extent.	3
"The paper presents an original approach to code completion which allows to predict “holes” where the model is not certain about the prediction and keep prediction going after the “hole”. Training a model that is able to generate completions past “hole tokens” is challenging, and the authors present an elegant and clever technique for doing that, based on reinforcement learning.
Overall, a nice original idea and nice execution. I think this paper should be accepted. 
"	3
This paper clearly defines the RED problem and conducts experiments to show that a classifier optimized for extracting adversarial perturbations can extract such perturbations better than 2 other baselines for a variety of adversarial attacks. A main concern is that the problem setting does not seem well-motivated: in what case will we _know_ whether an image is an Lp norm adversarial image and need to extract the adversarial perturbation? What could these perturbations be used for? Does the authors' proposed method work better than performing some PGD to go back to that true class? Finally, in table 2, the d(x, x_{RED}) is actually lowest for the DO baseline, and the DO baseline has comparable PA to CDD-RED. The results for CDD-RED do not seem significantly better.	2
"The major strengths of this paper are:

•	The paper is novel and could be have a considerable impact.

•	The paper provides a strong background that establishes a foundation for reverse engineering adversarial perturbations.

•	The ability to reverse engineer adversarial perturbations could be a powerful tool for understanding adversarial examples better.

•	Because the methodology is demonstrated to be able to handle benign inputs well the methodology could lead to effective defenses against adversarial examples.

The major weaknesses of the paper are:

•	The work only evaluates the ImageNet dataset. And so, it is not clear how it would generalize to other scenarios.

•	While the work compares against attacks it wasn’t trained to recognize, it appears that these methodologies represent similar attacks. (L2 or Linf) attackers It is not clear from these results that the methodology would extend to other types of attackers including L0 or Wasserstein minimized attackers.

Additional Comments:

•	The Adaptive Attack does not appear to be cited in the text.

•	The Interpretation function in the second sentence of the “Attribution alignment” subsection appears to be formatted differently than the rest of the text.
"	4
I believe that the paper explores an interesting problem but does not show any relevant practical applications for an end-user who might be interested in improving the robustness, interpretability, or fairness of their models before/as they are deployed in the real world. I encourage the authors to study which parts of the deployment pipeline can benefit from the estimated REDs.	2
Overall, I think it is a borderline paper. So I am fine with either accepting or rejecting this paper.	2
This paper has two core contributions: a dimension reduction technique (random freeze) and a method to scale gradient clipping norm. However, as mentioned in the discussion on weakness, this paper does not sufficiently demonstrate the benefits of the proposed methods. Therefore, it does not meet the standard for publication at a top conference like ICLR in my opinion.	2
"Overall, this method seems purely heuristic. Without a theoretical or first-principles explanation, this paper relies mostly on an empirical analysis. Theorem 1 is included, but the Gaussian assumption for SGD is extremely limiting and deviates from the accepted standards of analysis for SGD, which generally assume unbiased estimates with bounded variance, but not Gaussian shape in particular. Furthermore, the connection between Theorem 1 and the proposed method of random freezing does not seem that solid.

For the systems analysis, the authors do not report two key quantities: (1) the memory consumption (in bytes) of the method and (2) the runtime of the method. For (1), sparse representations must generally pay a memory overhead because an index is also required. A sparsity % of X% does not imply X% of the memory used. For (2), it seems that although a sparse representation can reduce memory, I am not sure how the runtime of the sparse gradient computations might also be affected here in the DP-SGD. It is important to also benchmark and report this, or at least mention that the runtime is not significantly affected if that is the case.

To summarize, the work is neither a complete systems analysis nor provides a sufficient fundamental analysis. I feel that at least one of these two aspects should be present.
"	3
Interesting idea but numerical experiments are not yet fully convincing and a lack of theory and discussion	2
The authors introduce a novel sampling procedure for the classical VAE that produces quantitatively and qualitatively strong results. The sampling approach leverages a novel theoretically-based interpretation of the VAE latent space structure. The paper itself is intuition building (e.g., toy example) but difficult to read in some places. It also appears the authors may have neglected a related body of work.	4
"Overall, I believe the paper needs revision to better support the claims in the paper. The idea of incorporating geometry in the VAE generation procedure is interesting and has potential. I believe the authors need to improve clarity on experimental setup and evaluation, and better support the variability of samples generated. The current form of paper has a large room for improvement and I hope the authors are able to incorporate the concerns raised and better position their work.

----
Update: I appreciate the authors for answering the concerns raised and am increasing my recommendation to 6. The additional experiments and details on the algorithm in the appendix improve the clarity of the paper. The proposed sampling strategy shows improvement (FID metric) in conjunction with a simple VAE model. It might be interesting to see if the proposed sampling can be used to improve other VAE models. The experimental setup with OASIS is not as compelling (my suggestion of resampling is only a simple strategy and I believe there are better class-balancing classification methods), but highlights the shortcoming of using earlier VAE methods in this setup."	2
"The document is good, easy to read, and fits within the scope of the conference.
The Riemannian point of view on VAE surely brings interesting insights that might lay the foundation of other works. 
Nevertheless, I have doubts about the importance of the contributions and their impact on the field. 
Moreover, the analysis is pretty basic. Maybe because there is too much to say/study for a conference paper.

"	2
"The geometric interpretation of the VAE is interesting. Moreover, the author's proposition for resampling seems sound in the light of their proposition and the experiments are well conducted. Yet, the theoretical link between the VAE and the Riemannian distance based VAE requires a more thorough discussion.
"	4
"I like the paper and the idea of using Riemannian geometry in VAEs. However, the paper presents the idea at a pretty general level and it is hard to see the details that matter from the computational perspective. I doubt it would be easy and straightforward to code the proposed approach up. This is my main problem with the paper, namely, the reproducibility. And the follow-up problem is about the computational burden of the proposed approach. Since it is unclear how to sample and how to calculate various components of the VAE, it is hard to assess how “heavy” the method is and whether it could be scaled up. As we see nowadays, models that are easy to scale up (e.g., hierarchical VAEs like NVAE or diffusion-based models) allow achieving SOTA performance.

=== UPDATE ===
I appreciate the rebuttal. Overall, I still doubt whether the paper is a strong contribution. Nevertheless, the new additions (especially Appendix C) make the paper better. Therefore, I decided to increase my score to 6."	3
"The paper seems to be a step in the right direction to address the issues with detection transformers. The provided figures of architectures are well illustrative and helps readers to understand the differences. The performed experiments to some extent justify the decisions made in the design choices. However, there are missing complementary experiments that I feel are required to fully explain the results. The empirical approaches are lacking as also described in the main review which damages the credibility of some of the claims.
Therefore, I am leaning towards a reject for this version of the work. But I do feel like the paper has the potential to be a great submission in a future revision, assuming the points of concerns are covered with additional experiments. I have provided more details on how to improve the paper. I look forward to see how the authors respond and address my concerns before I finalize my decision.

Edit: I thank the authors for responding to my comments in details. 
I am glad to see that the authors found some of my comments useful and took steps to further improve their work.
Some of my main concerns are resolved (5 and 9), while some other concerns are only partially addressed and they still remain a concern in my opinion (6, 7, and 8).

- The authors have not provided even a hint of performing a somewhat systematic/objective hyperparameter search to convince the readers that the hyperparameters are not biased towards the proposed method in the ablation studies such as in table 3.
- Some improved variants of the Deformable DETR paper including the two-stage Deformable DETR and results from the table 3 in that paper, seem to be still missing with appropriate comparisons in the updated draft.
- The authors have provided new results using a couple of alternative architectures using their method which shows potential for further improvements. However, these are not provided side-by-side in comparison to other methods, So that the proposed method can be judged on its capacity for improvement compared to the alternatives discussed in the paper. I do understand that such experiments are time consuming, however, the paper having taken such a comparative approach to motivate its contributions, needs such thorough empirical comparisons to prove its claims on improvements.
- It seems that the concerns of reviewer zkuH regarding contributions being marginally significant or novel, are not sufficiently resolved.

Typos I noticed in the updated draft:
- ""Some previous work also has similar analysis and confirmed this."": ""work"" --> ""works""
- ""The modulated attentions can be regarded as helping perform soft ROI pooling."": ""perform"" --> ""to perform""

Overall, the updated draft is improved and so I increase the recommendation score to 6."	3
This paper has the merits of good performance, clear presentation, and a neat idea. Although more analysis and experiments could further improve the manuscript, overall this is a good paper. 	3
This paper proposes a DETR variant that incorporates dynamic anchor boxes to accelerate model convergence. Though promising result has been achieved, the method lacks novelty and the motivation is not very clear. Thus, the paper is not ready for publication. 	2
It would be great if the authors offers more discussion about the variation of the upper bounds in equations (4,5) if we vary the robust radius $r$. The current analysis is done with a fixed $r$.	3
See comments above.	3
Overall, I can see that the paper is clearly written and addresses an important research direction that explores a better ensemble scheme of smoothed classifiers. Although I feel a slight logical gap in Theorem 1 → EBS, the paper presents a novel and theoretical justification to motivate the new training method, and it is further supported with a strong empirical results.	3
"The paper is well-written and easy to follow, and the theoretical and empirical contributions are interesting and appear meaningful. I would have liked to see different values of $N$ and a study of the *certifiable non-robustness* of the ensemble models. 

Edit: I have increased my score based on the authors' response."	4
The paper has a lot of proof, which is great, but there are some questions about the experimental results and motivation that need to be clarified.	3
A new approach for causal discovery and some interesting ideas but the utility should be demonstrated by comparing with simple baselines since interventional data is assumed to be available. 	3
Overall, the paper is a significant contribution to the area of learning causal graphs. Besides several minor points, the article is well written and should be accepted.	3
"The strength of the paper is the proposed way for enforcing acyclicity and the good properties of the proposed model. 
However, the convergence guarantee may require justification for its significancy and Condition 3. Moreover, the loss function in the graph fitting procedure may need a justification and discussion for the penalty term."	2
Interesting proposal if some of the claims can be better justified and if we can bring the algorithm closer to practice. In its current formulation, both interventions on all variables and unlimited sample sizes to guarantee convergence are not plausible in my opinion.  	3
Overall, this paper is well-written. The claims are supported by rigorous theoretical analysis and extensive empirical studies. It seems that there is only a minor issue in Section 3.2. I think this is a good submission if the authors could clarify this point.	3
The paper lacks a clear motivation why this method, and many important baselines are missing.	3
All in all, i find this paper informative and useful, but it does not provide enough truly novel insights for acceptance. I think it would make an outstanding workshop paper, or complement to some new conformer generative approach, but in and of itself it does not provide enough novelty and applied impact	2
The idea of the paper is novel, still experiments do not confirm practical significance of the method. The method is dependent on hyperparameters choice, still authors do not provide an advice how to find suitable ones.	2
The paper proposed an interesting idea for molecular conformation comparison. However, comparison with some important baselines is missed, and there is some essential technical issue, and thus I vote for rejection. But I'm willing to change if I made any mistake or the author can address my concern.	2
While the theoretical results seem new, they are not well motivated (at least practically), and the paper does not seem to provide sufficient justification of the success of the proposed methods in practical applications.	3
"Motivated from continuous dynamical system, the authors studied the convergence behaviors of discrete first-order methods. However, there is a gap between the continuous and discrete setting, and the obtained results need to be clarified. 


edit: the responses look good to me, and i raised my score by 1."	2
I found the manuscript to be clearly written and technically sound. I am, rather unfortunately, inclined to recommend that the manuscript be rejected on the grounds that the work may not have sufficient merit / novelty to warrant publication.	2
These results can be potentially useful for advances in this field. I found the paper clear and insightful in general.	2
Overall I think the introduced defense is novel and sound. The new defense is faster at training time than adversarial training, but slower at inference time. It achieves slightly better accuracies than SOTA and is properly evaluated to the best of my knowledge on white-box and black-box attacks. I will give a score of “marginally above the acceptance threshold” since the paper quality can be improved, and since I have several questions which I think require clarification from the authors (see above). 	3
The proposed method appears to achieve state-of-the-art performance. However, fully demonstrating this will require addressing the possibility of obfuscated gradients. Moreover, understanding the role of noise in the active defense via an ablation experiment will be important to see which aspects of the method actually improve robustness. If the authors can address these points, the paper would be significantly improved.	3
The motivation of its current version is unclear. The performance gain of robustness is not convincing. 	2
"The ""adaptive attack"" is not sufficient and needs improvement.  I am highly skeptical that this defense will stand up to scrutiny, and I don't think the current experiments demonstrate that it will."	2
The paper seems to be a relatively incremental contribution, that is not embedded in the related work on causal discovery and does not provide any theoretical guarantee that one would learn a causal graph. 	2
"I think this is a strong paper, both for the main result as well as for the unifying framework in section 5. My only concern is that the proposed method has many ""moving parts"" making it hard to see how it might or might not succeed in particular situations. But the empirical results are very strong, so that I recommend acceptance."	3
"The authors address an important and hard problem in learning DAG. The method is a novel and interesting contribution to the continuous optimization literature 
"	3
The paper develops a unified fiew of existing continous optimization methods for learning DAGs, which provide insights into how those methods relate to one another. However, I have some concerns about the experiments, and am willing to increase my score if the authors could address my comments.	3
In summary, while the paper has some interesting idea of combining variational Bayes algorithm with NOTEARS (continuous optimization) for nonlinear DAG learning, the significance of the contribution is not yet strongly supported theoretically or empirically. 	2
This paper proposes a novel and effective threshold selection method for semi-supervised keypoint localization. Meanwhile, I think there are some weaknesses in practicality. I currently choose borderline accept. The author may can explain about the weaknesses.	3
The paper is well-written and easy to follow. The proposed method is described in detail. The method is evaluated on four popular datasets for the keypoint localization task. The proposed method demonstrates superior performance especially in cases with only 5% of labeled data (out of no more than 10,000 examples). Ablation studies justify the design choices. The method, while combining existing techniques, is proven experimentally to be superior to the previous works and will add to the body of knowledge on keypoint localization.	3
"The paper is interesting and well written but the novelty is limited (using PPO2 to update the thresholds), the results are not very impressive (especially compared to SSKL) and the evaluation is limited to a single application (keypoint localization) while PLACL is application agnostic. Thus I believe the paper is not ready for a publication at ICLR. Please answer my questions from the ""main review"".
"	2
Overall I think this work proposes an interesting extension to previous supermask works but lack necessary experiments, making the results less convincing.	2
"This paper presents an interesting new approach of training tertiary neural networks with high sparsity levels. The paper is well written and several of the results look promising and would be of interest to the community. However, I take some issue with the motivations of this paper. As a method that is proposed to produce insights on neural network training and enable interpretablility of networks, I find a lack of new insights provided in the paper. As a method that improves efficiency and compressibility, I find it to be very intriguing and promising, but the paper currently lacks the appropriate performance comparisons with other tertiary neural networks to fully convince me of its advantages.

### Update after rebuttal

Thank you to the author for the update. I agree that there is value in training the most bare-boned network that we can. Even though not all the value has been demonstrated in the present work, I think it will be useful in many, perhaps unexpected ways in the future. Although this is not really discussed, I'm particularly interested in how this form of training can act as a particular form of regularization that is orthogonal to most other forms. I'll raise my score from 5 to 8 and encourage the authors to continue in this direction."	3
Since this paper introduces only marginal improvements over original Supermask work, this reviewer cannot find significantly new insights from this paper. Supermask needs to involve much higher model accuracy (than previous works) or minimal efforts to be computed. 	2
While the performance achieved by untrained NN is interesting, the motivation and experiments need more work. 	2
Overall, I vote for accepting. I like the idea of paralleling the HMC sampling for Bayesian-based GAN methods. My major concern is about the clarity of the paper and some additional ablation (see cons above). Hopefully, the authors can address my concern in the rebuttal period. 	3
The main contribution, the FHMC algorithm, is not presented clearly. The same for the theoratical justification. The paper would be strengthened if a formal and more detailed derivation of the proposed methodology is presented.	1
"It was hard to judge the merits of the algorithm and the contributions of the paper due to a lack of clarity in the explanations, and (it seemed to me) an exaggeration of the results.
I would be happy to re-evaluate if the authors could provide a clearer explanation of the method and results."	2
This paper proposes a complex solution for what is claimed to be a novel problem, but the problem setting as stated seems well-known and the solution is unclear both in its motivation and its details. The results appear promising but lack discussion, and the experiments are poorly documented. My conclusion is that the paper is not ready for publication at this time.	2
The paper proposed a reinforcement learning method for early classification of time series. Although it seems to have some technical novelty, the presentation and evaluation will need to be improved.	3
This draft proposes a heuristic approach for streaming data classification. The method stores some selected replicas of received data and retrains the model at each time, which could hardly handle real-world streaming data.	2
This paper proposes an adaptive solution for sustainable time-series forecasting. Although it seems to have some technical merits, the presentation needs to be improved a lot, and comprehensive evaluations will be required.	3
This is an interesting approach, and the authors provide extensive high-quality experiments that cover both end-to-end accuracy and illustrating how the method works.	2
"The proposed query/key sampling may be useful for the semi-supervised semantic segmentation task, and the idea was verified using extensive experiments. However, the simple thresholding used for sampling negative key and generating pseudo labels may be often sensitive when deployed in real-world dataset.

* Post-rebuttal: The rebuttal letter has addressed some concerns in the original comments. The additional ablation study in the appendix supports the effectiveness of computing the pair-wise similarity in mini-batches. The argument on the the fixed threshold is also reasonable. Thus, I improve my rating score from '6: marginally above the acceptance threshold' to '8: accept, good paper'."	3
"The contrastive learning for semi-supervised semantic segmentation has been presented in [1]. 
The differences/novelties compared to [1] are not significant enough for a new publication. 

- Post-rebuttal:
I slightly improve my rating score from 3 to 5 (marginally below the acceptance threshold). 
ICCV paper [1] is released on Arxiv in May, accepted in August and officially published in October.
Paper [1] must be addressed and compared in the paper. The author must make a clear statement on the differences and improvements (compared with [1]).

- For the rebuttal: ""[1] and [2] both apply contrastive learning with stored feature banks, but we sample features on-the-fly"".
The momentum memory bank has been shown effective in [MoCo, 1,2]. It boosts the scope for selecting the positive & negative pairs. ""Sample features on-the-fly"" can't be argued as an advantage over existing work without supporting experiments. Intuitively, this limits the selection of the contrastive pairs into the current mini-batch and would lead to worse accuracy. Experiments must be done to support this argument. 

- After further discussion, 
I would like to raise the rating score to 6 (marginally above the acceptance threshold).


"	3
Although this paper lacks novelty, I think that it will provide a good future direction for many researchers in this field, if it is verified that the proposed method generally works in more settings. If my concerns related to experiments are resolved, I would consider upgrading the score.	2
I think the idea of explicitly trading off the action focused vs. state focused loss to generate representations from unsupervised experience is new but closely related to precedents. It speeds convergence on easier PlaNet tasks and seems empirically to make a difference in achievable expected reward for the harder tasks in the Distracting Control Suite.	3
"The paper proposes a simple and elegant idea and shows that the approach ""works"" in practice. However, it is difficult to evaluate the effectiveness/significance of the approach. I look forward to the author's response and interacting with them to understand their approach better."	3
As discussed in Main Review in detail, I think the derivation of LCER seems a bit misleading, and further clarification compared to PI-SAC is required (since both PI-SAC and LCER optimize the CMI objective). In addition, some important baselines (PI-SAC w/ gs=2 or gs=4, Dreamer, CVRL) seem missing in the current manuscript. Considering these aspects, I larn towards rejection.	3
"While I liked the motivation of their approach, my main hesitation for recommending this paper lies both on the strength of their contribution and issues with their empirical results. On the strength of their contribution, to me it seems borderline incremental, so I would be curious to hear the opinion of the other reviewers. Moreover, the results seem marginally better than PI-SAC, which makes me wonder to what extent the better results can the better results be conclusively attributed to the better objective and not to implementational factors, like the choice of $h()$ function architecture or a better approximation of the mutual information objective.

On the empirical side, their explanation of transfer in Section 5.4 could have been clearer, and they could have compared against stronger baselines. I would also have appreciated insights into how much the representations are fine-tuned to the policy being used. Finally, it would strengthen the paper if the authors could provide insight into how well do these representations transfer to different objectives as well as to perturbations of dynamics in the same environment. The main motivation behind the authors' work is to have an agent learn state representations that capture controllable elements of the environment only. Evaluating transfer on different goals in the same environment would be a test of this, since an agent that truly captured the controllable features should not be adversely affected by a change in goal or reward function."	3
"Strengths:  The paper studies an important and well-motivated problem and provides sample complexity bounds for an interesting setting. Overall, the paper is well-written and the main results and contributions are cleanly stated.
Weaknesses: The first theoretical result does is not very surprising and its proof is rather standard.  It would also be good to discuss the technical novelty of the second theoretical result and compare it with the prior work.
Given the above, I am leaning towards rejection of this work but I am willing to reconsider if the authors or other reviewers can show otherwise.

 "	2
I think this is an interesting paper and vote to accept it, though it would be nice to see the experiments for the questions I've asked. 	3
"The problem is important and the idea of replacing sparsity with a generative model is very interesting. However, technical concerns remain.  


Revised comment: I think the authors' for their detailed and pertinent responses to comments. My major concerns are now addressed, and I am happy to assign an imporved score. I think this paper can be a major contribution, and I agree with the authors that some of the criticisms (for example, on computations) that can be leveled at this paper are relevant to the entire gamut of papers following Bora et al. Since there is no free lunch, I am willing to accept the argument that there must be a bit of give in terms of training cost or some toher tings to obtain this new kind of PCA. "	4
I think the high-level strategy pursued by the paper is reasonable, and the results are promising. Thus, the paper is a useful contribution overall. However, I have reservations about the setting of the empirical evaluation, and think more care could be taken to ensure a fair comparison.	3
This is an interesting and relevant direction to study, with promising empirical results for tasks with low- and medium-quality datasets. However, the algorithmic design decisions and empirical results (for good-quality datasets) require more explanation, and the presentation requires polishing.	3
In summary, I believe there are a lot of positives about this paper. It identifies and studies an important problem in the literature. The technical contributions and main ideas are interesting and appear to be new to this problem setting. The experiments are reasonably thorough. Unfortunately, I feel that the mentioned weaknesses (requiring online access, lack of comparison against hyperparameter sweeping other algorithms) are significant and leave open questions about the practical applicability of the proposed method and comparison to prior algorithms.	2
"Overall, I think that the paper provides a novel and valuable method with a satisfactory set of experiments to support the overall claims. I have outlined some of my questions and concerns above and think that further clarification will give me a better sense to re-assess my recommendation during the discussion period. 

----- Discussion Period Comments -----

I am adjusting my score based on the authors' response to my questions, their modifications in the paper draft and response to the feedback of other reviewers. I will follow up with specific technical questions in the individual response. 

"	3
I think it's a nice paper, that would spark a lot of discussion. Some more ablation studies are required to understand the relative importance of the different building blocks in the proposed architecture.	3
A novel contribution that brings the graph structure more strongly into the architecture than previously, building on existing work in the domain. Evaluated on relevant benchmark datasets (although one or two more could be added) against benchmark methods (again where one or two more could be added) and ablation studies included to give some insight to the impact of the contributions on the qulaity of the estimations of the missing data.	3
My score is 5 for now as there are some clarity issues. But I am happy to increase my score if the authors can answer these questions.	3
"There are a few aspects I'm not completely sure of, hence my rating and confidence level:

1. Time series imputation itself is sometimes motivated from the point of view of making good predictions. To this end, the authors might want to motivate this aspect a bit more, and also compare to methods that can make predictions from incomplete data. I don't see that addressed in the paper at all. If this is a completely orthogonal problem, that is something that the authors should also clarify. 

2. I'm not sure how scalable the method is, which is of importance. Multivariate time series can often get quite large, and the authors should address scalability either via a computational complexity argument, or some empirical evidence. 

3. combining time series and graphs is not new. There has been work in the context of forecasting (like the authors mention) and some other papers that have not been cited. The major contribution in this work seems to be adding graphs to a ""GRU"". But this kind of RNN + graph architecture has also been considered before. "	2
I only have minor concerns with the paper, and I have stated those concerns in the Main Review. Other than that, the paper is novel and the experiments demonstrate the usefulness of the proposed Partitioned Batch Ensembles (PBE).	3
Overall I lean towards rejection. While the paper presents experimental results likely not found elsewhere, in my perception, that appears to be the sole reason for the paper. Specifically, the paper presents experimental results on the robustness of V-MoE’s (vision sparse MoE’s) and adds ensembling on top of the V-MoE model to increase robustness. However, no further insight or development (e.g., in architecture or optimization) is provided.	2
The paper still has a big space for further improvement. I recommend rejecting it this time.	2
Overall, I think this paper is good and interesting. My main concern is 1) whether the comparison between autoencoder and contrastive learning is fair, and 2) how much insight can be extended to the more realistic scenarios.	3
Overall the theoretical results are quite interesting, for a very relevant problem of contrastive learning. The main negative point is about the relevance of the results to more practical settings.	4
See the above.	3
"It appears an interesting paper, but the algorithm wasn't explained with much detail which especially didn't address critical components. 

The results show sample efficiency. The asymptotic rate is not very much supported. 

The theory's message isn't very clear. 

"	3
Overall I think the contribution of this paper is not significant to the high standard of ICLR. I am leaning to the rejection.	2
"In general, the proposed model-free reinforcement algorithm, called AQE, is simple and its performance is competitive with that of other baselines on five MuJoCo environments. An ablation study is also provided at the same time. My main concern as regards this work is its novelty. As aforementioned, AQE constitutes a slight variant of the already existing REDQ algorithm.
"	2
"I recommend rejecting the paper.
A method that performs better than REDQ is beneficial from an engineering perspective. 
However, as noted above, the innovation made in the paper is incremental from what made in the REDQ paper. 
Also, the paper contains flaws in the theoretical analysis section that should be substantially revised. 
"	2
My major concern is that the empirical results are not informative at all.	3
Overall, despite its weakness as I described above, this paper does provide some interesting approach in understanding the NC with CE loss. Considering its theoretical nature and the serious study, I would say it is marginally above the acceptance threshold.	3
Overall, this paper provides new analyses for understanding the neural collapse phenomenon. The analyzed formulation removes the constraints and regularizers on the features and constraints, which is different from the existing approach.  	3
I believe this is a valuable contribution, but have some reservations about the presentation of the results. I would be happy to increase my rating after reading the responses by the authors.	3
(see above)	3
An admirable work on mining parallel sentences from building meaningful sentence cluster assignment. Tweaks including rank-based entropy and filter suite are important to make it work.	3
This paper proposes empirically useful methods to fine-tune multi-lingual sentence encoder for mining data for Unsupervised NMT augmentation. I believe that this approach shows novelty, and the paper itself good motivation and inspiration from previous work, making it a pleasant read. I have only minor concerns regarding terminologies and interpretations.	3
Overall, the authors’ proposed approach is novel and their empirical results show the effectiveness of their approach on several datasets. I accept the paper in its current form, but with some weaknesses including the requirement to hand tune thresholds for the filter suite used on the mined data, and the paper is missing the rationale behind using per batch statistics to re-balance cluster assignment without flowing gradients through them. 	4
"A good idea overall but with many details and choices difficult to understand. The paper would benefit from being restructured and simplified.
The evaluation is very questionable and prevent a publication of the paper.
"	4
This manuscript addresses an ongoing and open problem of sound source localization. I like the main idea in the paper, however, I think this work requires some minor improvements to be in proper shape for this proceedings.  	3
Overall I think the authors address a curious idea. The presentation is not entirely widely accessible and can benefit greatly from revision. The experiments are interesting but not conclusive enough to understand the root cause of improvements. 	2
"**Note**: I am not an experienced researcher in the sound source detection. My reviews may miss some published results or technology. 

Overall, I think this submission has solid novelty and state-of-the-art performance on the sound source detection task. Thus, I am inclined to accept the submission. However, my comments may not be professional and I would wait for authors response before making the final decision."	3
"The authors address a very interesting topic considering the computation of the suitable signal representation as the input of a deep neural network to address the problem of estimating the signal parameters of interest.
Despite the presented ""good"" results, the experiments are not reproducible since relevant information related to the protocol and the implementation are missing. Moreover, the theoretical concepts introduced in this study to describe the main contributions from the authors  are too superficially presented. 
Hence, I recommend to reject this paper in present state but I strongly encourage the authors to resubmit a more clear and more transparent study (for example to DCASE 22) to introduce their possibly novel approach."	3
The paper shows that existing semi-supervised objectives are proper lower-bounds on the log-likelihood of a generative model of curated data. These are interesting and valid results, but I am not confident they are very insightful. The experiments essentially show that uncurated data is harder to model, but unless I am missing something, this is not a surprising fact in itself nor is it a direct consequence of the bounds provided. Finally, I do think taking the curation process into account in semi-supervised learning (and other fields in machine learning) is quite promising, but the main methods of the paper, namely the generative model of curated data, are not new to this work.	2
"I recommend accepting the paper. First, it draws attention to an important (yet under-studied) aspect of common SSL datasets like CIFAR-10: they are heavily curated and thus their so-called ""unlabeled"" sets are really just labeled sets with labels redacted. Second, it offers insight into why several widely-used SSL methods (entropy regularization, pseudolabel, and FixMatch) work, by framing each as a lower bound of a log-likelihood for a generative model for how many annotators produce consensus. I thought this was insightful and backed by nice toy data experiments and real data experiments (perhaps the curated vs uncurated versions of GalaxyZoo used here can become nice benchmarks for future SSL methods).

In the rebuttal and revisions, I hope the authors can address my questions about experimental results and why the bounds say about when each SSL method might be preferred over alternatives."	4
The paper is generally very well written, clearly presented, and a pleasure to read. The problem is well-motivated and well-defined with respect to the background. The technique look sound and novel.	3
The theoretical and experimental contribution of the work are not enough to justify publication at the current form.	2
An interesting paper but currently the approach would need to be tested on more complex problems to more fully demonstrate the advantages of using an RNN+plasticity architecture.	2
The main challenge to this manuscript is the lack of intellectual insights, except a new combination of several existing techniques, and a lack of strong empirical results.	2
"The main issues with the paper are language and the minimal novelty of the proposed methods.
"	1
"While the introduction of yet another meta-learning framework should be supported, as the area is extremely promising, and the combination of plasticity learning rules plus evolutionary computation plus recurrent networks shows great potential, this paper is not in a shape proper for publication in a major conference, and the results on the overly-simplistic custom benchmarks are far from conclusive.
"	2
Overall, I think the paper shows lots of properties of a solid paper. It has an interesting hypothesis, a solid experimental section, and some valuable insights with great results. Taken together, I find that it clearly surpasses the bar for acceptance. However, it still leaves a few things to be desired, which prevents me from giving a higher score. 	2
"The paper shows good empirical results on mitigating dataset bias by devising a novel scoring function for re-sampling. The scoring function is the key in this work, but is not well-motivated enough. The paper can gains a lot of values by providing more analysis on the connections between per-sample gradients and the majority/minority sets.


---

Updated on Dec. 6. 
Though the paper still lacks of theoretic motivations, it does provide convincing empirical findings. I raised my score from 5 to 6."	2
This paper present a novel idea for de-biasing and its performance is validated on limited scenarios. More validation and analysis of the used hypothesis and the model would strengthen the paper. 	3
The paper presents a novel and simple debiasing approach for neural networks, as well as a denoising module for training with label noise. The proposed debiasing method shows promising results especially on synthetic benchmarks, comparing against various state-of-the-art algorithms; however, I believe the concerns listed in the section above needs to be addressed to fully justify the contributions of the work. I am setting my initial score to borderline reject but would be happy to update it based on the reviewers' response.	3
"I think the problem is definitely interesting, to understand the limitations of narrow networks and of course the extent of positive results that can be extracted from it. I think this work takes some steps in that direction, but definitely requires a bit more investigation (as specified above) and also the presentation could be made better with some illustrations/visualization of the sets in the main results, which may help communicate the ideas in a better way.
"	3
In my opinion, the article is asking an interesting question by restricting to certain compact sets. I believe that the manuscript is making a reasonable theoretical contribution to the analysis of deep narrow neural networks.	3
"Nice motivation about the question, but the results presented are relatively weak. There is not a ""main"" message of the paper that is in par with ICLR standards."	3
"In full disclosure, I am curious to know what the other reviewers think of the first contribution, but at this point this is the most positive part of the paper in my view.

However, the organization and discussion of the second contribution really confuses me. I am not comfortable with the current organization of the results related to Theorem 2 and the corresponding experiments in Section 6."	2
The paper presents a method with promising empirical results. However, the authors frame the main contribution of the paper as making a principled a theoretically sound version of deep ensembles. I do not fully agree with this interpretation given the details of the method. So, I vote for a weak reject.	3
Overall, I find the contributions presented in the paper novel and important to the Bayesian deep learning researchers to know about. Such contributions open the door for more works adapting Bayesian deep learning methods.	4
The paper tackles an interesting subject in an intuitive manner, and the extensive experimental evaluation showcases the performance of the model in several different settings. On the downside however, some of the writing needs to be heavily improved, while the connections to related work should also be reconsidered to further highlight the original contributions of this work.	2
Overall this paper demonstrates theoretical soundness and evaluates against standard benchmarks in the space of models with uncertainty estimation. However, it lacks some originality as well as comparisons with additional baselines and ablations to better separate its significance and reason for performing better / worse. There is also no clear indication of the problem being addressed and why being Bayesian matters.	2
This is an empirical work that aims to demonstrate the uncertainty calibration of DE-GP and its improved performance over a set of DE variants. The results appear somewhat marginal where reported performance of DE-GP is within the error range of other models. More trial runs are necessary to obtain more reliable estimate. Also, I disagree with the claim this work exposes a Bayesian rationalization for DE since this is not what it is doing. Instead, a GP parameterized by DE is introduced and its performance is analyzed empirically.	2
The proposed MNMs and MDAE are very appealing, and novel. I believe this will be a great publication if the authors could please address the issues listed above. In particular, I think the authors should give (1) a better motivation on why this methodology outperforms other generative methods, (2) how to compare different parameterization schemes, (3) a more thorough discussion on the experiments regarding the permutation invariance, efficiency over MCMC or other methods, and (4) companion on the quality of the generated samples with existing methods. 	3
This paper is in general a good paper. The new generative method it proposed can be of interest to the ICLR community.	3
The paper proposes an original work by offering a way out of the problem of approximating very complex densities by passing through a smoother variety. The method can be inspiring since it makes some connection and gives another point of view of the Denoising AutoEncoder proposed in the literature. The experiment shows that the method allows for generating stable and mixed samples and does not get stuck to some mode. Even if some discussions are not well-motivated, I recommend acceptance for the paper.	1
"The paper tackles an interesting problem in a clear and reasonable way. The main issues I see here are the evaluation, that to me still looks quite limited, and the lack of a theoretical interpretation for the weighting scheme.

While a possibly negative outcome of the evaluation of a node classification task does not concern me too much (the method itself is defined as a baseline and I think it does not require to be state-of-the-art in every single benchmark for it to be valuable), I think motivating the weighting scheme by formally demonstrating how it impacts smoothing when $K$ grows is an important step to make the paper more convincing.

----
I have read the authors' replies to the other reviewers and myself and they look convincing to me. I particularly appreciate they addressed my comments on related works, theoretical analysis, and further experiments on supervised tasks. They also took care of adding mean/std results for both new and old experiments (even when results are deterministic, which is great to emphasize). This makes the paper way more convincing in my opinion and worth being accepted, which is why I increased my score accordingly."	3
In summary, the paper is well presented. The motivations and the authors' insights to this model is well explained. The method is clearly described. The authors performed many experiments to validate the model's performance, with additional ablation studies. Would be nice to have more discussion on which scenarios the model works better/worse.	3
The approach is very simple, it offers no significant theoretical or methodological contributions, and the experimental study is not executed well enough, i.e., it only uses small-scale, old or synthetic datasets. Further, the are some problems in the experimental protocol, e.g., no standard deviations are reported. 	2
The paper provides an intriguing idea of soft-labeling to improve active learning (AL) on graphs. It seems more efficient than the conventional AL approaches on graphs and is more cost efficient. The results are mostly strong, except it is unclear what is the impact of the number of classes on the output performance.	3
"This paper considers a new AL setting with relaxed queries. Under this setting, a new AL query criterion is proposed to incorporate soft labels and information gain propagation. The setting and idea of this paper are interesting, but there are some concerns, e.g,, the lack of computational analysis of the AL query criterion and comparison to some state-of-the-art GNN-based AL methods. 

"	3
In this submission, in my opinion, the authors consider using relaxied queries to replace exact labeling for active learner and define a new criterion with IGP  for learning. Using the relaxied queries indeed provides the oracle a relatively easier labeling andthus more reliable. While Theorem 3.1 gives some insight. The authors provide their codes for REPRODUCIBILITY and the experiments are also relatively sufficiently conducted and compared, the results are convincing! 	4
"Strong points:
1.	The paper provides a relative new framework for GNNs where the information gain propagation is maximized. 
2.	The setting of relaxed queries seems to be facilitative in this case.
3.	Results show some advantage over other methods
Weak points:
1.	Motivation: except for the motivation for the relaxed queries, I do not see much motivation and discussion in the paper, especially a discussion around the state of the art in this area and how it related to the proposed method. Based on section 4.4 is the only advantage of IGP over others is the use of relaxed queries? What about the query criteria itself? What are its advantages relative to prior works.
There is lacking motivation throughoutt the paper for example after the IG definition why is this a good a good tool to use?
Why relaxed queries are important, where in real life could this make a difference?
2.	Paper organization: is section 2.3 meant to be related work? How is it related to section 4.4 on prior work?
3.	Technical errors:
a.	equation (8) isn’t that supposed to be v_i – on the last component?
b.	Equation (4) the indicator is on I – an index of a node equal l – a class? That is incorrect use of the two different symbols in the indicator function
c.	With respect to what is the entropy in eq (7) computed? What are the probabilities?
4.	Theoretical foundations: there is no sample complexity analysis or any optimality analysis for the given criterion. The only theorem provided is a rather trivial one showing that entropy doesn’t coincide with IG selection criterion.
5.	Clarity:
a.	Unclear sentences like: “Since the influence magnitude is diverse to the influenced nodes…” what does it mean?
6.	Experimental validation: I am left unimpressed by the rate of improvement shown by the proposed algorithm. Moreover, the analysis lacks error bars and standard deviation for the 10 executions.
7.	There is not discussion of parameter tuning nor running time of the proposed method.
"	2
This paper provides a novel augmentation strategy that leverages the latent space of pretrained VAEs to produce adversarial samples. The technique part is generally correct but is limited to InfoNCE loss, and the experiment results are not completely consistent with existing papers. Based on these concerns, I had difficulty in judging the value of the proposed method. I am currently inclined to recommend a reject and I suggest the authors keep the same configuration of the baselines to justify the improvement of their method.	3
I believe the paper presents a new method and serves to a useful problem of machine learning for a number of tasks. Due to the simplicity and utility of the method, I lean towards acceptance of the paper.	3
"While the empirical results of this paper are interesting, I still have concerns about the theoretical claims and the comparison to baselines. I would be happy to increase my score if the authors addressed these.


#### Post-rebuttal Update ####

I thank the authors for their detailed response. Based on the additional experimental results that the authors presented to all the reviewers, I have decided to increase my score to a 6. I still have two comments:

- The authors' response regarding theorem 1 does not completely address my concern. Theorem 1 as stated right now allows z to be perturbed by an arbitrary $\delta$, regardless of x. (In contrast, in adversarial training x itself is perturbed by $\delta$ which is constrained by some $\epsilon$.) Thus, it is still not clear how the bound in Theorem 1 can hold for some arbitrary $\delta$. For instance, imagine we have two images $x_1$ and $x_2$ and their corresponding latent $z_1$ and $z_2$. I could simply define $z_1' = z_2$ by selecting $\delta = z_2 - z_1$. In the case, I would expect that the bound is violated. I believe that there should either be an assumption on $\delta$, or it should feature in the resulting bound.

- In the caption for Table 8, there is a typo: `copmaring`.

- The authors should incorporate all the additional results they reported during the rebuttal into the paper (or appendix).


#### Post-discussion Update ####

Over the course of the discussion between the authors and the reviewers, it has become apparent that several of the experimental settings chosen in the paper are non-standard, which make it hard to verify the validity of the results. I do share the concerns of my fellow reviewers in this regard, and thus will downgrade my score to a 5."	3
This work introduces a novelty augmentation strategy with sufficient empirical and theoretical arguments. Although some concerns remain about its scalability to larger batch sizes and more challenging datasets, the authors clearly show significant performance improvements when using IDAA across all experimental settings and empirically justify all claims/hypotheses made.	4
"Overall, the idea is interesting, however the current draft is not well-justified on the actual algorithm design, the motivation, and (perhaps the most important) empirical comparisons to existing CL methods that considers hard view mining. Further comments/questions are listed in the weaknesses / questions part.

The paper has its potential to the field, but issues need to be addressed first. I'm happy to change my score if the feedback addresses my concerns. Please refer to the points in the weaknesses / questions part. I would like to see feedbacks on these comments/questions."	3
"Recommendation 
------------------------
I recommend accepting the paper because the proposed method is new, addresses an important problem and empirically demonstrates its benefits."	3
In summary, I believe the idea in the paper is novel and promising. However, the authors have not thoroughly/carefully developed the picture (as explained above). Therefore, I suggest rejecting the paper for now --- but I look forward to upcoming revisions.	3
I think this paper contributes a new idea in this field.	3
This is a nicely written paper for enforcing orthogonality of convolutional kernels with a computationally efficient method. However, I find the experiments to be shallow relative to related works, and cannot fully justify the practical value of the proposed method. I also have some minor concerns on the technical approach, that hopefully can be clarified during rebuttal. 	4
My concerns are primarily due to similarities to some of the works in semi-supervised DA techniques. Considering that the paper addresses the problem in a more general setting of classification and regression, I am leaning towards accept for now. 	3
Approach seems promising, especially the empirical results. I am lacking a bit of information to set this into relation to previous works as outlined above. Without these details it is hard to judge how much can be attributed to the proposed approach. 	2
In general, the proposed method is well-motivated with sufficient experimental comparisons. Most importantly, the paper seems to be the first step toward data-efficient regression leveraging strong stochasticities from both data and model. To this end, I believe this paper has enough merit to be accepted.	3
The paper proposes a good analysis of provided robustness guarantees, but further clarification of experiments would be very helpful.	2
This paper is overall well-written and provides new insight on intersection of robustness and privacy.	2
While the paper seems to propose many different things, I find that these are mostly small variations on existing work and I think the authors tend to overstate the papers' contribution (see main review for more detailed comments). The paper generally also feels disorganized due to having several disparate topics (algorithms for training DPFL models, some results that would fit privacy accounting paper better, certifiable properties of DP models). In general, I think this paper could clearly benefit from more work to sharpen the focus and the contribution it has to make, and to make it more clear and readable as well.	2
I do not recommend for acceptance because of (1) lack of novelty; (2) incoherent presentation.	2
I give this paper a strong recommendation, in spite of some missing ablations. 	4
Overall, the paper proposed an interesting idea and showed strong empirical results, hence I vote for accepting.	3
Overall, the paper's idea is powerful (but of somewhat limited novelty) and the results are good (but not great).  Its greatest strength IMO was the ablations.  My biggest complaint is that it's not completely clear the instructions themselves are important at all - I suggest a few more experiments, though they don't seem crucial.	2
While the method is not new, the empirical results are strong and comprehensive. Though I disagree on the interpretation of some empirical results, overall the additional analyses bring us further insights on what method works for very large language models (i.e. > 100B dense model). I highly recommend the paper to be accepted to ICLR 2022. 	1
Overall, this work has some merits but I expect more than that to get in for ICLR. Some investigation is rudimentary and is suggested to provide more deep insight. 	2
"
Balancing the strengths and weaknesses of the proposed method, I would like to recommend a rating of weak rejection for this paper.
"	2
It would be better to provide more explanations for some findings, and further improve the experiments for more general Transformer cases and different types of attack methods.	2
ViT demonstrates the potential to work as an alternative to CNNs. The adversarial robustness of ViT is indeed an important topic. As claimed by the author, this work makes the first comprehensive study on this topic. However, some concerns listed in weaknesses above remain to be addressed. Therefore, I rate this paper blew the acceptance threshold. I happy to raise my rating if the concerns are well addressed.	3
"In summary, the paper needs some adjustments before it is ready for publication. The related works need to be updated with the relevant works (Self-Cond-GAN and PGMGAN mentioned above). The experiments and empirical results need to be compared with these relevant works or discussed why the comparison might not be fair. 

EDIT: Given the proposed method is moderately novel and the newly presented comparisons after the discussions with authors, I increased my rating from 5 to 6."	2
Overall, the paper is somewhat novel and easy to follow. The assumptions and decisions are well supported. The stepwise experiments are helpful and provide good insights to evaluate the proposed algorithm.	3
Although the derived Stein latent optimization for imbalanced attributes is interesting and promising, the current experiments are not enough to support the claims.	4
"I think this is a reasonable combination of components that make progress on the task: facilitating learning of samplers from data with highly imbalanced class attributes. Given the performance increase compared to [1], it looks like using implicit reparameterization and a contrastive loss (the main novelty here) make a clear difference in performance. I think each of the components are justified and perform well. 

[1] Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and R Venkatesh Babu. Deligan: Generative adversarial networks for diverse and limited data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 166–174, 2017."	3
Overall, this paper is difficult to read and understand, the experiments on CIFAR-10 are not convincing enough, and some claims are underspecified. The innovation of multi-level error correction does not outway these negative factors and as such I recommend rejection.	2
"
The results in this work are rigorous and the motivation is well explained. The theory is done under reasonable assumptions and the rates are close to what is expected. Unfortunately, there are a few big issues with the proposed solution. First of all, there seems to be a way to make EF memory-efficient without any algorithmic changes. Secondly, the guarantees for the method imply that the methods are significantly slower than standard EF and may be even slower in runtime than SGD without compression. Finally, the experiments may not be the best possible, but I personally think that this is not a big issue as the main contribution is to propose new methods and analyze their convergence."	2
The need to compressing the local error in distributed training is not well justified in the experiments. The proposed application scenarios of training large models and federated learning are not demonstrated by the experiments. Besides, the scalability regarding the number of workers and the model size is not very convincing.	2
Overall, this is a good paper that has good motivations and reasonable solutions. The experiments are thorough. I recommend to accept the paper and hope the authors can revise the paper according to my suggestions. 	3
The motivation is somehow interesting; the techniques are kind of empirical and less interesting; the improvements over Deformable DETR are not very surprising on both accuracy and speeds.	2
The paper suggests a novel approach for improving computational efficiency of Transformer-based object detectors. Most contributions are supported by experiments, ablations and insights. The contribution with auxiliary losses in encoder requires more investigation. Overall I recommend accept.	4
Overall, this paper provides valuable insight into reducing queries for DETR-based detectors and I prefer to accept it.  	3
This paper solves an important problem in denoising diffusion models by parametrizing denoising with a multimodal conditional GAN. Experimental results show that the proposed model achieves good sample quality, mode coverage and speed without struggling in the trilemma. The minor issues are in the experiments. It is encouraged to test and discuss different setting. Therefore, I recommend to accept this paper.	3
Diffusion models come with a lot of desired benefits such as high quality samples, data coverage and stable training. However, their slow sampling speeds limit their applicability and the problem of improving this aspect with a new generative model is very relevant. The idea of combining diffusion models with the benefits of GANs by modeling the reverse process with conditional GANs is plausible. The main question is whether such an approach can really combine the advantages of the different models. To that end, the experiments are quite convincing and I think the paper provides a positive answer to that question, which will make it significant for future research. Thus, I recommend acceptance.	3
"In summary, the paper provides a nice combination of adversarial and likelihood-based training and, in my opinion, represents a natural and novel extension of DDPMs towards faster sampling times. The experiments validate the claim that denoising diffusion GANs can indeed help overcome the ""generative learning trilemma"", and most of the ablation studies justify the design choices made in the paper. I think this is a good paper worth publishing at ICLR 2022, and I am willing to increase my score if my remaining questions and concerns are adequately answered.
"	3
"I think the proposed idea is clever and very well executed. Experimental results
validate the method in terms of the triple sample quality / mode coverage /
sampling efficiency trade-off. Experiments demonstrate their conditional
generator exhibits the strong multi-modality required for the large reverse
diffusion steps.  I expect this hybrid between conditional GANs and diffusion
models to become an important player in generative modeling.
"	4
See above.	2
"Overall, it is my opinion that the data collection are extensive and covers many different scenarios, although it is quite confusing to read it due to many terms and different type of bugs. 
However, the novelty in the learning-based auto program repair is quite lacking. Furthermore, the evaluation is not suitable (e.g., uses cross-entropy to evaluate the model,  where you can actually report the difference model performance on quixbug). The paper will be much comprehensible if you employ similar metrics for all experiments and evaluations (e.g., top-1 fixes)
"	2
Authors present a bug fixing approach using tuned transformer models. Authors generate synthetic bugs by using reversed commit data. They additionally train the model on stack traces with errors caused by synthetic bugs. Authors propose using additional textual context to improve bug fixing. All this results in improved bug fixing model. I am uncertain if the contribution is significant enough for accepting the paper. 	2
In summary, this paper presents some potentially state-of-the-art performance on neural code repair. But there are a large number of nontrivial details that are left out from the methods, results and discussion parts of the manuscript, which I respectfully request the authors to address. If the authors can satisfactorily address these questions and concerns, the manuscript will be a strong candidate for publication in this conference in my opinion.	2
This paper proposed a simple method which incorporating entity abstraction to transformer language models. The method is simple while somewhat trivial, and the novelty/technical contribution is not very significant. The experimental results on synthetic dataset is good but not as well on realistic datasets. Therefore I believe this paper is below the bar.	2
Technically, the paper presents novel approaches to incorporate entity type abstraction with transformer language model. As the related work section discussed, there are a lot of other related works also incorporating knowledge from named entities. To me, using only the entity type is marginally novel. Also, the empirical findings lack deeper analysis and were merely conducted on a single and toy-sized pretrained language model. For empirical results, the only significant improvement is on a synthetic dataset while the gain on real-world datasets is also marginal. Therefore, the significance of the paper's contribution is limited.	2
Overall, this is a solid empirical paper with convincing experimental results. It will be of great help for revealing the intuition of the model if introducing more analytic experiments.	2
This paper tries to address an important problem, but the experiments conducted show that the proposed methods are not scalable to more real-world datasets. I do not recommend acceptance of the paper in its current form.	3
Overall, I agree that it is important to study practically successful and well-used algorithms (even if they do not have any particular benefits from a theoretical standpoint). However, considering the theoretical nature of the paper, I think the contribution of this paper is marginal given a long line of work in the convergence analysis of practical RL algorithms. I lean toward rejections.	2
I think the paper provides interesting theoretical results and analysis. The idea of using a reverse experience replay buffer sounds interesting for practical use, though I'm not entirely sold there as mentioned in the main review. The work is not groundbreaking but is a solid advancement in its niche.	3
Overall, the paper is clear with solid theoretical bounds. Their results show that the Q-learning algorithm with online target learning and experience replay can provably converge to the optimal policy with near-optimal sample complexity. One concern is that the assumptions may simplify the analysis of the problem. Besides, more discussion on the related works needs to be added.	3
Overall, due to the weaknesses I mentioned above, I think this paper does not cross the bar due to the high selective of ICLR. Since the theorems are correct from my point of view, I will choose weak reject.	3
The paper describes a relatively simple modification of the DQN algorithm using a proximal term. The approach improves the performance of DQN, but it does not improve on the current state of the art.	2
My reasons are outlined in the weak points section above. While this paper provides a set of promising empirical results, the relative lack of explanation and analysis around why proximal iteration helps in deep Q-learning significantly detracts from the impact of this paper. I would like to see these aspects of the paper improved, perhaps drawing on previous works that have tackled this problem. Relatedly, given that other works have attempted to improve deep Q-learning via regularization, these works need to be cited and, ideally, compared against. Finally, it would help to expand the scope of the results section to include other environments or setups in which deep Q-learning is used, e.g., actor-critic algorithms, demonstrating the generality of the proposed method. In its current form, this paper will leave readers a) confused regarding what problems the proposed method is tackling, b) unaware of previous works in this area, and c) unsure about the method’s generality. For these reasons, I cannot recommend acceptance.	3
The authors use a simple trick to improve the performance of DQN-type algorithms on some Atari environments. Although the proposed method has achieved good results, the novelty is somewhat limited, and the reasons for the performance improvement are not explained clearly.	2
This paper is innovative and well written, the experiment parts are very solid and the authors give a clear analysis. But I'm concerned about its technical contribution and conservatively rate it as around the borderline. I hope the authors could address or explain their thoughts about my concerns.	3
"This paper should be very careful to define a class subsampling for federated learning applications since it can leak clients' private information. Also, this paper requires more comprehensive experiments with considering other communication-efficient algorithms.
"	2
The paper is generally well written. However, the experimental results and technical novelty are less convincing. I would lean towards objection for now.	2
This paper introduces a novel and simple approach to a practical cross-device problem. The experiments are well conducted and support the claims of the paper. I think this paper should be accepted. 	3
"Overall, I enjoy reading the paper as it pointed out an interesting fact and problem. However, the proposed method seems to be too straightforward with insufficient novelty --- negative class subsampling has been widely used and including positive classes of the client seems to be quite intuitive. I have also listed several potential ways to strengthen the paper (please see the main review). I think the current version of the paper is below the acceptance bar, and thus I give a score of ""3""."	2
Strong and interesting results. Slightly incomplete, but still a good contribution.	3
This paper makes an interesting technical contribution, giving further evidence that sketching techniques from the streaming literature preserve differential privacy without further randomization. The paper can be accepted, although it would be stronger if the authors gave applications of their results. 	3
"Theoretically, the paper seems nice but it could be made stronger by providing motivations for the specific problem being studied in the paper, giving better intuition for what technically is new in this paper and by possibility solving the problem for the case when the updates can be negative. These reason being the paper a bit below a clear accept for me.

Post rebuttal comments
----------------------------

The authors have addressed most of my concerns about the paper. I'm still not super-convinced about the usefulness of the result for the entire range $p\in (0,1]$ since the entropy applications seems only to need values $p=1-\Delta$ for $\Delta\to 0$. However, _theoretically_ studying the whole range still makes sense. After the rebuttal the paper is more of a 7 for me."	3
"1) They provide reasonable bounds to show that the $\mathbb{F}_p$ sketch of (Indyk, 2006) is $(1/p\cdot \ln\rho_p)$-DP.  I like the flow of the exposition. This is a valuable contribution to the DP literature.

2) Inadequate privacy guarantee: In the literature, it is recommended that the privacy parameter be a constant. But instead it is $(1/p\cdot \ln\rho_p)$ which might provide no privacy at all for small $p$. [mostly addressed]

3) Experimental Evaluation: the experiments are rather sparse; also the ""real-world"" dataset is not specified. Where is it from? [addressed]

4) There might be some typos in the proofs. Also one calculation (Theorem 5) isn't fully specified. [now addressed]"	2
Overall, the paper contributes important ideas where the core ideas are well-supported by experiments and the strengths outweigh my concerns. So I vote for accepting the submission.	3
The proposed model, SAINT, seems like a well-motivated model, but the presentation of the experiment results and related analysis is either misleading or incomplete, therefore making it difficult to accurately evaluate the work in its current form. Reading the supplementary material further decreases the reviewer's confidence in this work's claimed contributions. 	2
This paper considers an important problem and presents interesting technical contributions which extend what was done in previous works. However, the experimental evaluation was not done rigorously enough to support the main claim. Different models had a different number of hyperparameter tuning iterations, and the tuned hyperparameters were not necessarily the most important ones. Therefore, based on the current data, the paper does not meet the criteria of ICLR. I would reconsider it if the authors provide the required data along the lines described above (even if the updated results would show a smaller improvement, its significance and soundness could be much higher).	3
Overall, the paper has important contributions, especially in improving semi-supervised learning for tabular data. However, there are different issues in the content listed above, that need to be addressed. 	3
Overall, the proposed loss is not novel enough and needs to have a further comparison with other pertaining losses such as the losses from BART. And also missing some strong Seq2Seq baselines with knowledge distillation losses.	2
Overall, this paper presents an interesting experiment to discover the trade-off between the compression rate and the performance. The experiment seems a little bit naive without detailed analysis.  I recommend rejection for this paper.	2
 In this paper, the baseline system is far from literatures and is lack of comparisons to existing methods.	2
This paper describes a simple approach to reducing the dimensionality of the decoder to be used for a pre-trained encoder.  The experimental results may be of interest to practitioners concerned with reducing model sizes.  However, the original contribution is rather small and the restriction to a single downstream task leaves questions over its general applicability.	2
While this paper provides some interesting insights into the training of infinitely deep and wide ResNets, it is not well-demonstrated whether the results are novel and significant enough compared to various existing works. Some detailed discussion and comparison may be very helpful to improve the quality of the paper.	2
"Interesting results, but contribution should be clarified.

I am willing to raise the score if the issue of novelty is clarified."	3
Overall, I think this paper has useful ideas for understanding the training dynamics of ResNet in the nonlinear regime, and it complements the existing research in the literature.	3
This is a technical paper that removes a strong assumption from the analysis of Lu et al 2020, though the techniques are not surprising. A number of edits are necessary. Though giving a confirmation to a result that is expected, the paper is still a good theoretical contribution, in light of the heavy technical works that are required.	2
As a reviewer mentioned, the idea behand the paper is interesting. However, there are couple of comments needs to be addressed. I recommend this paper marginally accepted.   	3
"Although there are some items that should be better explained / clarified, overall I enjoyed reading this paper and find the addressed topic of high interest for the community working in the field of low-precision training/inference of neural networks. Although I am not familiar with the prior art, the paper seems to be the first to propose a method for analyzing the error-memory trade-off in a deep learning low-precision training/inference scenario without exhaustive search. 

I like the two-step meta-training/meta-test approach, however I am not yet fully convinced about the main property of the meta-training algorithm, namely the rapid decaying of the singular values of the error matrix. In terms of evaluation, I appreciate the authors analyzing the performance of the meta-training approach using multiple metrics and providing recommendations for the optimal sampling conditions. 

Also for the meta-test phase, the authors analyze the performance of the ED-MF approach to other methods such as bayesian optimization, random selection and QR with matrix factorization. ED-MF out-performs these methods in most of the cases. Some results are not clear unfortunately - see my questions in the main review above. Hopefully the results will become more clear during the paper discussion / rebuttal phase."	3
I believe the paper proposes interesting approach in selecting low-precision training configurations. Formulating into multi-objective optimization and using meta-training/testing to find Pareto frontier of configurations for the overall accuracy seems like the key contribution of the paper. I am not an expert on low-precision training, but to the best of my knowledge, the overall approach seems reasonable and the evaluation seems okay. However, I believe it would provide better reference for the readers if the authors provide evaluation on ImageNet.	4
"The motivation and contribution need to be made clearer. In addition, the experiment part that supports the contribution needs to be thoroughly supplemented. Please address my concerns in ""Main Review"" through the rebuttal process."	2
"I am leaning towards accepting the paper because the approach is technically sound and the results are competitive. However I would like to see more of a discussion on the comparison of the proposed approach with the less expensive CRE baseline (see my recommendations above) to be completely convinced.

Comments after rebuttal: I am satisfied by the additions made to the paper. The experiments on the Kather (medical) dataset do illustrate the superior L1 ECE (canonical) of the proposed approach, and the time measurements show that the overhead introduced by the proposed approach is not significant. I am therefore increasing my score to reflect the same."	3
Justification of the recommendation is based on the strengths and weaknesses stated above.	3
The paper presents a conceptually simple approach to improving the calibration of classification models.  The paper is interesting, but could use some restructuring.  It is not totally clear how to use the method in practice (especially on large datasets where $O(N^2)$ would be prohibitive) and it is not clear if the proposed methodology substantially improves calibration over existing approaches.	3
Overall, I think this is an interesting paper, but not ready to publish in ICLR in its present form. I hope that the authors can improve the empirical aspects for this paper. 	2
The approach based on maximum information gain is very interesting. The uniform generalization bound for overparameterized neural networks given in this paper is neat.	4
"I think the contributions of this paper are incremental, and is limited to using already known techniques to generalize results in [7] and [6] for $s > 1$. Another contribution is to derive the bounds in Theorem 1, 2 and 3 for RF kernels in addition to NT kernels. However the results for RF kernels are implicit in the other works.

## References 

[1] David D Lewis and Jason Catlett.  Heterogeneous uncertainty sampling for supervised learning.  In Machine learning proceedings 1994, pp. 148–156. Elsevier, 1994.

[2] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.  On exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.

[3] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint , volume 48.  Cambridge University Press, 2019.

[4] Arthur Jacot, Franck Gabriel, and Clement Hongler.  Neural tangent kernel: Convergence and generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.

[5] Dongruo  Zhou,  Lihong  Li,  and  Quanquan  Gu.   Neural  contextual  bandits  with  UCB-based  exploration.    In  Hal  Daume  III  and  Aarti  Singh  (eds.),Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research,pp. 11492–11502. PMLR, 13–18 Jul 2020.

[6] Kassraie, Parnian, and Andreas Krause. ""Neural Contextual Bandits without Regret."" arXiv preprint arXiv:2107.03144 (2021).

[7] Geifman, Amnon, et al. ""On the similarity between the laplace and neural tangent kernels."" arXiv preprint arXiv:2007.01580 (2020).

[8] Alberto Bietti and Francis Bach. Deep equals shallow for ReLU networks in kernel regimes. arXiv preprint arXiv:2009.14397, 2020.
"	1
This is a well-written paper with a non trivial advance over the state of the art and a reasonably large number of computations, with one semi-novel technique involved. 	3
Though Lemma 1 is a nice observation, Theorems 1 and 2 are trivial. Theorem 3 is not proved and should be wrong, to this reviewer's opinion. 	1
The paper makes important contributions to the study of RKHS associated to the NTK. The main drawback, in my eyes, is that the scope of the paper is limited.	3
The approach taken in this paper is very creative and the insights gained from this paper will inform research in this space for quite some time. As such, I strongly recommend acceptance of this work. I would be happy for this work to be selected as an oral presentation.	4
As a reviewer mentioned, the benefits and motivation of the proposed works is not obvious and the experimental result have not shown the advantages of this approach compared to state of the art works in 8-bit integer quantization. Therefore, I recommended this manuscript is marginally rejected. 	2
"Overall, I think this paper lacks some motivation for the fix-point format. Given the fact that quantization has been studied extensively by the previous work, I am not convinced that 8-bit fix-point can achieve a better hardware efficiency than the previous work. Some hardware results may be helpful.

"	3
"The authors propose a method to estimate the optimal fractional length of a fixed-point format given a float tensor, as well as other minor improvements to a quantization-aware training flow such as (1) combining PACT and quantization, (2) a two-pass method to handle batch norm. However, the novelty of the proposed methods is low, and it's not clear to me what improvements they offer over existing standards. The end-to-end accuracy improvement of the authors' technique is fairly small. As a result I rate the paper marginally below acceptance. If the authors can provide experiments that show specifically how each of their proposals improve over existing standard, then I would consider raising my score.

EDIT: The authors have addressed most of my concerns and I will raise my score to a 6. I still think that the formulas to compute the fractional length are not novel. The idea of estimating the dynamic range of the tensor and using that to determine the quantization range is well known."	3
The authors proposed to add adversarial perturbation to enable large mini-batch finetuning of  sequence models. However, the empirical study is limited to BERTBase model on GLUE datasets.	2
Generally, I feel this paper presents an interesting solution to solve the convergence and generalization problem of large-batch fine-tuning. However, the proposed method is somewhat similar to existing work, which limits the novelty. I would like to hear back from the authors for the final opinion.	3
In summary, this paper is well-written and i can see the efforts of the authors to make it rigorous and well-organized, the ablation studies and analysis are also very comprehensive. But I'm not sure about the novelty of this paper, ScaLA to me is more like a combination of several methods, incorporating adversarial perturbation and layer-wise optimizer. Moreover, the benefits of adversarial training on normal batch size in both NLP and CV domains are well-studied, so I think the novelty is restricted. I vote for a weak accept for the written quality and the rigorous analysis of this paper.	2
"I believe that as written the paper is not ready for publication. I believe that statistical analysis of the results is necessary, and can be reasonably completed in the response period. I would be willing to upgrade my scores (correctness and significance) based on that. I also believe that the edits for clarity would significantly strengthen the paper.

Update: The authors incorporated most of the comments, and the statistical analysis bears out the major claims."	3
Overall, while somewhat incremental, I think this paper adds a valuable contribution, is very thorough, and pretty coherent. The focus on simply modifying the inductive biases in the architecture (rather than dramatically changing training) is also a good venue fit.	3
I have some issues with the novelty of the work presented, the accuracy of the claims made, and the somewhat lacking comparison with the related work and key ablation experiments. See main review, above.	2
"I think there is interesting work in here, and a nice story to be told. Unfortunately, the current structure of the paper is not good enough to support this, and the paper is in need of a substantial amount of rework to improve it's clarity.

At present, it lacks the clarity required to best showcase the results, and I cannot confidently say I understand this paper, because it is not explained or presented clearly enough.

==== Post rebuttal comments ====
Thank you for revising this work, I think it is much stronger now after the restructuring and clarification, so have increased my score."	2
"The paper presents a concrete algorithm with good results in general. It would be helpful to provide some more details and insights in what makes the method work and what are some limitations of the current work.

"	3
This paper provides an interesting perspective on efficient agent design with reasonable technical approaches, and the results look empirically good. I recommend acceptance.	3
"Strong paper. The proposed approach is novel and can be impactful in other discrete-continuous optimization domains. Experiments validate the advantages of the proposed method. The paper is clear and easy to follow.

====

I am satisfied with the response of the authors and I maintain my score ""8: accept, good paper""."	3
"The paper introduces the (to the reviewers knowledge) novel concept of learning the morphology with RL instead of evolution. The experiments need more seeds, but the advantage of the presented method appears significant. The method to allow asymmetric morphologies appears less significant and one might find a more elegant way to ensure this property (if it is at all needed). 

The reviewer would be willing to increase the score if the authors could substantiate point (1) and promise to discuss (or convincingly argue against) the other points of criticism.

**POST-REBUTTAL**

While the response does not answer 100% of this reviewers questions, the resulting paper is nonetheless interesting enough, and the results clear enough, to merit publication.  The reviewer is therefore raising the score to 8."	4
The authors proposed an novel approach to improve the ViT's robustness. Comprehensive experiments are conducted on various robustness benchmark. Results showed the proposed method can improve the robustness of ViTs. My main concern is that the major claim of the paper, that ViT's fail to make adequate use of global information, is not supported by the experiments. I'd like to accept the paper if the authors can make the claim more supportive. 	3
"In the paper, the experiments are conducted primarily on ImageNet robustness datasets. It will be interesting to see, out-of-domain generalization of the features learned by the proposed approach e.g., ImageNet pre-trained features on fine-grained datasets (Flowers, Pets, Fungi, Sketches, Birds, etc).

I am not entirely convinced by the shape-vs-texture bias claims made in the paper, due to lack of principled study as in  https://openreview.net/forum?id=Bygh9j09KX

Also, it will be interesting to see if the proposed discrete tokens enhance ViTs robustness against common corruptions (e.g., artifacts introduced by rain, haze etc), and adversarial perturbations. The considered robustness analysis is too restrictive in my opinion."	2
The submission is pretty solid and the claims made in the paper are convincing. To the best of my knowledge, the proposed method is novel and the contributions are significant. The authors conduct detailed ablation study which are pretty interesting to learn. There are some observations unclear to me, but they are not critical issues given the solid experiments presented in the paper. Overall I believe the quality of this submission is great. 	4
"Suggestion.
1. The authors organize splendid discussions in the introduction and related work section, and some discussions look interesting. But the latter paragraphs could not work well in concert with the previous writing.  It is suggested that supplement enough in-depth analysis to demonstrate the mentioned illustrations. After reading this paper, the reviewer could just observe ""ViT + some global information would be better"" rather than your method is necessary."	3
Based on the previous comments, I tend to recommend accepting this paper. But I would like the authors to resolve my concerns first.	3
"(1) I suggest the authors to include a section discussing the background of Neural ODE, adjoint method etc. in the Appendix.

(2) Technical assumptions should be added, and the proofs should be made rigorous (if possible) throughout the paper. Otherwise, the paper stands as a non-rigorous treatment of the subject.

(3) On page 7, '3' and on page 9 'Invariant Imbedding'  are typos.

(4) In the caption of Table 1, the stop somehow appears in the next line."	3
Overall, this is a good paper which provides an exciting novel architecture, with good theory and some simple experiments to justify the quality of this architecture, with comparisons to related work. My current issues lie with the clarity of writing and the explanations of the models used in the experiments. Given that these issues can be easily resolved during the rebuttal process, I lean towards accepting this work, pending some extra explanation provided by the authors.	3
The paper has an interesting idea to condition the Neural ODE on the parameter p that can be interpreted as the “depth” of the network. The Results section does not explore the new architecture enough and provides only the basic results to demonstrate that the model works. I suggest the acceptance of the paper, but recommend the authors to expand the results section. 	4
This paper provides convergence guarantees of MCES algorithm under mild assumptions, hence resolving an important open problem in RL. I enjoy reading this paper and I think it has enough contribution to be published at ICLR.	4
To summarize, while I enjoyed reading the paper and its simplicity (which I appreciate in general), its contributions are correspondingly minor and not surprising. The combination of the highly restrictive realm of tabular MDPs in which states are never revisited, together with the strong assumption of ability to sample every state-action indefinitely, generate an easy problem that is not particularly hard or interesting to solve.	2
Overall, I think that the paper provides a novel contribution. However, given that the considered assumptions are, in my opinion, very restrictive, I have concerns about the significance of the theoretical results. For these reasons, I opt for a borderline score.	2
Overall I like this paper because of the strengths listed in the main review. There is some weakness but it doesn't weaken the main contribution of the paper.	4
Overall this is a solid contribution on the question of preventing capacity loss in deep RL. The ideas are novel and the point of view developed on representation collapse, and the proposed way to prevent it, is a significant contribution to the community. A number of points could fruitfully be discussed, both in the main text and in the appendix, which would increase the practical impact of the paper.	4
"Overall:
This is a well written paper which gives strong evidence that capacity loss/representational collapse is indeed a root problem for deep RL. As long as the authors address the weaknesses, I am willing to bump this up from weak accept to strong accept."	2
"Overall I think the paper makes some interesting and novel contributions, the problem of capacity loss is rarely studied in the RL setting, and research in this direction might help us better understand the difficulty in DRL training. Though I currently do have some concerns. I look forward to reading the authors' rebuttal. I will consider increase my score if my concerns are fully addressed. 

========================================================

post-rebuttal

I now increase my score to 6 since I feel a lot of the concerns are indeed addressed in the response. However the authors should go through the paper carefully and try to make things more clear in the paper, would be nice to incorporate some of these reviewer discussions into the paper or appendix. "	3
While the paper tackles an important problem, the proposed solution and it's empirical exhibition are insufficient in my view for acceptance.	2
I generally like this paper and am in favor of acceptance. I think the paper presents a pretty good narrative and addresses the noted optimization issues with a somewhat novel and theoretically grounded approach. The experiments are pretty comprehensive although admittedly of very limited scale. What stops me from giving a higher score is mostly the somewhat disconnected nature of the proposed contribution to the theory of MARL, which is at this point mostly based on intuitive/empirical findings.  	2
The authors identified a concrete problem in offline MARL training and proposed a method that is addressing the issue. The evaluation shows that overall the proposed method outperforms the existing baselines.	3
Overall, this paper addresses important question, but is not well researched. So, I consider the paper to be at the level of borderline for now, and I recommend marginally below the acceptance threshold (5).	3
Overall, I think this paper contains some interesting and useful ideas. Some parts can be further improved as mentioned before and I hope that the author response will address them.	2
SWARM parallelism is an elegant approach to train large neural networks in non-HPC settings and it has been shown to achieve decent compute throughput in a cloud environment. However, it requires several model modifications to run efficiently, and it has not been shown that these changes do not degrade the learning quality or slow down training.	4
The topic studied is definitely of interest, yet the authors do not manage to sufficiently explain their method. Furthermore, they fail to demonstrate the advantages of their method that they claim. The experimental part is lacking substantial thoroughness, and more studies should be conducted in that regard.	3
"I recommend rejecting this paper in its current form. Although the paper is well-written and I liked the proposed algorithm, I found the experimental section to be lacking crucial experiments to justify the paper’s claims. My biggest concern is that there is not a single speedup comparison to existing model-parallel techniques. Furthermore, several key details regarding accuracy drops and computation overhead negate the paper’s empirical impact. Having some speedup experiments and comparison to existing algorithms would help increase the score.


### Update Following Revision #2
Throughout the rebuttal process, most of my questions and concerns were fulfilled by both answers and paper revisions. I'm still concerned about the proposed compression technique, which I find the weakest part of this paper. However, with that, I think this paper in its current form should be accepted. The paper addresses an important topic with a high potential of impact on the ML community, both as a tool and research direction."	3
This paper proposes a novel and practical method to do pipeline-parallel training for large scale models. The literature review is adequate and experimental evaluation is convincing. Although some details of the system design is missing, I am more than happy to raise my score is they can add those details to make this paper more concrete and useful. 	4
I have no objection to the importance of the theme or the validity of the idea. However, almost all of the experiments did not show the superiority of the proposed method, and it would be difficult to accept it to ICLR, a leading conference.	2
The idea is similar to variational information bottleneck. The main concern is the experiments, especially table 4, that is too far behind other state-of-the-art models that have a similar size. 	2
The core idea presented in this paper is strong, but the lack of an appropriate baseline and the significant rewriting required make it unsuitable for acceptance.	3
Proposes a novel method and clearly elaborates it. Extensive experiments verifies the effectiveness, as well as the design choices.	4
The idea of leveraging linear property predictors trained on latent dimensions of deep generative models for molecule manipulation/controlled molecule generation is not novel. The work does not consider comparison with. existing baselines, nor does it report performance on real-world property manipulation tasks (such as protein activity manipulation). It is not clear what is the real-world significance of continuous property change. 	2
While the direction pursued by the paper is interesting, there are many strong assumptions made with little justification, and overall it's not clear how one should interpret the results in this work, or what the impact on realistic optimization tasks is. Finally, some things are also unclear, and the relation to prior work is somewhat misrepresented. Thus, I believe this paper falls below the acceptance threshold.	2
"Overall I am happy to see a paper addressing this worthwhile problem, but feel that there are too many issues to recommend acceptance, and therefore I must recommend rejection. In order to change my mind, I think the following would need to be addressed:

1. Clarify differences between your method and similar methods for discovering directions in latent space proposed in [10-13]
2. Further clarification or justification of finding a hyperplane using a classification method
3. Comparing against a set of non-trivial baselines and having superior, or at least reasonably similar performance

I also think the paper could be made much stronger by:

1. Using a more difficult set of molecular properties
2. Improving the definition of molecular manipulation and the metrics proposed"	2
I think this paper does not demonstrate any significant contributions compared to existing studies.	2
Overall, I think this paper is well-written and it provides a novel, simple, yet useful algorithm for further increasing the batch size for large-batch training.	3
Above all, the authors should improve the theory for a better understand of the adversarial data. It would be better if the authors propose the strategy on how to choose a suitable large batch size for a given problems.	2
New empirical findings regarding the advesarial training helps generalization in large-batch training scenarios, but some results lack of deeper understanding. 	3
A paper with a lot of potential, but lots of important details are left out and lots of decisions are stated without sufficient justifications. As of right now, this paper is not in publishable state. After some revision however and some improvement in their exposition, I believe this can be a good paper. 	3
The motif-based heterogeneous graph construction method proposed in this paper is interesting, but the experimental results are difficult to prove the effectiveness of the method. The network architecture is slightly simpler, the chemical bond information of the molecules is not used in the atom-level feature extraction. Therefore, I am not in favor of recommending this paper.	2
Overall, I think this paper is at the borderline. Although the paper is among the first to use motif for molecular graph representation learning, the paper lacks sufficient literature survey. Moreover, it would have been better if the paper had had discussions about why motifs are helpful in a data-driven perspective.	3
In summary, I think the idea of using motif in molecule representation learning is interesting, but the propose method has a lot of design issues that are not well-justified or well-explained. Moreover, the proposed method is not tested on commonly-used datasets. Overall, I think this paper is below the borderline and I tend to reject this paper.	2
This paper propose a cross-attention Transfomrer architecture for the multi-label text classification problem. Such modeling architecture is not new in MLTC literature. In addition, the rational of latent labels are not techincal sound and justified. Time complexity with respect to number of labels is also missing. Finally, the author should consider experiment on large-scale multi-label datasets such as Wiki-500K and Amazon-3M. Given those reasoning, I would vote this paper being marginal below the acceptance threshold.	1
The key idea has been proposed in previous studies, so the novelty is rather limited. The intuition is not well explained.	1
This papers proposes a simple method to model the label correlations implicitly via latent labels. The reported experimental results show the proposed method is effective. Whereas the method lacks of theoretical and empirical analysis. In summary, this paper is not good enough for ICLR.	2
I would like to reject this paper since overall contribution and impact seem quite limited. Also, the authors neglect recent advances in related fields.	2
"- It is an interesting and useful study on multi-label text classification.
"	3
This paper deals with the measurement shift in the source-free DA problem. The authors propose a simple but effective method to approximate source distribution in a lightweight manner. I recommend to accept this paper. 	3
Even though the intuition of the algorithm is not given yet, the designed framework showed the state-of-the-art performance, which means that the measurement shift would be one of the hidden keys to improve the domain generalization. When the authors present adequately the additional analysis for its intuition and revise some repeated parts, I am preparing to increase my score.	3
"This work focuses on specifically tackling measurement shifts (subset of general domain shifts). Most of the justifications to highlight the importance of measurement shifts are based on evaluations on the proposed synthetic EMNIST-DA dataset. While the improvements are significant on EMNIST-DA, the simpler and more generic DA techniques obtain competitive performance on the real-world measurement shift dataset (Camelyon17) which casts doubt on the requirement of specific methods to tackle measurement shifts. See the main review for detailed comments. While the paper addresses an interesting problem with good results, I am not convinced by the paper’s argument that such a specific method is better suited for realistic measurement shift DA problems than simpler, generic methods like pseudo-labelling. Thus, I give the rating of “5: marginally below the acceptance threshold”.

**Pose-rebuttal comments**

I thank the authors for their efforts in the rebuttal period. Most of our concerns have been addressed by the authors. Further, having read the other reviews and considering that this work is the first that addresses measurement shift DA, I increase my score to 6: marginally above the acceptance threshold. I hope that the authors will include all the discussions from the rebuttal in their revised draft."	3
Overall, I think the paper has some interesting ideas and has a compelling experimental analysis. However, there still remain few concerns/confusions which require further explanation.	3
The authors propose a relevant new scenario for SFDA and propose a new method to address this setting. Results show that the existing methods struggle for this setting. The proposed method outperforms existing methods. I think the proposed setting is relevant and I would therefore recommend acceptance. The novelty of the proposed solution is not that large though. 	3
Overall, the proposed system is a practical and usable system for pose authoring based on sparse input. The demo is impressive and seems a great addition to inverse kinematic systems. On the other hand, the technical novelty seems lacking and some important baselines are missing. In all, I think this work is borderline and would like to hear the author's response before making the final decision. 	3
This paper focuses on a very interesting topic and proposes two datasets that may facilitate the research area. The proposed methods are reasonable, and most of them are well evaluated.  Although some experiments should be added and some details are missing now,  I think it is still a good submission and ok to be accepted. 	3
This paper is interesting and technically sound in general. Therefore, my current rating is boardline accept.	3
"The authors tackle an intriguing problem in modelling functions on multi-sets. The paper is overall promising, and I would be in favor of accepting if the authors could clarify some issues on the theory and add some more details in the empirical results.
"	3
"The conceptual advantage of exclusive multiset-equivariance is interesting but lacks some additional comparisons in order to demonstrate that it is the deciding property in practice.
Implicit DSPN works very well but on its own is just an application of existing methods to a new task, which is not necessarily bad but limits the technical novelty of this part of the contribution. "	3
Overall, I found the paper to be a strong addition to the deep sets literature with many important contributions. However, I did have some overall complaints about the general structure given in the weaknesses and nitpicks sections above. As such, I am a bit reticent to accept. If these issues are resolved, I am more than happy to increase my rating.	3
The paper is well written, nicely connects a novel theoretical observation with a modeling improvement idea, which then culminates into a clear empirical improvement. More empirical investigations would be great, but I feel the paper is well rounded as is.	3
This paper makes an intriguing claim about the effect of the input on LM learning, which is backed up by a formal argument. These kinds of arguments are valuable. However, the argument is difficult to follow for a non-specialist audience. Also, the empirical results the authors report have some confounds due to questionable task selection.	4
The paper addresses the inductive bias of in-context learning. The studied problem is meaningful and could be a potential breakthrough for pretrained language model. However, the written of the paper is a bit hard to follow and the experiments is somewhat limited.	3
I was not clear about the impact of the theoretical analysis presented, compared to past work. It will be good to rewrite the section clarifying the contributions and novel conclusions from the analysis. The empirical section sounded disparate from the theoretical part of the paper, and the results are weak. I suggest evaluating on a wider variety of tasks. 	2
Despite its theoretical focus, the paper is easy to read and to follow. The theoretical analysis is supplemented by empirical results.	4
I thought this paper was interesting and brings a new perspective to practical design decisions used in pre-training language models. I think this paper is relevant to the ICLR audience, and that said audience would be excited to know about it.	3
The paper proposes a simple idea and conduct a wide range of experiments that seem to focus on producing SOTA results instead of illustrating how and why WPA works. However, given the amount of effort that goes into this paper, I'm on the fence of whether to recommend accept or reject.	1
I enjoyed reading this paper, and I think its proposed method is mostly effective and can be valuable as a new regularizer for CNNs. Moreover, the insights into how different parts of a CNN are affected by such a regularizer strikes me as a very helpful piece of knowledge for the wider research on neural net architectures. As such, despite having some concerns listed previously, I think the paper merits acceptance.	4
"Unfortunately, I do not believe that this paper meets the standard for publication. Besides that, the paper is not well written and therefore hard to understand its experimental evaluation is missing a comparison to previous approaches.
"	1
"On its own, the methodology used to gather the results disqualifies the paper for publication at this point.  If the reviewer is mistaken about this, then all of the sub-scores would increase.  

However, even if the reviewer is mistaken about this methodological error and the results are valid, it does not seem that the paper is ready for publication because the connection to the ""shortcut"" literature needs clarification (which impacts how almost the entire paper is written).   

# Update based on author responses

The author's responses have addressed some of my concerns (eg, using testing data as a validation set, the lack of overlap between CIFAR10/100).  However, the paper still needs to be revised to emphasize that it is not focused on ""perceptible patterns"" as shortcuts (eg, cows are usually in pastures) but rather on ""imperceptible patterns"" (eg, high frequency signals in an image).  So I've changed my score from a 1 to a 3.  

I'm very familiar with the work on ""perceptible patterns"" and this paper is not up to par for that area because  it does not identify specific shortcuts and then demonstrate that it has reduced them.  I'm much less familiar with the work on ""imperceptible patterns"".  So I've changed my confidence from a 5 to a 3.  

"	2
The work seems to be valuable in the sense that it conducts extensive experiments on simulator learning for turbulence. Meanwhile, the main claim of the paper is somewhat vague because the difference between the proposed architecture and the baseline architectures is not explained in full detail.	3
"Although using some existing learning models, there are certain contributions of this paper in applying the ml models on predicting the turbulent flows, which shows some advantages over the classical solvers.

===============
Updates: I thank the authors for the response. The response addresses my concern about the contributions of the paper. I would agree that  the empirical work of this paper is impressive. I decide to change my score towards acceptance. "	3
The authors do make efforts on this paper, however, only some statistical presentation of some low-precision turbulence simulation data is not convincing enough to make it attractive.	2
"In summary, the paper represents an interesting read, and contains a variety of very interesting results. There are a few open points, but, giving the authors the benefit of doubt, I hope that these can be cleared up during the rebuttal. Given my current understanding of the work I would argue for accepting the paper to ICLR.
"	3
This is a nice combination of mostly existing methods with some good insights for how to put them together.   It's not a major contribution, but it is well described and the experiments seem good, and most importantly, I think it is heading in a very good direction.    Lifelong learning and compositional generalization are critical aspects of moving forward to bigger and more interesting problems and so I think it is important to encourage work in this area.	3
Overall, I would recommend this paper for acceptance. The application of functional compositionality to RL policies is quite original, and the approach has been validated empirically. But the paper would benefit significantly from greater clarity in its exposition of functional compositionality and in better defining certain variables and functions in Algorithm 1.	4
"A well chosen topic, with a highly interesting solution.

Strengths:
- good research direction
- paper is extremely well written
- the core of the paper, the introduced algorithm, is a well-engineered solution to the problem

Weaknesses:
- all details relevant to contextualize both the approach and the results are in the appendix
- the paper jointly introduces a domain and an approach, opening it to the critic that the experiments are hand-crafted towards the approach
- the experiments seem too simple
- I disagree with some assumptions behind compositionality"	3
"The paper aims at addressing a very timely and important, and notoriously difficult problem: lifelong RL with task-families that have compositional structure. In principle, learning about this compositional structure should greatly facilitate learning-speed for new task-variations (forward-transfer). There are (at least) four main problems to solve: (i) how many modules are needed and what is the required computational depth (i.e. maximum number of sequential module calls), (ii) how are modules selected for a particular training instance (when this is not clear a-priori and brute-force search across module-combinations is too costly), (iii) how are modules trained initially after random initialization (how do they acquire specialization), (iv) how can it be ensured that modules only process relevant information (and thus become invariant to irrelevant variation). The fifth problem, avoiding catastrophic forgetting, is actually solved in the paper by borrowing a technique from batch RL. All of these problems are hard and active areas of research. Admittedly the answer to (iii) might be a curriculum, similar to what's proposed in the paper.

The current paper makes strong simplifying assumptions to reduce the severity of each problem. While this is a sensible starting point to establish baseline results, what I’m missing from the current manuscript is an attempt to tackle (some of) these problems. I am not saying this is trivial, and I would consider it a significant addition to the paper. Without such an addition the paper shows promising and interesting results for situations where these problems do not appear (which is often unrealistic) or for situations where these problems could be solved through some means in the future. The paper is well written, and clear, results are good, and I have no complaints regarding the correctness of results and claims. However, given the strong simplifying assumptions the results are not very surprising, which ultimately limits the impact and significance of the paper. I currently see no reason to reject the paper, but also have a hard time assigning a high score. I think with a bit of extra work (admittedly beyond what’s possible in the rebuttal) the paper could become a landmark piece of work in lifelong compositional RL."	2
This work lacks novelty and significance. The experiments also have many drawbacks, such as only validating on the weak baseline or small dataset.	1
The proposed method lacks novelty, and the technical details of the proposed method are missing. Moreover, the proposed method is not evaluated on a common benchmark dataset (i.e., ImageNet) and the results are not impressive. Therefore, I recommend rejecting this paper. 	1
I think this paper is not good enough mainly due to the very limited novelty and results.	1
The core idea of using one-parameter families of states as ground-truth is nice, but this idea appears to be already present in Kohl et al. 2020. Overall, it is not clear to me what technical or empirical novelty the authors demonstrate over past works such as Kohl et al. 2020.	2
The paper has good quality. The proposed entropy-based similarity model is novel and seems to be useful. Although some technical details need to be further clarified, overall the work is nice and worth to be presented in ICLR.	3
"I believe the contribution is relevant to a large audience, is well and clearly written, and method aspects are clearly argued for. The datasets include a lot of variation, and the results are thoroughly discussed. I think the semi-supervised approach to learn a metric for comparing simulation or measurement data based on the proposed similarity model is useful for a wide range of applications and believe the authors also present a nice line of argumentation for design choices which are applicable to other tasks, hence I recommend accepting the submission.
"	4
The paper tackles an important problem, but leaves much room for improvement. The population entropy should be backed up by (toy) experiments that analyze the effect of the quantity, since on paper it seems that the lower bound exhibits a huge gap compared to the empirical range of entropy values observed in Table 1. The experiments can also be improved (TrajeDi is missing in the human experiments), and the current results are hard to interpret (the reward in Table 1 does drop significantly when the entropy term is pushed up).   Improving the overall writing would also raise the paper's potential impact.	3
"Overall, the manuscript presents an idea that seems compelling and useful, but the experimental section doesn't provide enough signal to compare this method against the literature. This makes my recommendation borderline at this point, so I'm looking forward to discussing the manuscript with the authors and the rest of the reviewers to understand how to improve it towards possibly acceptance.

---

Bumped up score to weak accept."	3
This paper presents an interesting solution to train the diversity policy with a population, but more clarifications (as stated above) are required. 	3
"The paper is interesting and well written, and addresses an important problem. The method itself combines two known methods (TrajeDi and PFSP) but does not provide sufficient credit, since it claims TrajeDi is ""concurrent"" and it only discusses PFSP in related work. Accurately positioning the new contribution w.r.t the previous would make this a solid contribution."	2
There are several crucial missing baselines and ablations and clarifications before I think the paper is ready for publication. However, pending their inclusion and a review of the new results, I am very open to raising my score to an accept.	2
"My weakness is listed above. Besides, I also have some questions:

1. If Threorm 1 & 2 are correct, why your model does not reach exactly the same accuracy as ANN when $L=T$?
2. What is energy consumption? It is higher or lower?"	3
The paper's effort to provide more similar ANN activation as SNN to reduce conversion error of ANN-SNN at low time step is interesting and promising. However, there are some concerns as I have detailed in my review that can further clarify the paper for good.	3
The achieved experimental results are notable. However, some key points need to be addressed (see the main review section).	3
This paper proposes a novel ANN-SNN conversion framework and achieves SOTA accuracy with fewer time-steps. The suggestion for improvement is listed above.	3
The idea is clear and reasonable, but lack of novelty and promising performance.	2
Overall, I think this paper presents a novel and interesting idea. However, the current experimental results are relatively weak and cannot show the strength of the method, which limits the contribution. There are also some concerns with the self-attention supervision that are not explained well and not supported by the ablation study.	3
The authors propose an nice, simple strategy to associate keypoints in multi-person pose estimation, but I don't know that a compelling enough case is made that this method offers advantages given the restrictions in terms of additional annotation requirements + compute overhead while falling short of and excluding more recent methods in performance comparisons.	2
Please refer to the Main Review of the paper. 	2
Although the paper has a clear motivation and is well-written, lack of experimental demonstration and weak experimental results are weak points.	3
While it is interesting to see a new line search method that is competitive to existing line search methods, the comparison doesn't seem entirely fair given that the proposed method shares essentially the same idea used in momentum. Maybe authors could give an estimate as to the required amount of total computations and compare it to that of momentum SGD.	2
Interesting take on a previously explored strategy for adaptable learning rates in SGD.  However, the results and (lack of) comparison to previous related methods does not look correct.	2
The proposed method doesn’t seem practically useful. It introduces many additional hyperparameters, and it is unclear whether it performs better than SGD with Momentum, and in settings where it is not confirmed whether the empirical observations that justify the method hold. Therefore, I recommend rejection.	2
It seems that most part of the paper (including figures and statements) is taken from one of two previously published papers and hence the novelty contained in the paper itself is very limited.  This inclines me towards rejection of the paper in its current form. 	2
This paper introduces an interesting idea to model Bayes label transition rather than clean label transition based on several existing literatures. However, the overall contribution of the paper is a bit weak for ICLR standards IMHO, and some theoretical justification are currently missing to support most of their claims.	2
The method has made certain theoretical contributions to the label noise learning community.  The algorithm also clearly outperforms the current SotA. Therefore, I would like to lean on the positive side of this paper.	4
This paper has no novel and significant contribution. The correctness and clarity are also of serious concern. Therefore I vote for a reject.	1
The idea is new and interesting, but final solution is not satisfactory, especially for theoretical guarantee.	3
I strongly recommend accepting this paper, given the over-emphasis of toy tasks in the emergent communication community.	3
"**Recommendation**

I recommend to accept this paper based on its valuable experimental results, which demonstrate that scaling up emergent communication can find more efficient and generalizable representations. Supported by empirical contributions, the paper steps beyond toy experiments that are common in emergent communication literature and towards increasingly complex and realistic settings."	3
The commonly used Lewis game setups with synthetic data for language emergence are indeed quite limited for real-world applicability. Thus, scaling up the game for the emergence of communication protocols is an important problem. I think this paper is a great contribution, but I would appreciate some more understanding of the structure within the emergent languages. Alternatively (or additionally), demonstrating an ability to systematically generalize can also be very valuable.	3
Overall, I find this work thorough and well-motivated. The results would have significant impact in this area. I recommend acceptance. However, I do not work extensively this area and may not follow the latest work.	3
I enjoyed the paper; however, because I'm not familiar with the math I will defer to other reviewers on their assessment on this front. I'm giving this paper a 6 because I felt that in the end it did not prove it's new formulation was better but rather showed it experimentally. For a primarily experimental paper, I felt the experimental domains were rather simplistic.	3
"Overall, I think this paper provides a good understanding of the different SPRL variants, their limitations, and proposes a new method which provides some empirical benefits (over other SPRL methods) and is theoretically motivated. 

My main concerns are the incomplete related work section and the limited benefits demonstrated by WB-SPRL over other curriculum-based methods. Hence, I believe the paper needs some more work to warrant acceptance at ICLR.  

I'm willing to increase the score if the authors include references to the relevant works from the literature, a discussion of how WB-SPRL compares with other curriculum RL methods (theoretically and empirically), and include more experiments that (more convincingly) demonstrate WB-SPRL's benefits over other approaches (with some intuition of why / in which settings we can expect WB-SPRL to be better). "	3
The paper proposes a well-motivated extension to the SPRL algorithm and demonstrates its usefulness in interesting experiments. The significance of these results is somewhat unclear, though, as the naive context discretization seems to perform just as well as the proposed metric in every but a very limited case. Additionally, the nature of the work is rather incremental with limited impact and novelty. 	2
The novelty of the work is limited and the authors did not show the results when using a standard MLP approach on the spatial direction.	1
An interesting combination of existing ideas evaluated on three computer vision tasks. Overall novelty seems limited, the paper does not explain its approach very well, and the gained understanding is limited. Empirical results are interesting but not super-convincing.	3
I think this paper is around the acceptance threshold, mainly due to the limited technical novelty besides applying Shift, an alternative to convolution, to MLP-mixers, and achieving on par performance as previous transformer/convolution methods. I will raise my score if the weaknesses are addressed.	3
"Overall, I think this is an interesting paper. The authors design a new benchmark for continual learning and propose a simple yet effective method. My primary concern is that “how likely is it for following researchers to refer to this benchmark?” As there are already many different benchmark protocols in continual learning, I think the following researchers will tend to choose the benchmark protocol already used in many popular papers. Nevertheless, I still think the paper might be useful for the continual learning community. 

My rating is “borderline accept”, and I will consider upgrading my rating if the authors successfully address my questions.

&nbsp;

### Post-rebuttal update
The authors addressed most of my concerns in the rebuttal. They also provided the results I asked for in the revision (e.g., the results on ImageNet-1k). I think I tend to accept this submission. So I upgrade my rating to eight."	2
There are unsolved concerns about the concepts/comments in the current version. Without addressing them, the proposed problem setup and method could not be that significant for continual learning. 	2
"### Basis for the scores

1. The experiments justify most of the information present in the paper. Some additional information regarding the sample importance based memory management scheme is required. Also, some decisions regarding hyperparameters need to justified, given the *i-Blurry* setting is online and the main motivation of the paper is to propose and solve a more practical CL setting. 

2. I'm not sure about the novelty of the sample-importance based memory management scheme and a discussion regarding prior work related to sample-importance is required. The proposed setting on *i-Blurry* is marginally novel as it is a generalization of the disjoint and blurry configurations. Any-time inference and $A_{AUC}$ are  likely novel.

3. Empirical results shown cannot be easily (and extensively) compared to some prior work as there isn't a strict adherence to prior dataset and hyperparameter choices. Some justification for the decisions is required. 

4. Insights into the proposed CLIB method along with a nuanced discussion regarding the differences between the algorithms is lacking. There is no description regarding efficiency and scalability.

## Update
The authors clarify almost all of my questions pertaining to the paper. The paper now contains experiments on Imagenet-1k with additional insights into the algorithm and its efficiency. As such, I've improved my scores. "	3
A nice paper describing an effective and somewhat novel replay-based CL method, evauated in a demanding setting.	3
Overall, the paper is well motivated and provides promising results for leveraging suboptimal datasets for effective offline imitation learning. The problem is well motivated and the authors provide some novel insights into better objectives for behavior cloning. While the authors demonstrate some improvement over the provided baselines, I would encourage the authors to consider adding couple of more ablations, particularly across datasets with a mix of human and random trajectories. 	3
The results presented in the paper are promising, but there are several issues with the clarity of the derivation and some with the experiments, and thus the paper is not yet of sufficient quality to be published as is.	2
My reaction to this paper is mixed. On one hand, the introduction, preliminaries, and related work are well laid out. On the other hand, I had several confusions about the method and have some concerns about the experiments (see the main review for specific details). As it stands, I think this paper is marginally below the acceptance threshold. I hope that the authors can diligently address the concerns that I raised, at which point I will reconsider my recommendation.	3
I believe this is an interesting idea in an interesting setup yet it lacks novelty and it would deserve more work, notably on the experimental part, before being published.	2
As shown in the Main Review, the motivation of the paper is clear and reasonable, and also, the experimental evaluations are convincing. While there are also some issues that need further refinement to be accepted, especially the statements in the experiment.	3
"Overall, this is a very well-written paper that studies an important topic and proposes an interesting new idea. While there are some limitations, they are quite well-discussed (although improvements such as those mentioned in the ""Suggestions"" section are desirable) and I think that the paper will spark some interesting discussions and new ideas in the context of FL."	3
The novelty of the proposed model is incremental, and the privacy issue of sharing features is not solved.	2
While this paper has some nice ideas to address the non-iid feature problems in federated learning, its contributions are limited due to novelty and practicability issues. 	2
The authors advocate for class-stratified weighted macro-averaging as an appropriate scalar-valued evaluation for classification under class-imbalance — justified by a reasonable number of case studies. However, they don’t describe a procedure for setting the weights in a decision theoretic way nor describe the statistical properties of the resulting point estimate for hypothesis testing. Thus, while a reasonable heuristic approach, it lacks rigor for use in critical systems (where high-loss rare events are important). Additionally, there is very limited novelty as this is performed both for training systems and evaluating them in general ML research, application-focused ML research, and industry practice. This paper has some interesting observations, but is not ready for ICLR acceptance. 	1
"Recognizing that the problem is interesting, however, taking into account the above, I found that the contribution of this work is insufficient for this conference.

"	2
"Overall, I vote for acceptance as the class imbalance problem is chaotic. It gives classification suboptimal results. Especially when allied with class disjunct
"	3
"In this paper, the authors have proposed a novel evaluation framework for imbalanced data classification. Specifically, the proposed evaluation metric is designed to improve the grading of multi-class classifiers in domains where class importance is not evenly distributed. Generally speaking, the problem authors paid attention to is really existed and is important in imbalanced classification problem. Moreover, the writing in this paper is very well and is very easy to follow. However, the paper still exists the following problems:

The major problem of this paper is that the importance of each class is difficult to define. In this paper, the authors propose four methods in Section 3.2, i.e., importance criteria=user-defined, importance criteria=rarity, multiple importance criteria and partially-defined importance criteria. But in my opinion, none of them is very accurate and objective to define the importance of each class because in real-world applications, even if domain experts are difficult to distinguish the subtle importance differences between some classes. Anyway, it is an interesting and valuable problem for researches to discuss the importance differences between different classes. I think if the authors can provide some other importance criteria that are more objective, this paper will be more valuable.

Besides, since this is a novel evaluation metrics, I think the authors should use a collection of datasets to fully verify the effectiveness of the proposed framework, instead of just three datasets in this paper. 

Finally, if the number of some classes are too rare, according to the rarity importance criteria, these classes will be given a very large weight, if so, whether the accuracy of majority classes will be largely decreased?"	3
See above for the detailed comments.	2
This a well-conducted and strong paper which achieve is goal and give interesting insight about neural network architecture elements.	2
I think the paper has merit, but it will benefit from revision, which empahsizes more motivation on why to choose a suggested approach as opposed to the other leading approach PIM and also why certain empirical results make sense. 	1
"The paper proposed a new meta HIL method called DMIL. The method is clearly explained. The background is described in a proper manner and is informative. The novelty is clear. Also, the empirical evaluation is appropriate though the number of environments can be increased. 
As a whole, I admit that the paper is an excellent paper. 
"	3
I vote to marginally reject this submission because the experimental results are not thorough enough to demonstrate robustness across environments and number of subskills and the novelty of DMIL mostly comes from combining known approaches. If more environments and subskills could be tested, I think the paper could be a candidate for submission.	3
While I think the technical contributions of the paper are sound and the experiments are generally thorough, this paper at its current form is limited by the writing quality. A signifiant amount of work needs to go in re-writing the introduction, improving the figures, and fixing grammatical issues.	3
"The paper proposes an interesting approach for jointly learning adaptable high and low-level skills. While the method is well-explained it seems to more complex than alternative approaches and the current experiments don't fully convince me that it would be easy to train on new tasks without a significant amount of tuning to get the iterated learning loop to converge. If the authors can provide experimental evidence that the method is no harder to train / tune than alternative approaches for few-shot imitation I am leaning towards accepting the submission, but I am open to changing my opinion based on the other reviewer's feedback.
"	3
The quality of the whole manuscript should be further improved. The motivation of the whole task is not clear, especially compare to action segmentation task. May raise the score if author can handle all my concerns. 	3
"In short, this paper studies an interesting problem, and proposes an convinient and effective method as a solution. Although the proposed solution is not very novel, and not strongly techniqual, it is marginally above the acceptance threshold for the potential impact beyond video scene segmentation. 

I also would like to see the opinion from other reviewers. "	3
This paper presents an effective framework for the task and reports good performance. Yet I am still concerned about its lack of insights and generalizing ability.	3
This is a theoretical understanding paper for lable smoothing when learning with noisy label. However, its practicality is limited. 	2
The main problem with the paper is analyzing the effect of label smoothing on the minimizer of the expected loss of a classifier. There is no need for any regularization when minimizing the expected loss. This is a fundamental conceptual problem with the paper. Furthermore, the results are fairly elementary and do not significantly contribute towards theoretical understanding of label smoothing beyond existing literature (c.f. Chen et al 2020).	1
"I tend to accept this paper. It does explain some interesting things, and the experiment is also sufficient.

========== post rebuttal ============

The rebuttal of this article solves my concerns to some extent, but as other reviewers pointed out, some problems still exist, so I changed my score."	3
"This paper provides understandings for a generalized notion of label smoothing (GLS) when learning with noisy labels and theoretically show that negative label smoothing improves the expected model confidence over the data distribution. The empirical experiments on multiple benchmark datasets demonstrate that with the presence of label noise, negative label smoothing (NLS) becomes competitively robust to label noise.

However, to the best of my knowledge, the following points need to be addressed before it can be published:

1)	My first concern is the originality and novelty of this work. In this work distributed soft label learning is used. To my knowledge, label distribution learning is a related research topic about soft label-based multi-label learning, which should be discussed.
2)	For Figure 1, more detailed descriptions and explanations should be given.
3)	The last paragraph in Section Introduction, please don't cite so many references at the same place. I suggest to delete some unimportant references and then dispersedly cite these references. 
4)	Two UCI datasets are used to evaluate the effectiveness of the proposed algorithm. My question is why the authors choose these datasets. To my knowledge, there are a large number of UCI datasets can be used for evaluating the effectiveness of the proposed algorithm. More datasets may be needed. At least, please explain the reason why these datasets are used in the current experiments.
5)	It is more convinced if the authors can compare the proposed algorithm with some other state-of-the-art label distribution learning algorithms.
6)	With regard to the comparison results, statistical tests are needed in the comparison results.
7)	The presentation can be further polished. For example, some equations have punctuation at the end and some don't have.
"	2
"The paper provides great value in the research communities that focus on the impact of label noise. The paper is well written and easy to follow. The observations shown in the paper are supported by extensive analysis. There are only some parts that require more explanation, but it does not harm the overall readability.
To make the paper more complete, I suggest the author make more connections toward a broader community that focus on label noise."	3
The paper collects additional manual annotations for CIFAR10 and CIFAR100, which will be of great value to the research community, for ex. helping practitioners study the robustness of their algorithms to label noise.	3
While I find this paper to have a clear strength, the contribution is not enough to justify acceptance to ICLR. The dataset proposed is good but not crucial, and the paper does not provide enough new insights. This paper might have a better chance at other venues specific for dataset papers.	2
"As a paper that proposes insightful analysis and some datasets, there is room for further improvement. In particular, resolving the above concerns will add more clarity and novelty to the paper. I will decide my final rating after the rebuttal.

> [rebuttal] The score increases 5 $\rightarrow$ 6 since the response addressed my concerns properly."	3
I recommend accept since there is clear novelty in the paper, problem is important and well motivated, empirical results are convincing	3
"Abstract:

”It is crucial to fields where fine-grained labeling (e.g., breeds) requires strong domain expertise thus is prohibitive, such as medicine, but predicting them is desirable.” --> I found this sentence confusing, consider rephrasing.

Introduction:

“During the treatments…task” --> I understand the general point here, but the example seems a bit strange.  For instance, if I could predict an event before performing dialysis, would I not consider delaying the procedure?  If I delay the procedure, the events never happen.  If it's useful to be able to say ""we predict event type A vs. event type B"" to help physicians assess the risk of performing the procedure, that would make sense...but that doesn't seem to be how it's presented here.  Consider reframing this to make it reflect the situation you're trying to describe more precisely (or consider providing a slightly different example).  This is a minor issue, but the framing could throw some readers off.

“The data constitute a support set” --> In the sense that they provide support for the different fine-grained annotation schema?  Or for training/fine-tuning?  Or for both?  Additional precision would help here.

“Unseen siblings of the superclasses” --> What is meant by this phrase?

“Subclasses may arbitrarily spread within each superclass” --> Maybe point out ""and in real datasets, it is often the case that they are unevenly represented in a way that causes subclass performance gaps.""

“However, their approach is confined to images” --> Why is this true?  Certainly the referenced work focuses on images, but it is not obvious to me why it could not be directly extended to the type of data in medical records.  I am also not sure I agree that there are not reasonable/standard approaches for obtaining positive/negative pairs on other types of data.  Even if this were true, however, I would argue that it's not a downfall of Bukchin et al.  Recommend rephrasing or cutting this piece.

“Also, since contrastive learning does not model subclasses…Sec .4).” --> At this point in the paper, it's not clear at this point how you support these statements in Sec. 4, and the claims may seem unwarranted.  Recommend either deferring this discussion or providing some quantitative previews of your results here to mitigate this.

“It explicitly represents the unobserved subclasses as latent variables…” --> It may be worth clarifying here that you do not assume knowledge of the subclass identities.

Related Work:
“…cannot trivially be extended to non-image data…” --> See comment above, I don’t think this is a great argument.  Recommend cutting or rephrasing.

“…cannot leverage coarse training to guide their pseudo-labeling…” --> This claim is unclear. Coarse training explicitly does guide the pseudo-labeling in e.g. GEORGE (Sohoni et al. 2020), as clusters w/in the coarsely trained representation define the pseudolabels.  The argument about end-to-end optimization of the pseudo-labels/model parameters seems reasonable, but not sure this statement is accurate.  Maybe rephrase to indicate that a two-step approach could lead to suboptimal psuedolabeling rather than arguing that the coarse representation doesn’t contribute at all in these two-step methods.

“These methods are unsupervised, and cannot be used…” --> Why does the unsupervised nature of these approaches mean they are not capable of application so this problem?  I found this part unclear…I suspect it may be a writing issue rather than a technical issue, but clarification would be appreciated.

Superclass-conditional Gaussian Mixture Model:

“unseen superclass” --> What does an unseen superclass mean?  Is it an unseen subgroup that spans existing superclasses?  Or an OOD example?  Need a more precise description of what this means, recommend providing mathematical definition.

“Unlike some previous works (Sohoni et al., 2020), which allocated a predefined number of subclasses to every superclass…” --> This isn't true, as far as I understand it.  Sohoni et al. perform a search over the number of subclasses w/in each superclass and optimize an unsupervised cluster quality metric.  One could argue that fewer searches are required in the method this paper proposes – because all of the searching over number of subclasses is baked into the r parameter – but I don’t think that argument is particularly compelling from a practical standpoint.  I would suggest cutting or softening this claim. 

Figure 1 --> Appears to be a typo in the caption, all of the subclass means have the same subscript (“1”)

“…and tune \sigma to adjust the relativity between super- and sub-classes” --> What range does this usually fall in?  Worth reporting this.

“transport polytope” --> Many readers may not be familiar with this.  Recommend providing a brief description/background on this in the Appendix (and reference this in the text)

“It is noteworthy…” --> This is an interesting set of observations.  Nit: typo in “contrasitive”
Experiments:

“visual granularity” --> What exactly does this mean?  A comment would be helpful.

“20/6/8 splits” --> So the superclasses are disjoint in train/test/val?  That seems a little confusing.  Suggest clarifying

“which adds an embedder” --> What exactly does this embedder do/why is it added? More representational capacity?  More detail would be welcome.

“dim” --> “dimension”

Table 1: Was there a statistical test done here to support improvements over ANCOR?  Seems to be overlapping confidence intervals in a couple of cases.  Please provide detail on how confidence intervals were computed and what statistical approach was used to determine superiority/non-inferiority.  Also, what's similar about the cases where the method outperforms and doesn't outperform ANCOR?  Is there any additional explanation that can be given here?

“baselines on contrastive learning cannot be applied…"" --> see above comments, I don’t think this is a particularly strong argument.  I would cut it, or at least acknowledge that this is not a fundamental limitation of the contrastive approaches.  Augmentation schemes certainly exist for non-image data types.  The strongest response would run these baselines using data augmentation approaches for these other modalities.

Appendix A:

Eq. (10) --> I am not following exactly how Jensen’s inequality results in the move from line 1 to line 2 here.  I could be missing something, but it would certainly help if the authors provided more explicit detail in the intermediate steps (e.g. laying out the terms in the inequality clearly – i.e. a_i and x_i here https://en.wikipedia.org/wiki/Jensen%27s_inequality).

“the second step in Eq. (11)…” --> I think this is a typo…should be Eq. (10) referenced here?

“\lambda \ge 1” --> Am I correct in understanding that there is strict equality for \lambda = 1?

“equal partition constraint” --> If I understand correctly, this statement deserves more discussion.  Does this imply that the constrained transport polytope formulation here effectively assumes that subclasses are of equal size?  This would appear to be consistent with Eq. (6), and if this is indeed true this would represent a substantial limitation of the method that should be further discussed.  On this note, what is the distribution of subclass sizes in the experiments run in this paper?  Are the subclasses all of similar size, or does it vary?

Eq. (13) --> the last equality/definition here seems a bit strange.  You remove the 1 constant, and define the term without the 1 as equal to the term without the 1?  Why is this true/necessary?

Appendix B:

“…robust learning rate…” --> Could be worth searching over robust learning rate for this baseline.

Appendix C:

Table 8 --> Suggest adding bolding to this table as well to maintain consistency with other tables.
"	3
The paper introduces the interesting problem of few-shot multi-granularity adaptation through hierarchical Gaussian mixture and demonstrates its effectiveness across multiple datasets. The relation between the goal (embedding learning) and evaluation (classification accuracy) is not very clear.	3
"The paper proposes some interesting ideas for using multi-scale inputs and feature hierarchy and diversity for training equilibrium models. The results on CIFAR-10 look promising, but I have some concerns about the writing and the novelty of the work. 
"	3
I believe the idea of the paper is interesting and the authors show the impressive results on CIFAR experiments. However, there are some concerns regarding scalability (since the proposed method performs unrolling), additional overhead, and experimental fairness. Most importantly, the proposed approach is trained with unrolling to a fixed depth and does not have a property of equilibrium models. Hence, I give a score of 5.	2
This paper propose a new method, which can handle different input scales. The  proposed method is well-presented with good motivation, obtains better results with smaller model sizes in experiments. There are several questions remaining, I'm looking forward to the responses and discussions. 	2
I failed to collect evidence to reject this paper, so I believe it might be a solide work. I hope authors could address the interpretability issues here. I hope AC and SPC could hear from other reviewers.	3
"The paper proposes a reasonable and interesting meta-learning approach towards leveraging (pre-trained) generative models for constructing synthetic labelled training sample sets. The overall approach is interesting, the results are promising yet the paper (in text and in experimental analysis) seems to have various weakness, as discussed above. 

While I do *not* consider that the existence of all suggested experimental evaluations is a pre-condition for acceptance, the current experimental analysis seem to be underwhelming in several aspects. In addition, all the textual weaknesses & ambiguities definitely need to be addressed/clarified in the comments/revision.

— Post revision/rebuttal update —
I have gone over all the reviews, responses and the updates in the paper. The revision and responses address my concerns. The text and experimental evaluation have been improved on several aspects, though there is still room for improvement in experimental analysis. Overall, based on all positive improvements, I increase my recommendation score from 5 to 6."	3
"This paper proposes an intuitive method based on clear motivation. In addition, sufficient experiments have been conducted to prove it, and the paper is written in an easy-to-understand manner. However, one big concern (#1 above) remains, so I give my initial rating as 6.

**[Comments After Author Response]** I appreciate the efforts of the authors for the detailed rebuttal. The authors have well addressed my concerns (other reviewers' as well), so I raise my rating to 8. I strongly recommend that the authors will reflect those contents in the final version."	3
"The submission has a few limitations, including the limited demonstration of its effectiveness in low-data regimes and grammatic errors. However, this work extends [Zhao et al., ICLR 2021] by combining the idea of DatasetGAN and RepurposeGAN in an interesting way. The merits of this work seem to outweigh the demerits.

===== After rebuttal ===== 
This reviewer stays at the same rating because the merits of this work seem to outweigh the demerits even after the discussion phase.
It would have been good to show the limitation and break-down point analyses in the submission."	3
The contributions might need to be clearly summarised and I vote 'the marginally below the acceptance threshold', based on the limited experiment and the unclear experimental settings. 	2
This paper achieves very good empirical results and build on-top of several recent works on VQ + Transformers for image modeling. However the paper lacks some details and clarifications. Given the significant improvement of FID and IS, the paper could also do more to analyze which components of the proposed method was instrumental in achieving these gains.	2
"* I’m seeing the potential and practical benefits of the proposed method, even though it is a fairly straightforward approach by replacing CNN into ViT in the VQ-GAN framework. In addition, including the linear probing experiments is also a plus.
* However, I have many concerns on the experiments and reproducibility. Without resolving these issues, my recommendation is borderline reject. 

== Updates after the rebuttal ==

My major concerns have been addressed by the response, thereby raising my score from 5 to 6. I really appreciate the authors reporting these additional results in such a short time.

Though the empirical results of ViT-VQGAN are quite impressive, I still have a concern about the technical novelty. In the response to Reviewer 6eTH, the authors have argued that ViT-VQGAN is the first approach to consider vector quantization image modelling on both generation and recognition, while iGPT didn’t consider the vector quantization. However, according to the original iGPT paper (http://proceedings.mlr.press/v119/chen20s/chen20s.pdf) published in ICML’20 proceedings, they have reported the results of linear probing of iGPT on the top of discrete codes from VQ-VAE. Please revise the response to Reviewer 6eTH. 
"	3
Overall, the value of this paper is mostly experimental, or more like “bag of tricks”. The detailed analysis and explanation of why some components are essential are not stated very clearly, which may make the whole paper more interesting, but it indeed shines some light on the image generative model. Hence currently I tend to accept this paper.	3
I believe this paper introduces a simple and effective method for improving VQGAN using vision transformers. The findings in this paper and the model would be useful to the community.	2
The paper studies a novel problem, input length extrapolation in language modeling, and proposes a simple solution with good empirical results. The paper is also well-written. One way to further improve the paper is to add experiments on other tasks. Overall, I recommend acceptance of this paper.	3
The paper is well written and easy to follow. The contribution is concrete and practically useful since a transformer is a building block of many machine learning models. More importantly, the size of language models becomes bigger, so their training cost is prohibitive. ALiBi improves the efficiency of language model (or transformers in general) training. 	3
The submission proposed a simple yet effective method that helps pre-trained language models to extrapolate beyond the sequence length used in the training, but I think the paper could've delivered a stronger message. I am open to discussions.	2
This paper proposes an interesting and novel idea for enhancing the extrapolation capability of transformer-based language models. A few additional experiments and discussions will make the paper stronger.	3
This paper is an interesting proposal that attempts to apply the ideas of neural subspaces to produce a set of compressed models at varying points on the accuracy / efficiency curve. Yet, these methods in the end seem more about learning robust compressible models and stray far from the original neural subspace idea, especially with compressible points. In my current understanding, these networks seem to have no demonstrated advantage to universally slimmable networks, which have a simpler validation process, runtime switching method, and more intuitive training procedure. The comparison against these networks and others needs to be better justified since I currently do not understand why fine-tuning is not allowed. If I misunderstood the method significantly, I would be willing to increase my score, but currently I suggest rejecting the paper.	3
"The paper proposes a method that reasonably extends the learning subspace method to allow performing accuracy-efficiency tradeoff according to runtime available resource. The method has been evaluated on several classification tasks and find to be useful.

However, the paper does not clearly explain how the compression is performed, with important details like choice of alpha missing. The measurement of speedup is not that quantitative, lacking realworld test stats. It is very difficult to evaluate the contribution of this paper under these conditions."	2
Many important details are missing in this paper. For example, the finer-grained compression level is a major selling point of this paper, but the authors did not even provide the compression level function $\gamma(\alpha)$.	1
I think the paper is well written; the method is novel and interesting; the experiment can well support the claims. Just need to clarify some details	3
The paper is technically interesting, but the authors should carefully revise their experimental section to address the concerns.	3
An interesting idea but the authors should specify better what is new and what comes from existing work.	3
The paper presents interesting ideas of extending kernels, here the Fisher kernel, to neural networks. I find that the paper would need to make a stronger connection with existing literature (eg, Belkin et al, 2018) and give more motivation/discussion/intuition on the derivation of the different $U_x$s. My grading suggestion is a 6, as I believe the paper presents nice ideas but would require more work.	3
The idea is very promising but some technical details are not very clear. Besides the proofs in the appendix, it is recommended to describe more about the architecture and implementation details in the appendix too, if length limited. It'e better also to add some discussion about how the conditions are satisfied in real applications and what to do if the conditions are not satisfied	3
Very promising paper but there is some lack of clarity in the descriptions of the architure used, preventing future readers to use this paper at its full potential. I would be happy to increase my score if this issue is resolved. The baselines should also be run on the real-world datasets for comparison with the proposed approach when the data generating process does not exactly matches the assumptions made in this work.	3
"The problem setting seems to be completely new, experiments were sufficiently performed, the results show the superiority of the proposed method, whereas the technical contributions are derived from mixing the previous techniques. 
Since I think the strengths in this paper totally outperformed the weaknesses, I provided a higher rating at this stage (but note that I am not an expert in this field). "	3
"An interesting paper with relevant results on the identifiability of latent causal processes. I hope the authors could clarify my above concerns in their rebuttal.

POST-REBUTTAL UPDATE:
I found the authors had done a careful job addressing my original concerns. I don't have any remaining significant concerns. I find the article interesting and significant, and I support its acceptance at this stage. I will increase my score from 5 to 8."	4
"I like the method - it is simple and intuitive, and the initial results appear impressive.
But for such a simple model the burden of proof is high, and there should be many more detailed comparisons with SOA zero-shot methods allowing the reader to compare and contrast benefits.
So I would like to see more detailed analysis on datasets and methods, for example, I would also like to see study on:
1) how does the dimension of the feature vector affects performance?
2) what happens when a set of new classes are very similar to a known class?
Perhaps some synthetic data would allow to demonstrate the strengths and weaknesses of the method.

"	2
Paper difficult to review in its current shape.	2
My recommendation for this paper is a strong reject. I am not convinced of the novelty proposed in this paper and there are too few comparison with existing work to strengthen the validity of the performance on the proposed datasets.	1
The paper lacks novelty and significance both for the presented approach and the empirical results. Further, the quantitative evaluation is based on non-standard datasets, evaluation protocols, architectures and baselines, hence does not provide a proper basis to evaluate the effectiveness of the proposed approach.	2
"The paper has a limited applicability to large-scale problems. Considering the modern methods and tricks has already dealt with the large-scale problems on a more diverse set of distribution shifts (such as Hendrycks 2019 ICLR), the results in the paper seem not significant. Also, the experiments can be performed more precisely if they involve more proper baselines.
"	2
The paper proposes a methodological framework to study DA under missingness and covariate shifts. It lacks a clear formulation, a theoretical analysis and it overlooks existing methods. Hence, the contribution is not significative. 	1
My major concern is the novelty of this work. Aligning distributions of latent representations by MMD has been extensively investigated before. It seems to me the only technical contribution is to generate a mask that mimics the missing patterns in test data. 	2
"The paper addresses an important topic and is interesting. Nevertheless, at this time I vote to reject it for the following reasons:

* There is not enough intuition/theory regarding situations in which the method is expected to perform well compared to existing work, nor are the empirical results sufficiently detailed.

* Related work on supervised learning with missing data is completely omitted.

* The authors would need to provide additional detail to facilitate the reproducibility of the results."	2
"- The work has not taken into consideration the existing literature on analyzing reward shaping on RL. 
- The validity of CRP as a way to explain the behaviors is well-motivated and supported in the paper.
- Since the experiments are performed in a simple environment, experiments on a complex task should make that paper stronger."	2
"The paper studies an interesting topic, the impacts of rewards on the properties of emergent language. However, I find the contribution is very light and limited in a very specific setting. 

It considers only a simple sender-receiver navigation game with two specific bias rewards, Euclidean distance and horizontal distance. The results are also very straightforward and specific to the setting:
1. ""Since the standard shaped reward takes both dimensions into account, we do not see any bias in the direction of the learned actions. With the trivially biased reward, though, we see the the learned languages exclusively favor actions near to the horizontal axis. In this setting, nothing explicitly prevents the agents from learning vertical actions, but the fact that horizontal dimensions receive the shaped reward makes those actions easier to learn.""

2. ""we plot the language entropies against different world radii. In both settings, we observe that entropy decreases as the world radius increases, but the setting with no shaped rewards shows a much more rapid decrease in entropy. We offer one possible explanation for this effect in Section 5.4. When the only reward signal agents have access to is the base reward, they can only accomplish the task by finding the goal randomly at first; as the size of the environment increases, the chance of finding the goal with random movements decreases and the agent pair often fails to learn a fully successful language at the highest world radii.""

The current results can be summarized in a workshop paper. "	2
"I think after addressing these questions in earnest, and giving the study a major overhaul, there could be some very interesting findings from this line of questioning, of interest to both the RL and Emergent Communication communities. But given the major issues it currently has, I will not reccomend for this paper for acceptance. 
"	1
See above.	2
"Like the paper
- use of PPO in emergent communication is new
- empirical exploration of effect of reward shaping on the entropy of the emergent language is new
- theoretical analysis of why the reward shaping affects the entropy of the emergent language is new"	4
This paper introduces an existing scheme into the federated setting, and provides an experimental proof of the gains it brings in terms of local computations and communication. Despite some minor issues which can be fixed, I think this paper could be accepted.	3
see the comment above.	3
"Good submission with a simple idea that seems to work well in practice. Extensive evaluation and nice compatibility with more traditional compression and quantization. There are some points that I would like some input from the authors which, if turned out positive, would strengthen this work further. For this reason, I am leaning towards acceptance.
"	3
"In summary, I have the following concerns:

1. The proposed ProgFed algorithm seems a little bit incremental, since it seems like an ""A+B"" method where A is progressive training technique and B is FedAvg (local-SGD).
2. For the model or the theoretical analysis, either the model does not consider data heterogeneity or the theoretical analysis needs to be modified.
3. (minor concern) ProgFed is a combination of two techniques, and comparing with compression schemes like quantization is not very fair."	2
In general, I think the client-adaptive quantization part is quite interesting and novel. But the time-adaptive quantization part is incomplete. The authors fail to demonstrate the effectiveness of their proposed method.	3
"This paper is well-organized with a clear presentation. However,  there exist concerns regarding the efficiency of the proposed algorithm
 
The authors are strongly encouraged to address these shortcomings by:

(1) Benchmarking the total training time and the computational overhead in quantization

(2) Comparison with other  communication efficient  FL strategies such as sketching


"	3
The paper is lean towards empirical studies. So the reviewer does not judge from the theoretical contribution perspective. Overall, the paper well presents the intuition and the logic of the data-adaptive quantization approach. The empirical study part is well-rounded and the results are relatively solid. 	2
The paper proposes an interesting communication-efficient FL algorithm. However, the theoretical analysis is not convincing and the commonly used analyses, like communication cost via communication rounds, are missing. 	2
Overall I appreciate this paper's idea of using score-based models for representation learning. However, I feel the writing and experiments can be improved before it gets accepted. 	3
Authors propose a simple idea to extract multiple-scale features with denoising score matching. However, very similar approaches have already been proposed in previous works and authors fail to cite and discuss.	2
"1. Pre-rebuttal rating.

    Despite I admit the idea, presented in the paper, is interesting and novel, the evaluation does not seem conclusive for me. I ask the authors to motivate the choice of the baseline and explain what the use case of the presented approach is. Currently, I tend to rate the submission below the acceptance threshold.

2. Post-rebuttal update

    Having read the authors' feedback and other reviews, I keep the initial rating. While the presented ideas may be of some interest to the community, the evaluation of the approach should be more thorough."	4
"Pros:
- Interesting extension of current non-adversarial generative models, with expanded theoretical results
- Powerful representation learning capabilities and SoTA on semi-supervised classification
- Adequate experimental assessment and ablation
Cons:
- Results on image generation are not very impressive
- Statistical significance of the classification results could be better assessed

I think that overall the paper is a valuable contribution and expands the knowledge on representation learning techniques in a meaningful way. "	3
Overall, the well designed structure makes the workﬂow clear and easy to follow, but the further analysis and discussion are expected to clarify the contributions in the techniques as well as in the evaluation section.	2
The paper has its merits. The paper tackles an important problem. The proposed method is novel and evaluated extensively. The writing is clear and easy to follow. However, the authors may want to justify some minor issues raised in the reviews. I am willing to raise my recommendation of this paper upon further clarifications. 	3
Previous works on federated learning either focus on the mechanism of parameter aggregation or the aspect of privacy. This paper opens a new direction in FL where clients may not be willing to share their unique model designs. From this perspective, I think this paper has promising impact on the research field of FL. 	3
"* The clarity of the presentation needs improvement. First of all, the underlying modeling assumptions on the datasets are not clear. What do you assume about the data distributions $\mathcal{P}_{c}$ and how are they related to the network architectures $\mathcal{A}_{c}$. 

* It is nice that the authors write down two research questions at the end of the third paragraph Section 1. However, it is not very clear how and where these questions are answered later on. 

* Section 3 is very short. Maybe it could be expanded with a discussion of using graphs to represent neural architectures. 

* There needs to be more explanation of Algorithm 1 in the text. How can we tune the input parameter of Algorithm 1 ?  The steps of the inner loop in Algorithm 1 should be fleshed out to provide a sufficient level of detail for implementing Algorithm 1. Should the client architectures \mathcal{A}_{c} be listed as input to Algorithm 1 ? 

* The literature review is missing a recent line of work on personalized FL (multi-task learning) using total variation minimization: 

D. Hallac, J. Leskovec, and S. Boyd, Network Lasso: Clustering and Optimization in Large Graphs, Proceedings SIGKDD, pages 387-396, 2015.

A. Jung, A. O. Hero, III, A. C. Mara, S. Jahromi, A. Heimowitz and Y. C. Eldar, ""Semi-Supervised Learning in Network-Structured Data via Total Variation Minimization,"" in IEEE Transactions on Signal Processing, vol. 67, no. 24, pp. 6256-6269, 15 Dec.15, 2019, doi: 10.1109/TSP.2019.2953593.

Y. Sarcheshmehpour, Y. Tian, L. Zhang, A. Jung, “Networked Federated Multi-Task Learning,” CoRR abs/2105.12769, 2021

* The following wordings are unclear: 
- ""... train a strong model ..."" what is a strong model ? 
- ""all clients need to share the same network architecture"" pls provide more justification for this claim. in general we can use arbitrary parametric models for clients as long as the parameter vectors are ""compatible"" e.g. have the same length. 

- "".. often run different networks .."" what do you mean by ""running a network"" ? 

- ""... current FL approaches do not support mixtures of different architectures."" im not sure if this is true. consider e.g. using different pre-trained nets as feature extractors for clients and then using the same last layers. as long as we use paramter vectors of same length for clients we can use e.g. network Lasso to implement FL. 

- "" Since different architectures have different layer compositions, representing layers as nodes allows meaningful knowledge aggregation across architectures. "" I do not understand this argument 

- ""...from some predefined family of models..."" what exactly is a family of models?

- ""...for a suitable loss function ..."" what precisely makes a loss function suitable?

- Figure 2 could include more details e.g. indicate hyper network parameters W_{G} , W_{H} and node representations h_{v}

- ""... are learned from data using updates from all clients."" unclear 

- consistently use abbreviation (e.g. HN) once introduced ! 

- ""... to represent non-parametric operation in the network ..."" what is a non-parametric operation ? 

- ""... see supplementary for more ... "" 

- "" ... and trained in a standard FL."" 

- "" ... when local data is ..."" 

- ""...when local data percentage is at ..."" 

- ""...our GHN can immediately populate it ..."" 

- "" ... we have proposed a new setup ..."" pls refer to the relevant equation, algorithm or figure 






"	2
In general, the paper is well written. The task is novel and well supported by theoretical proof and experimental evaluations. 	3
"This paper provides useful analyses on the fairness issue in the semi-supervised setting. Although I have some comments as described above, the paper is overall well-written.
Thus, I believe that this paper is worth publishing after clarifying several parts.
"	3
This paper studies an interesting problem. But some key references are missing, which should be discussed in related work and (potentially) evaluated as baseline methods.	4
"The prime objective of the paper is to demonstrate the Matthew effect of SSL and its fairness consequences.  
As described in more detail above, I have concerns regarding both the aspects of their contribution."	2
The perspective is novel, and the presented solution would be beneficial in detecting noisy labels. But the significance of detecting noisy labels is uncertain and needs more experimental proof.	3
"This work proposes an interesting perspective to analyze how the representations could help with noise detection. Yet, the major assumption of this work that a good representation is given still needs further justifications. Additional experimental results are also needed to fully support the claim.

===Update after rebuttal===
Thanks to the authors' response, which resolved some of my confusions. I am increasing my score for the additional discussions and results"	2
Although I appreciate the idea of using pre-trained models to provide guidance in selecting corrupted samples. It is also important to ensure the proposed method works well in practice, especially for imbalance or more complex large-scale settings.	2
The paper proposes a training-free instancewise noise label detection method. It supposes a good representation extractor is given and generates the initial soft labels based on the clusterability of representations using kNN. Followed by that,  the authors perform a local voting and global ranking-based scoring system to detect the corrupted labels. In general, the paper is well written and easy to follow. The main idea of utilizing good representation instead of training a deep inference model to avoid memorizing corrupted data sounds interesting and seems to work on CIFAR10 and CIFAR100 datasets.  However, I have some concerns about the definition and criterion of good representation which is the prerequisite of the method. As the proposed method uses kNN to analyze the clusterability of representations, I also wonder the increase of object categories may cause a significant accuracy drop in discrimination. Besides, the author only perform experiments on CIFAR10 and CIFAR100 instead of the acknowledged noisy-label datasets, which is not convincing enough for me. There are also some minor issues in the paper to be taken care of.	3
"To summarize, the idea of introducing node-specific, private information into a graph of multiple interactive systems seems to be novel and interesting and improves the prediction performance. However, it is not entirely clear to me if the private information can be propagated to other nodes (question 1) and if the model can even be applied in the envisioned scenario (question 2). I think the answers to both questions are important to make a reasonable assessment of the paper. Hence, my recommendation is rather tentative and I will update it as soon as the application setup becomes more clear to me.

After rebuttal:
Thanks a lot for the author's replies. I still think the applicability of the idea may be problematic, but I think the idea of NSI is sufficently interesting and novel to be accepted to this conference. Hence, I raise my score to accept."	4
Overall, this paper has demonstrated strong results on neural relational inference tasks. The concerns I have are about the assumption of the approach and unclear novelty points. 	2
"This paper presents a neural relational inference model with node-specific information. Experiments on real-world datasets validate the merit of the proposed method. 

Stengths:
1. This paper is well written and the idea is clear. 
2. The results are promising compared to various baslines. 

Weaknesses:
1. The codes are not availabel, it is difficut for others to re-produce the results. "	3
"While the proposition of understanding what aspects of code processing are represented in different brain regions is interesting, in its current form, I think the paper doesn’t successfully disentangle this. Several of the effects reported are not significant above chance or random baselines and further, not significantly different across brain regions. The features considered are also confounded with low-level properties of code comprehension that haven’t been controlled for. To this end, it is hard to infer what properties different systems are encoding and if there is a take away beyond prior work that identifies regions robustly responding to code.
"	3
The paper is well written, the motivations are clear, and the references are enough. Related work is adequately established the existing research in the field and compared the goal of the current research to previous work. However, the ideas of the paper are incremental and mostly applications of existing methods which have been put together to approach an interesting open problem. Moreover, usually further experiments on more datasets are required to evaluate any correlation between the code models and the brain representations in the proposed framework.	2
"The paper has done thoughtful analysis and comparisons to investigate how the brain represent computer code and has great potential to be accepted but more in-depth analysis are needed to make it a more informative paper for both the neuroscience and/or machine learning audience. 
"	2
Overall, the paper investigates an interesting and timely question, but offers limited technical and empirical insights. The work can be strengthened by a more in-depth discussion about what is learned by the comparison with complex ML models, and by additional analyses that strengthen the empirical conclusions (see under Questions).	1
The paper is generally clear and well written, and the empirical results are impressive.  Still, there are several opportunities to clarify the details of the approach as well as its advantages, to note its limitations, and to provide intuition for the empirical results (that may be surprising to some readers).	3
The present paper presents a framework to ensure the fairness of deep clustering algorithms. The paper is well organized and the motivation of this paper is clear. However, some definitions and roles about sensitive attributes are not clear. Besides, these tested datasets do not include sensitive attributes. The experiments are not enough. 	2
Lack of own contribution, proper motivation and sufficient justification of the proposed method. 	2
Please see above. 	2
"The topic of the paper is very nice, but I have some concerns and hope to have a nice conversation with the authors!
"	3
Given my concern about the novelty and contribution of this paper, I do not recommend acceptance.	2
I cannot recommend an acceptance to this paper as most of the results are more or less incremental given prior work.	2
"I think this is a very useful and well written paper.
Even though the scope is small, the results are convincing
and it shows a very clear way on how to effectively use contrastive
representation learning methods  while learning to control directly from
pixels.


## After Rebuttal

After reading the other reviewers comments and the rebuttal 
I see that i missed some literature that needed further comparison.
I think it is a good and well written paper but I would lean for rejection 
given this new data."	3
"I am concerned about the lack of clear novelty in the paper, and other experimental issues highlighted in ""Weaknesses"". I will update my rating based on the author's responses."	2
The paper is highly similar to a prior work as mentioned above, and as a result, the contribution of the paper is very limited. I would vote for a rejection.	1
I recommend a weak reject due to the fact that the datasets and models used in this paper are too simplistic; understanding how this method scales as those factors change will improve the work.	3
The contributions of the paper are unfortunately unclear. In addition to this, based on the empirical evaluation, the made claims are to general. 	2
My current decision is acceptance. However, the motivation of the fixed feature extractor is unclear for me. I'd like the authors to clarify the motivation for calculating the lower bound on the adversarial loss with the fixed feature extractor.	3
This paper provides an important application of previous work while also introducing a practical algorithm to find collisions in feature space and calculate a lower bound of adversarial robustness. While neural network architectures for which this algorithm is not applicable seem to be ignored and some notational issues made it take longer to read, I think the paper should be accepted.	3
"Overall I find the idea interesting and the results reasonable. Once my questions are addressed, I can offer better advice on the paper organization. I'm giving a score of 5 now, but I'm willing to raise the score after my questions are answered.

===================================================

After viewing the author response, my technical questions are resolved. I'm raising the score to 6 for the completeness in the idea. The paper can be enhanced by considering considering more data sets. MNIST and FashionMNIST is slightly too simple. The in-sample v.s. distributional robustness can be an interesting future direction to explore too. Currently it's addressed by using a practical attack algorithm as a probe. More explanation to this end can enhance the paper too."	3
Overall, I believe that there are some merits of this work regarding the simplicity of the implementation of the algorithms, theoretical justification of the convergence rates, and numerical instances of verifying the efficiency of BGPO and VR-BGPO.	3
This paper has limited novelty in both algorithm design and theoretical analysis. It is also highly similar to existing papers from several perspectives.	1
"The main comments are provided in the main review part. Here I will add a few comments. 

1. The authors have used ""mirror descend"" several times in the paper, please unify terminology and use ""mirror descent"".

2. Because the second algorithm proposed by the author is a STORM type variance reduced mirror descent PG method, the authors should also mention the work by Yuhao Ding Junzi Zhang & Javad Lavaei: \emph{On the Global Convergence of Momentum-based Policy Gradient}. The algorithms of this paper also apply the STORM technique.  For the same reason, the following work by Nhan H. Pham et al. \emph{A Hybrid Proximal Stochastic Policy Gradient Algorithm} should also be cited. 

3. Regarding equation (4), the author should mention that the objective function with a horizon $H$, there is a truncation error of $\frac{\gamma^H}{1-\gamma}$ compared to the original infinite-horizon MDP. 

4. Regarding Assumption 2 and Assumption 4, the author should mention that both of them are satisfied automatically given Assumption 1 and the fact that all the rewards $r(s,a)$ are bounded. 


5. Regarding Assumption 3, the authors should also mention that the importance weights can be bounded theoretically (instead of an uncheckable assumption) by using the truncated gradient step instead of a gradient step. See the work of Junyu Zhang et al. \emph{On the convergence and sample efficiency of variance-reduced policy gradient method}. The importance weights actually as some nice properties,   for soft-max policy it is bounded by $e^{c||\theta_1-\theta_2||}$, in Gaussian policy it is bounded as $e^{c||\theta_1-\theta_2||^2}$. So a simple truncated update is enough to control this value. 

6. There are still a few typos/gramma errors in the paper, for example, in the first row of contribution (a), the ""based the"" should be ""based on the"". The authors should carefully check the spelling and gramma in the revision. 

7. Regarding the experiments, the authors compared with several different algorithms. However, there is a small issue w.r.t. the selected algorithms. 

VR-BGPO: variance reduction (VR) + mirror descent (MD)

VR-MDPO: VR + MD

MDPO: VR only

TPO & PPO: no VR, no MD

From these comparisons, I'm not able to see if the MD really works. The authors should also compare with algorithms such as Prox-HSPGA, (A hybrid stochastic policy gradient algorithm for reinforcement learning) which applies STORM technique while not using mirror descent. 

"	3
The paper is nicely written, easy to follow, tackles and under-explored problem and provides a good analysis, but lacks sufficient novelty in methods or experiments and also does not have a strong justification with respect to some of the findings.	2
"Defining and studying hard episodes in meta-learning is an interesting direction that may lead to more robust models that can be more safely deployed in various scenarios in the real world. So this type of study has the potential to be impactful. Certain findings in this paper are also quite interesting, like the large discrepancy between ‘end’ and ‘max’ for certain episodes, and connections to forgetting. However, the paper lacks clarity in certain important areas that makes it difficult to correctly interpret the results. I also found that the paper can be improved in terms of comparisons with related work and more thorough experiments. I believe that another round of review would be required to address these issues, so I do not recommend acceptance at this stage.


#######
Edit: after rebuttal, and reading the other reviews:

I maintain my opinion that the paper, in its current form, falls below the bar for publication. To summarize, albeit some clarity concerns resolved during the rebuttal, I still find the narrative of the paper unclear and the proposed approach not sufficiently well motivated (please refer to my latest response to the authors for details). I also agree with reviewer uYEr that OOD tasks are a natural candidate for ‘hard’ tasks. In fact, an important advantage of defining hardness in this way is that it no longer is dependent on each particular meta-learner. Based on my understanding of the author’s response to my review, the results reported in the tables are computed on different episodes for each meta-learning model, which is problematic for making direct comparisons between them. Finally, I feel that the experimental section is weak and lacks comparisons with relevant previous work. For instance, ‘hard-task meta-batch’ employs a different curriculum learning approach which they find beneficial, whereas the simple curriculum learning proposed here doesn’t work very well, and I felt that they prematurely declare this to be a negative result for curriculum learning. As discussed, running additional experiments to compare against different design choices and related work would strengthen the paper for future revisions."	2
In its current form the paper is not ready for acceptance. The main issues are the lack of discussion of out-of-distribution literature and shallow empirical comparisons.	2
"The submission examines a very interesting topic, but falls short on execution in its current state, especially in terms of experimental clarity and generalizability of the reported observations.

---

**Post-rebuttal**:  I appreciate the clarifications and think the paper has made good progress towards better clarity.

At this point, however, I'm not ready to reconsider my score, mainly because my concern with how results are reported remains unaddressed. In the absence of confidence intervals it's hard for me to determine whether the observed improvements are significant or not."	3
This paper provides various experiments to show the generalization effect in policy learning with the pre-trained language model. However, most results cannot clearly support the main claim of the paper and it remains some concerns and questions.	2
This work presents limited empirical studies and has several critical weaknesses.	2
The paper offers novel, interesting results that contribute new knowledge about pretrained LMs. I recommend acceptance. 	2
"The paper has some interesting results, but the paper lacks novelty. Pre-trained LMs have already been shown to be effective in similar text/symbolic RL environments (text adventure games) and the paper is not as impressive even as some earlier work (see [1]). It does not provide new insights/methods/architectures compared to prior literature and so I think would not be accepted.
"	1
With more transparency, clarity and experimental evaluations, this paper would be very strong.	3
While the paper has much theoretical parts, experiments on more complicated dataset and comparisons to recent equivariant methods are needed to verify the soundness of the method.  	3
"Whereas this paper does rely on some heavy lifting in the recent Melnyk 2021 paper (which itself builds on past work), this past work is carefully reviewed in Sections 2 and 3. The meat of the novel contribution is in Section 4. The derived steerability constraints in the present paper and the construction of the basis spheres using placement on vertices of a tetrahedron, followed by the proof of concept use of the steerable spherical neurons, for particular tasks (e.g. recognition from skeletons seen under rotation), demonstrate the advantages of the approach. Whereas this might by some be considered by some to be a niche contribution, and others might argue that the experimental results could be strengthened, I for one appreciate the geometric reasoning and the relevance to ICLR themes. Overall the paper is quite dense and in places there are sentences that I was not able to parse, e.g., ""Equivariance is a necessary property for steerability as the group acting on the input space needs to be represented in the co-domain of the operator"". However, I consider these issues to be somewhat minor. I believe the strengths sufficiently outweigh any such weaknesses.

In terms of demonstrated applications of 3D steerable geometric neurons, I think that could come later. This is not primarily pitches as a benchmark oriented or empirical results derived paper. Rather, it is an attempt to add new and likely quite useful theory, and methods to apply that theory."	3
I find the proposed approach interesting yet its impact seems hindered by some potentially strong limitations, and additionally the empirical assessments do not include relevant prior work.	3
Although I carefully reviewed the paper, many of the theoretical claims were beyond my expertise (as indicated by confidence score). 	3
In its current form, I cannot recommend this paper for publication.	2
This is an empirical RL paper introducing a new algorithm, but the results are not convincing either based on their strength or intuition. At present the paper does not convince the reader to use this method or build upon it, thus it does not meet the bar for acceptance.	2
"The paper describes a new approach for learning a state-state mapping.
Pros:
- It is a well presented and clearer approach for learning a state planning
policy, than previous work (in particular, D3G)
- It opens a new research avenue in terms of exploration comparisons between
state-state and state-action policies
Cons:
- An alternative formulation of a previously published idea (D3G) which can be
seen as an incremental contribution
- Not completely clear why and under which circumstances the state-state
mapping is preferred over the state-action mapping
- Lack of assessment on the performance of the approach with different
state consistency threshold values (b)"	3
"While the paper appears to adduce some novel results. However, the presentation is unclear which makes it hard to verify the results of the authors and properly situate it in the context of previous literature. 

I do however think that the authors have a reasonable chance to address my concerns for a camera-ready version, why I recommend a marginal accept. 

"	2
The paper should be accepted because it is well-written for the most part, tackles an important problem, and provides a novel generalization of a well-known method.	4
"This paper is well-written. 
Real datasets should be used in the experiments. 
The computational time of the Cholesky factorization should be included in Figure 2. "	3
Though the paper could benefit from additional analyze on explaining the performance of the proposed algorithm, I think this paper brings about some valuable insights which may benefit and impact how off-policy algorithms are implemented in the long-run.	3
The paper presents a useful and interesting improvement to REDQ but novelty claims are not backed by experiments. 	3
"I think the paper has potential to make some good contribution, the authors already obtained some good results showing that Dr.Q can achieve the same performance as REDQ while greatly reduce computation and memory consumption, which is very nice. However, what is lack is some more in-depth investigation on why the proposed method works. Why dropout on its own does not work? And why layer norm will deal with the difficulties? What are the unique advantage of this combination over other methodologies? Very little discussion and empirical/theoretical analysis is provided in the paper. I think if the authors try explore these questions more, they should be able to make a bigger contribution and make this paper much stronger. 

Though the paper studies an important problem, and has some good results, due to this major concern, and considering other issues in the paper, I don't quite think the paper is ready for publication in its current state, thus I currently lean towards rejection. 

==============================================================

Post-rebuttal: after reading the authors' response and the revision, I feel that a large number of my concerns are addressed and the new results ablation and analysis look quite interesting. Though I still have some concerns, I now increase my score to 6. "	2
This paper lacks sufficient comparison to other more contemporary weak supervision work, lacks sufficient motivation and/or ablation w.r.t. the heuristic design choices taken, and has potential issues with experimental protocol.	2
This paper is clearly-written, but it has significant flaws in its evaluation, and the method is only marginally novel. Therefore I recommend reject.	2
The work may be interesting for the automatic label annotation area, but the idea novelty is a little trivial and experimental results are not convincing.	3
This paper tackles a practical problem. The idea is relatively novel. However, the writing of this paper needs to be improved to reach the acceptance line. The current version is hard to follow. Therefore, my score for this paper is negative. 	2
The paper presents a simple approach to post training activation quantization that should be easy to implement and test by other researchers. The authors present both a mathematical treatment and empirical results to back up their claims and I wasn't able to find any glaring error. Hence, I think the paper is good and should be accepted. I expect their code to be jointly published with their paper.	3
Overall, I think this will be an instructive work if the authors can tackle the reviewer's concerns. 	2
"To sum up, the paper is well-written and generally interesting even though there are a few glitches. Therefore, I may change my rating according to the authors' rebuttal to resolve the above concerns and questions. 

***** Post-Rebuttal Comments *****
I appreciate the authors for their detailed response. The newly updated manuscript addresses most of my concerns. Therefore, I am happy to raise my rating up and genuinely recommend this paper to be accepted. "	4
Please see the Cons in the main review.	3
The paper contributes adequately to the community and provides enough details regarding the problem formulation and empirical performance-cost tradeoff. Including a more complete comparison with the literature will make it a stronger submission.	3
The paper is well written, polished, and easy to follow. The anistropic part of the proposed approach is well-motivated, however the sample-wise part has several issues (see main review) making it unsuitable to use in practice. Moreover, some of the theoretical results are not novel (see main review).	3
"The paper presents anisotropic robustness certificates, which seems to be technically correct and conceptually interesting.
While not stellar I believe the novelty and motivation are sufficient, given the results.
However, my reception of the results, and thereby the overall paper hinges on the data-dependent optimization procedure considered in the paper, of which I am currently not convinced."	2
This approach advances the state of the art in an important line of work. A slightly nonstandard problem setting is considered, but even when comparing to the standard problem, AnCer improves upon prior works. The analysis is clean, the results surprising, and the experiments are thorough. I have no qualms recommending acceptance.	3
"Overall, I highly question the use of the certificate. It is not certifying the actual smooth classifier as desired, but it certifies some other classifier (with highly undesired properties) in a non-scalable way. Thus, also the experimental comparison is questionable.

I am currently giving a score of 3. I might be open to increase it in case the authors clearly state that they are actually not certifying the smooth classifier. Put differently: The presentation has to be updated in various places since at the moment it is highly misleading and readers might get a wrong understanding of what is actually certified. Moreover, the authors should elaborate whether the experimental analysis is still useful or not."	3
The idea of this paper is interesting and the paper itself is a solid work to improve model robustness by optimizing the certified bounds w.r.t. the noise variance. However, there are missing comparisons with existing techniques/methods, which I hope to be solved.	3
The author(s) might want to discuss in detail why it is desirable to maximize the radius $R$ in their proposed objective function. 	3
See the comments.	2
"Although I believe that this manuscript is well-written, tackles an interesting problem, and has an evaluation that is consistent with its claims, I do not think there is enough evidence to back the strong claim that SOTA HTP models are unable to learn almost any interactions for well-studied datasets that have cases of high interaction, but are able to learn interactions for a dataset with more interaction.
"	3
The paper clearly presents a variant of Shapley value for trajectory prediction tasks. The proposed measure helps identify if a model uses a feature for prediction. My main question is about the novelty of the proposed approach.	2
Overall, I think that the contributions of this paper are interesting and could raise more discussion and exploration on the interaction modeling in multi-agent systems. I have a positive feeling about this paper. My major concern is the suitability of the proposed Shapley value metric in the task of trajectory prediction. Hope to see more explanations in the rebuttal.	3
The paper is valuable to the community of trajectory prediction models. It directly discusses if the “social” interaction exists or not in previously reported models. Some questions were raised regarding the choice of the static, non-interacting agent and the feasibility of the presented social interaction score. 	4
"Overall, I think it is a good paper with some minor problems that presents a new perspective that the past trajectory of the target is the only feature used for predicting its future for state-of-the-art trajectory prediction methods on standard benchmark datasets. On the balance of positive and negative points, I think it is a borderline paper and tend to accept.

Post Rebuttal： Thanks for the authors' feedback. The rebuttal answers most of my questions, which raises my score. It is suggested to further analyze why the interactions have limited effect in trajectory prediction. It is unintuitive but makes sense. There are some potential reasons in my views: 1) There are biases in interactions? 2) The multi-modality generation breaks the effect of interactions? 3) Is this finding generalized to rule-based methods, such as Social Force? I will further raise my score if these questions can be addressed.

 "	3
"This paper studies positive and unlabeled (PU) learning by constructing a transformed the probability distributions of input data by a re-grouping operation, such as the irreduciblility assumption holds on the data to the subsequent classifier. Overall, this paper is clearly written. But because of the following concerns, also mentioned above, I suggest the authors to further improve this paper before publication:
1. Please proofread this manuscript carefully to improve the writing.
2. More comprehensive experiments to compare deep neural network based PU learning should be added to convince the readers."	2
The authors give an insightful demonstration of limitations of existing CPE methods, and also give feasible solutions to the limitations. Theoretical proofs are given, and experiment results show significant superiority of the proposed method compared to existing CPE methods. So I believe this work would be beneficial to the community and therefore give a score of 8. 	3
"This article focuses on practical issues, and its writing style is clear and easy to read. The structure of the article is reasonable, and the experiments are full and practical.
However, there are many parts that need to be improved in the method part, which are listed in Main review. In general, although this research scenario is very meaningful, the ReCPE method proposed in this article is too strategic. It only propose solutions intuitively, and there seems to be no strong guarantee in theory."	2
"This paper considers a particular PU learning setting where the irreducibility assumption is not satisfied, and gives a new method to address this problem. The proposed method can be combined with any class-prior estimation method and the experimental results show positive effect of the proposed practical implementation. This paper is well-motivated and novel, and it will be better with the weaknesses addressed.
"	3
"- quantitative evaluation is weak 
- missing related works and quantitative comparison 
- incremental work"	2
As discussed above, the proposed method simply inject the idea of the global context in Chen 2017 into the Skip-gram topical embeddings by Shi et al, 2017, which is very incremental. Even regardless of that, the performance gain of the proposed model is very marginal, even worse than STM-diff. 	1
"Now, the mainstream of representation learning is attention based models or Transformer based models, 
so we would like to see a comparison with these and discussion.
For example, we use BERT and then get a similar expression from [CLS] token.
"	1
In general, this paper has not been carefully checked and well organized, and the experimental evaluations are not solid. This paper needs some major updates for future improvements.	2
Overall I think the work is good.  I think the contribution is significant enough to merit publication.  Most of my questions/suggestions I believe can be addressed through writing (eg more verbal justification or clarification)  I did not carefully check the proofs, but nothing stood out as suspicious.	2
"Pure exploration in bandits is a well-studied problem and Thompson Sampling is a well-known meta-algorithm. The paper provides a TS-based algorithm for best arm verification in MAB (or super-arm verification in combinatorial MAB). While the analysis is correct, more discussion about why the simpler (in comparison to [Agrawal and Goyal 2013] or [Kaufmann et al 2012]) analysis works in this problem would be helpful. Motivation for why TS should be used for verification could be described more, maybe by simulations or by describing its greater applicability. The introduction mentions two broad challenges in the analysis of TS strategy but does not describe at a high level how these challenges are overcome. Due to these shortcomings, I put it below the acceptance threshold.

====== After reading author response, I've increased the score.
"	2
The algorithm works well, but the contribution may not be enough.	3
I think the formulated problem is a bit strange which is not a standard pure exploration problem. And the algorithm will not stop if the input arm is not optimal. So I would suggest authors give a complete pure exploration algorithm together with the complexity analysis.	3
This paper studies the problem of approximating the inverse Jacobian matrix in hypergradient computation for bilevel optimization problems solved with gradient-based methods, in an attempt to reduce the computational bottleneck of computing the exact inverse Jacobian matrix in high dimensions. The proposed method leverages quasi-Newton methods for such approximations. Experimental results have demonstrated the effectiveness of the proposed approach. 	3
"Overall, I like the core idea and theoretical insights of the paper but have questions regarding the experiments as indicated in my main review. I am willing to update my scores based on author responses.

Update:

I would like to thank the authors for thoroughly engaging with reviewers on the platform. After reading the author responses to my review and other discussions on this forum, I am convinced that the improved draft should be presented at the conference and vote to accept the paper."	3
The paper's main motivation is to computationally improve the backward pass. They show speed improvement compared to the other methods involving inversion of the Jacobian and minimal decrease in performance. However, they do not show performance improvement compared to the Jacobian free which is faster than the method in DEQ experiment. Given this, why one may SHINE that approximates the inverse Jacobian when their method does not outperform the Jacobian-free method? More experimental results is needed to highlight the advantage of their method against Jacobian-free. This advantage is already shown once in the regularized logistic regression.	3
This paper presents major problems: it is severely unaware of previous work in the field, does not present a sufficient contribution and it is not well evaluated.	1
Overall, the authors propose a technically sound sampling technique and experimentally show its benefits over traditional sampling techniques, such as faster and stable convergence and higher test accuracy, in the context of action detection from videos using LSTMs. However, the authors also attribute the superiority of their technique to the claim that it is closer to how humans learn from data, but this is not grounded with any relevant prior studies or observations. Without such grounding, the claim comes off as post-hoc and the technique comes off as a successful engineering effort rather than a novel research contribution, thus making me lean towards rejection.	2
This paper proposes a very simple idea: repeatedly inputting previous frames into the same units in LSTM structure attempting to imitate humans' repeatedly memory system. But the results seem ugly, the training parameters (batch size) seem to be randomly selected. This method seems impossible to generalize to other training parameters, tasks and network structures.	1
"I find this work is an interesting exercise in the context of data augmentation (an illustration of how classical models from physics may benefit the learning), although the technique is not novel and the general consequences are not discussed in detail. 
"	2
This paper proposes an interesting idea and is very well written. The issues (with colour jittering) pointed out in this paper were previously known. Thus I view this paper as more geared towards practical purposes. However, the experiments are not comprehensive enough to strongly support the practical claims. Thus, my vote is for the paper to be rejected in its current form.	2
This paper proposed a general color augmentation that performs better than the typical color jittering and demonstrates better performance in contrastive and self-supervised learning schemes. The only concern I have is the speed.	3
"I am currently **leaning to reject the paper**, because in its current form, the empirical support is too weak to sufficiently support its claims. 
1. I believe the experiments need to include an additional baseline
2.  The writing should have more details about the domain and methodology. 
3. Some line of empirical questioning seems disconnected from the main claims described in the motivation.  
4. I cannot be sure the proposed architectural components make a statistical difference, because the reported confidence intervals overlap.
"	2
"The authors claim to have obtained stable cognitive maps in a network trained to do path integration. However this claim is not supported by the results, as they show only a correlation between learnt embeddings and location in the environment. A cognitive map is a spatial knowledge about the environment, that could be used to guide behavior in a flexible manner. From the neuroscience literature when know that this knowledge is based not just on absolute position (place cells), but also on 
- an allocentric sense of direction (head direction cells)
- boundary position (boundary vector cells)
- and cells that support coding of metric distances as the animal moves through the world (grid cells). 

This if we only report the most well known. So there is a lot more than just absolute location, but none of these features are present in the paper. For this reason I don’t think the authors can claim to have learnt cognitive maps.
"	2
In principle, a study of this type  is well suited to ICLR, and I think some of the analysis of internal dynamics in particular could be interesting. However the manuscript was difficult to follow at parts, and was missing critical details that make it impossible to evaluate. I also have reservations about the significance of problem being studied, as a full discussion of contemporary RL agents that can perform path integration is lacking.	2
This work proposes a simple yet effective way to fuse external signals and proprioceptive signals. It is benchmarked on a 2D maze-like environment and shows better performance than LSTM. I am not able to fully understand the contribution of the paper since it doesn't compare with other state-of-the-art cognitive mapping methods, and it doesn't use standard benchmarks. 	2
The author should make it clearer what the expected target audience is, those who are interested in modelling biological systems, those interested more in artificial systems or those who search for inspiration in biological systems to build artificial ones. The paper does have its merits for each group. It might not be directly applied for e.g. robotics, but the fact that you can learn a state space that combines local and global information as suggested in this paper is worth reading about and could possibly lead to more applicable solutions in areas beyond biological modelling.	2
"Although the privacy-preserved complex-valued DL is an interesting problem, I’m not sure this paper reports enough contribution for pushing the development of this research field. The theory for complex Gaussian Mechanism seems to re-use the prior theoretical results. The used gradient computation method based on Wirtinger calculus is similar to the previous works on complex-valued deep learning.
"	2
"The authors suggest a new framework for differential privacy in complex spaces, which works well with complex notions of derivatives.
However, as far as I have gathered, this is not particularly different than the existing notion of differential privacy in real spaces and I was not convinced there is an actual benefit to using the complex derivatives.
"	2
The paper is very well-written and insightful. It provides new DP notion, new Gaussian mechanism, new DP optimizer and exciting experiments. Minor weakness exists: the extensions are not well-discussed and experiments can be enhanced.	4
The paper has studies an interesting problem but it still has lots of space to improve.	2
I think the method presented in this paper is a good strategy for defining the state/action space for bin packing. The paper is clear and the experiments are convincing, so I recommend acceptance. It would be stronger with more compelling evidence that the graph network is learning non-trivial policy behavior, but overall the results are certainly strong over prior work and existing heuristics.	3
"The reviewer is not actively working within the scope of this work. Still, it is a pleasure to read this paper, and it clearly describes and motivates the problem and proposes plausible and effective solutions with detailed experiments.

====
The reviewer has read all other reviews and the rebuttals and would very much thank the authors for the speedy and detailed response."	3
"The idea of PCT is holistically novel and seems effective for the online 3D-BPP, which is a very challenging issue anyway. Therefore, although the paper has some issues as listed above, I am still slightly positve to it.

------------ Post rebuttal -------------- 

Most of my concerns have been well addressed during the rebuttal period of time. I'd appreciate very much the effort made by the authors and am happy to lift my recommendation to a full acceptance. Btw, it seems that some references are missing in the revised version, such as the two recent ones for 3D-BPP listed below:

- A Generalized Reinforcement Learning Algorithm for Online 3D Bin-Packing. AAAI 2020.
- PackerBot: Variable-Sized Product Packing with Heuristic Deep Reinforcement Learning. IROS 2021.
"	3
"The authors address a real problem with gradient boosted decision trees. But the results are limited and do not appear very convincing. They need to evaluate their approach on more datasets to make a convincing argument. The improvements from the proposed approach do not look very significant either.
"	2
Overall, the paper proposes an interesting new method, but has an insufficient empirical evaluation and is additionally quite imprecise throughout.	3
"I recommend a rejection based on the weaknesses I list above. Specifically, in the current state, it is very hard for me to follow the proposed scheme and why existing schemes do not apply. It is important that the relevant baselines are properly compared and contrasted against. Moreover, we only see a marginal improvement of the proposed scheme over the considered baselines.
"	3
The problem considered in the paper is important and actual as I describe in the paper summary. The idea on developing algorithm with the shared tree construction based on tasks losses makes sense and in some sense parallel to the things done in deep learning. However, there are a lot of ambiguous, not clear places (even I think some places are wrong, see the main review) in the paper that does not allow to conclude on the correctness and applicability of the proposed approach. Moreover, the paper lacks of the proper baselines and comparisons, statistical significance testing for one of the datasets, and ablations on the parameters introduced in the algorithm 2, 3 (details in the main review). For the current paper state my recommendation is to reject due to all aforementioned things.	2
I don't claim to be an expert in ML theory, but given the simplicity of the concepts used in the proof, it should have been easy to follow. This is not the case. An opinion from a theory expert should be more appropriate. It is my impression that the reviewers that would be capable of giving a clear opinion are probably not at ICLR. This paper should be submitted to venues like COLT or SODA. The paper needs more work and more clarity in the notation. My recommendation is to reject the paper.	2
Considering the difficulty to understand the proof presented in the article, I do not think the paper is ready to be accepted in its present form. I can reconsider this position if the needed clarifications and improvements are provided.	3
Unclear and unconvincing.	2
The theorem is either trivial (it is impossible to find an optimal task split) or wrong (solving the task in the task-split optimally does not mean solving the original task optimally).	4
The paper makes novel theoretical contributions to the challenge of deployment costs in practical RL algorithms. The problem is well motivated, well written and good fit for the venue.	3
"The paper introduces a new framework, DE-RL, and provides lower bounds under different settings. By drawing inspiration from reward-free and provably efficient RL, the paper provides efficient algorithms for the task.

The setting is interesting and motivated by real-world concerns.The results are comprehensive and reasonable, with clearly explained proof in the appendix."	3
This paper provides a solid study in deployment efficient RL from the theoretical perspective. Therefore, I choose acceptance.	3
Overall, this paper provides a significant contribution to the field. I found the theoretical results insightful and, although the present submission contains no empirical results, I believe the derivations may inspire future more practical algorithms.	4
Based on the above analysis, I am slightly leaning towards acceptance for now.	2
"In summary, the method is simple yet effective, showing strong performance on various benchmarks.
"	4
Overall, this paper is well-written and well-organized for both theories and experiments. The idea is interesting and is easy to implement. The experiment results are empirically sound. 	3
"This paper is solid and well written, however, there are some points i am confused. I thus give the score of 6.

====================================update========================================

First of all, I appreciate the authors' hard work in the rebuttal. Most of my concerns have been addressed in the discussion. I would like to raise the score to accept."	3
"I am on the fence about this paper. On the one hand, the authors proposed a new threat model and provided with theoretically-grounded algorithm for tackling that problem. On the other hand, the motivation of the problem is not clear to me and the evaluation is also insufficient. I'd like to see other reviewer's opinion and the author's comment before making my final recommendation.

Post rebuttal:
I appreciate the author's effort on addressing my concerns and answering my questions. Most of my questions has been addressed. Thus, I'd like to raise my score to 6. However, I am still not fully convinced about the significance and motivation of studying online adversarial attack. My final recommendation is perhaps we could accept this paper and see how it can inspire future research on this direction."	3
In the review, I proposed two major concerns toward the paper along with some minor ones. I did not check the proof carefully but they look correct.	3
The new algorithm VIRTUAL+ is interesting, but please make it clear about the motivation and novelty of this work towards adversarial learning. Please also provide convincing explanations towards some observations in the experiments.	3
The method is interesting and presented well. Weaknesses in presentation and experiments are mostly lower-level and affect only parts of the method and its description or validation, and are not central to the overall paper.	3
"My main concern is that the technical novelty of this work is somewhat limited. Other than that, this work is solid and has comprehensive results and analysis. So, I feel this work is borderline and am slightly towards borderline accept.
"	3
"The proposed approach is simple and has nice accuracy-FLOPs tradeoff. However, the proposed approach does not seem to yield actual wallclock speedup due to its incompatibility with current parallel chips (GPUs). I'm not an expert in HPC, but I'm not sure about the assumption that this will change in near future. Without much positive signal on that, I'm doubtful about this general direction.   
"	3
I'm not convinced that the proposed problem is novel. Furthermore, considering the conceptual similarity to several prior methods (see the discussion above), I believe that the technical novelty of the paper is also quite limited. Lastly, in my opinion, the current experiments are lacking. Therefore, I would recommend rejecting the paper.	2
While the paper showcases that adding these extra constraints (kGLM and Lipschitz) is useful for initialization, it's not clear if how useful the kGLM-based DEQ is for real tasks. 	3
"As mentioned in the main review, I think this paper provides a good and solid theoretic perspective to understanding the deep equilibrium networks, which is a form of implicit network that has drawn growing attention these days. While the model is simple and constrained, the idea that the plausibility of DEQ models may be provided by some underlying optimization problems is an interesting and profound property. This also reminds me of the Hopefield network paper [1], which suggests that the self-attention layer is (sort of) minimizing an energy state, which somewhat implies why a Transformer-based DEQ layer would work. In addition, the authors have shown that establishing such connection between DDNs and DEQs allows us to make informed initialization, which provides appealing stability properties to the DEQ model training. Despite the limited scope and practicality, I believe this paper is still a good theoretical addition to the current set of works on implicit modeling (and how to make sense of it).

[1] https://arxiv.org/pdf/2008.02217.pdf"	2
Overall a good paper on the theoretical connection between DEMs and DDNs that brings insight into the initialization of DEMs. The theoretical justification is well written and the empirical experiment is a bit on the week side.	3
The paper is well-written, and provides a useful result that gives insight into a rich class of models. One set of experiments, namely the comparison of a simple DEQ with a DDN solving empirical risk minimization with a kGLM, should be added to corroborate the theoretical claims. 	4
Overall, this is a borderline paper with promising results. The major concerns include the limited technical novelty, missing references, and unfair experimental comparisons. I would like to see the rebuttal and comments from the other reviewers to make a final decision. 	2
In general, I feel that the paper provides very good experimental results but I have concerns about their model design. I am willing to change my rating if the authors can address my concerns above.	2
Although the proposed augmentation method is simple. it shows state-of-the-art performance for domain generalizable semantic segmentation. The proposed method can be also adopted in different methods, backbones, and tasks, which is beneficial to the field. However, the authors should address the raised concerns regarding the technical clarity and experimental comparisons.	3
"My vote is towards accepting this paper. Similar concepts have been applied in the past to improve domain generalization but never reaching the quality achieved by the proposed approach. It is exciting to see how the application of adversarial style learning can boost results even surpassing existing state-of-the-art solutions, especially considering the ease of implementation of this idea. The experiments section is quite strong, leveraging well-known datasets, relatively modern segmentation architectures, and very recent baselines.

"	3
Imputation is a key and general ML problem. EMFlow is an intuitive framework methodologically and in practice seems easy to train and performs well on real data. It is principled in its approach — relying on the rigorous EM algorithm as its base. However, EMFlow (albeit fairly due to training complexity) does not compare to what might be state-of-the-art GAN-based imputation methods. It is also unclear how EMFlow would scale to even higher-dimensional datasets given the modeling assumptions.	3
The presentation is clear, the model is theoretically sound, and experiments show impressive improvement against baseline models for most of the dataset and missing rates. However, the novelty is somewhat limited, especially when compared with MCFlow.	2
The work focuses on MCAR and MAR cases and extends NF and online EM for the missing data problem, which can be interesting for applications from a practical perspective. But the assumptions and main idea of the work can be lack justification, i.e., the relationship between the dependencies in the latent space and observation space needs to be elaborated and clarified. Moreover, a more thorough comparison with other related works would be helpful to better evaluate the proposed method.	2
The study and the expected results are acceptable; however, this framework was tested on imaging dataset, which has unique characteristics. for example, one can not assume similar performance when the method is tested on longitudinal clinical data, unless tested systematically on actual data. 	3
"Although the authors propose a general framework to deal with multi-armed bandits and RL problems, it seems to me that the work is an extension of the existing work i.e., Wu et al., 2016 to a more general problem setting.  If I read the paper correctly, I did not see any new techniques developed to tackle this more general problem setting. 

Detailed comments:

__Before section 2__, ... All these works focused on provide an upper-bound to the regret of a conservative algorithm...: provide -> providing

__Notations__, ...there exists a constant $c$ such that $A \geq (\leq) cB$... : constant -> positive constant

__After Assumption 1__, ... there exists $\theta^{\pi} \in \mathbb{R}^d$ such that $Q_h^{\pi}(s, a) = \langle \phi(s, a), \theta^{\pi}_h \rangle. $: $\theta^{\pi}$ -> $\theta^{\pi}_h$"	3
The problem studied in this paper is interesting and I think it would interest the ICLR community. While the theoretical derivations heavily reliy on Kazerouni et al 2017 and Wu et al. 2016, the theoretical results are interesting. I have not checked all the proofs, but they seem sound and correct.	2
While the reviewer tends to accept the paper, the reviewers has some concerns.	3
The paper studies an interesting question and presents a natural reduction-based approach that yields optimal upper and lower bounds. However, the relation to previous work is not described clearly, the proof sketches are either unclear or missing and the authors put their focus on a lower bound that is less tight than previous ones. Overall, while this paper has interesting contributions, it requires further revision before being published in my opinion.	3
The approach is well-motivated. The comparison to Polyak DQN is great. More experiments on complex tasks with relatively large action space, with sparse rewards will be great. A few minor concerns/questions in the main review. 	3
The paper is easy to read and provides a significant contribution to a very widely known problem. The solution can easily be integrated in existing applications and frameworks and is hence of broad interest to the community. While I am missing a few experimental benchmarks against approaches that address the same problem but differently, in my opinion the paper provides sufficient experimental evidence to judge the advantages and limitations of the proposed approach.	3
This paper brings a new perspective for the use of target networks in DQNs, and the experiments show some promise. However, the interpretation on why it works has some flaws, and a more rigorous analysis could make it more convincing. Furthermore, the authors need to compare with the related work above.	3
The paper presents a straightforward and well-motivated idea for using online q-values for calculating the bellman loss. The paper is well written and easy to follow. Since there are essentially no theoretical contributions, the empirical evaluation is important but unfortunately has several problems. The authors consider the toy environment of four rooms, which I believe is too simple. The authors also consider 7 Atari environments, I think it would be better to consider all environments from the Atari suite. Most importantly, for the Atari environments, the baselines considered are very simple, and improving upon them does not show that the state-of-the-art is being advanced.	2
This paper offers a new and interesting view on the well-known ensembling of DNNs. Some details are not entirely clear to me, details see above. 	2
"Overall, the measurements provided here demonstrating the effectiveness of this simple and sometimes overlooked approach, are welcome evaluations.  I would have liked to see more on the importance of the search and variation between model configurations.  However, I think there is significant practical interest to see measurements that establish the effects of cascades from a candidate model pool, in the context of current classification models.
"	3
I find that this work provides a thorough study of the efficiency-accuracy trade-off offered by ensembles of shallow models w.r.t. single deeper models. However, I have difficulties recognising the significance of the new insights this study delivers in light of the previous work (e.g. Lobacheva et al., (2020))	2
This paper points out a good direction to dive into unsupervised learning of compositional scene representations. However, technique strongness and novelty may need to be further improved. 	2
"The authors presented a novel idea. The system is engineered well and the authors have shown success on three synthetic datasets and various applications. However, all evaluation and experiments are done on synthetic datasets with the same set of objects and background. Thus it is unclear how this approach can generalize to solve real world problems.

Edit: The authors addressed my concerns in their revision. I think it is a good paper and should be accepted."	3
I vote to accept this paper for publication at ICLR 2022. I like the idea of modeling scenes as a combination of disjoint objects, which can be added, removed, modified, and recombined to form new scenes. I also think the paper is well written, well-motivated, and provides extensive experiments. In my opinion, the paper adds to the literature on neural scene representation/decomposition and is interesting to the community. I have some minor suggestions and questions (see above), which I hope the authors clarify during the rebuttal.	3
I admire the motivation, idea, and possible impact of this paper. However, I am not entirely convinced that the experimental results are convincing enough. I would like to update the score after interacting with the authors and other reviewers.	4
"While the ideas presented in this work could be very impactful, as the paper is currently written, its main claim seems incorrect which is grounds for rejection.

The paper claims to develop a general framework for approximating offline algorithms using online algorithms. But to me, it seems the online algorithms do not approximate the offline algorithms.

I think the paper could be made substantially better in one of two ways:
1) The authors clarify in what sense the online algorithms approximate the offline algorithms.
2) The authors modify the claims to more accurately reflect the the method, and add additional experiments to support those claims."	3
Overall, this paper lacks an evaluation for the proposed method, and thus cannot be accepted. The proposed approach seems interesting, and I encourage the authors to resubmit after incorporating a proper evaluation by comparing to other methods on established datasets and addressing some of the other comments above (in particular the dataset issues).	3
I like some of the ideas of this paper, but overall I think that it falls just below the bar because of the reasons I stated in the weakness. Please correct me if my understadning is incorrect.	3
"To the best of my knowledge, the proposed offline-to-online framework by predicting behavioral structures of the offline algo through a multi-task learning scheme is novel. 

For weaknesses, more explanations/discussions on the following aspects would improve the paper: 1. How the performance of the proposed framework relates to that of the offline algorithm; 2. Choice for number of decisions in a structure; 3. Comparing predictions across different tasks. The paper’s exposition in terms of explaining the key concepts can also be improved.
"	3
"Post-rebuttal:

I thank the authors for their careful responses to the reviewers’ concerns, comments and questions. Their response addresses most of my concerns. I highly recommend to include the responses to the major issues in the paper to rectify the ambiguities, and re-write Section 3 accordingly. I would like to change my evaluation based on the authors' response, to accept.


-------------------------------------------------------------

In a nutshell, the submitted manuscript suffers from the following issues:
1) lacking a clear motivation/description on the proposed denoiser (4) and (5),
2) ambiguity of the proposed PnP method (9),
3) unsupported claims and lacking a clear proof for Theorem 1 by which the contribution of paper is built upon, 
4) insufficient experiments,
which overall could not convinced me to accept the paper, but to evaluate it as “reject”."	2
"From my point of view, this is a truly innovative work with significance on both theoretical and practical sides. Since PnP is a very important tool in image reconstruction domains, this paper is thus worth being highlighted and would inspire further works in future. 

Disclaimer: I don't check the full details of mathematical derivations. The justification here is therefore contingent upon the prior assumption: all the Theorem/Remarks in this paper are rigorously established. "	4
This paper is proposing an interesting alternative to training contractive/nonexpansiven denoiers for PnP and RED with convergence guarantee. Although I would expect a little bit more technical depth at least via numerical simulations, I believe this is a well-written paper with some new results that could spark further research in the important topic of designing explicit regularizers for PnP/RED. 	2
"In general, I vote for *weak rejection* based on the current evaluation of the paper.

—— After rebuttal ——
I now vote for acceptance."	3
The proposed method gives a new way of enhancing Transformer models in the graph domain. However, the experiments are not sufficient to justify the efficacy of the new model given its added complexity.	3
This paper proposes a model that tries to improve the performance on the transformer on graph structure data. The proposed model has pretty good motivation and reasonable theory support, with the performance being relatively good in comparison to other baselines on some tasks. Overall, this is a solid paper, but it might more experiments to justify the importance of the proposed model.	3
The paper lacks the motivation and the proposed model SpecTRA is incremental.	2
The idea that different filter shapes are necessary to discriminate the spectral components associated with different graphs makes sense, therefore, learning the filter coefficients of the GNN used to distinguish between these graphs is a compelling solution. However, other architectural choices---such as learning the spectral GCN filter weights from the transformer's attention matrix---are not well motivated. Moreover, the numerical results are unconvincing. 	3
It is a good submission that provides an interesting way to improve the updating rule of SAM. However, some modifications for the technical part of this submission are still needed.	3
The paper has some smaller weaknesses (see main review) but if these are addressed in the rebuttal I am inclined to increase my score and recommend acceptance of the paper.  Overall, the work proposes a simple and elegant method, which is well motivated and gives state-of-the-art performance in an extensive numerical evaluation.  Therefore, I believe it will be of great interest to the ICLR community. 	3
The idea is nice. The empirical performances look promising and the theoretically analysis generally sound (though sloppy). I recommend a weak accept. 	3
The idea of minimizing the surrogate gap and the perturbed loss simultaneously is interesting. The experimental results are sufficient. However, the theoretical analysis is not quite convincing.	3
The paper tackles an interesting problem of long distance estimation by introducing a novel dataset and an attention based learning framework. The authors need to clarify some my concerns before acceptance. 	2
Overall, this paper proposed an interesting solution to a very hard visual perception problem. By modeling the pairwise relationship between targets and reference objects, the proposed framework can achieve good performance for the problem of interest. The paper is well written, experimental results are solid.	3
The work addresses a practical problem in autonomous driving. The motivation and the proposed method are clearly desribed. Experimental results are pretty good on two datasets. Of course, there are also room for improvement in both the experimental setting and evaluations as described in the above reviews. Nevertheless, I lean to acceptance to the paper.	3
"In general, I lean towards accepting the paper since it tackles an important but underexplored task with a reasonable design of the network. Also, the proposed method achieves better results than the previous works (not a surprise though) and the authors verify the design choices with the ablation studies.

However, since I do have some concerns about the paper, my final recommendation will be heavily dependent on the responses of the authors."	2
This paper investigate an important and interesting question for CL. The analysis performed is interesting but more justification may be needed for some propositions. However, the experimental validation is quite limited. 	2
See above.	3
"This paper empirically demonstrates and theoretically investigates an important problem of representation learning in general and contrastive methods in particular. The paper provides useful insights, related it to a current heuristic architectural design (projection layers) and proposes a novel training technique to get rid of that heuristic.
While the theoretical analysis is performed under simplified settings and its applicability to deep networks are only shown empirically, its limitation is inline with other theoretical works of the field.
For the reason above I believe this is a useful contribution to the field and hope to see this get accepted at ICLR.

##### References
[1] Chen, Ting, et al. ""A simple framework for contrastive learning of visual representations."" International conference on machine learning. PMLR, 2020.

[2] Chen, Ting, et al. ""Big self-supervised models are strong semi-supervised learners."" arXiv preprint arXiv:2006.10029 (2020).

[3] Le-Khac, Phuc H., Graham Healy, and Alan F. Smeaton. ""Contrastive representation learning: A framework and review."" IEEE Access (2020).
"	4
"The theoretical analysis of the embedding space are insightful, but it seems not well connected with the experimental results.
Also, the gap between the model in the proof process and the ones in the experiment part are quite large.
I think this paper need some additional work on the experimental part."	3
The analysis is insightful for some toy settings, but lacks some extensions to the real settings in Sec4. Also experiments are not adequent to validate the points.	3
This paper is overall structured clearly and well written. However, the novelty may be limited as it can be viewed as an application of the previous contrastive distillation method to the quantization task.	2
In summary, this is an interesting paper with clear motivation and novel techniques. I will raise my score if my concern about the training time is well addressed. 	3
Regarding the existing problems in the paper, I recommend rejection for now, but it may be adjusted according to the rebuttal.	2
The authors may clearly discuss the differences between CMIM and existing works. The current draft makes it hard to fully evaluate the contributions of this paper.	3
All in all, the paper needs a lot more thorough experiments to justify the usefulness of the proposed activation functions.	2
In short, there is very similar work in this area of research which was not mentioned by the authors. 	1
The paper is very poorly written and requires a significant improvements before it is ready for submission. As is, the paper is very far from begin ready even for a proper reviewer's evaluation.	1
Some parts of the paper are novel, but the current exposition (and experimental results) are too shallow to warrant publication, which is why I am suggesting a rejection of the paper.	2
I find that the paper is not really well-written as it lacks references and the main contribution is not introduced clearly. Moreover, I think that the contributions are not significant enough and not well supported by experiments for ICLR. Hence, I recommend rejection of the paper.	2
"Novelty
Clarity
Significance"	1
Segmentation loss functions are employed in different medical and real-world datasets, particularly the medical domain can present a wide variability in available datasets.  It might be interesting to discuss results on additional datasets, compared with the DSC (as the paper discussed, it is widely used in the medical domain). 	3
The decomposition of the losses into two terms is interesting and largely rigorously studied. However, the regularization schemes proposed are not well justified.	3
"The theoretical perspectives states the intuitively well-understood fact that Dice loss is better at capturing small objects while CE provides prediction distribution that matches training data. Thus, although it is an alternative perspective from label marginal bias, it did not lead to significant new knowledge about both losses. The proposed new loss, although achieving better accuracy on two datasets with two different networks, is not sufficiently validated in terms of how \lambda choice should be made regarding to different datasets and network structures. The comparison against ""CE + w*Dice"" is also strongly recommended, both theoretically and experimentally. The lack of Dice-related losses results on Cityscape dataset test set needs to be addressed. The poor quality of GT makes the retinal lesion dataset inappropriate to conduct segmentation experiments on.  "	2
Overall I think the paper introduces a valid research idea on improving the communication efficiency of federated learning. However, several concerns exist (please refer to “Cons” in “Main Review”). If my major concerns are addressed, I will consider increasing my overall evaluation score.	2
The paper is mostly sound, while the contributions are marginally significant.	2
The work proposes an effective low-rank parameterization that shows the superiority of communication efficiency in federated learning tasks. The idea is relatively interesting to the reviewer. The overall presentation of the idea and empirical results is clear.	2
I am not very familiar with reparametrization and its corresponding related works, and I am not able to comment on the significance. However, I think that the technique used in this paper is interesting and simple, which can be applied to real applications. The experiments shown in the paper are enough. Some questions and small suggestions are listed in the main review part.	3
The research topic is important and new. The theory and method developed looks sound to me, and the findings are interesting and could have potential use.  However, as mentioned above, the readability of the paper should be improved, and more related work or details should be provided.	3
In my opinion, the contribution is mainly in the definition of structural causal interpretations and how they relate to existing representations. For the moment, I think the contribution should be presented much more clearly in a major revision. I believe that only after this the significance of the results can be assessed much clearly. As a result, I do not recommend acceptance at this point.	2
"While I liked the overall idea of interpretation (which I still think about explanation) of a certain variable having a certain value through causal mechanisms, the paper seems over-promising what interpretation is about with a restricted, simplified causal model.
===after discussion
I liked the overall idea of interpretation (which I still think about explanation) of a certain variable having a certain value through causal mechanisms. Some of the assumptions in the paper seemed too restrict at first, but I later found reasonable for the sake of generating interpretation.
"	3
The idea of large patches to capture high-level shape, as well as concentrate on detailed texture and parts information using smaller patches is very good. However, the justification could have been improved. The paper is well-written and the experimental evaluation is comprehensive. However, the improvement in accuracy is not significant in comparison to the way the novelty of R2L attention is described. There is an advancement to the transformer model however the impact is limited. The related work is very nicely done.	3
The idea sounds direct and reasonable, although introducing local inductive bias into ViT is not and there have been lots of paper to achieve it. The paper is well-written and shows efficient experiments to evaluate the proposed RegionViT.	2
Overall the paper proposes a fairly interesting pyramidal modification to ViT. It seems like a somewhat incremental approach though. While the performance is empirically shown, there is little that can really be learned about the underlying approach from the experiments. Nothing is really wrong with the paper, I do think it could be significantly better though. As such I recommend the paper to be accepted, but I do think it has a lot more potential that is not uncovered at the moment and I would actually be very happy to read such an improved version of the paper.	3
This work aims to solve the interaction between global and local information in a ViT architecture and to save the computation at the same time, the performance is good. However, I am concerned with the novelty of this work and the contribution may be limited.	3
"In summary, I think the proposed quadtree attention is a reasonable efficient design of attention mechanism. It builds coarse and fine tokens, which is able to model both long-range and local interactions. Empirical results also demonstrate that the proposed approach achieves good performance with less computation on many vision tasks. However, I still hold some concerns in technical contribution, unclear design of the pyramid tokens, and performance on classification task. Thus, I would like to rate this paper as ""marginally above the acceptance threshold""."	3
The paper proposed a clear method to handle long-distance and short-distance attention. However, important related work is missing in the discussion. As mentioned above, I think more discussions and experiments should be added to support the claim. I am leaning to reject the paper if the authors can't address my concerns. 	2
The paper studied a challenging and important problem in efficient attention mechanisms. I think the overall method is novel and the experimental results are promising, which leads me to a positive rating. I hope the authors can further address my concerns in the rebuttal.	3
The paper proposes a novel efficient vision transformer based on quadtrees, which is interesting and technical sound. It achieves competitive performance in various vision tasks.	3
The paper contains an important contribution as discussed above. The fact that predicting the locations of invisible points improves camera pose estimation provides further support that the method is useful in downstream tasks. 	4
"I tend to reject the paper because of the lack of contribution in technique and insufficient experiments. I would like to see 
1. the technique differences, compared with [Germain 2021] .
2. improve the baselines, and compare with the state-of-the-art camera pose estimation methods."	2
In summary, based on the strength and weakness I mentioned, I like this paper's idea for hallucinating correspondence so that we can obtain better accuracy for camera pose estimation, although it would be better to have more analysis and ablation study for the impact of correspondence hallucination on camera pose estimation accuracy.	3
"The paper considers a challenging problem of Byzantine-tolerant learning in the decentralized setup (without a parameter server). However, the paper lacks in many details and does not quantify costs and gains of the proposed algorithm.
"	2
Overall solid piece of work. It is worth considering the paper modulo addressing the comments above.	3
Though the paper addresses a very important and timely problem, I believe it cannot be accepted as-is as it requires to be revised in a few aspects. In summary, (1) the authors make several assumptions that seem unrealistic and unjustified. (2) The most important parts of the paper are deferred to the appendices; this makes it hard to understand the algorithm and to see its benefits. (3) As the main aim of the paper is to provide a computationally efficient algorithm, the paper should give clearly the computation complexity of all parts of the algorithm. 	2
"- unclear description
- limited novelty"	2
Although the paper has some significant weaknesses, I tend to recommend an acceptance given the technical depth and significance of the problem addressed.	3
Our field is producing papers at a fast rate. Reviewers can provide scientific checks and balances only up to a certain level. This type of rigorous effort can offer valuable information about highly influential papers to the scientific community. 	3
Overall, I think this type of works is important and can lead to important insight. However, I think experiments with more problem types and more solutions are needed to draw general and interesting conclusions. Also, the paper needs to discuss some relevant works that are currently not mentioned.	2
Overall, I like this paper and think it makes a solid contribution to the intersection between deep learning and combinatorial optimization. However, I think a paper that “debunks” a highly-cited work should also establish convincing avenues for future research which are currently beyond the scope of (ML or non-ML) existing methods; I argue that this is missing at this stage. I would like to see the authors’ responses to my questions and concerns before making a final decision, but am generally positive about this submission.	1
"Despite the paper being purely experimental, its main point can have significant net-positive impact within the wider research area. Not only by casting doubt on the entire ""DL based tree-search"" but also by serving as a long-term warning -- claims about ""outperforming SOTA of classical combinatorial optimization"" can fall apart under proper experimental methodology.

 For this reason, I am comfortable disregarding the usual demands for technical novelty, beating benchmarks, or providing theory.

However, in its current form, I do not believe the paper would realize its potential impact due to the issues listed above. I believe they require significant changes in the paper structure so I cannot, at this point, recommend acceptance."	1
Overall, this is a pretty solid paper. Please refer to the previous section for detailed discussion.	2
"I’m not deeply familiar with the recent literature on (more sophisticated) VAE architectures, but the method, while a combination of existing techniques and ideas, appears to be novel and I would expect the empirical results to be of interest to the community. Therefore I am **leaning towards accept**.

**Post-rebuttal note**: Increased score to accept."	3
"I have enjoyed reading your paper, nice work! The introduction of the layer-wise attention mechanism is a great contribution. Using non-local blocks within the model is less novel, but this is still new for such a model. 

The results on MNIST, Omniglot and CIFAR-10 demonstrate the effectiveness of the method, and the ablation study clearly shows the positive effect of the two attention modules (layer-wise and spatial). 

Experiments remain however limited. Large-scale experiments (ImageNet) and qualitative studies (inspective learned attention) would be a great addition. They would give the reader a better understanding of the impact of the attention modules. The paper overall writing quality and structure need to be improved.

---- edit 26/11
Raised my score to 8."	3
An interesting work was proposed with experimental justification. The experiments can be furthered strengthened.	3
"The paper presents a simple method for important problem that outperforms recently proposed methods on three challenging benchmarks. The paper is clearly written, backed with experimental evaluation and ablations, where it shows good performance.
"	2
Based on all the strengths (extremely well written and well motivated; clear; a simple but novel architecture; comprehensive experiments and good ablations; great results), I'd recommend this paper be accepted.	3
Overall, this paper proposes the ARTEMIS for image-search with free-form text modifiers. Although simple and effective, the methodology may not be novel enough: it is more like a simple combination of matching scores and does not provide many insights.	2
"At least one highly related work is missed and the statements of the theorem require small $\epsilon$.

However, I think the proposed algorithm based on MOM and theoretical analysis are interesting."	3
"In general, the authors propose a novel and interesting MOM objective for robust imitation learning from corrupted data. However, there are some weaknesses (listed above), including no sufficient experiment results supporting the effectiveness of the proposed solution, preventing me from giving an acceptance.

"	3
This paper presents a novel and in my perspective interesting approach to tackling IL in the corrupted data setting.  However, I believe it would benefit from a refinement of the theoretical results to highlight the role that the fraction $\varepsilon$ of corrputed data plays, as well as from more extensive empirical evaluation, as described above.	2
"The paper itself is not especially well written, having numerous typos and grammatical mistakes, and is evaluated on the barest minimum of tasks (two relatively simple gym environments), with no convincing baselines or alternative methods compared to.  I am also currently unconvinced by the core objective (eq. (3)), and the relevance of the math presented, although I invite the authors to justify these both to me.  Finally, I think the setup itself is both too restrictive to have significant practical impact, and is actually in conflict with the original motivation for the work.  

Overall, I do not think this paper is of the requisite quality for acceptance at this point in time.  I encourage the authors to explore removing some of the inherent restrictions on the method, validating the method on a wider range of environments, seek out alternative methods to compare to, and to submit this work to a workshop to further develop the work and obtain feedback.  
"	2
 I don't think this paper is ready to be read. I think there are many places that can be improved.	2
I think this paper's contributions are not well justified. Given its current status, I cannot vote for its acceptance. 	1
"I would be more confident about my review of the work, if the authors can include some set of experiments in the paper. It will be very helpful if the authors can point out the benefits over the existing SOTA techniques on why to use their proposed causal representation. Some comments on the scalability of their approach will also be appreciated. 

I am willing to reconsider my ratings if the authors address some of my concerns."	3
In my opinion, this paper is not clearly written, and does not contain compelling and convincing evidence that the proposed method adresses a gap in knowledge or improves on current method. It suffers from major structural problems as outlined in detail above. It is quite possible that the contribution is, in fact, significant but I could not evaluate it because of these issues. 	2
"This paper is missing details on neural architecture, configuration parameters and experiment details. I am not sure ""what"" the novelty of the paper is. It is most probably due to the fact that the paper significantly lacks in details. "	3
The paper gives good insights into combining machine learning techniques and traffic engineering. However, it lacks solid evaluation to demonstrate the overhead of this approach and lacks analysis of problem handling. Moreover, the representation of background and some of the technical details does not help readers to understand. Therefore, the paper should 1) add evaluations demonstrating the overhead of ML techniques, analyze the possible problems that may arise from prediction errors; 2) rewrite ambiguous sentences to provide more clear technical details; 3) show more concrete difficulties in translating traffic engineering policy into protocols.	3
This paper applies existing machine learning techniques to solve the traffic engineering problem in computer networks. The contribution toward the machine learning methodologies seems to be limited in this work, and the evaluation needs to be conducted in a more realistic environment to demonstrate the feasibility of this proposed method for the targeted application.	2
"The paper studies the use of ML to design protocols for a data-center application; it adds to the sparse set of papers on the application of  ML to problems in communication networks, and is a welcome change from papers dealing yet again with MNIST. 

My main concern relates to generalizability: Can the framework be extended to non-tree topologies (in particular, where links may appear as well as disappear, and where multiple routes exist), to protocols that must support multiple policies, and to incorporation of contextual information associated with nodes (perhaps also links). It is not clear what the model has learned, and some additional details about the test setup are needed.  "	3
The submission fails to cite a previous paper which proved their main theoretical result, and the experiments leave room for improvement.	2
See above.	2
Overall I really like the main idea of this paper. But there are just so many things, including those that are key to their main results, that confuse me. I believe that this paper would become an important work to the field once it is clear.	3
Overall, there is no theoretical justification of why the proposed method is better than vanilla TRPO/PPO, and the empirical advantage is also limited. The so-called \textit{theory of relativity} for RL is nothing but basic regret decompositions well-known in the RL community.	1
Overall, I think the weaknesses of this paper outweigh its strengths. While the experiments are thorough and the human study results are quite interesting, I am primarily concerned about the novelty of the empirical findings (+ missing related work) and the limited scope/breadth of experiments (e.e. analysis of easy and hard examples). 	3
"Unless I have not missed anything very obvious, this submission seems very
polished, significant and novel.
Therefore, I recommend 8: accept, good paper.
"	4
The authors show the high consistency between the decisions of CNNs trained on the same dataset. It is also shown that the validation images can be split into three parts: the trivial part, the impossible part, and the part that’s between them. However, the current work lacks the test of a naïve hypothesis about these two parts: the trivial part is the easy images, and the impossible part is the difficult images. Some results are also not described clearly. I can only recommend for acceptance with confidence marginally above the threshold, but fixing these points would increase my confidence.	4
The authors tackle an important topic and perform fascinating experiments, but there is insufficient evidence for the claims they make and, in my view, the claims they could better make were not deeply explored.	3
"In general, the paper presents a dataset, which is a positive step towards concept learning tasks and will hopefully encourage future works to move beyond object categorization based only on appearance. The paper presentation seems too dense, and it’d be good to see some concrete directions of presenting the paper better to make it more accepted by the community.

I will wait to hear back from the authors and other reviewers, and accordingly adjust my score."	3
In summary, I think the paper brings little technical innovation, and as such it should not be accepted.	2
The issues mentioned in the weaknesses part above are minor. They do not affect the value of the proposed benchmark dataset and baseline algorithm. But it would be great if they could be addressed.	4
I think the dataset part is a solid contribution of the paper. However, for the proposed model, I think the authors are missing important baselines and discussions with related work.	2
Overall, I am positive about this paper. The method is well motivated and justified. I believe the experiments could be much stronger, especially since the experiments with real datasets would still be considered toy experiments due to the choice of pre-training data and % of real data used. It is hard to say whether these results would generalize, but I believe they warrant a discussion by the community.	3
The paper tries to answer an interesting question (though practical interest might be limited) in estimating the number of samples needed for pre-training, however there are many simplifications (as also stated by the authors) and the empirical evaluations, on which the claim is based, lack measurable results.	2
"Overall, I consider the paper contains some nice ideas and the experimental analysis to be interesting although  I  feel what we can get out of this paper may be a bit limited. I also believe that presentation of the work should be improved and the work would benefit from a major revision. I also recomend the authors to focus on the more general transfer learning scenario since the narrowing of the scope to a ""sim-to-real scaling law"" may seem a bit artificial."	2
"It is an interesting paper, and possibly make important contributions. For now, I give the score ""marginally below the acceptance threshold"" due to the poor writing. But if the authors can well explain all the related conclusions and definitions, I can consider to increase the score.

After reading the rebuttal and explanations from authors, I think the revised manuscript is much more understandable, and I change the score to ""marginally above the acceptance threshold""."	3
I am awaiting author responses at this point and will update my review based on their explanations.  	3
The paper is well-written and presents an interesting method for a methodologically interesting problem with a nice application.  Some of the key aspects of the paper (e.g., what does an approximate equivariance look like in practice? why does the method outperform chained NPE? how should one set the kernel width?) are a bit unclear, but overall the paper is a good contribution.	3
Overall the paper is well written and the experiments seem to demonstrate the power of GNPE. There are a few technical questions that the authors could do well to answer which may change my score and opinion of the paper.	3
"In summary, I believe that the proposed method can be useful for a variety of deep learning tasks, although clarification of scalability of the proposed method on large matrices is expected.
"	2
This paper proposes some strategies to reduce number of matrix multiplications both forward and backward propagations, aiming to improve the efficiency of computation of matrix square root. Particularly, the idea of approximate Lyapunov equation based on matrix sign function and Newton-Schulz iteration seems interesting. The more experimental comparisons and discussions could be strengthened. Overall, I like this work.	3
"The paper has two separate contributions, one in forward pass and one in backward pass in dealing with (positive-semidefinite) matrix square root in deep learning.

In the forward pass, two approximation approaches MTP and MPA both seem to trade stability for speed. MTP is the fastest but also the least accurate. MPA is slightly slower but on average is more accurate. However, the current paper is not complete in the sense that it lacks treatment of ""defects"" (regions where the numerator and the denominator approach zero) in Pade approximant approaches. Therefore, the stability and convergence of MPA is somewhat concerning.

In the backward pass, viewing the computation of the gradient of the matrix square root as solving a Sylvester equation is interesting. However, experiments in the generic settings lack accuracy tests to reveal how close these Newton-Schulz-based approaches are to the real gradient.

There is a bit of concern when in section 4.2 the proposed approaches are not compared against the closest possible candidate (Huang et al (2019)) for the ZCA whitening problem."	3
I think the manuscript is technically sound with a solid understanding of the PTQ problem. The experimental results are quite good. Since there are still clarifications to be made, and the fact that similar deductions and flipping methods exist in previous works, I recommend a weak acceptance.	3
This paper is well written, and the proposed method is novel.	4
This paper introduces a few reasonable assumptions to alleviate the efforts to estimate the input data distribution. Successful approximations lead to improved model accuracy even with a very small number of bits to represent weights.	3
The motivation to decompose the Hessian matrix into diagonal, block-wise diagonal and low-rank components is the crucial contribution of this paper. Experiment results show that the multi-scale optimization objective lead to good performance after quantization. I don't follow some steps in the derivations of the optimization objective. This paper will be better if the authors can illustrate these steps well.	3
an interesting idea, but there is room to further strengthen the paper, particularly for the experiments.	3
I recommend acceptance of the presented paper. A novel idea, combined with a good practical justification, and strong results make this paper worth for publication.	3
I recommend this paper to be accepted, as its novelty on the newly developed variational prior differs from existing works. This is based on the hypothesis that the conditional posterior model to approximate the optimal prior shall depend on the whole data set. This claim is supported by the assumption that the variational model conditioned on the aggregated set with larger context shall be closer to the optimal task-specific prior, than only conditioned on a subset, which is a specific task. In the experiments, the variational prior shows a robust regularization effect.	3
This paper proposed a new augmentation method for low-shot classification in histology images. It is well-motivated and reasonable. The experiment results also confirmed its effectiveness. There are also some issues regarding the proposed method, i.e., the influence of K-means clustering randomness, and the explanation of CLP's superiority over FSP. Overall, this paper's quality is good.	3
The paper has slight novelty and interesting findings but neither novelty nor finding is significant enough. It may be better to be submit it to medical image journals.	3
The paper shows very promising results for few-shot learning in histology images and introduces a simple and effective method for doing data augmentation. The paper would benefit from some additional experiments with competing augmentation methods and a few straightforward ablations. Overall, the paper is very well written, feels complete, and has an extensive discussion.	3
Overall, I vote for acceptance. While the novelty might be relatively limited, the comparison between fully-supervised and contrastive pre-training in an FSL setting (with and without latent augmentations) for histopathological images is interesting. The experimental design and ablations provide a valuable overview of the inner workings of these methods, which could be valuable to a large portion of ICLR’s audience (potentially beyond applications to histopathology).	2
"The paper addresses few-shot learning for the classification of histological images. This topic is of significant interest to the biomedical community, as annotations are very expensive to obtain. Exploring the generalizability of different pretraining methods and augmentations to exploit the available ground truth to its fullest extent is of great importance. However, I found it very hard to understand the text and followingly some of the experimental setup. On multiple occasions, the phrasing was so ambiguous, that it became unclear to me what the authors tried to convey. The text needs revisions to correct for semantic and grammatical errors. Due to this unclarity, I suggest to reject this submission, but encourage the authors to revise the text and submit to another venue.



------------- After author's response and revision of the submission ------------------
The authors addressed my concerns and questions in detail and the revised submission has improved in clarity. I believe the topic is of significant interest to the biomedical imaging community, which is why I am suggesting to accept the revised submission."	3
This work introduces a nice idea that improves image translation methods by distilling the knowledge of unconditional GANs. The setup is more flexible compared to prior work, where different architectures for image translation can be used. While the clarity of the writing can be improved, I think it is good to accept this paper to the conference.	3
I think the paper could be strengthened by further analysis, experiments, and presentations fixing. However, I like the general idea and I am favorable to accept it considering a large number of work they have done and signifance of this problem.	2
"The paper would benefit from the following improvements:
- Clear definition of main technical contribution in the Introduction
- Evaluation of the semantic-to-image task in terms of mIoU (i.e. comparison of the input segmentation with the results of a segmentation model on a synthesised image)
- Extended explanation of the semantic diversity loss
- Extended evaluation of the semantic-to-image task with modern models such as [a,b]"	2
The paper considers an interesting problem and presents interesting results that encourage trying KNN. However, due to the concerns mentioned above regarding the correctness and representativeness of the results, and since it does not have a significant technical contribution other than comparing existing methods, it is below the bar for ICLR.	2
The experimental setting has a significant limitation, the hyper parameter optimization is not conducted on the right data set, which can explain the superior performance of KNN. The authors claim that they are the first to report these results, which is not exactly true. 	2
This is a nice study with important messages for the community. The experimental section can be enriched with a more in-depth evaluation of the effect of hyperparameters on the performance of data imputation approaches.	2
After reading this paper several times, I start to understand the underlying connection between OOD and invariant property. I think the readability of the paper can be greatly improved by introducing the connection first then talk about their implementation. However, the creation of multiple environment while keeping the invariant property is unclear to me. I might misunderstand this part. Currently, I think it is marginally below the acceptance.	2
The paper is well written, polished, and easy to follow. The problem that is studied is important and relevant for the community. The proposed approach is well motivated. The theory is sound, however, the technical novelty is limited. The experiments are well executed and mostly convincing, however, an ablation/sensitivity study is missing.  I am willing to increase my score if the authors address the issues with the experimental evaluation that I raised in the main review. 	3
Please see the weakness and questions section.	3
I recommend a reject. This paper developed a new approach to tackle a problem that has already been discussed. Although the presentation is well organized and easy to read, the limited novelty and the lack of excitement bring me to this recommendation.	2
Overall, this is an interesting paper. It may help to understand the paper if adding one or two examples other than ranking (constrained optimization problems such as shortest path/knapsack in the modeling section), to illustrate how the new notion of total group preorder and algorithm can be applied. The proofs need to be written in a clearer way.	3
"The paper deals with an interesting problem and the proposed method is well-motivated and presented in a logical manner.
The experimental results based on three different datasets show that the proposed method may have potential advantages over other existing schemes.
However, despite the conceptual similarity between SPO loss (which the TGP/ATGP tried to improve) and regret/MOCU in decision theory, the authors do not discuss the relation of their work to the rich existing literature on regret and MOCU.
Furthermore, additional performance evaluation results are needed to clearly demonstrate the robustness and benefits of the proposed scheme.
Including discussions on potential limitations of the current method (if any) would strengthen the paper.

---

The evaluation has been updated after reviewing the authors' response."	3
Overall, my recommendation is “weak accept” because I think the motivation is compelling and the “strong ranking property” seems to open up this predict-then-optimize framework to important combinatorial optimization problems (though I’m not very familiar with prior research on this specific topic). As I mentioned in the main review, there are a lot of ways the paper could be fleshed out to increase it’s potential impact.	3
Unfortunately, I am not knowledgeable on recent theoretical work for meta-learning to provide a very educated critique of this paper. The proposed $\Pi$-relatedness notion seems like a novel contribution and the PAC generalization bound involving mn samples seems stronger than existing work, so for that reason I'd lean to accept.	3
This is a well-written paper with vast implications in the meta-learning society. The almost Pi related concepts are useful and easy to understand.	4
"The paper presents novel and interesting generalization bounds for meta-learning, they offer new insightful results, in particular with spectrally-normalized bounds that are adapted to learn with neural networks in different settings (binary, multi-class, regression).
The practical implication of these results is not really discussed and the perspectives offered by these results for designing new algorithms are not sufficiently developed."	3
The paper makes an important and meaningful contribution to the theory of meta-learning. The paper is well-written, and the ideas presented are sufficiently novel. I think the paper would make for a good submission in ICLR 2022.	4
The paper proposed a solution to an interesting problem but as mentioned in the weakness section above I could not comprehend clearly the use-case of the multilingual decoder. But I would be highly interested in discussing with the authors and other reviewers during the rebuttal period if I might be missing the aspect of using multilingual data for the framework. 	2
Overall, the combined unclear clinical utility, lack of support for a major claim in the paper (that RTLP generates more clinically accurate reports), and concerns about the evaluation strategies lead me to recommend this paper for rejection.	3
The paper is well-written and the motivation sounds reasonable. The targeted problems, i.e., cardiac signal captioning and multilingual captioning, are novel and important. The presented approach is novel and interesting. Thus, I tend to accept this paper.	4
The proposed model is novel and interesting. While the evaluation metrics in clinical text generation are not persuasive. And some of the important details were missing in the current draft. 	3
"This paper provides a generic fairness metric for both continuous and discrete protected attributes in various tasks. The kernel estimation has good error and complexity properties. The experimental comparison is comprehensive and well designed. 

The connection between GDP and binary demographic parity is built, but I wonder about the relationship between GDP and discrete protected attributes with multiple values, e.g., intersectional fairness. The extension from binary to multiple to continuous would be more smooth."	3
Overall, this paper is a bit unclear in some parts and lacks a thorough literature review. The experiments are not convincing enough as well. 	3
"
In my opinion, the paper touches upon a very important and practical topic in fair ML. Not all attributes are categorical and privacy concerns might require fairness in a neighborhood or region of attributes. Thus, accounting for continuous variables in fairness definitions is crucial in many practical contexts.

The paper is methodologically sound. It provides a quite extensive theoretical justification and guarantees as well as a complete and convincing empirical evaluation with several datasets. It is well written and has the potential to lead to follow up work (both in terms of other definitions and also inspiring research on continuous attributes in fair ML). 
"	4
The paper is interesting but the technical novelty may not be sufficient	2
"
Due to lack of novelty and the performance of the proposed model is marginally above the existing approaches."	2
"The paper introduces sample probing to train a generative model that synthesizes data in feature space for generalized zero-shot learning. The proposed approach is restrictive to certain zero-shot learning models. Some ablation studies and analysis are missing.

=================

post rebuttal:

I thank the authors for the extensive experiments and clarification made in the rebuttal. The rebuttal has addressed most of my concerns, especially showing the generalizability of the proposed approach (different types of generative models and different types of closed-form ZSL models), so I raised my score. However, I am still a little concerned about the novelty of the proposed approach and its marginal improvements."	2
Considering the weaknesses of the paper, I would give this paper marginally below the acceptance threshold.	2
"Overall, I enjoyed the paper and would have liked to recommend acceptance. The approach makes intuitive sense, and can no doubt be extended with multiple future works, offering the community a parallel line of investigation into improving GZSL results.

However, I think the flaws outlined in point (1) of the weakness section are fundamental enough that I am worried that any future works which try to build on this one might simply be wasting their time. I do not believe this to be the case, but I would like to see additional experimental results that would convince me otherwise.


*I have marked my confidence as 3 due to limited familiarity with related work. As an extension to that, I may have missed prior art which already suggested similar ideas.

******************************
Post rebuttal update:

The authors conducted an extensive set of experiments and addressed my primary concerns (the method's generalizability to additional baselines). I am still concerned about some aspects of the evaluation (considerably worse results for all baselines compared to their originally reported values). However, since the paper suggests a method for improving other models, the relative improvements are what matters most. As such, I am willing to accept the current demonstration of a (fairly) consistent improvement when using the same hyper-parameter selection approach across the board. 

Even if accepted, I urge the authors to better highlight and explain the difference between their experimental results and the original baseline values."	3
This paper addresses an important issue existing in current generative ZSL models. The paper is well-motivated and the solution seems to be effective. The major concern lies in the performance gain and some details regarding the method need further clarifications.	3
The authors propose a simple but novel clustering method for DNN compression. Overall, I believe it has useful findings but also has some flaws that I mentioned in the previous section. Hence, my score is 6: marginally above the acceptance threshold.	2
In sum, a very simple idea is proposed. I think simple is good, but empirical results don't quite convince me. I am seeking more ways of validating this idea in order to recommend it.	2
Mostly well-written paper with good experimental results on benchmark data, but hardly any insights into the method itself.	2
"This paper proposes a novel differentiable k-means clustering layer (DKM) for deep neural network model compression. The DKM utilizes attention mechanism to align the weight-to-cluster assignment with the training loss function. Overall, the idea is novel but the paper is not prepared enough. 
The comments are listed as following: 

1.	In Figure 2, there are some confusions in logic. To my understanding, the criterion of convergence should be put after the calculation of Attention Matrix   to decide whether to continue update the Centroid Matrix   or not. However, it was put after updating the Centroid Matrix and weight approximation. 
2.	The description of multi-dimensional weight-clustering is not so clear. It is better to help to understand by adding a figure with more details. 
3.	In Section 3.2, the procedure of DKM is elaborated. However, theoretical interpretation for each step of DKM is also required. 
4.	Some details should be noticed, please check and correct them: 
a)	The format of reference should be unified; 
b)	Some format errors and spelling mistakes exist in the paper; 
c)	The Figure 1 and Figure 2 are not clear, I recommend to use vector graphics; 
5.	In Section 4, more related clustering-based quantization methods should be added for further improving the reliability of experiments. 
"	2
"Due to the conceptual novelty of incorporating rational inattention into multi-agent RL, I recommend weak acceptance. 

---
Update after author response: Thanks to the authors for their detailed responses, and for clarifying the results in Section 5.1. I would encourage the authors to provide a more thorough discussion of these comparisons in a future version of the paper. Moreover, although it would be useful to consider multiple agents facing inattention (e.g. in Section 5.2), I found the existing results to be sufficiently interesting for acceptance to ICLR. Thus, I keep my assessment the same. "	3
"Overall, I think this paper needs more convincing results that tell us something general either about PA+RI problems or just RI itself.
"	2
The paper can be a nice methodology paper but lacks empirical results for an ML venue. 	3
The paper presents a new model that accounts for human rational inattention and evaluates the model in a specific problem, however, the technical contributions seem marginal and the literature review is insufficient. 	2
"The general idea of the paper is interesting, but it seems to be an early stage of this work. Investigating the work with more complex datasets and tasks and more baseline comparisons would make the paper more convincing. There are a lot baseline of LDP or task-aware work to compare with the work, for example
[1] Wang, N., Xiao, X., Yang, Y., Zhao, J., Hui, S. C., Shin, H., ... & Yu, G. (2019, April). Collecting and analyzing multidimensional data with local differential privacy. In 2019 IEEE 35th International Conference on Data Engineering (ICDE) (pp. 638-649). IEEE.
[2] Liu, R., Cao, Y., Yoshikawa, M., & Chen, H. (2020, September). Fedsel: Federated sgd under local differential privacy with top-k dimension selection. In International Conference on Database Systems for Advanced Applications (pp. 485-501). Springer, Cham.
"	2
"
The main reasons for my recommendation are:

1. similarity to prior work, the novelty of this work and it's benefits are not well identified

2. How useful the extracted dimensions would be in high dimensions and for models that might use different set of features is not clear.

"	2
This paper proposed a good analysis for task-aware local DP.  However, my main concern is the setting of local DP.	2
This paper considers and important problem and brings an efficient solution. It is well-written. The experiments shows the performance improvement of the proposed method. However, the experiements could be extended on larger datasets and they can be compared with SOTA.	3
The theoretical parts on generalization contradict the literature. The experimental results also mismatch literature, and I got a far worse result on GAN training using the authors' code (could be the wrong version, I can re-run if the authors can re-upload code into a single file). I tend to reject for now and would increase the rating if the authors can resolve my comments above.	2
"Overall, this manuscript presents an interesting modification of Adam to improve empirical performance. However, the technical novelty is very limited and there is no new theoretical insight. The generalization analysis using SDE comes from [Zhou et al. NeurIPS 2020], and the optimization analysis is based on a problematic assumption pointed out by [Reddi et al. ICLR 2018]. There are also some unrealistic assumptions (e.g., Assumption 5) which needs to be further justified. 

For its current version, it clearly does not reach the bar of top venues such as ICLR.
"	2
This paper proposed a adaptive type gradient methods named ADAMOMENTUM  by the intuition that gradient with momentum contains more accurate directional information and therefore its second moment estimation is a better choice for scaling than that of the raw gradient.  The intuition, theory, and experiments seems OK. Therefore, I tend to accept this paper at this time. I am not an expert at this area correct me if I am wrong, and I am willing to change my score to align with other expert's score if needed and reasonable.	3
"The paper is in general well-organized.  However, as pointed out by other reviewers, the empirical evaluations are not fair and the technical statement could be incorrect.
"	3
This might be a game-changing direction for the future however the paper as it is now is lacking solid ground and mature enough progress in this direction. 	3
The neural network-to-human path heavily overlaps with (Ge et. al 201). I think the novelty of the work is mostly in the human-to-NN path. 	3
The paper tackles a relevant problem for the conference. While the empirical evaluation is rather insightful for image processing, the paper is motivated for any data modality, but the claims are not backed up by evidence. The paper could also improve the readability / clarity of presentation.	2
"
Overall, the reviewer think that this is a ok, but not great, paper.  With a more realistic experiment (ie, with a naturally occuring dataset and interventions from users based on the explanations), the paper would be much stronger.  "	3
"The core idea presented in the paper is simple and quite general, going beyond image steganography. The authors heavily outperform current steganography algorithms with their method and the community would benefit from its publication.

However, the article has some weaknesses that if the authors can address I will be happy to raise my score.
"	2
Overall I enjoyed reading this work. See main review for details. 	3
"pros : 
- it is clever to rely on adversarial examples and iterating to obtain a stego image

cons : 
- the evaluation in terms of undetectability (which is what matters most in steganography) is insufficient: one should provide the error rate of a few SOTA opponents. The method must also be compared to gold standards steganographers. 
- the zero error of message retrieval is only empirically checked and one cannot hope for perfect generalization performances
"	3
"In my opinion, FNNS can provide a contribution to the steganography community. The proposed algorithm is novel and includes sufficient results and the evaluation and analysis appear valid. The authors also show an interesting use case of FNNS. However, the paper still has some weak points the authors should address to increase the quality and completeness of the paper. 

====== Edit November 23 ======  
Raised my score to Accept after reading the author's response. "	3
In general, I find the paper interesting. But the readability can still be improved.	3
The application of multi-agent reinforcement learning for economic problems seems very promising, particularly given the high complexity of these problems. The main contribution is algorithmic, but without a precise description of the algorithm, I find it hard to really understand the method and to assess the impact this work could have.	3
The paper studied a fundamental problem and tested an efficient algorithm for solving it. However, I do not think the paper can be accepted in the present form because (1) the success of the algorithm highly relies on a trick proposed in another paper; (2) besides running the algorithm empirically, no theoretical analysis of the proposed scheme is provided in the paper. 	2
I recommend rejecting this paper in a large part because (a) the main contributions (RBC model/reward shaping) have unclear/unspecified novelty, (b) weaknesses in the analysis described in the main review, and (c) need for contextualizing this work in both the MARL and economics literature (help the MARL community understand that this work is important!). 	2
"The paper theoretically develops the steerable PDO which enlarges the family of equivariant operators. However, the proposed method is quite limited in terms of performance. Also, the theoretical findings are not very surprising to me given the previous literature. Thus, I doubt the significance of this work to our community. But in case of any misunderstanding, I would like to hear more from the authors during rebuttal. 

> Post-discussion: the authors addressed my concerns. So I'm very happy to improve my rating. "	3
The paper studies equivariant Partial Differential Operators towards building equivariant networks. The work is both novel, interesting, and brings new perspectives to the equivariant networks literature while maintaining an exceptional level of polish in both writing and execution.	4
The theoretical contribution of the paper is significant. Even though experimental results are not as satisfactory to demonstrate the utility of equivariant PDOs, I am leaning towards accepting the paper.	3
The proposed algorithm represents an improvement in theory over the nearest competitor (the Heuristic IntSGD algorithm), requires only all-reduce style communication, and does not require error feedback.  Though the performance improvements in experiments were not enormous, there certainly were improvements.  I'd like to see the paper in ICLR.	3
This paper proposes a new compression algorithm for distributed deep learning where the compression results are addable). There are existing work on this topic, and the main contributions of this paper are providing theoretical analysis and giving better scaling factor choice.	2
The scaling factor proposed by the authors helps them show convergence of their quantized SGD. Empirically, the authors were able to show that their method beats competitors only one task out of the two considered. 	3
In summary, this paper is novel, but the experiment should be strengthened as detailed in the main review.	2
An excellent paper with exciting ideas, clear presentation, and technical depth. 	4
"To my knowledge this paper demonstrates significant technical and empirical novelty.
I believe the main weaknesses can be addressed prior to publication.
Therefore I recommend acceptance.
However, I am not an expert on this topic, so my confidence is only a 2."	4
A very interesting paper, dealing with an interesting and timely topic. Unfortunately, the paper is not perfectly clear throughout all sections.	4
I suggest NOT accepting this paper at the current version, given that a few issues mentioned above are critical.	2
"This is an empirical paper. The key contribution is an investigative, ablation studies that build up intuitions and practical guidelines to fine-tune the generic federated SSL framework. The fine-tuned version is shown to have significant performance improvement. But on another note, this success might be dependent on a correct choice of lambda which is only possible if one knows the data heterogeneity of the clients. Perhaps this is a key restriction of the method. It would be great if the authors can expand on this during the rebuttal.
"	2
"This paper provides an interesting insight into self-supervised training in FL. It motivates its proposed method by empirical insights on cifar10(0). My major issue with the paper is that its empirical scale is quite small in terms of what makes the FL scenario challenging. On the other hand, they have very broad empirical analyses. 
If the authors extend their experimental evaluation along these scale-related dimensions and clarify their positioning within the FL literature, I will consider raise my score.

=== Post-Rebuttal ===
The authors have addressed my concerns sufficiently and I have updated my score accordingly."	3
Overall an interesting idea. However, I thought the novelty proposed is a bit incremental and should have been more thoroughly investigated. For example more experiments could have been run and it would be good to theoretically show why this works. Although from Section 3.5, I can understand the intuition behind why this works. Overall, I felt that the contribution that this paper proposes does not meet the requirement to be published at ICLR.	2
"The originality of the paper content needs further justification. The method introduced were well-established in photonic computing and the machine learning community.
"	1
While the efficient parameterization guaranteeing orthogonality is not entirely novel in classical ML, showing a single architecture that works on GPUs and on NISQ devices can gather significant interest from ML and QML communities.	3
The overall idea of pyramidal circuits as neural network layers may have some novel significance. My overall understanding of the evidence supporting the claims in this paper is weak. It is missing any type of demonstration/concrete-proof that network weight matrices remain orthogonal during training, in addition to missing any demonstration of the claimed convergence rate proof. Despite my greater confidence in understanding implementations, I believe its reproducibility is questionable, and the experimental descriptions are lacking. The experimental results appear to be weak, such as achieving an 85% test accuracy on MNIST. Overall it was a difficult paper to access.	2
This paper proposed a training method for orthogonal neural networks that run in quadratic time, which is a significant improvement from previous methods based on Singular Value Decomposition. However, the experiments are not very convincing since they lack the comparison of running times.	3
An interesting work that tried to improve adversarial robustness through test-time fine tuning using self-supervision techniques. Answering/commenting on the points, that I posted under the main review, can significantly improve the quality of this work.	3
"The contributions are summarized as (1) introducing the framework of test-time fine-tuning for defense, (2) meta adversarial training, and (3) the adaptive attack against the proposed defense. However, the first and most novel contribution (1) needs qualification, as there is substantial prior work on test-time optimization for defense, and none are included as baselines. This lack of discussion and experiment makes the work incomplete. (2) is indeed a contribution. On the other hand (3) is more a requirement for proposing a defense than an independent contribution. As outlined in the main review, more could be done here to evaluate that the defense is more robust and not superficially interfering with attacks. As there is a wave of such defenses at present, it is important to verify improvement on what has already been done, and to work toward common evaluations techniques across them. As such, I encourage the authors to further test the proposed defense, but must recommend rejection of this edition of the work.

**Final Review** The rebuttal responded to the weaknesses of computational cost and the need for more attack types to assess gradient obfuscation but only partially addressed the prior and concurrent work on test-time optimization for defense. On the plus side, the potential issue of gradient masking is resolved by evaluation of a decision attack and reporting that robustness fails as the adversarial budget increases. However, on the minus side the computational cost of the method is significant and the transfer attack reduces the improvement in robustness from fine-tuning. More importantly, the response does not acknowledge the prior method of runtime masking and cleansing, which likewise updates model parameters. Furthermore, because the parameter updates in this work are _episodic_, it is perhaps not as different from purification methods as claimed. More work is needed to situate the contributions with respect to prior defenses, such as analysing when the model parameters (for this method) or input perturbation (for purification methods) is shared/unshared across the batch."	2
Paper is well-written and lacks any obvious concerns regarding the content, other than the lack of comparison against existing methods for leveraging test-time training for adversarial defence.  	3
The authors proposes a costly training and slow fine tune based testing strategy to get rid of robust overfitting issues associated with AT. In this context the authors should justify the increased cost of training/inference with gain in performance trade-off. The authors should also compare with at least the one relevant paper I mentioned.  I can comment on the overall incremental novelty once I get to know the purpose of doing few things differently. For example the difference between TTT an proposed solution is less, and I am not convinced the $\theta^0$ initiation is necessary always. Please justify.	3
"Probably, this is a preliminary work, and it could become a good paper with lot more research efforts put into it.

Algorithmic contribution of sampling is interesting but incremental and not throughly studied. Empirical analysis is not very insightful either. I don’t see much justification in mapping 32k contextual embeddings to 2k latent embeddings to build a so called latent representation.
"	3
This paper is well-organized, has a novel contribution and I believe it will be of interest to various subgroups in the ICLR community. However, the experiment setting does not make much sense to me, and the empirical results are not convincing. I understand that I might be asking for too much for a single paper, but I think this paper would be much stronger with better experimental settings and more thorough analyses. I'd recommend the authors to split this paper into two papers. For example, the first paper can be an in-depth analysis of the proposed randomized dynamic programming algorithm with applications in different domains and different sampling strategies, and the second paper can be about structure discovery in pretrained models with a correct setup. In its current form, I am leaning towards rejecting this paper due to the issues mentioned before.	3
Interesting approach for inferring large-scale structured latent variable models in contextualized representation space. Empirical results are insightful but methodological contributions seem a bit limited in the context of prior literature.	2
I think this is an interesting paper and is above the threshold of acceptance since the presented method is somewhat novel and inspiring. I’m willing to increase my score if my concerns are addressed	3
The paper is well organized and easy to follow the authors' claims but the qualitative comparison results seems difficult to be interpret as the captions state. And from the perspective of transformation of the latent vector z, there can be more discussion in the comparison with other methods such as StyleGAN that transform z before putting it into the generator.	2
"The paper explores an interesting idea of adding adversarial robustness into GAN training to improve latent distribution sampling and diversification. Unfortunately, the paper falls a bit short in the experimental validation of the approach, and comparison to prior approaches.

---

Post rebuttal. The rebuttal makes a good case for their final algorithm using additional results. However, I still do not see what the paper adds on top of baselines, or how the problem setup in Figure 3 (interpolation artifacts) is actually addressed. The rebuttal mentions some experimental evidence that seems to indicate latent-space sampling can helps. However, I would need to see these results in an actual paper submission for review to feel comfortable about accepting it. As is the paper seems interesting, but not ready for publication."	2
The paper is overall well written, and the idea is very clear and elegant. Meanwhile, I have some doubt on the sufficiency of experiment to show the improvement from quality. Also the improvement over baseline method MSGAN seems minor for me. Expect the rebuttal to clear my doubt.	3
Please refer to the above comments. 	2
It is a contribution to show that current SOTA anomaly and intrusion methods are quite sensitive to adversarial attacks through a systematic set of experiments. However, no new algorithms or methods or insights are proposed in this paper, and so I lean to rank the paper as borderline. 	2
I feel the major issue is with the limited contributions and originality of the work, and the unclear focus on CAN-only attacks, when the proposed methodology does not seem to tackle this domain specifically. 	2
"The paper has its merit, but the poor and unrealistic threat model undermine the “impact” of the results. It is true that attacks against time-series based ML are rare, but many efforts studied the impact of “traditional” adversarial attacks in intrusion detection. The extremely powerful adversary is able to thwart state-of-the-art ML systems… but such outcome is obvious considered the attacker’s assumptions.

In the current state, I do not believe the paper passes the bar for ICLR. I believe the paper requires a more realistic threat model that clearly outlines the real capabilities and knowledge of the adversary. In its current state, despite some originality, the paper represents just a “yet another adversarial ML paper”. The complete lack of references to security-focused works is emblematic of the issues affecting this paper. I acknowledge that ICLR is not a security-focused venue, but since the claimed contribution of this paper is its ""novel"" application, then I expect a stronger security background.
"	2
The paper provides an interesting variation on recent JKO-based methods for computational Wasserstein gradient flows, but its limited novelty and empirical evaluation diminish its contribution, and make it a borderline paper in my view. That being said, if the issues I raise in my review are properly addressed, I would be willing to increase my score. 	2
The approach is reasonable, the simulation promising but I do not see a significant technical contribution (They essentially reparametrized a min max problem over a function space using neural nets and running Adam to alternatively maximize and minimize). 	1
"The paper studies an important challenge in JKO steps encountered by recent works, but the contributions are incremental without demonstrating convincing practical advantages.
"	2
My overall impression of the paper is that it is unfinished. While the idea of variational approximation is reasonable, I suppose this paper requires a major revision with a dozen text improvements and experiments. Therefore, I vote to reject this paper in its current form.	2
Robust methods for equation discovery respecting physics are of utmost importance for the application of data-driven methods in science en engineering. Although the method presented in this contribution seems interesting, its presentation lacks clarity. As discussed in the core of my review, comparisons with already existing techniques are biased (e.g. SINDy with constraints is not considered) and are not presented for representative cases or well-tested benchmarks. Other issues are also discussed in the review. Hence, I cannot recommend this work for publication.	3
Although the paper highlights weaknesses of existing approaches for a very important learning problem and proposes interesting novel modeling ideas, it has major weaknesses in terms of theoretical and experimental soundness and clarity. Hence, I cannot recommend it for publication at its current state.	3
This paper solves the problem of estimating physical equations by a physical neural networks (PNN), with the improvements to take into account “Range, Inertia, Symmetry, and Extrapolation (RISE)” that are important property of physical equation learning. The paper is well motivated and the method has been clearly described. Experimental results on both synthetic and real data support the presented method.	3
I think that study described in this paper would be beneficial for the conference readers, and especially the complex systems modeling community. 	2
"We think that this paper is not of sufficient quality to be accepted in ICLR, for at least the reasons mentioned in the Main Review section.
"	2
"This paper presents a new paradigm involving multiple sub-models which has several advantages compared to ensemble.
It has been validated effective by the quantiative results in terms of robustness, but comparison in other aspects such as training cost could be also helpful."	3
A novel and interesting collaboration method for advancing the robustness of multiple sub-models.	4
"1) Unfortunately the main idea is flawed and the proposed adversarial training may converge without providing enough coverage.
2) The experimental results do not show advantage over previous non-ensemble method."	3
I don't see any major issues with the paper and I find the problem addressed very relevant. 	3
Generally speaking, I recommend acceptance of this paper.	3
"All in all I think this is a very nice analysis and improves the state-of-the-art. Also, this is a very important problem. With small modifications I think this ought to be accepted. My only critique: I am just not entirely convinced that randomness in the number of repetitions is crucial for having tight bounds for DP hyperparameter tuning, I hope the authors can clarify my concerns.

"	4
This work provides careful privacy and utility analysis of private algorithms for hyperparameter tuning. All of the analyses and the proofs are sound, and the experiments give good comparisons between various distributions.  To make the methods widely applicable, the authors might want to comment on how the model’s evaluation on a hold-out validation set can be integrated into the DP workflow. Overall, this is a strong paper and I recommend it for publication.	4
The paper shows several interesting generalization results for Uniform-LGI loss functions. While the paper proves several insightful generalization bounds, it seems Theorems 2-5 do not explain the connection between the optimization length and generalization error. Also, the paper has no numerical results validating the generalization results.	3
The paper is well organized and clearly written. It is interesting to introduce the Uniform-LGI as an extension to the PL condition. However, I think the novelty of each of the two main results is not significant enough, and putting them together may not support the claim “short optimization paths lead to good generalization” exactly.	2
I find that this paper is worth of publication since it provides an interesting contribution to a relevant research direction in the theory of machine learning. However, I believe that the authors must improve their numerical results to provide a convincing final version of the work.	3
"My major concern is how explicitly the length of optimization path influence the generalization bound in the current analysis, instead it is based on the function value gap.
"	3
This paper presents Neural Voice Camouflage, a real-time attack method that disrupts in streaming ASR systems. The methodology is clearly explained and the main contributions are well supported by a solid evaluation framework and carefully designed experiments. Thorough analyses on the results provide useful insights for researchers working in the same domain.	3
Though this work has established the important problem and nicely tested the proposed method, it has failed to address an important component of ASR, LM. Thus I'm initially giving a score of 5.	3
I think this paper proposes a contributive NVC model and can give many insights. However, I personally think that this paper is a little difficult to read and the experiments are a little weak to support its contribution. Therefore, I give a score of 5 for this paper.	3
I am very positive about the potential of this work, but it needs a substantial improvement.	2
Overall, this is a very interesting paper with convincing results. I believe the key idea from this paper can have a broader impact on the community (e.g., 3D and video synthesis) in the future. I recommend accepting this paper but would kindly ask the authors to incorporate the suggested discussions in the final version.	3
My preliminary rating for this submission is marginal reject. While I like this novel task that aims to generate diverse images with high quality, the key novelty and contribution are unclear explained to the prior related works. In addition, interesting experimental results are provided, but they are not well analyzed in the representation domain. The realistically visual results cannot demonstrate the contribution perfectly. 	2
This is an important piece of work, but I would like to see how they contrast their work with Chen/Hall, which I think they missed. 	2
In overall the proposed method for modeling the event data is novel enough and seems to have superior performance in comparison to other state-of-the-arts methods. It also can capture more complex behaviors in events data.	3
Theoretical results appears to have very few links with empirical results and the behavior of the proposed approach in practice. Furthermore, the experimental part is not properly studied and does not support the advantages of their approach. Limitations of the proposed approach are not investigated. The paper could be a good paper with more complete work.	2
The paper focuses on a specific yet common circumstance, injecting backdoors into a trained clean model in backdoor learning, explains the phenomenon of AWP in this circumstance and provides interesting theoretical insights. The paper also models the concept of consistency in backdoor learning, and proposes a novel logit anchoring method for better instance-wise consistency. The extensive experiments and analysis are comprehensive and solid. However, the reasons to choose the metrics or indicators to evaluate the instance-wise consistency are not clear and need further clarification. In general, this paper is novel and solid, and I recommend a strong acceptance.	4
The paper proposes a novel logit anchoring method to improve the consistency between clean models and backdoored models. The paper formulates the concept of consistency. However, it may benefit from a discussion of the difference between the concept consistency and the concept instance-wise side-effects proposed by the existing work, such as Neural Network Surgery. Overall, this paper is well-written. It evaluates the lower bound of the parameter variation in the backdoor learning and explains the phenomenon of AWP. The experiments are solid. The authors also conduct many extensive analysis and ablation studies. Thus the paper is comprehensive, therefore I am glad to recommend a strong acceptance of this paper.	4
In this work, the authors propose a novel and interesting concept of consistency in backdoor learning, and propose to evaluate them with several metrics. They propose a simple yet effective logit anchoring method for better consistency. Extensive experiments are conducted on three CV tasks with the ResNet model and two NLP tasks with the BERT model. The experiments are very solid. Overall, I think this work opens a new angle of understanding model behaviours in backdoor learning, and I would recommend to strongly accept this paper.	4
The motivation of this work seems a little tenuous, and the presentation is overly complicated.  Nonetheless, the experiments do a good job of showing that their method accomplishes their goal.	2
Good motivation and observations, but the solution is also not novel enough and lacks of theoretic justification. 	2
"Strength:
+ Focusing on an important problem
+ Paper is easy to follow

Weakness:
- Over-simplified theoretical analysis
- Some technical terms need to be explained / discussed
- Missing certain ablation studies"	2
This paper is well written and novel enough for this conference. The presentation can be improved and more justification can be added to further improve the paper.	3
The paper proposes a new architecture, called Adaptive Channel Mixing, with the aim to address several cases of harmful heterophily in GNN node classification settings. The paper is well written and motivated and contains convincing experimentations. It could also have a big impact on GNN node classification practice and future research.	3
the idea is quite simple and intuitively reasonable, and empirical results seem extensive and significant, and theoretically justified to some extent, but the results analysis and some experimental design could be more insightful or improved. 	2
"This paper address the spurious correlation by augmenting the data via interpolation. Although intuitions are provided and empirical effectiveness is illustrated accordingly, the contributions over previous work (e.g. mixup) are marginal.

=========After Response==========

The response from authors has addressed my major concerns, so I raise my score accordingly."	2
The paper is well-written, the approach is efficient and observed to work well on a number of benchmarks. However, experiments supporting key claims are lacking, and it's unclear whether the observed improvements in terms of invariance (either at feature- or prediction-level) hold true since no supporting experiments are reported. Contextualization of the proposal relative to past literature also needs improvements since cross-domain data mixup was studied in the past under both settings considered by the authors. The provided discussion on related work does not clarify what and how the authors' proposal improves upon previous work.	3
The paper is generally well written, clearly presented, and a pleasure to read. The technique look sound and contributions look solid.	3
"The paper proposes a useful tool for integrating imaging and genetics with improved accuracy shown on schizophrenia prediction. The proposed interpretability strategy also seems useful. While many parts are based on existing methods, working out the details for combining various components is nontrivial.

Post-revision Summary
While the authors have addressed all my comments, the results are not quite at the level for a score of 10. So I will be keeping my score as 8."	3
"The basic idea and premise of the paper are promising. But the quality is presentation is very poor along with insufficient explanations and experiments to justify publication.
"	1
While the paper is well organized and written overall, the methodological innovation is limited and the arguments were not well supported from the experiments.	3
Clearly written paper with key insights nicely presented. I had some comments/questions on the key insights, but I don't think it should affect acceptance, unless I misunderstood.	4
Recently, there has been heavy interests in prompt tuning such as P-tuning, Prefix-tuning, WARP due to its efficiency in use of large pre-trained language models for downstream tasks. The proposed DART has similar motivation. However, DART is more efficient than previous prompt tuning methods and is on par with them. Especially, focusing on differentiable label would be important observation for the community.	3
Overall, the approach is technically sound and is also very easy to apply to other pre-trained language models. The empirical results on the 10 popular datasets seem closer to LM-BFF and it’s hard to interpret if the approach empirically performs better than static/fixed prompts.  On the other hand, the proposed method performs much better on relation extraction and event extraction datasets, and it is not clearly discussed in the paper on these variations in the results. 	3
This paper provides a simple, yet effective approach to the few-shot learning problem. While it has a couple minor issues, I think the broader community would find this paper interesting.	3
This paper has merits in multiple directions. It applies the state-of-the-art language modeling in NLP in protein representation learning named OntoProtein. Also, OntoProtein help creates a new augmented protein benchmark with aligned GO annotations (enhanced views for heterogenous BioKG with multiple domains). The knowledge-aware negative sampling strategy has underlying insights on utilizing the hierarchically structured gene ontology which can be generalized onto KG negative sampling with ontology information, not limited in bio domain. The critics of this work are mostly about some missing references and baseline approaches, as well as the limitation on combinations of existing works.  Overall, the reviewer agrees that the merits of this work outweigh the flaws. 	3
"The research direction of this paper is very important and that the work described here is adequately framed and motivated. A valuable contribution is the ProteinKG25 knowledge graph put together by authors, which could be valuable to other researchers. While there are several points to clear out (see main review), the ideas are overall well presented and the paper reads nicely. 
However, the methodological and empirical contributions of this work are somewhat limited: the Knowledge Embedding for the knowledge graph (while applied for the first time to GO data) is largely borrowed from Bordes et al. and not particularly tailored to this very specific data modality; the negative sampling is only marginally different from random sampling (constraining on entity group). To the author’s own account “the gains in our proposed OntoProtein compared to previous pre-trained models using large-scale corpus is still relatively small” since “the knowledge graph ProteinKG25 can only cover a small subset of all proteins, thus, limiting the advancement”. In my view, this is precisely the main challenge to resolve for this type of work attempting to instill established knowledge into large protein language models, which would have made this work a very meaningful contribution.
All things considered, I’m leaning weak reject given the minor methodological contributions and weak experimental results."	2
"
pros:
1. In computational biology, the prevailing approaches of learning protein representation rarely consider
incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts. This paper seems to be the first to impose the knowledge graphs of protein.

2. This paper provides a meaningful architecture for simultaneously learning protein knowledge and embedding. 

3. To demonstrate the effectiveness of the proposed method, the author constructs a large-scale KG dataset, promoting the research on protein language pre-training.

Cons:
 
1. To make the paper more friendly for the general ML/DL researchers, I think it should be made clearer the definition of the downstream tasks used in this paper.  

2. This paper usually considers one or two baselines in the PPI task, which is are not enough and convincing. It is possible to compare the proposed method with [1] and [2]? If not, please provide the reasons.

3. The presentation of this paper should be further improved. For example, Figures 1 and 4 are hard to read.

[1] Patel, S., Tripathi, R., Kumari, V., & Varadwaj, P. (2017). DeepInteract: deep neural network based protein-protein interaction prediction tool. Current Bioinformatics, 12(6), 551-557.

[2] Liu, X., Luo, Y., Li, P., Song, S., & Peng, J. (2021). Deep geometric representations for modeling effects of mutations on protein-protein binding affinity. PLoS computational biology, 17(8), e1009284.


"	4
Whilst the GANCO framework is novel as a whole, it is the combination of existing ideas and methodologies (e.g, learning construction heuristics for CO problem and adversarial training).  The field of ML4CO is active the particular problem considered in the is work, generalising to OOD test-time data, is ubiquitous - therefore, in my opinion, the paper comfortably reaches the required standard in terms of novelty and potential interest.  Ultimately then, I believe it comes down to whether the demonstrated empirical performance is sufficiently strong, or if the core idea of adversarial instance generation is sufficiently well justified and investigated.  My feeling is that it is not clear cut, with the experimental results impressive, but to me only unequivocally demonstrating that training on a broader distribution of tasks results in a more general agent.  Overall, however, there is enough justification of the GANCO framework in the latter sections, for me to lean towards acceptance (ultimately, I find the ideas and results in the paper interesting, and therefore feel the same can be said for others working in the field).  With that said, I would be more confident and comfortable in my recommendation if the adaptive nature of the adversarial instance generation that is core to GANCO could be demonstrated as a key element in the frameworks empirical success (as discussed in detail in my main review).	3
This paper attempted to tackle an important problem which hasn't received the attention it deserved, namely ensuring that neural networks trained to solve combinatorial optimization problems generalize well. They proposed a theoretically sound approach to augment the original dataset with additional training examples created under the guidance of an auxiliary 'generator' model to cover the gaps in the original dataset. However, they failed to demonstrate that the effectiveness of this 'generator' based approach.	2
The paper presents an intuitive approach to generating training examples, makes it work and does a good evaluation. However, similar work has been done in the RL domain and other supervised training domains before (e.g. using GANs for data augmentations) so giving a marginal accept for now.  If the missing citation is added, my questions are answered and the edge the *adversarial* augmentation has over *random* augmentations (or simply perturbing the network slightly and training for longer) I'm willing to update my score.	3
The generative adversarial training for combinatorial optimization in this paper is novel, the experiment results are detailed and comprehensive. However, the motivation, writing, and the experiment part of this paper seem to have room for further improvement, and I am suggesting a rejection for this time and I am wishing to see an improved version in the future.	3
This is a somewhat novel but solid work. The comparative experiment based on clip is very sufficient, but it also lacks other important experiments, such as the effect of CoOp on other vision-language models, just as the title of the paper is learning to prompt for vision language models	3
The paper achieves better few-shot image classification performance improvements but lacks enough technical novelty and explanations for learned prompts	2
"Overall: the work is generally clear with thorough and convincing
experiments. While there were a few presentation/technical concerns,
the main drawback of this work is novelty: the proposed idea is
identical to Li and Liang (2021) [arxiv in january], Zhong et
al. (2021) [arxiv in April], and perhaps Lester et al (2021) [arxiv in
Sep, this one is more recent and I haven't read it yet]. While these
papers consider only the NLP case and not the vision+language case,
it's still difficult to call this method ""novel,"" as the authors do in
the intro."	2
Despite extensive evaluation, the paper presents in very low quality and lack of description of key methods and the theory behind it. The overall quality is clearly below the threshold of the ICLR standard.	2
The idea of the paper makes sense, but the theoretical and experimental results cannot support the usefulness of the proposed method well.	3
"The underlying logic is not clear (see the detailed comments above). 
Some statements/derivations are not rigorous and thus are not convincing. "	2
As a non-expert, I find that the method proposed in the paper is novel and empirically well-tested. However, the proposed approach for training QNNs seems to be computationally expensive on its own, and I am not sure if the required computational budget defeats the purpose of training QNN -- it will be great if the authors can help me understand in this regard.  	3
It is interesting to incorporate manifold learning in QAT, but more concerete analysis (experiments) in deep learning field should be conducted.	2
Although not perfect and build upon existing works, the paper proposes some new techniques and quantity to refine the analysis of distributed non-convex optimization algorithm. The theoretical novelty is sufficient.	3
The paper improves the analysis of MARINA algorithm, but the introduced new assumptions need to be further justified.	2
Could you take a look at my main review and address the concerns on motivation, PermK and experiments?	2
See above.	3
This paper provides solid theoretical analysis for several EF21 variants. But the novelty is limited.	2
"The paper is well-written, and good in theory and practice. 
However, I have some concerns in the contribution and the experiment settings."	2
More discussions are needed to clarify the theoretical novelty, and numerical results should be extended.	3
"Please refer to the main review for the details.

I recommend the authors to update the manuscript to make these points clearer."	2
"The paper's idea is interesting, as evidenced by the experimental results. However, the theoretical depth and novelty may not be enough to meet ICLR's standards. Additionally, the concerns raised in W2 and W3 should be addressed.
"	2
This paper studies an important problem, but the proposed solution is not well-motivated and the experimental results are inconclusive.	2
I vote for clear rejection. The proposed method doesn't show any strength in training large dataset and help on overfitting with more regularization. 	1
"Despite the pros summarized above, the paper lack enough novelty and requires more effort on experiments and clarification on several important aspects as mentioned above (e.g., clarification on the why neighborhood size variance and LSH).

"	2
"The main concern with the paper is the lack of comparison with any other methods that sparsify the graph. Firstly, the contribution here is a separate pre-processing step of the graph that uses edge pruning. There are numerous approaches to prune edges for example proabilistic methods that use the node attribute similarities to remove edges with negative correlations in the node attributes (a graphical lasso approach) and this shows to improve performance for then using the pruned graph as side information, in graph theory there are more methods for simply removing edges; It is not clear why LSH is a better choice. Secondly, there are other methods for sparsification of the graph for graph convolutional networks (GCNs) that not only sparsify but improve the performance. I think comparison with these existing GNN sparsification methods is important as the end goal of speeding up inference time while preserving predictive performance is the same.

With these comparisons, it will better position the paper. The area of sparsification of GNN is I believe an important one, so I feel this work has a lot of potential but there is already work on this that should be compared."	2
A clear rejection based on the weaknesses mentioned above and no intuition to the community.	1
The shortcoming noted in the main review suggest that the proposed mechanism for generating interpretable policies is unlikely to provide a viable alternative to previously proposed methods. Considerable improvements would be required to meet the threshold for acceptance.	2
"1. This work does not present a systematic approach that can be broadly used. 

2. The interpretability of the final model is not shown. 

3. This work attempts to make connections to DRL, but the proposed method is not applicable to the same problems."	2
This paper proposes a novel idea for building explainable neural networks but lacks strong theoretical and empirical evidence to support its claim. I do not recommend accepting this paper.	2
This paper is poorly written and the contributions do not seem significant, so I cannot recommend acceptance for this paper. 	1
There are several key issues about this paper that are not well-supported and are not clear. 	2
I do not doubt the theorems in this paper have any issues. But I do have trouble understanding the motivation of this work, as I have detailed in my weakness section. Without showing a learning algorithm achieves state-of-the-art prediction performance, I do not get why interpretability is of concern. As a result, I do not see how this work is a significant contribution to the ML community and would not recommend the paper to be published by this year's ICLR. 	1
The proposed model of progressive constructive of neural networks does not seem to be novel and has been shown to provide universal approximation as well as have limits, which the authors of this method do not seem to be aware of.  While the resulting architecture is capable of arbitrary accuracy of approximation on the training data, I don't see that it would generalise or cope with catastrophic forgetting, at least according to the typical notions of what constitutes generalisation and catastrophic forgetting in machine learning.  	1
"The contributions are not novel or even not supported. 
1. Interpretable universal approximation is not novel. It is written in a book in 2015. 
2. The resistance to catastrophic forgetting is not supported by theorems. Settings of the claimed theorems are different from that of catastrophic forgetting. 
3. The universal approximation and generalization are not the ones machine learning cares about. 
"	2
I overall like the paper and appreciate the geometric formalism and results. That being said, from a mathematical point of view, I am unsure about the amount of novelty the paper would have had, had the same approaches for integration been used without the overarching geometrical point of view.	2
"Mathematical link is presented between FS classification and voronoi tessellation, description of a novel ensemble FS method based on  CIVD. Paper is well written and conclusions are throughly validated. Paper needs clarifications about implementation details, and discussion of limitations, effect of outliers and generalization to other tasks.
"	4
The main idea is novel and interesting, and the experimental results demonstrate the superior performance of the proposed method. However, this paper lacks some ablation studies and time comparisons. In summary, the reviewer thinks this paper is marginally above the acceptance threshold.	3
"The manuscript is technically overloaded but not self-contained. Important descriptions, in particular of the proposed methods, are missing. The experimental results look impressive, but it remains unclear what exactly has been used in the DeepVoro(++) methods.

Post-rebuttal:
most concerns have been addressed and the correctness has improved. The assessment has been raised accordingly."	2
"Summary: My current rating is mainly based on two reasons: 1) The experimental results are strong and the idea of utilizing voxelization for de-noising/refining/densify-ing point clouds makes lots of sense to me. In addition, the proposed transformer works well for further improving the results and recovering points from voxels. 2) Even though the numbers are great, I do think the experiments can be improved. Multi-task baselines should be devised and compared. It's not enough to only demonstrate the proposed method out-performs single-task baselines. Moreover, fine-grained experiments should also be conducted to help us understand the performance on each single tasks. 

I'll also use this section to justify my ratings below. This paper is mainly proposing a new problem setting and develops a new framework correspondingly.  Technical wise, this new framework doesn't carry significant new components. It does a great job choosing the proper sub-modules,  combining and slightly improving them for the final tasks. However, most of these components existed in the literature. Empirical wise, the results in Tab.1 is exciting and the proposed framework greatly improves over singe-task baselines. More strictly speaking, single tasks and two-task (X--> score-PD) baselines. The qualitative results in Fig.5 are also encouraging and clearly show the advantages. 

I have worked in the field of point-cloud recognition and pretty familiar with the recognition frameworks.  However, I'm not an expert of point-cloud de-noising/completion/densify-ing, and not very familiar with the SOTA methods in these fields. 

"	2
"

Justification:-
I think the paper does interesting contributions but lacks extensive qualitative results and the “Snowflake-PC → Score-PD” and similar things make me wonder if things are fair. The performance gap between the method and baselines is quite huge and I’m not able to completely understand the exact difference in impact between Stage 1 and Stage 2. Depending upon the rebuttal by the authors I will change my rating upwards if given reasonable explanations."	3
This paper creatively integrates different tasks such as point cloud denoising, completion and reconstruction into a two-stage task, and the generalization ability of the experimental results on ScanNet and ICL-NUIM datasets is much better than the latest existing methods. This paper creatively uses amplified positive encoding to compute the relationship between a center voxel and its neighbor voxels, which has certain enlightening significance for the follow-up work.	3
In summary, I find the paper well written and the problem interesting. The approach is well presented and technically sound for the most part. However, due to the lack of a significant body of works in the related work section as well as the missing evaluations I can not recommend this paper for acceptance at this stage. 	2
In general, I believe the $\beta$-Intact-VAE is novel and is useful in treatment effects estimation. The authors have provided theoretical guarantees for the proposed method. Experimental results also demonstrate that $\beta$-Intact-VAE outperforms other generative models. 	3
"In summary, the aim and approach of this paper are interesting and I believe the approach could be a methodological gain for the causal machine learning audience, but its underlying assumptions are presented in a way that makes it difficult to link or compare them to other works and its significance for real-world examples is therefore difficult to judge. Especially, the details of the underlying causal model are not clear to me and make it difficult to conceptually compare it with other methods such as CFR (Shalit et al., 2017) or CEVAE (Louizos et al., 2017).
I will read the rebuttal carefully and am willing to increase the score if the authors address the raised concerns."	3
I believe the paper solves the interesting problem of estimating CATE with limited overlap. It seems to me that most of the theory, and the estimation method were developed before this article, so in a certain way it is an incremental contribution. Nevertheless, both theoretical, and empirical analysis of the article seem serious.	3
I vote for weak acceptance. The paper proposes a technically and theoretically sound approach and it studies an interesting setting -- limited overlapping, though it has some presentation issues.	3
Overall, I think it is currently a borderline paper since the idea is good, which can benefit researchers in related fields. Meanwhile, this work is presented well, and the experiments are comprehensive. However, I hope to check the answer to my questions before making further judgments.	3
Based on the strengths and weaknesses listed above, I am slightly negative to the paper.	2
"This paper clearly states the problems of unsupervised RGB-D saliency detection and proposed appropriate solutions.
The contribution and novelty are clear. Extensive ablation studiesy reveal the effectiveness of its components.
Based on proposed solutions, the proposed network achieves state-of-the-art results.
"	3
Overall, this paper has publication merits, but some issues need to be addressed.	3
I recommend this paper for acceptance. This seems like a fitting extension of existing work, and the authors have provided a corresponding framework for quantifying performance loss in the presence of byzantine agents and heterogeneous data for federated learning. Their results match existing work in regimes when there are no byzantine agents, and when there is a lack of heterogenous data. Furthermore, the non-trivial loss of performance in the regime where both byzantine agents and heterogenous data is present is accounted for in a matching lower bound. These results will be of interest to the growing body of work in federated learning at ICLR. 	3
"Although there are weaknesses in this work, considering the challenges from heterogeneity, I think this work is slightly above the threshold.

--------
(Post-rebuttal)
My major concerns have been properly addressed by the authors and the quality of this paper has been improved after revision. Thus, I have raised my rating."	3
To sum up, I am sure that the paper should be accepted to the conference after minor improvements. If the authors apply all necessary corrections and address my comments properly, I will increase my score: the paper deserves acceptance as a spotlight or even oral talk.	3
Overall, non-trivial theoretical analysis has been conducted to study the effect of compression on convergence. However, some concerns exist in terms of both theoretical justification need to be addressed. The experimental analysis could have been more thorough, though I consider this a relatively minor weakness given the extent of theoretical contribution in terms of convergence analyses. 	3
"The paper studies the communication needed in order for a group of distributed players to collectively solve a variational inequality problem. In particular they provide two algorithms MASHA1, MASHA2 to do this that solve both the deterministic and stochastic cases. 
They also provide experimental results on applying their techniques to Bilinear saddle point problem and adversarial training of transformers. 
Both the algorithms basically extend the extragradient/extrastep method of solving variational inequalities to the distributed setting. I think this is an interesting modification of the prior known work but I do not believe that this is a significant research contribution and is an incremental adaptation of the extragradient/extrastep method."	2
I have been working on related areas and have read this paper carefully.	2
The paper has some interesting aspects for example large-scale adversarial training of transformers. However, given the issues regarding theoretical results, connection with related work, and clarify and presentation of the paper, I recommend rejection. 	2
The idea is interesting and seems novel, and the method is domain agonistic that may be applied to other domains. Experiement conducted are convincing, I am leaning to accept the paper, provide the minor concerns raised are resolved.	3
The idea of the paper is interesting and potentially useful for the community. However, the experimental results seem limited for now.	3
This paper addresses an important problem and proposed a framework to learn content and style representations. The experiments show that the performance of the proposed model is relatively well. But some details are missing, so it is difficult to follow the paper and it is unclear the roles of the loss functions and how the model avoids information leaking.	3
The author introduce an original and highly effective optimization scheme for controlled SDEs for modeling stochastic dynamics. While there is not a thorough analysis of the algorithm (aside from a discussion of the theoretical optimality in the appendix) the experiments amply demonstrate that the approach is productive. 	4
"The paper brings several interesting approaches to address the sub-problems that appear due to the construction of the SDE controlled by multiple agents. As much as I can follow, the equations are accurate and the methodology does not suffer from any theoretical deficiency. On the other hand, this makes it very difficult to evaluate how important different assumptions and building blocks are. As such, the model seems like a magical combination of a number of blocks but it would be much better to isolate each block individually and show how it contributes to the overall performance. Also, I believe the writing should be significantly improved (see my detailed notes in the ""main review""). Although the method is interesting and timely, I recommend a reject and re-submission after the above-mentioned points are addressed."	3
This paper proposes some novel ideas that works well in practice. I don't fully understand this paper, but I feel it deserve a broader audience.	3
"The authors propose  a new method for learning dynamics from continuous-time data using neural Markov controlled SDE. It connects stochastic optimal control theory and deep learning which is pretty novel in my opinion. So my recommendation is ""accept"". "	4
"The paper was organized logically. However, the applications were not well presented. Many details are unclear and missing, especially on datasets, data processing, feature selection, cell types. Also, it seems that the authors don’t fully understand cell-type gene regulation for misusing different datasets (train orange to predict apple).

"	2
The problem tackled by the manuscript address an important need in repressing genome data using both sequences and additional measurements. The results presented both in performance and diversity of tasks measured is impressive. However, I am not completely sure if the submission will be of broad interest to the ICLR audience. 	3
The authors explore a new method for self-supervised pre-training before tackling several regulatory sequence analysis tasks, but are not able to deliver a clear method description or compelling empirical results.	2
The paper tries to address an important problem in genomics: how to effectively embed DNA sequence for downstream tasks. They have used some recent transformer architectures and loss function to effective do this. Their main contribution, which is an important one, is to use the transcription factor information in the accessible genomic regions which are cell type specific. Therefore, the model would be more generalizable than the previous methods like DNABERT. The ablation study of the model and hyperparameters are comprehensive. However, some biological experiments and their importance are overstated. I provided some comments for the authors to enrich the experiments. I think the paper is interesting and if some comprehensive and important experiments are coupled with it to show that it can really find interesting biological events, it would be a great paper. As such, I choose the score 6 for the paper. 	2
"This paper proposed CrossFormer that aims to model cross-scale information through Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA). The design of both modules is reasonable, and it demonstrates performance improvement on dense prediction tasks. However, due the potential limits of LSDA and DPB and missing comparison with some prior works, I would like to give a ""marginally above the acceptance threshold"" rating."	3
Overall, the ideas of CEL and LSDA are sound and solid experiments support most claims in the paper. However, some designs are not new and some related works are missed. I would rate this as a borderline paper (marginally above the acceptance threshold).	3
The idea of the paper is clear. Comprehensive experiments are performed, and superior results over several benchmarks demonstrated the effectiveness of the proposed architecture. My main concern is that the idea of the proposed components is not new in the CNN-based works. The overall novelty of the paper is limited. 	2
"The paper is overall well written and easy to understand. I think there are some strong contributions that are a bit oversold (in my view). However, I am leaning towards accepting the paper given that some of the concerns and questions are properly addressed. I am willing to raise my score in case those concerns are addressed.

UPDATE after rebuttal. I am willing to raise my score after reading the rebuttal and the updates to the paper. I think the paper is a good and clean contribution as it is conceptually simpler (in my view) than most of the previous works while showing stronger performance."	3
"
Strong reject given the poor writing, limited novelty, and unconvincing experimental results of this paper."	1
I would recommend strong reject for this paper, which introduces NO novelty and the experimental results are suspect.  	1
"Following the aforementioned consideration, l recommend to reject the paper ""3: reject, not good enough"" except if the authors make major changes, mainly providing an extensive analysis and discussion of the empirical results and considering U-net in the ablation study."	2
This work provides a new attention module to improve the performance of backbone networks. The main concerns are the limited techinical contributions and insufficient experiments. In my view, this work is much lower than the standard the ICLR.	1
This manuscript needs careful refinement. This version far from reaches the level of ICLR. Maybe the authors can improve it according to the weakness part.	1
"The paper made some contribution by proposing a lightweight architecture and achieving real-time inference. 
However, the design is not well explained (figures and texts sometimes don't match). The experiments didn't explain the function of each module and focused on too-low-level ops. The performance gain and speed gain are marginal comparing with other real-time methods."	2
Reject due to lack of novelty, claims not explained and supported.	1
Mainly because of the weak novelty of the proposed method and poor organization of the paper, I will reject this paper.    	1
The novelty of the paper is quite limited, and the paper should be rewritten to include important metholodogical details as well as comparisons with the existing studies to assess its impact and significance.	1
"There are multiple major issues about the paper. 

- Methods description is very difficult to understand. It is not very clear to me how the proposed method works
- Experimental evaluations are quite limited. There are some Dice score and IoU values, but there is no comparison with another method in the literature."	1
The paper makes no significant technical contribution or innovation and does not offer any strong results.	1
The paper does not have any meaningful contribution.	1
"The proposed model may have some selling points to it, but the presentation
is so poor and so many details are missing that it is impossible to tell.
This manuscript is far from being up to ICLR standards. 
"	2
"This manuscript talks about two popular topics: attention mechanisms and physics-informed neural networks. I believe something significant will happen in this particular overlap but not in a ""1+1=2"" form. I expect more explanatory theories or experiments and domain-specific modifications."	2
The authors introduce a new architecture for PINNs, which can be seen to be novel up to a certain extend. However, modules of similar flavor have been proposed in previous works and simply proposing a new architecture that augments an existing architecture with additional modules or layers is not sufficient. The experiments need to clearly study and demonstrate the advantages on a range of problems, in particular if no theoretical justifications are provided. The provided experiments do not live up to the standards that I would expect from an empirical paper. Further, the presentation and discussion of the ideas is insufficient and many details are missing. Also, no research code is provided to reproduce or better understand the proposed architecture. The overall quality of this paper is below the acceptance threshold for ICLR. 	2
"This paper proposed attention augmented physics informed neural networks (AA-PINN). It uses a combination of channel and spatial attention module in addition to the normal Physics Informed Neural Networks. This paper would be an interesting paper however it is poorly organized in the current version and also has a lot of flaws as I pointed out above earlier.

In summary, I would not recommend its publication in ICLR until all my comments are addressed.
"	3
Overall, I think the paper is well-written and reach the acceptance bar of ICLR. Both theoretical and empirical results support the main claim of the paper. Therefore, I vote an accept for this paper.	3
"The current work represents a solid piece of work that takes a step forward towards robust conformal prediction. Within the context of CP under distribution shifts, prior works studied settings where structured/constrained shifts are present. This work focused on a different setting which hasn't been covered in conformal literature before where at the test stage adversarial examples invalidate the vanilla CP. The reasons for lowering the score include: limited theoretical contributions, a limited study of the types of adversarial examples, unaddressed questions stated in the main review.

**Update after rebuttal**

I would like to thank the authors for the detailed responses. Taking into account the general contribution of this work and points mentioned in this and other reviews, I tend to keep the current score."	3
Overall I like the idea proposed in this paper. Their theoretical and experimental results demonstrated the effectiveness of the proposed approach. My major concern is about comparing with other robust conformal prediction methods in the experimental section. It would be helpful if the authors can provide insights on this.	3
"Overall, this paper handles an interesting and timely problem, but the theoretical and empirical results are less significant compared to the known literatures as described in the main review; I lean to reject, but willing to discuss to adjust my understanding.
"	2
Even though I consider the technical contribution a bit limited I positively acknowledge the fact that they're the fist to study the problem of hot-refresh model upgrade in image retrieval in particular and the problem of model regression.	3
"The paper introduces a very nice problem with some technical details to be clarified. The reviewer's opinion is that paper is not ""uniformly"" developed, many technical parts would have required more attention.  "	3
"Overall, this paper is a good read and does provide useful insights to the proposed problem and potential remedies. 
However, I think this works is just so slightly short of passing my threshold for acceptance due to the lack of exploring more on its main contribution, which is the RFCT loss, and potentially giving more valuable information to the community on the synergies between old and new features in the model upgrade process. 
Moreover, the slight overclaim in the ""Regression-Free"" naming requires some extra attention. 
Therefore, I am recommending ""5: marginally below the acceptance threshold"" now, but this decision is marginal and I am willing to consider boosting the score if the authors address my concerns well in the rebuttal."	2
This is an interesting paper, targeting an important application, proposing a novel observation and method to improve hot-refresh image retrieval. However, some aspects of the paper are not well-developed (see weaknesses listed above). Due to the latter, my rating is “marginally above the acceptance threshold”.	3
I appreciate the novelty and I think the idea is worth being noticed. Nevertheless the presentation (story) seems a little misleading, which may not be a proper way to present the merit of the method.	4
This paper proposes to use kernel transfer operators for generative models, by leveraging a kernelized embedding of the “forward operator” (generative model). The authors experiment with various kernels and obtain favorable results on high-dimensional image generation tasks.	2
"To summarize, the authors address an important problem and propose a new generative model with several benefits over existing methods in the low sample size regime. The results demonstrate that the method leads to improvement in retrieval tasks. However, the technique has some limitations (namely, the inference may take a long time, and there might be memory limitations for large datasets). The experiments do not provide a complete picture of the method's capabilities, namely how diverse the new points are and whether they really differ from the original samples. To demonstrate that a generative model is useful, the authors could try to use the model to enrich imbalanced datasets and see if clustering or classification improves.
For these reasons, I recommend a weak rejection of the paper.
"	2
Great idea, though there are some issues with presentation and experiments. Not very convincing as is, but a lot of potential to be improved.	3
The paper is well motivated, clear and well evaluated with strong results.  It could benefit from a deeper analysis on what is being learned, how that contributes to different data settings and if novel aspects of datasets can be learned from this approach (eg does the model identify previously missed domains).  Overall a good paper that could be better with more detail.	2
Based on the above analysis, I am leaning towards acceptance for now.	3
Overall my opinion is that the paper explores an interesting problem of learning latent domains present in data, when no explicit domain information is provided. I feel the experiments performed in the paper can be improved a bit. I've scored the paper accordingly.	2
"The new problem setting, latent domain learning, looks interesting and practically useful. However, the baseline model is limited in terms of novelty, its performance is not impressive (decent though), and the paper misleads readers about the important previous work (i.e., RA). 
The paper has both pros and cons, but I would value novelty and correctness more, thus leaning towards rejection.
"	2
"In summary, this is a good exploration into the transformer architecture for speech recognition. The paper provides valuable insights into the transformer's attention map structure and connects it with the phonetics. Then the paper uses these insights to improve the model.

I would like to request the alternative hypotheses on why the attention becomes diagonal in higher layers. Or some discussion on this.

Then, the figures need to be improved for reading when printed."	4
Overall, the authors present an interesting analysis of the attention maps, which in the reviewer’s opinion is the strongest part of the work. It is interesting to categorize lower layers as phonetic, and upper layers as linguistic. That being said, similar analysis has been done in prior work. The presented results are also lacking; it is unclear if the method is very specific to CTC and whether it generalizes to other more popular end-to-end techniques. The authors have also not considered alternative strategies to minimize computation that directly addresses the quadratic computation of attention on long sequences.	3
"The paper is well-written and provides a lot of insights into how CTC models with MHSA learn. 
I think this is a very interesting experimental analysis of the properties of the Conformer/Transformer models that can help to understand non-autoregressive end2end ASR models. The claims are very well supported and the practical conclusions and implications are very interesting. The code is provided and it seems possible to reproduce the paper.
I would have liked to see more experiments on the paper but just because the results are very interesting."	3
The analysis proposed in this paper can be a good contribution to the community. The task focused in this paper, however, is on the weak side. Overall the analysis is beneficial to the community, and can be considered for acceptance.	3
I think the authors have come up with a very interesting technique of methods and are targeting a novel issue of data storage for efficient implementation of neural networks. This paper falls under the same category as pruning or quantization, but provides a very fresh perspective. However, their metrics of evaluation using parameter reduction is weak and does not really support the energy efficiency claim that the authors make. Thus, I have given a rating of 5 for this paper.	3
The paper introduces WFN, weight fixing networks, an algorithm that transforms a neural network into a representation with very few unique weights, the ones most frequently used being powers of two, all while maintaining good inference performance.	3
The work appears interesting but I am unsure about the potential impact on data movement. I found the paper quite difficult to follow in general. For this reason I am currently recommended that the paper is not accepted.	3
The idea of this paper is novel and may inspire broadly future works. The technique design is clear and correct. The empirical design can be improved.	3
"This paper is insightful, clearly written and presents good empirical results. 
I vote for acceptance. If the concerns mentioned above are addressed, it can be better. 


"	3
"Creative use of data for pretraining, experiments could be stronger


_________
after author answer:
Score adjusted to 8"	3
The idea presented in the paper is novel, interesting, simple, and direct.  The results seem to be promising. 	3
The motivation for this research area is strong and there have been several papers addressing similar problems at previous top-tier machine learning conferences. I think ICLR is a good venue for this manuscript.	4
Overall a simple idea and seems to work well. However, some more intuition around cyclic sampling would be worthwhile and might improve the paper. Exploring alternate sampling strategies would be interesting (random sampling or some other pattern can be explored). 	3
Overall the comprehensive and varied experimental results give the paper its strength.  The results are either competitive, such as on segmentation, or better than the previous MLP methods.  The idea is simple and a natural extension of previous ideas for fully connected layers.  There is a question on the relative contribution of the patch embedding when comparing to previous MLP methods, but generally no major weaknesses in the paper.	3
This paper shows competitive results of MLP-like architecture on image classification, object detection and segmentation. However, there exist some unclear descriptions. Currently I rate it as a borderline paper and the authors need to consider the above questions.	2
The proposed method is simple and interesting. Compared to modern MLP architectures, the CycleMLP can cope with various image sizes and achieves linear computational complexity. The benefits of each component and the performance difference are shown. All experiments of this paper is detailed and the overall content is sufficient. The proposed method is applicable to many vision tasks in the future.	3
Overall, I think this work is quite reasonable and does not have any serious issues. I find it only borderline on the accept side, due to its incremental nature.	2
"The paper is well-written and the method is clearly explained. The proposed method builds on SPiRL and proposed a meta-RL learning of the high-level policy. It shows reasonable improvements when the number of samples is limited (maze and kitchen tasks) and also provides insightful experiments (meta-training task distribution analysis).
However the performances are quikcly matched by concurrent baselines after not that many steps and the comparison to SPiRL seems unfair in the current setup. Mainly the set of mazes solved seem more restrictive than SPiRL, is it a restriction of the method? Also the extra annotations used by meta-RL during evaluation (target trajectories), is not made available to the strongest baseline the authors compare to, SPiRL. The experimental section could be strengthen by evaluating SiMPL against SPiRL where the extra-data provided to SiMPL is also provided to SPIRL, and by evaluating on a richer set of environments."	2
"The paper proposes a nontrivial and effective way of combining skills extracted from offline training data with meta-learning from training tasks.  Following the details is hampered by notational ambiguities and inconsistencies.

Post-discussion edit: I upped my score to in response to the authors' improvements, although limited novelty remains a concern, as expressed by my fellow reviewers."	3
Given the marginal novelty and the lack of empirical evidences to backup the authors' claim I do not recommend to accept the paper. I am open to change my mind if the authors can provide a convincing rebuttal and more solid experiments.	2
"Authors study choosing the optimal variance for the reverse process in DPMs and propose to Monte Carlo estimate it for improved inference.  Technical quality, writing, and experiments are mostly good with the two minor caveats I described above. 
"	3
The paper provides valuable insights into optimal reverse process variance of DDPMs and DDIMs, and makes connections between the proposed optimal variance and previous handcrafted choices, etc. The improved understanding of these model classes could have been sufficient to recommend acceptance. However, the empirical results, especially those around faster sampling are also strong and convincing. DDPMs / DDIMs achieve high sample quality and it's primarily their sampling speed that prevents practical application off this model class in real-world systems. This works makes a significant step towards enabling faster sampling for this model class.	3
"Correctness: I think the paper is mostly correct. I am not sure if using timestep is a good metric to demonstrate the efficiency of the proposed method. Since efficiency is a major aspect of the proposed method, it would be desirable to make a clarification on this issue.

Novelty and significance: I think the derivation of the optimal reverse variance is novel and insightful. The lower bound and upper bound of the optimal reverse variance is also useful in practice. While the proposed method is performant compared to existing DPM variants, these existing variants achieve better or comparable FID or negative log-likelihood with enough timesteps. These strong baselines suggest the (potentially limited) headroom left for improvement for the proposed method.

Overall, I think the paper solves an interesting problem in DPM. "	4
The paper contributes interesting analytic results to the DPM models and the methods perform well in practice. However, the DPM is a simple Gaussian model and similar results can appear in other similar models.	3
"Overall, I vote for accepting. The paper provides an important insight in DPMs and shows improved results for pretrained models by a simple post-processing technique. My major concern about the paper is that OT [4] does not work well in combination with their method, which in my opinion makes the work slightly less significant. I hope the authors can address this concern in the rebuttal period.

**Post discussion period update:**
I strongly vote for accepting (and also changed the correctness score from 3 to 4). All of my concerns, questions, and suggestions have been addressed by the authors in the discussion period. I thank the authors for this productive reviewing process."	4
Based on its strength and weakness, I think it is a good and quite novel paper considering both its framework and applications. I recommend accepting this paper.	3
I did not find big issues with this submission. It is a very good paper, and I think the proposed techniques can benefit other large image generation research. I am only concerned about the $z_g$ in both $G_S$ and $G_T$, which I think might be redundant. Therefore I recommend accepting this submission.	3
"This paper is a solid submission, that really pushes the state of art in image synthesis for outdoor scenes. The problem is well justified and placed with respect to related work and the contributions are well described. The results prove that the method is outperforming baselines with comparisons on both quantitative and qualitative results. Given the complexity of the framework, I would have appreciated a more in-depth analysis (i.e. ablation study) of the various components to help other researchers to reproduce the results and focus on the right modules, however I acknowledge that authors did their best with experiments and evaluations: the supplementary material is actually impressive. 

I would suggest authors to also show some results on temporal sequences, to assess temporal consistency across frames and/or long sequences. This would also help to establish a baseline for future work in this area."	3
"This paper proposes very interesting ideas on image generation, including a padding-free StyleGAN architecture, feature unfolding and neural implicit representations for adding variation to the generated image contents, as well as hand-crafted priors on landscape image generation, including positional encodings and disentanglements between global and local styles. It includes sufficient details on implementation for allowing reproducibility. The proposed applications could be impactful in the image generation literature. 

Overall, even though this paper shows potential for somewhat significant algorithmic contributions for image generative models, it falls short on important aspects that need to be adressed before it can be recommended for acceptance. First, several of the claims of the paper should be toned down. The paper argues it proposes a generic framework for infinite image synthesis, but the hand-crafted priors embedded in the method and the datasets used for validation are very specific for landscape photography, making it much more limited in its scope than the paper claims to be. Second, many decisions taken throughout the design of the framework are not well justified or validated empirically, making it hard to understand what the actual contributions of the paper really are.  Finally, this paper misses important references and comparisons to prior work, the comparisons that are present in the paper are somewhat arbitrary and the proposed evaluation metric lacks implementation details and justification. 

** Update after rebuttal period** 
The authors have adressed many of my concerns during the rebuttal period, including new extensive comparisons with other methods, ablation studies and discussion of limitations. I am overall more positive about this submission and thereby recommend it for acceptance.  "	3
"I empathy the difficult of arguing the ""betterness"" of a machine learning framework. However, I think the paper in general has not made a convincing argument or provide enough scientific contribution. So I recommend rejecting this paper. However, I am very interested to check out the open source implementation of Flashlight, and see how amazing it is."	3
Overall the paper was interesting, and I commend the authors for their work. It seems the Flashlight framework could be beneficial for researchers looking for a boilerplate template framework on which to build their very highly customized solutions. However, for anything other than an solution requiring rewriting large portions of a neural network framework, the researchers using this library will give up optimizations and other benefits of using a more mature library such as PyTorch C++ (for which custom ops and some other features mentioned in this work are possible without rewriting internals).	3
Flashlight is well-designed and shows promise as a research tool for systems research in machine learning. However, it has not demonstrated its usefulness in practice.	2
In summary, the paper presents a solid deep learning framework written in C++, and has been put in production for a wide range of applications. However, in terms of paper writing, it would be more desirable for the readers and reviewers to understand the novelty and quantifiably justify the claims in the paper.	2
"The authors propose a general framework to approach regression problems using binary encoding and a set of classifiers. The paper is detailed, some interesting theoretical aspects are discussed and experiments seem to demonstrate the high accuracy of proposed approach even though only using shallow network after the feature extractor.

=====POST-REBUTTAL COMMENTS========

The added experimental results and manuscript modifications address all of my concerns. 
I have raised my score."	3
"Idea is novel and interesting.
Results and findings are mainly empirical and look remarkable, achieving SOTA performance.
I believe these findings require more careful description of the exact empirical process, and that as a reviewer I should make sure the comparison to other baselines is fair. A provided code would also be helpful and more assuring."	3
The idea is interesting. The method is simple yet effective. I think the some issues needs to be clarified.	3
"Overall, I vote for weak rejection. I like the motivation of the work and the authors provide a nice analysis of the problem from both theoretical and empirical perspectives. My major concern is about the clarity of the experiments in which the proposed BEL approach is compared with the existing regression models, as mentioned in the main review above.

=====POST-REBUTTAL COMMENTS========

Thanks for the authors' response. The newly added experimental results and manuscript modifications address all of my concerns. I have raised my score to recommend this paper to be accepted."	3
Although the theoretical contribution seems clear, I believe we need additional experiments to support the authors' claim.	4
"I feel this paper does present an interesting study but my main concern on is its presentation. I feel this paper can become a good paper if the weakness and questions mentioned above can be addressed. I currently rate the paper at the borderline but I am willing to reevaluate the paper based on the authors' response.
"	3
"The idea of exploring adversarial robustness of distributional RL is interesting and novel, but the analysis presented in the paper are marginal, disconnected, and does not support the understanding of the robustness of distributional RL. 

===== AFTER REBUTTAL ===    
Increase the score to 3"	2
"The paper studies stocastic convex optimization in shuffle model; however, to my understanding, most of the results are known or are folklore. Also, they fail to compare with the most relevant and recent already published work. 

I have studied the proof and they are correct (more so, because most of them follow the standard ideas or the results follow from previous works)."	2
Overall, I recommend acceptance. This paper is well-written and has significant advances in the field of shuffle differential privacy. One possible weakness is unclearly in the tightness of bounds in the sequential model.	4
Overall I greatly enjoyed reading the paper and think the results have a lot of potential for impact, both theoretically and practically. In particular the vector sum estimator provided by the authors is a tool that I could easily see being used by other papers to establish results in the shuffle-DP setting, and even just focusing on the error bounds, the optimal excess loss in the full shuffle model is quite a nice result, and the results for the sequential shuffle model also nicely improve on the results for SCO in the standard local DP model. In addition, the paper is nicely written and easy-to-read, and in turn ready for presentation at a major conference. For these reasons I recommend accepting the paper.	3
"The paper is interesting, well-written, and seems correct. It is a solid work, but would be greatly strengthened by providing *tight* bounds for sequentially interactive algorithms and/or strengthening the motivation for sequentially interactive algorithms. Additionally, there are a number of minor issues (described above) that should be addressed. I consider the paper marginally below acceptance threshold in its current form, but would be open to recommending acceptance upon satisfactory revision. 

__

Edit: The authors have addressed my concerns and revised the manuscript. I recommend acceptance. "	3
This is a well-written paper describing an idea of substantial interest to the physical reasoning and the visual reasoning communitties. I only have clarifying concerns with this work and feel the paper will be well-rounded if the limitations of the work are discussed upfront.	3
The paper presents mainly two new ideas, the RISP network for state estimation and the gradient loss for regularization. Both are novel and can be interesting and impactful for the community and push forward the state-of-the-art in using synthetic data for parameter estimation. The quantitative and qualitative experimental results on synthetic data show clear improvements compared to the previous methods.	3
This paper is a solid submission, which solves an important problem and proposes some novel ideas. I expect the authors to evaluate the method on more datasets, especially real datasets. 	3
Although this paper firstly studies new problems of online NDPPs, it has a lack of algorithm novelty, theoretical analyses as well as writing quality for addressing their methodology. Hence, the paper should be improved for acceptance.	1
Overall I think the problems that this paper studies are interesting and the proposed algorithm are effective.	3
"This paper proposes the first analysis of MAP inference and learning of nonsymmetric DPPs in a streaming setting; the authors propose novel algorithms, provide guarantees and complexity analyses, and evaluate their algorithms empirically across a variety of benchmarks. Startlingly, the authors show that their online algorithms are competitive with (and often outperform) their offline equivalents.

My main concern with this paper is novelty: there is significant overlap between the MAP-inference section of this work and previous work by Bhaskara et al. (2020), both in terms of the key ideas (using a stash) and in how the algorithms are analyzed. If this overlap is only in appearance, the authors should discuss in detail where their contributions depart from this previous work. Currently, it is difficult to understand the extent of the novelty of this work."	3
This paper has strong contributions in the area of streaming and online MAP inference algorithms.  However, there are some notable issues with the contributions regarding the online learning algorithm, including the comparison with prior work, and the correctness of the approximate optimization objective (Eq. 4), as described above.  Thus, unless the authors can address these issues in the rebuttal, it is hard to recommend this paper for acceptance.	3
"The work is reasonably sound overall (if somewhat incremental) but is let down by the slightly unclear set of ablations and discussion of the empirical results. I would consider raising my score if the authors can compellingly clarify what conclusions can be drawn from their study.

----------------------

I believe the authors have addressed all of my concerns in their responses and updates to the paper."	2
"This paper presents an approach that addresses an important problem in RL, namely how to act optimally in an environment with varying actions. The proposed method seems to perform well against the compared methods. Though more baselines are needed, especially from closely related methods to draw conclusions. Evaluation is done using a selection of benchmarks and application domains which I view as a positive. The architecture is described in sufficient detail, though some sections need further clarifications. 


[1] Huang, S., & Ontañón, S. (2020). A closer look at invalid action masking in policy gradient algorithms. arXiv preprint arXiv:2006.14171."	3
The key idea of the paper is illustrated well but many details regarding the approach and the experiments are left out.	3
"This work has much to admire, and could make an important contribution if the uncertainty regarding its hyperparameter tuning didn’t cast such doubt on the experimental results.

--- POST-REBUTTAL UPDATE ---

The authors have provided much more information about their hyperparameter tuning, and have performed additional experiments which provide additional insights. These changes address my concerns, and I have revised my evaluation.
"	3
"The paper presents a new rotation equivariant transformer model for atomic prediction. The resulting method obtains good results on 3 popular datasets, while being computationally efficient at inference time. The paper also provides a nice analysis of the attention weights that shed light on the inner workings of their model. While I would recommend the authors to clarify this analysis for the final version, I am leaning towards accepting the paper in its current form.
"	3
"1. The introduction is well-written and relates the work to previous works.
2. Sections 2.1 to 2.4 are not detailed enough. Considering that these Sections is at the very center of this work, they deserve more details:
   1) is the function $a_n$ defined anywhere? 
   2) where are the matrices (?) Q and K coming from?
   3) how is this attention matrix A computed?
   4) how did the authors arrive at the update rules in equations (8) and below?
3. Specification of the training details is welcome.
4. Did you consider using the updated version of MD17, rMD17, for your experiments? Why did you decide against using the updated dataset?
5. There is a typo in Table 3 (malonaldehyde).
6. In the abstract, the authors claim to have gained ""valuable insights into the black box predictor"". In the manuscript, the value of their insights is not properly laid out. What did the authors learn about the model that would help them improve it, for instance?

# Update

The authors updated the manuscript and addressed my questions. For this reason, I am willing to increase my score."	3
"The presented approach shows iterative improvements over previous work. Unfortunately, at the current stage it is not clear whether those are due to the ET architecture or the increased model size. The analysis of attention scores is promising, but still lacks the necessary interpretation.

Update:
Based on the response of the authors, I have raised my score."	2
Overall, this paper is a good paper with strong experiment results on various datasets. The components of the model look very interesting. Possible improvement include more discussion on model components and comparison with more baseline algorithms.	3
While this paper offers an interesting angle into OOD detection with few shot examples, several flaws regarding writing/methodology/results must be addressed before this work is ready for publication.	3
The paper proposes an interesting method for combining generated OOD that is customized to the training data with the general natural outlier images for outlier exposure. However, the paper needs to be further polished such that the method and results can be clearly presented. 	3
"The paper touches interesting problem and proposes a valid solution, however lack of comparison with adversarial and few-shot open-set methods raises some questions as to novelty and quality of the experimental results. Additionally, experiments on more modern datasets should be performed, since CIFAR-10 and SVHN are quite simple.

Post-rebuttal: The lack of deeper discussion on related work, the lack of comparisons and modern datasets still causes me to keep my initial score. I believe more work needs to be done in order for it to be accepted. "	2
Generating low-confidence samples from random vectors is a common practice. All the terms in Eqn. (2), which is the key component of the proposed method, are trivial. The technical novelty and contribution of the proposed method is somewhat weak. 	2
I thought the authors consider an interesting problem in this paper. Negative transfer in federated learning is a challenging problem and if you can avoid this issue by building a local model, that would be great! However, there seems to be several issues with the proposed definition. Moreover, the methods work under certain assumptions and I am not sure the assumptions are realistic.	2
There is a lot of related work in the cooperative game theory and federated learning literature.  I think the authors need to do a much better comparison with the existing literature and propose why their work is different from it and carry out clear comparisons (in their experiments) to show the efficacy of their approach. 	2
The introduced model does not seem to be complicated enough to capture the real details of the problem. Further, the algorithms and guarantees introduced do not seem to be rigorously justified. 	2
Overall I think this paper proposes a novel theoretically principled method for computing stable outcomes of data sharing or collaboration in general, which can also be applied to personalized federated learning.  Some of the theoretical results appear quite insightful.  The experiments are quite informative and support the main claims of the paper.  (Since I'm not familiar with the literature on federate learning, my evaluation regarding the novelty could be misled.)	3
The proposed algorithm is an interesting way of learning end-to-end meta-features across tasks to improve the efficiency of Bayesian Optimization on new similar tasks. The benchmarking experiments are extensive and convincing for the efficiency of the algorithm, but does not support clearly the claims about negative transfer. The latter point should be discussed more clearly. The additional hyperparameters of the algorithms (network size and REPTILE hyperparameters) should also analyzed empirically to see how importantly they affect the performance of the algorithm.	3
Overall, the authors propose a new method that is well-scoped within existing literature, and provide positive empirical results on a large-scale benchmark against an adequate set of baseline tasks. It is a reasonable paper for acceptance.	3
Overall good, well-written, self-contained article that presents a novel combination of existing techniques, and addresses the issue of negative transfer when using transfer learning for Bayesian HPO. The experiments conducted are well designed and support the conclusion.	3
This is an interesting paper, but its novelty is limited. A more detailed analysis of the proposed method (in particular, ablations that use a deep kernel GP with different learned meta-features) would increase the impact of this work.	2
"The proposed method, DKLM, for transfer-learned HPO seems interesting and is, as far as I'm aware, novel.
The experimental evaluation seems to demonstrate benefits of DKLM over prior work.
Unfortunately, the paper is (sometimes severly) lacking in clarity, and the draft requires significant further work before being, in my opinion, acceptable for publication.
The exposition leaves me with too many open questions as to how DKLM actually, as well as some of the experiments, actually work.
Further, one of the main contributions of the paper – avoiding 'negative transfer' – is not discussed sufficiently and not evaluated at all, and I have trouble following the authors in why it is appropriate to say that DKLM learns ""landmark meta-features"".
If the authors can address these main points of my review, I am happy to reconsider my score.

Thank you for including reproducibility and ethics statement. Please make code for reproducing experiments available as soon as possible.

"	3
"The proposed method exploits the existing OLE loss to the few-shot learning problems. The method, which imposes class-wise orthogonal distribution in feature space, is elegant for generalization to the novel classes, and the evaluation is satisfactory. 

==
Post rebuttal
==

Most of my concerns are addressed. 
However, I have degraded my score slightly because the novelty is incremental. 
"	3
Despite interesting perspective, this paper lacks detail on the methodology and ablation study. Meanwhile the performance is not convincing enough. 	2
The primary concerns of this paper are the weak performance and limited novelty. Though the usage of meta-learning is interesting, the proposed learning strategy is borrowed from the existing method, and the final model performance does not surpass the existing techniques.	2
"In short, the term 'anomaly detection' has been applied to a wide range of unsupervised techniques, with little in common among them.  This literature, using deep learning methods that have no connection to the existing literature that is more than a decade old, calls into question whether the proposed methods are competitive with previous methods that do not originate by applying deep learning techniques.  One really has to look at the history of the field to claim to have made an advance. 

My apologies for a shallow review due to a lack of familiarity with this branch of the field, however there are some basic questions that need to be raised about the lack of connection of the field with the vast work that precedes the current trend in deep learning to really understand the significance of the work. "	2
"The paper proposes an interesting anomaly detection approach and an experimental evaluation that also involves fully supervised models.

Results are competitive, and some of them appear close to the upper bound performances provided by supervised alternatives.

The authors also provide an exhaustive description of the architecture and hyperparameters in the Appendix section.

I think the main merit of the proposed method is the ability to achieve satisfactory anomaly detection performance without large availability of normal samples while adapting to new tasks."	3
"- Good industrial use-case paper
- Limited technical novelty -- less innovative combination of known methods. "	2
To summarize, this paper proposed a fast adaptive anomaly detection framework for anomaly detection in images. The proposed method is technically sound. However, the novelty is limited on the adaptive space coding layer with receptive field, while the rest of the network structure seems to be a big melting pot incorporating the Energy Bassed Model and episodic training in the context of meta-learning based few-shot learning. 	3
The paper mixes existing techniques in a nice way to make the overall solution very effective.	3
"In order to better understand the improvement coming with the proposed method, it would be useful to have a deeper understanding of the reason for which it is indeed beneficial. More details on the differences between the models f and g are to be provided: different random
initializations are probably involved as well as different batch shuffling. I guess that mainly the g models are going to converge faster on the training data, presenting more variance in the performance for non member data, which in turn makes it easier to find an effective threshold for the MIA problem. However, this does not seem effective on some datasets. Could the authors investigate those issues and shed more light on the relation between the type of data and the success/insuccess of their method?"	2
I like this paper’s focus on reducing high false positive rates. However, unless the authors clearly explain the difference with prior works (Long et al. (2018), Carlini et al. (2020)) and show the advantage over other attacks (Jayarama et al. (2021)), I suggest not accepting this paper.	2
A nice paper with an idea that is not entirely novel but with a rigorous empirical analysis.	2
The theoretical reasoning the performance of the proposed calibration method is not provided yet necessary. 	3
Overall, the idea is novel and interesting. The results look promising. However, there are some critical details missing. Further clarifications are needed to understand better the pros and cons of the transformer-based scheme as compared to the prior work. 	2
"I think this paper is ""marginally above the acceptance threshold"" since the use of transformers as entropy models for image compression is novel and interesting. The paper needs a more thorough empirical evaluation to warrant a higher score."	2
The paper present a novel contribution by using first Transformer based method to image compression task. Experiments validate the performance of the proposed approach named Entroformer compared to CNNs based methods.	3
While the paper presents an important and technically sound contribution, the quality of presentation dramatically reduces the value of the manuscript.	3
The results are interesting. I feel the authors should make a more thorough investigation of recent work that highlight similar ideas and yield better results on more complex and large-scale datasets. I am giving a rating of 5, but I am willing to chnage my score if the authors can convince me of the scalability and the novelty of their approach.	3
The presented method could become important for further studies on Deep SNNs, as a replacement of poorly fitting weight initialization schemes borrowed from ANNs. It remains to be shown that the method scales to more difficult tasks and larger networks. If the method still works there, this could be a paper with very high impact.	3
Overall, it is a nice paper with solid theoretical basis. It has the potential to convince researchers to adopt SNNs and exploit learning of deep SNN. I consider to raise my score if the authors can resolve the concerns above.	3
"In short, I find the paper very well motivated, the problem is very important, authors show a familiarity with related work, and the method is sufficiently novel.

However, I do not find the claims in the paper to be well supported. Specifically, I don’t find enough evidence to support the fact that adapt-then-combine is the issue preventing convergence of previous methods, and that the proposed method resolves this issue by using a combine-then-adapt strategy, especially since:
- with a simple re-parameterization, the proposed method can actually be written as an adapt-then-combine strategy
- propositions 1 and 2 showing issues with adapt-then-combine strategies provide upper bounds, but these are completely uninformative since the relevant quantities should provide a lower bound on the sub-optimality of the iterates with these strategies
It seems to me that the proposed method actually works because of the fact that consensus steps are incorporated into the first/second-order momentum buffers, not because the order of adapt/combine is flipped."	3
"I want to thank authors for their work. The heterogeneous input is very important and I think that the adaptation of Adaptive Distributed Adam to it is an important step. 
The only thing I am curios is the following: how do authors propose to calculate the global average and how long will it take in the end of the algorithm to calculate it? 
Furthermore, I think that this result can be extended to the case when instead of weighted update with neighbors we select every neighbor with some probability and communicate only among them. This can save a lot of communication cost and make an algorithm even faster.

"	3
Please refer to the main review. I'd be happy to increase my score if these concerns are properly addressed.	2
The problem studied is interesting. But the method is not novel and the theoretical analysis is not solid. 	2
Recommend rejection as the paper did not demonstrate its utility in practice, nor corroborated their abstract in a clear manner. 	3
The paper presents a simple approach to characterize robustness. Some limitations of the techniques need to be clarified for the reviewer to understand the contribution and limitations of the proposed approach.	2
"The current version is quite incomplete. It lacks comparisons and distinctions to prior arts for technical novelty, and it also lacks comparisons to empirical robustness and other baseline methods.

"	1
"Reasons for score: 

 
Overall, I vote for rejection. I like the idea of measuring the probability of random inputs being adversarial 
instead of finding the extreme case. However, I think whether the proposed method is measuring such 
probability is questionable. Some notations and terms are unclear, so it is possible that I missed some 
components. If the authors can address my major concerns in the rebuttal period, I'm ok to increase 
my score and accept the paper."	2
My evaluation for this work is that it is below the acceptance threshold, but I tend to give a score 4 (but I don't have option). Major reason is the posing of the DLTH claim and the contribution. I also think the writing could be fundamentally improved. I still think the technique and the idea are interesting and encourage the authors to try the next venue with enough improvements.	2
"The paper's main claim as presented seems to be false --- it is expected not to hold true that *any* randomly picked subnetwork is transformable to a network as performant as the original dense network if trained. 
For the particular method presented (RST) dependency on cycles leaves utility / insight unclear.

*Updated review in light of author responses [Nov 26, 2021]:* see above update. claims corrected, RST novel contribution, experiments for high sparsity in order to address high-sensitivity regime expected. 

"	3
My final score will be mostly based upon discussion with authors regarding my points outlined above. Currently, I think the paper is somehow off-topic, but I am open to hearing from the authors and willing to change my view if I had made mistakes. 	3
"Overall, this is an interesting and good paper. If the authors can address my concerns in the rebuttal phase. I will consider making the recommendation for the paper. 

-----------------------------------------------------------------------------------------
Most of my concerns have been addressed by the authors. I change my score and recommendation. "	3
Overall, it's interesting to investigate the proposed Dual Lottery Ticket Hypothesis. However current results are a little bit insufficient to validate the DLTH. Specifically, no winning tickets have been identified in current experiments and whether DLTH can be extended to the setting when layer-wise sparsity level can also to randomly arranged.	3
The problem of this paper is interesting, but the experiments are not sufficient to support the claim. Please see the main review for the details.	3
"The paper has a lot of pros and cons. The cons look solvable (clarity mostly), and the pros seem solid enough. The practicality of the approach, the thoroughness of experiments and comparisons, and the excellent performance attained are all very positive. As already mentioned, a good rebuttal that shows clearer explanations will mean I raise the score.
"	4
The paper combines the recent advances of efficient (low FLOPs and low latency) networks and some other techniques such as knowledge distillation, conditional conv (CondConv), and highly-engineering activation function. What is the true novelty is somewhat unclear to me. Also, the methods are not clearly written and how different modules work is confusing.	2
Good results, but lack of ablation study of each proposed components.	3
The idea of the proposed method seems to be effective as supported by the experiments, however, it lacks disccusion and comparison to self-training methods that are based similar motivation, i.e., globally and locally alignment should be achieved simultaneously to achieve good DA performance. There are also many unclear issues that require further validation. Therefore, the paper is currently with more weaknesses compared to its strengths.	3
In light of the aforementioned consideration, this reviewer believes that the paper has some merits but, the lack of a proper presentation, and the fact that there are not enough experimental results to support all paper claims, result in a submission that is not solid enough to justify an ICLR publication. These are two very important weaknesses that are likely to require more than a single major review round before the paper can have a proper shape. Thus, this reviewer is considering the submitted work below the acceptance bar but would be happy to change the opinion in light of more/proper evidence.	3
"Overall, I am wondering about the entire motivation of this paper. I acknowledge the strength of the proposed modules. However, these modules lack some novelties in my opinion. They are like the combination of some typical modules in a generative model. Thus I tend to choose marginally below the acceptance threshold.

I would like to upgrade the rating if the author addressed my concerns."	2
This paper introduces a new approach for cross-domain object detection but is somewhat weak in technical contributions.	3
"The paper investigate the interesting problem, but more rigorous experiments and clear statement stated in the paper. The improvements of the paper are not substantial and few of definition and hypothesis are vague and not well-supported. 

Details of Cons

* The key concern about the paper is the question being tackled in the paper is unclear, and the lack of rigorous experimentation to support the hypothesis.  For example, the purpose of investigating interplay is not well-stated. Why is it important and useful? Also, after reading the paper I do not understand the conclusion after investigating the interplay between KD and DA?
 
* The main things shown empirically in the paper are that DA is helpful to improve accuracy, which is different from the claim of the paper in understanding the success of KD via the DA perspective. rather than that, no other insights about understanding more about KD or fundamental reasons why DA improves KD are not clear (at least no empirical support). We observe data augmentation can improve most training models, it is reasonable and not too surprising it will improve the student model. The question is that the student model is improved because of simply more training data to be used or because the soft label provided by the teacher makes the training better (we observe in the baseline CE, simply adding more data performance gets improved)? One simple experiment to demonstrate that is to modify the current model using the soft-label of the original samples as the soft-label of the augmented sample in KD training. 
 
* Another concern about the performance is that the improvements are minor compared to the original KD and most of the experiments are less than 1%. 

* There are also concerns about the hypothesis (Eq. 2). I believe it would need rigorous experiments to demonstrate this because it depends a lot on the network architecture and dataset size as well as the data augmentation methods and hyper-parameters. Without that, it is difficult to make the final conclusion. Moreover, I wonder why and how this hypothesis is useful for the community to investigate? I understand when we have more training data provided to the model it will take more time for the model to  converge. By the way, not sure how the authors defined a “stronger” DA? It looks like the “stronger” one will give better accuracy to students which may be vague. 

* As claimed in the paper, the “stronger” DA will need to train more iterations to achieve good accuracy. Does the author consider the combination of multiple DAs together a “strong” DA?

* In the framework, can the author explain the reason to keep x and x’? Does that help to boost the performance? As in Eq. 1 we already have the CE part using the original samples x? 

* Many related state of the art data augmentations are missing: 

[1] AutoAugment: Learning Augmentation Strategies from Data.
[2] RandAugment
[3] Fast AutoAugment.  ​

Minor comments: 

Figures and plots are not high quality. 

Eq. 1 is the combination of CE and KD; the hyper parameter $\alpha$ may lead to different observations and behaviors. Would be good to study on that."	2
"The analysis and proposal methods covered in this paper are interesting in terms of performance improvement.
However, it is unclear whether the DA algorithm is for knowledge distillation only.
In addition, it is necessary to analyze whether the performance improvement is due to augmentation or a large amount of learning.
Data augmentation seems to be the most important contribution point of the authors, so it seems that clear verification is needed."	2
Although the paper has several strengths, I am mostly concerned about the lack of arguments supporting the main purpose of the paper and the technical depth of the paper. Also, the related work and the experimental analyses of the paper can be further improved.	3
I like the idea of maintaining dynamic isometry during pruning. But there is still great room to improve in both the presentation and analysis. 	3
Overall the paper is interesting. The connection to BN regularization and DI could be explained better to improve clarity.	2
This paper has moderate novelty with the well-studied kernel orthogonality method. Besides, the experiment design is not sufficient. The performance gain in ImageNet is incremental and not sufficient compared to the state-of-the-art methods.	2
The paper is well written in general, and the proposed methods are intuitive. However, the proposed claims are not explored well. More precisely, the relationship between the proposed major claims should be analyzed in detail. Otherwise, the scope and claims of the paper should be updated accordingly. To this end, the paper proposes regularizing learnable parameters of NNs using different heuristics. Therefore, the novelty and scope can be redefined according to provided analyses and results.	2
The paper carefully studies recent thoughts on the meaning of pruning and made some rigorous investigations. Though some issues exists and DI may not be the ultimate solution to the question, the paper provides valuable thoughts for this filed, including fair experimental comparison and new explanations. Therefore I recommend to accept.	3
There are many unsubstantiated claims (or weak statements) in the paper. 	1
"I like this paper in that it thoroughly investigates existing arguments on the value of pruning weights. The perspective of OrthP is not that novel, but is simple and effective on MLP under large pruning ratios. 

My major concern is in the experiments design and results part. I would consider raising my score if my questions can be addressed.
"	3
"This paper does not make a solid case for the hypothesis that ""pruning+finetuning produces models with better generalization than training small models from scratch"", as there are issues with both the experimental and the analytical studies. In the experimental setup, the comparisons are not performed fairly, while in the analytical parts use flawed metric to measure model convergence and dynamical isometry. Even fundamentally, it is unclear why the proposed hypothesis must be true in the first place. For all these reasons, I would recommend rejection.

Post rebuttal update: The discussions did not change my opinion of the paper. However I appreciate the efforts taken by the authors to update the paper in light of reviewer suggestions, particularly to make the core argument more rigorous. Although I do not think the paper still succeeds at this, I am increasing my score slightly to reflect this change."	2
"The authors argues contribution of parallel subnetworks, while the experiments show that the traditional RepVGG-wise blocks adopted in ParNet contributes significantly more to prediction accuracy. Therefor, my rating is ""5: marginally below the acceptance threshold"".
"	2
Please erase the above concerns. 	3
- See the main review	3
The paper shows a significant result that a network with 12 layers can be competitive with deeper networks on well established image classification benchmarks. However, the architecture is more complex than ResNet or RepVGG, and the empirical evaluation is unsatisfactory due to weak and missing baselines, and the lack of statistical significance analysis. I hesitate between reject and weak reject.	3
"There are two axes that have to be evaluated in this paper: the mathematical value of the authors' contributions, and the positioning thereof. Regrettably, my assessment varies drastically along these two axes: while I find the authors' results mathematically interesting and suitable for ICLR, the overall narrative is encumbered by a series of vague and often confusing statements that are (a) detrimental to the paper; and (b) not needed in the first place. Thus, while on the mathematical axis I consider the paper to be a good fit for ICLR (I would rate it between 6 and 7), the reported results do not suffice to support the paper's (overly ambitious) narrative claims, hence my overall reject recommendation.

The detailed scores below do not refer solely to the paper's technical proofs, but they also take into account the various non-mathematical claims made by the authors throughout the paper."	4
The paper introduces some interesting examples accompanied with relevant commentary. The examples are simple enough to be easily understood, but may leave a bit to be desired in generality.	4
The paper has interesting observations including  for SGD, which are not carefully discussed in literature. Personally the reviewer would like to see the paper published on important venue like ICLR.	3
See the comments above.	2
"I think this paper provides an interesting and novel view on SGD, and brings forward scenarios for which SGD behaves differently from GD and from the intuition provided in previous papers. This is an important step in understanding the optimization process of SGD, its limitations, and its benefits. There is one issue which I would be happy to see the author’s response. There are also some issues with the presentation which I think can be fixed. In total, I believe the merits of this paper outweigh its flow and it should be accepted.
"	4
"Out-of-distribution generalization is one of the most pressing, largely open problems in the field. Concretely, the IRM framework has recently emerged as a promising line of research in this direction, but its characterization is far from conclusive both from a theoretical and a practical standpoint. Thus, I consider the paper's topic timely and of great relevance to the field.

I believe the manuscript  explores an interesting idea and is technically sound, albeit not without what I consider to be important limitations, namely,
+ The proposed approach is only applicable to the MSE loss.
+ It is unclear whether it would be sound to use the proposed penalty in a stochastic gradient-based optimization setting, as it requires backpropagating through the optimal linear regressor, which depends on all data.
+ The experimental results do not provide compelling evidence that the proposed penalty is statistically significantly superior to the original formulation.

All in all, I believe IRM to be a promising yet not fully mature idea with many crucial open questions remaining to be answered. Thus, I consider exploratory ideas such as those presented in this article to be worthwhile despite potential shortcomings that might need to be revisited by future work. Hence, despite the aforementioned limitations, I lean towards supporting acceptance of the manuscript. 
"	3
"Specific Comments
1）The similar theoretical result in this paper has been obtained in [2]. In other words, under the same condition, the theoretical result also holds for IRM. It confuses me whether the proposed method is essentially different from IRM.
2）It seems to me that the proposed invariance penalty doesn’t solve the problem of IRM. This paper doesn’t guarantee that the proposed IRMv2 can recover invariant predictor at least in the theoretical scenario where IRM fails.
3）I suggest that a detailed comparison with other alternatives for IRM should be given to better illustrate the strength and weakness of the newly proposed invariant penalty.
4）There are a few places that are not very rigorous. (1) In 5-th line of Algorithm 1,  is computed according to Eq. equation 4. However, the distribution is usually unknown and expectation can’t be computed. (2) In 6-th line of Algorithm1,  is computed according to Eq. equation 12 instead of Eq. equation 11.

[1] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.
[2] Elan Rosenfeld, Pradeep Kumar Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=BbNIbVPJ-42."	2
The idea behind this paper, while nice, is a repackaging of IV regression, and does not adequately analyze the implications of the objective (nor do they properly contextualize it within the great deal of similar related work in other fields). They provide no new theoretical justification (in fact their main technical result, very similar to another work, simply shows that this algorithm performs on par with the existing IRMv1). Further, their explication of performance in the non-linear setting is incomplete.	1
This paper analyzes the counter-example of IRM and proposes a new invariance penalty based on that. Although some intuitions are provided, it is unclear why the proposed invariance penalty is necessary. And baselines, as well as the empirical results, are not enough to support the method.	2
The observations are novel, the proposed model is simple but efficient, and the experimental results are impressive. However, the description of some preliminaries and implementation details is not clear enough.	3
"The paper is definitely very interesting and is fun to read. I think it would benefit from experiments on datasets which are not multiple-choice. 
Also, there are applications where the learned entity representations from GNNs are used for downstream tasks. Since this method is advocating for removing node embeddings, there should be a discussion added on how to handle those situations (Note: I personally think removing entity representations is a good idea as it has lot of advantages such as generalizing to new entities etc, but for completeness, I think a discussion regarding the same should be added)

======Update 11/26======
Most of my concerns have been addressed through the author rebuttal and I am changing my score to accept."	3
See my main review.	2
I am currently a little concerned about why the method achieves such high performance, but I would definitely increase my score after my concern/questions are addressed.	4
Overall the paper is well motivated and the problem is interesting. However, the approach is not compared to any previous works and is fairly simple (e.g., predicting a binary value if two objects are related or not). With some revisions and additional experimental comparisons, I think the paper would be much stronger.	2
In summary, the task of this work is interesting and I think it gives some insight into exploring the general functionality of objects. The paper is well organized. I would be inclined to suggest accepting it if the motivation and some technical details could be clarified in the revision.	3
"Essentially, the problem of inferring inter-object functionality boils down to link prediction tasks, but the meat lies in the fact that supervisory GT signals are mined from the outputs of a developed system within. 

I give credit to the self-supervision part, but then, the problem itself pivots towards the theme of extracting priors from data for learning a different task. Plus, the evaluation is not reliably interpretable with the results shown. As such, I am sitting on a borderline score for this paper."	2
The paper introduces a novel and relevant problem. In the process, the authors construct a dataset for experimentation based largely on previous ones. The paper presents a strategy to solve the problem while the experimentation showcases different aspects of the solution. Finally, the authors promise to make their code and dataset available upon acceptance. Still, some issues need to be addressed to clarify the message in the paper further.	3
"Could the authors show results of the baseline suggested above?
Could the authors explain the rational of the exploration reward?

It is possible that I am missing something from my understanding of the paper. I will be careful during discussion period to clarify any misunderstandings.



Post rebuttal : the authors have put together the requested baseline and they show significant performance margins over it. Thank you very much for this, I raise my score accordingly.
"	3
This paper has some interesting elements to it and their approach is validated by both real and simulated results.  While the task isn’t very novel, they at least validated their approach to show that it does better than previous approaches thus contributing to the field.  They also do a good job of explaining each of the steps in the appendix to make it easy to understand what exactly they are doing.  For these reasons, it merits inclusion in the conference.  	2
The paper proposes to solve a new problem of long term action trajectory generation for 3D articulated objects, but the method to solve the problem is more about an extension and combination of existing work. The overall quality of the paper, writing and experiment, is good.	3
See main review above. The paper does have a lot of potential, but perhaps it is not fully mature at this point. 	3
This paper provides a reasonable method for graphical modeling of dynamic systems, although the technical novelty is limited.   	2
"This is a fantastically written paper with an effective model for an important problem with solid theory and outstanding empirical performance. 
"	3
Interesting work, however the experiments section lack of qualitative results regarding the proposed approach (i.e. multimodality benefit is only quickly showed for one dataset in Figure 1)	3
I think the manuscript considers a subject of high current interest. The contribution is mostly theoretical, laying out conditions under which a pdf from a family exists (which is a fair question to consider). While I don't have any technical objections, the attention mechanism looks computationally costly, and I wonder if this is going to be an issue during training in practice. Also, I found the experimental evaluation weak, and hand-wavy. I would have expected a clear demonstration of how the method is used, what its computational cost is, and how it improves upon existing alternatives.	3
The proposed model in the paper seems a straightforward extension of existing works by Martins et al. (2020; 2021). Hence, the novelty of the paper is limited. 	2
"The paper definitely adds to the discussion in Federated Learning and by introducing an interesting new setting of hybrid local SGD. Therefore I am leaning towards accepting it. However while the analysis captures the important tradeoffs, the experiments are a bit limited and seem slightly unfair to the local SGD baseline. If these can be rectified (following my suggestions above or otherwise) I would be completely convinced about accepting it

Comments after Rebuttal: Thank you for adequately responding to my queries. I am satisfied with the revisions made to the paper and have updated my score to reflect the same."	3
Overall I think the proposed method is a natural extension from the previous work. And the novelty is quite marginal. The experimental evaluation can be largely improved.	2
The proposed algorithm is novel and the the performance is rigorously characterized. The improvement compared to existing algorithms is significant and hence I believe it should be accepted for publication. 	4
The theoretical study has merit while the empirical study is less convincing,   	3
The paper provides many useful implementation details for RNN based RL methods.	3
I'm recommending this paper with some reservations. Overall, I really think work like this is valuable and illustrates a large gap in knowledge that was previously unaddressed. I've always found it frustrating that the approaches to partial observability are so ad-hoc. I also appreciate the clarity with which this paper addresses the classes of POMDP and the considerations required for each. That said, I have some concerns with the thoroughness of the experiments and would like to see more baselines and fairer comparisons.	1
Good direction, results are inconclusive.	3
This paper proposes a nice framework to combine goal-conditioned RL and waypoint selection. The method is neat and effective. Though the connection with the HRL is unclear from the paper, the advantages outweigh the flaws. I tend to accept the paper.	3
"

This is valuable progress on the field of learning goal conditioned policies. curriculum learning have been a challenge and this work shows a clear advance and solid theoretical connection."	3
"I find the proposed method to be a nice step-up from existing prior work, both theoretically and in terms of performance. The proposed incorporation of planning (in terms of waypoints) appears well-justified, and seems to enable solutions to a variety of goal-reaching tasks that are not trivial. Heuristics also seem to be mostly avoided, which is appreciated.

I would recommend acceptance."	3
The paper presents a non-trivial extension of the previous work and presents an interesting viewpoint of formulating implicit train-time searching as variational inference. The paper is clearly written and easy to follow. However, I found that the experiment section is relatively weak and the presented results are less convincing. 	3
"The problems that this paper attempts to solve are interesting and would add value to the setting of unsupervised skill discovery. Specifically, it considers the question of what the objective of such skill discovery should be.
However, the geometric tools introduced in the paper and the subsequent analysis does not seem polished enough and is hard to follow. I am unconvinced of the analysis and thus scoring the paper conservatively."	4
Overall, I really like the paper and think that the results are important and should be presented to the community. However, I do think the paper could be improved with even a small empirical study.	4
The paper analyzes some of the challenges of unsupervised RL through a geometric approach of analyzing the space of learnable policies. The approach allows insights about the characteristics and certain optimality criteria that applies to skills learned through mutual information type approaches. 	4
This paper is written well and the idea sounds very interesting and promising. However, as stated in the review, I'm not fully concerned by the log reward technique. Overall, I think there is more merit in this paper than the flaw so I recommend weak acceptance. 	3
"The reviewer recommends rejecting the paper. While the idea is promising, the submission in its current state needs substantial improvements. Addressing the outlined concerns might increase the overall score.
"	2
The overall paper is motivated and clear, The concern I have is the advantage over previous works that address exactly the same problem with relatively simple mechanisms.	2
A good paper that introduces a novel model-based RL method, where the policy learning and dynamics modeling share a unified objective.	3
The authors' treatment of inductive bias in extrapolation is interesting and will interest many researchers. The proposed protocol, which takes its ideas from psychology, is also interesting. However, we are not convinced that the proposed protocol is practical.	2
The paper is ambitious but falls short in clearly explaining the framework proposed, and substantiating the idea with strong experiments.	2
"The authors proposed an interesting approach for measuring the rule- and exemplar-based  generalization for a given learning system. The perspective is novel and the writing is clear. My only concern is the practical utility of the proposed measures.

"	3
The paper is unclear based on the current presentation.	2
The wrapper of benchmark is a valuable contribution by itself, but this paper illustrates well the benefit of such benchmarks by providing insightful analysis of NAS algorithms. 	3
The authors did much engineering efforts and emprical analysis, whereas the technical novelty is a little bit weak.	2
I think the paper is of good quality and will contribute to the NAS community.	3
"In my view the two main contributions of this work are (1) showcasing the limitations of drawing conclusions from single benchmarks; (2) providing the community with a larger analysis tool.

I expect that the community will welcome these resources and thereby **recommend the paper is accepted** (provisional score: 8)"	1
I think the results of the paper are relevant however, I think that the exposition needs to be improved for the main paper, as currently it is hard to follow. 	3
DL-based PDE solvers are important and exciting direction of deep learning. The current paper provides several insights about the statistical properties of DL-based solver. Although there are several limitation (mentioned above), I think the contribution is significant enough for a ICLR publication and many researchers in the ICLR community will find it insightful (at least for me)   	3
Valuable technical advances on an important and popular topic.	4
In general, this paper is a good paper and provides solid theoretical results on solving PDEs by deep neural networks. But the writing and organization need to be improved.	3
Overall, I am positive about the work and I find the idea novel in the CMR setting. I am rating this paper with a 6, since I would like the authors to clarify some experimental details, but all in all, I expect to keep my positive score.	3
The problem setting is interesting and the qualitative results in the paper are impressive. However, the experiments appear not sound enough to support the necessity of the approach and important baselines seem to be missing. Hence I lean towards rejecting the paper.	2
"Overall, I do not find the major claim of exploiting 3D generative priors for single image mesh prediction clearly validated in this paper. The presented method still requires 2D mask and camera pose given as input, and ends up being similar to test-time optimization with existing methods. Moreover, the evaluation and comparisons are not entirely convincing. Therefore, I recommend reject.
"	2
The paper proposes an interesting idea for single-view 3D shape reconstruction using 3D shape priors built-in a GAN model. However, the experiments and comparisons are not convincing enough to support the major claims. A few technical details also needs to be clarified.	3
"The authors suggest a useful way to analyze loss function selection, but experiments are not persuasive.
I recommend expanding the evaluation part in terms of the number of datasets and real deep learning models with a sufficient number of layers."	3
Overall, the novelty of this paper is rather limited and the relevance with previous work is not clear enough, hence I would vote for rejection. I would encourage the authors to consider more recent research on deep AUC maximization, and possibly elaborate a more systematic empirical study on the new surrogate AUC methods. 	1
See above. 	1
The model is not convincingly valuable, either as one for study or for practical use. Almost exclusively using a single synthetic dataset with a brief paragraph on a small real dataset is insufficient to draw any conclusions.	2
It is a solid work, though the connections to [1][2] are not clear.	3
The authors adopt methods from physics and investigate local minima of  loss landscape. The methods are interesting and insights are useful. However, the current presentation is unsatisfactory and needs major improvements.  Sections of the paper seem disconnected or repetitive, and the methods sections lack details. 	3
I found the presentation could be improved, and the experiment results are too weak to support the paper.	2
"Overall, I think this paper presents an interesting set of ideas, but strongly lacks empirical support. The experimental section is poorly explained and is too simplistic to support the method presented (the only dataset considered in the main text is a spiral dataset). Also, the paper is difficult to parse in parts, not because of mathematical formalism but because of the use of physics jargon with a lack of clarifications. 
Therefore, in the current state, I would not recommend publication, but strongly encourage the authors to consolidate this interesting  research direction with more extensive experiments.
"	3
See above	1
The paper is well written, the work is presented in-depth and clearly. I believe that the presented work is solid and will contribute as a point for constructive discussion between the HPO and RL communities.	3
Overall this paper is interesting and has I think some original ideas. I have several questions about related works and importance of aspects of the method (e.g. the ensembles) that would be good to get answered. If the authors are able to do this, I would consider raising my score.	3
I found the paper quite difficult to follow in some places, with parts of the method and experimental set ups not adequately discussed. I feel that adding an algorithm of the proposed method and signposting it to each section would go a long way towards clarifying the method. I would be willing to increase my rating if these issues get addressed, but even then I think the paper lacks sufficient novelty to elicit a strong recommendation from me.	2
The paper contributes to the filed of automated ML by proposing a novel approach to find optimal HP configurations. However, the ideas contains several issues in terms of clarity, significance, and correctness of the claims, and I thus vote for a reject.	2
To summarize, I think the paper proposes a reasonable approach for RL with training-time privileged information. But I think further efforts should be made to better justify the proposed approach both theoretically and empirically, as discussed above.	3
See details above.	3
This paper proposes a method to leverage oracle observations with a variational inference framework. But the applicability of the method is rather limited as it  poses a very strong assumption that oracle observations need to be accessed for training the policy. The empirical evaluation domains are too trivial, e.g., very simple decision making tasks and there is no challenging POMDP task employed to demonstrate the KL regularization works. There are also some problems regarding to how the methods are implemented. 	2
This paper tackles an intriguing oracle-guiding problem. The proposed method is intuitive and effective, as an application of variational models. The paper is generally well written. The concerns I proposed are mostly minor and can be easily resolved within a reasonable time. I did not capture major bugs in this paper. The experiment is complete and promising.  I am happy to vote for an accept given other reviewers do not find major issues.	3
This paper makes an attempt at an important problem - offline RL model selection, however some experimental details are missing and the results seem to be too brief and not as informative as they could be. It also has significantly smaller page margins than the ICLR required format. 	2
The authors proposed the PMS to select ORL model based on the estimated worst performance to address potential overfitting issues in ORL. Both theoretical analysis and empirical performance have been conducted. The overall impression of the submission is that the authors should carefully check the writing and present more complete and comprehensive performance evaluation. Better presentation of the research results will be also appreciated. 	2
The theoretical result is weak, and the exposition can be significantly improved.	2
The paper studies an important problem and provide nice solutions. However, some important related papers are missing and there are number of concerns in the experiment. Thus, I would recommend weak reject at this moment. I would like to see substantial effort, in particular the improvement in the experiments to change my score.	3
Overall, even though it doesn't introduce any impactful contribution, I think that the paper proposes an interesting method that is able to obtain very good results using only global features, outperforming methods that also combine re-ranking with local features. However, I find it difficult to recommend this paper for its acceptance in its current shape since its presentation is not adequate. Introduction and Method sections are sometimes difficult to follow and I couldn't fully grasp some of the ideas and rationale behind the design choices. Another pass in the text is required, and figures and formulas should also be updated and improved.	3
Due to the limited contributions, I do not recommend this paper to publish on ICRL.	2
The paper describes the proposed architecture proving a complete explanation of all of its parts (with the help of a complete ablation analysis). The comparison with a lot SOTA models on two datasets confirms the robustness of the proposed model. Even if the improvements in terms of accuracy are not so high (not a negative point, there is however a clear improvement) there is a huge improvement in terms of speed and memory used. An improvement for the paper would be the addition of a complete error analysis that now is not present and a better description of the entire architecture that, at present, is not very clear. Moreover, is not clearly specified if all the models have been tested on the same GPU (by the authors) to test speed and memory usage, this is an important point to specify because is one of the main goals of the paper.	3
The paper addresses an important technical direction of fusing local and global image features. However, it has several major flaws, such as: claiming learnable matching/homography when none of that is happening, suspiciously excluding previous work’s performance numbers in some cases; poor writing.	2
I hope the authors could provide some insights/intuition why transformer can generalize well from source to target. And the comparison with other pseudo-labeling approaches is still important to evaluate the contribution.	3
"This submission proposes a transformer framework for unsupervised domain adaptive classification tasks. In this submission, they conduct an exploration about cross attention layer and found that the cross attention layer is robust to pseudo label noise. Inspired by this, they construct a three branches architecture in this submission, which includes a source transformer, target transformer, a source-target transformer. Due to the robustness of the cross attention layer, the source-target transformer acts as a teacher to guide the other two branches. 

As far as I know, no similar work was proposed before on UDA classification tasks. This work is of novelty and the experimental results are strong enough to demonstrate the efficacy of the proposed method."	4
Despite the concern on novelty, overall this paper lacks justification of their method to see whether the improvement is brought by the backbone change. The authors should compare other methods when replaced with the same backbone as the proposed method.	2
The method proposed in this paper out-performs existing methods, and targets an important setting (no access to target domain or model). However, the writing is error-ridden, and the proposed method is only marginally novel w.r.t. existing works. Therefore, I rate the paper as marginally above accept threshold, conditional on the authors correcting the mistakes highlighted above.	2
"This paper indeed identifies a more practical threat model, but the experiments do not closely match the proposed ""cross-domain"" scenario, and the performance gain seems to largely come from existing technique (perturbing feature space instead of decision space). These issues prevent me from recommending for acceptance."	3
I tend to accept this paper because it focuses on more realistic black box attack settings and proposes two modules to improve performance. The design of the module is insightful and effective, but the module proposed under some models is not always effective, which limits its application and requires more adequate analysis.	3
"The paper explores an interesting idea of using graph neural networks to better encode model states, with a hope to improve the existing RL-based meta-learning method. Despite showing some promising results, there are quite a few concerns about this work. 

First, the RL-based scheme (e.g., PPO) is very similar to Xu et al. 2019. Therefore, the remaining novelty is really on changing FFN to GCN to encode more model hidden states. 

Second, it seems there could be multiple factors that impact the final accuracy, such as the encoding scheme (e.g., GCN vs. other encoders), the features (last layers vs. all intermediate states), the optimization mechanism (RL vs. alternative methods), the choice of hyperparameters. However, the paper jumps to the conclusion that the encoding scheme and features are suboptimal without giving enough evidence to show that they are indeed the culprit. The paper also changes multiple factors at once, including tuning additional hyperparameters, making it difficult to tell where the improvements actually come from. The end-to-end results are useful but they do not reveal how the proposed method works internally. To be more convincing, it would be better if the paper could provide more insights on how encoding affects the final accuracy. For example, does the quality of the GCN matter when performing the encoding? If so, how do you measure the quality quantitatively? Could the existing method from Xu et al. 2019 still work by slightly adjusting its model architecture, e.g., by increasing the model depth/width or changing the hyperparameters?

Third, the evaluation on RoBERTa-base/large appears to use unoptimized baselines. For example, the RoBERTa-large baseline reaches 88.1 GLUE score on the leaderboard whereas this paper only reports 86.9. If we look at individual datasets, the RoBERTa-large results from this paper are also constantly worse than what has been reported (e.g., 90.0 vs. 90.8 for MNLI, 64.2 vs. 67.8 for CoLA). If we take the publicly reported results as the baseline, the proposed method actually performs worse than the RoBERTa-large baseline. Furthermore, it is also unclear whether the paper follows the standard practice when setting the learning rates schedules when producing the baselines for GLUE datasets. For example, RoBERTa models are often fine-tuned with warmups followed by a linear decay scheduling. However, it does not seem to be the case that the paper includes this basic baseline in the comparison. It raises concerns on whether the improvements in Table 2 are merely an effect of comparing with a set of suboptimal learning rate schedules. To be more convincing, it would be better to compare the proposed method with the best practice of fine-tuning RoBERTa-large. 

Fourth, given that the proposed method requires training an additional policy network and GCN to learn the learning rate schedule, which adds additional difficulties to the already complex training of large models,  it raises concerns about the usefulness of the proposed method in practice. 

"	2
"My initial recommendation is 5: marginally below the acceptance threshold. I believe that this paper has great potential to become useful to the community. However, in my opinion there are some important questions that need to be clarified. I have listed my comments/ thoughts and suggestions in the Main Review. If you could please address my comments, I could consider increasing my score.

-- post-rebuttal

Thank you for your response. I will raise my score to 6."	3
"In summary, the paper proposes a novel and effective way to learn a learning rate scheduler. However, some of the investigations such as the node feature and the meta-learning rate are missing. 

-- post-rebuttal

My concerns are well-addressed in the rebuttal. I will keep my score. "	2
I believe the problem addressed in this paper (how to learn optimal learning rate schedule) is very important to the machine learning community, and the techniques used to solve it (RL and GNNs) are powerful and effective. The authors has formulated the problem as an RL problem, which is the most correct way to do it, and has also thought how to make it architecture-agnostic, generalizable, and versatile in using the hidden layers' information by message passing through the computational graphs. Overall, it seems that this formulation and solution is the best one can do to learn the optimal learning rate schedule, which is validated by the experimental results and the ablation study. Therefore, I believe this paper is very interesting and I recommend it for acceptance. 	3
In general, the paper is written well and easy to follow. However, there exist aforementioned concerns that the authors need to address.	3
Important problem, but there are many unanswered questions making the submission feel incomplete.	2
Overall, the paper presents several new technical contributions and insights, but some parts are not entirely convincing.	3
"The problem of automatic merge resolution is compelling. The authors present a solution that is based on token-level diff (no ML contribution), identify 9 merge patterns (no ML contribution), and finally use BERT with minor adaptations to classify which merge pattern should be used. For this model, some details of what exactly is being fine-tuned are not clear. 

The suggested approach shows improvement compared to existing baselines. 

The experimental evaluation is hard to follow because neither of the existing baselines addresses the exact same kind of merges. The closest baseline in terms of technique is DeepMerge, and the comparison to that baseline is quite partial. 

Overall, this may be a great software-engineering contribution, but I'm afraid that there's not a lot here for ICLR.

"	2
Overall the paper is okay, I don't think the results are impressive enough, or that the proposed method is novel enough for a top tier conference.	2
Overall, I vote for accepting. I like the idea of combining Good-GAN and Bad-GAN together, which is novel and interesting to me. The proposed orthogonal loss is simple and effective as being used as the complementary of the existing dispersion loss.  The results on benchmark datasets shown in the paper are encouraging and outperforms many existing works in both scenarios without or few anomalies observed. 	3
"The paper introduces a novel method that could be potentially interesting should the authors clarify all the concerns raised in the previous paragraph and provide extensive experiment results to address them. In the current state and given all the concerns on experimental methodology, I would recommend the rejection of the paper with regards to the standards of the ICLR conference.  

UPDATE:

Please see the response I have provided to the authors. I decided to lower my score marginally and I encourage the authors to undertake the modifications suggested by the reviewers for future venues, especially on the empirical methodology and the writing. The paper in its current form may not be ready to my mind. "	3
The paper presents an interesting idea that involves multiple GANs, but limits the evaluation to relatively simple benchmarks. Moreover, the proposed method relies on the presence of labeled anomalies, which are very difficult to obtain in practice.	3
The contributions are not clear and the writing needs major work.	2
The paper can be accepted if some clarifications are made	3
The motivation of the paper and the intuition of the proposed approach is not clear. 	2
Overall, my recommendation is to accept this paper. While the paper could be more focused, make its own contribution and limitations more clearly, and experiments could be extended, I still believe that most flaws could be addressed with minor adjustments and that the core contribution of the paper is interesting enough to a wide range of scholars that publication is warranted.	3
"Overall, the paper is very well-written and shows fascinating results with continuous-time seq2seq model. 
However there are still some missing components in the paper, and more clarification needed to distinguish this method from the Neural ODE work. "	3
This work successfully proposed the first continuous-time generative model and discovered intriguing observations. The possibility of different applications might interest a wide range of ICLR audiences, and hence I recommend accepting the paper.	3
This paper introduces a new continuous time model for learning representations of handwritten data. The paper is well motivated and well written and demonstrates various compelling properties of the proposed model through thorough experiments. While the paper has a few minor weaknesses, I believe it deserves to be accepted.	4
"Authors propose a way of encoding online handwriting data in a continuous space representation by using Neural ODEs, and show interesting properties of the learned model. This is a novel idea interesting to wider audience.

Their main weakness of the paper, however, is the experimental section, and based on its' current content I can not recommend the paper for acceptance. 
* Most importantly, the comparisons are only done to an approach that operates purely on the discrete sequences of points, rather than the ones that, similar to the approach outlined by the authors, allow sampling a continuous trajectory as the output of their decoder (BezierSketch, CoSE), and whether the provided approach provides capabilities beyond those is an open question. Combined with the complexity of the training requiring ODE solvers, it is not clear whether the proposed approach provides any advantages.
* The ablation study is virtually non-existent and does not allow to understand the effects of some of the decisions made by the authors: how does continuous data augmentation affect the quality of the results, rather than the validation error? What are the trade-offs between using a single-stroke representation vs multiple strokes representation? How does the multiple strokes model behave with sequences containing longer than 5 strokes, and how does a single stroke model handle long strokes?"	2
Overall, I am still quite satisfied with where the paper currently stands. I listed things I would like to see improved or discussed. I also listed some potential questions for investigation. But these questions do not detract from my overall positive impression of the paper.	3
The paper is well written and the proposed approach is simple. In general, interpretability of representations is a subject worth studying, and I appreciate the effort that went into experimentation in this work. However, there are also issues that relate to 1) positioning with respect to related work, and 2) the ability for the reader to draw insights from the analysis, among other aspects, that would significantly strengthen the work. In the current state I believe the paper is not yet ready for publication at ICLR.	2
"While the problem studied in the paper is very interesting, at this moment, I don't see there are any obvious advantages of the proposed method over a simple linear probing, as discussed in the ""weaknesses"". Therefore, I'm leaning toward a rejection."	1
The proposed idea is sound and, to the best of my knowledge, novel. I believe the proposed method can perfectly complement the somewhat reduced amount of efforts along the line of interpretation of pre-trained models. However, I believe several aspects (please see my review) that should be polished/clarified before the paper is ready for publication.	3
Given the interesting idea and competitive performance, my initial recommendation is borderline accept. I would like to see authors response on new experiments and discussions during the rebuttal as I think the experiments are limited and somewhat unconvincing in the current version of the paper.	3
The paper is generally well-written and the method makes sense. However, given that the novelty of the method and some concerns about experimental results, I think the paper, in its current form, is slightly below the bar. I believe this paper can be improved largely if theconcerns about experiments are resolved.	2
"An interesting idea,
Numbers hold
Claims are not completely justified with experimentation.
"	3
"# Evaluation

This is a well-written paper with a simple but clearly motivated idea. The preliminary experiments and discussion are helpful in motivating the approach. I am not familiar with the benchmark chosen for experimentation, but the experiments appear to be sound as far as I can assess. There is a comparison with several reasonable baselines. See the main review for more questions and comments. "	2
The proposed weight initialization method is reasonable when adding neurons in model training. However, the experimental evaluation assumes a limited situation. The effectiveness of the proposed method in practical usage is not apparent.	3
"My weakness is listed above. 

----
### Post rebuttal 

After reading the author's response, I feel my questions are addressed. I will increase my score to 6."	3
This is a novel idea (as far as I know) for growing networks, with a good intuition: add units so that they maximize the gradient, rather than minimize the loss. This has good effects on the learning dynamics. Unfortunately, the writing and organization dampens enthusiasm for the paper, and the experiments seem a bit haphazard. I think this paper could be improved considerably by addressing these issues.	4
Not sufficient and convincing evidence that the motivation of the paper make sense and is met in practice. 	2
"In a nutshell, the reviewer regards this paper as a borderline paper, given the limited technical innovation. 

I read the authors' rebuttal. Some of my questions and concerns are replied well. However, I think that the core technique of this work does not advance the research in continual learning significantly. So, I adhere to my previous rating. "	3
"This is a nice and well written paper in which some details need to be clarified. Specifically, imbalance, selecting the core-set before/after adaptation, dataset diversity and selection, the ""transferability"" of performance of the datasets.  "	3
"In general the paper is well organized and clearly written. The technical details are easy to follow. Experiments on the benchmark datasets show promising results compared with baselines.

"	3
"Please find below my main concerns:
1. The following statement is not totally clear: ""The naive CL design cannot retain the knowledge of previous tasks and thus results in catastrophic forgetting"". What do you mean by 'naive CL design'? Please reformulate this statement.
2. Along the paper, you use repeatedly the expression 'target dataset'? What do you mean by 'target dataset' in a continual learning framework? It is confusing: If I have to learn 10 tasks, which one is the target dataset? I guess you refer to the 'current task'. Therefore, please use this formulation instead and change all over the document.
3. How many representative instances do you select from each mini batch?  Since a sample is presented several times during the training, shouldn't you use an acumulative measure and the final ranking/selection should be done at the end of the training? How do you guarantee the class-balance of the selected core-set? Please plot the distribution of samples per class resulted after the coreset selection process.
4. I did not understand the equation 3? What index 't' refers to: task ID? How is possible to measure the similarity between a sample and the minibatch it belongs to? Or you consider the similarity between a samples and the batch average? Something is missing there. 
5. Equation 4: Confusing in terms of notations and terminology! You refer as 'cross-batch', but the eq. 4 is about the similarity between samples in the same batch! 'Cross-batch' should not refer to the similarity between a sample from one batch and the samples from different batches? Why the similarity measure in eq. 4 is negative? Please reconsider the notation and formulation of eqs 3 and 4.
6. Usually, in exemplar-based CL, the memory allocated for exemplar is known and fixed from the beginning. While new tasks are being learned, the number of exemplars from previous tasks is decreased in order to keep the memory size. What strategy do you adopt in your approach regarding the memory capacity where the exemplars are stored?"	3
"For weaknesses 1 and 2, I hope the author could provide more detailed explanations.
For the last one, I am still concerned if the performance gain is provided by the switch scheme or just from the carefully designed BKLD loss and RCE loss."	4
based on the above discussion, I recommend borderline accept for this paper. I would like to adjust my rating for the paper after discussing with the other reviewers and AC.	2
The paper introduces domain-switch learning that simulates cross-domain scenario for training, and thus improves the model’s generalization. The evaluations and comparisons in the paper are a little bit weak. Some ablation studies and analysis are missing.	2
"This work bases on the domain switch learning (DSL) manner. Around this, the work proposes two modules (i.e., domain-specific prompter and the domain-general teacher). Two modules are reasonable and needed under the DSL. 

The major concern is the working mechanism of DSL is not well explained. More discussion/explanation could help eliminate the concern.  Moreover, experimental evaluation needs clarification.

***---Post Rebuttal---*** After reading the response and revised paper, my major concerns are well addressed by the authors. 

"	3
"- The paper is very constructed and has a clear motivation, comprehensive ablative studies, and good analytic discussions.

- The only concern from the reviewer is whether the complexity advantage is a sufficient contribution over the previous regularization method [1], given that Zero-CL shows only incremental performance gain. 

- The exhibited theoretical analysis of Zero-CL with previous efforts gives direct and clear comparisons. 

"	3
Overall the idea looks clear and efficient, it's not clear however how problematic for the current approaches the collapse is and some additional clarifications could strengthen the work.	3
The paper is satisfactory.	3
"Overall, it is a good paper. I am keen to understand why the symmetric framework can be consistently applied to features and instances given they have different properties in the contexts of image classification. I am also not clear about the complexity reduction, which seems to be claimed as a very important contribution in this paper.

Minor issues include some typos in equations and the discussion of low-rank property and ZCA background."	3
The paper proposed a simple multimodal fusion methods based on Jacobian regularization, However, there are minor issue in related work, and experiments are not extensive, i.e. only valided on smaller Toy datasets.	2
I would consider to raise my score, if the authors can address my concern.	3
"
This paper proposes a novel late-fusion algorithm based on Jacobian regularization and conditional independence assumption. It also provides theoretical guarantee on its performance. Further, this paper conducts comprehensive experiments to verify their method. However, there are still some concerns about it assumption and claims. "	3
The importance sampling procedure is new, however there have been many similar works in the past and this paper's results maybe very incremental.	2
I believe the paper has its value in combining importance sampling SGD with conditional variances approximation to reduce the asymptotic error as well as the computational complexity. I just have one question on the technical soundness of the convergence results.	3
"The major comment is that the analysis is for very limiting settings. It would be more practical to analyze the method for non-convex settings to see the effectiveness of the method in practice for training NNs. It would be useful to apply this method to NN training. 
"	2
See above	3
"- While the draft discusses a novel and interesting problem,  I feel it lacks in some of the aspects (listed in the Main Review). The work can be considered as a good early work in this direction. If the authors can justify the motivation for chosen complexity notion, and the effectiveness of the proposed posthoc interpretation along with other minor issues, this can become an excellent paper.

- Pre-rebuttal score: 5 (marginally below)
-----------------
post-rebuttal
----------------

- Rebuttal partially convinces me on some of the aspects (e.g. sample complexity) so I increased the score to 6. 
- However, after reading other reviews and authors' rebuttal I feel that the manuscript needs some more improvements. 
- For instance, primary aim of the manuscript is to investigate the effect of reducing information on classification, and I feel the findings are not very nontrivial or significant. In the end-to-end framework where the sample is presented to the learning algorithm as it is, one can easily see that he/she can find irrelevant information in the sample. And the framework deployed in the manuscript is very much inspired from the meta-learning approaches used for dataset distillation, condensation works. Also, the observations have not been leveraged (or harvested) in a significant way.
- Despite my earlier points, I feel that these investigations have potential to better understand the information that the classifier networks might be relying for learning. Hence, I score 6, but I will not fight for this manuscript in a discussion."	3
"The paper is for the most part well-written. Section 4 needs serious improvement in terms of clarity -- in particular, the authors should provide intuition and clearer explanations about how they performed posthoc explainability analysis using SimpleBits/a simplifier network (e.g., explanations about L_grad and L_pred in Algorithm 1 and why scaling of parameters of the trained network is needed). The authors should also consider comparing their methods of generating simplified images with existing posthoc techniques (e.g., saliency techniques). For a given input image, is the simplified image retaining regions similar to those identified as salient by an existing saliency method?

******************************
post-rebuttal:

Thanks for the clarification of Algorithm 1/2. After reading the response and the revised paper, I still think that the paper will benefit from a better explanation of how to synthesize a simplified image for a given input image for an already trained neural network. I understand that Algorithm 1/2 details the process of how to compute the loss function, but there is still a lack of explanation of how to optimize the loss function in the latent space. Essentially, I want to the following question be answered clearly: how do you perform the optimization in the latent space of the pretrained invertible generative model, using the loss function you computed in Algorithm 1/2? Right now, there is only an algorithm for how to compute the loss of the optimization problem, but I cannot find the optimization algorithm itself. Therefore, I am keeping my original score of 6.
"	3
"The paper proposes a novel direction of simplifying images (in terms of image complexity) to interpret neural networks. The proposed method is intuitive and easy to implement, having the potential to make broader impacts. 
The paper applies the proposed, SimpleBits, in three different settings *i)* simplification during training *ii)* simplification after training *iii)* simplification with dataset condensation. 

From the title, the paper's main goal should be understanding and interpreting neural networks, while I found the contents sway between interpreting models and simplifying images. Without further elaboration, better simplifying images does not naturally imply better model interpretation. 

Overall, I think the method is novel, and the experimental results look convincing, but the explanation and motivation are unclear, especially sections 4 and 5."	3
"In my opinion the methodology section must be more thoroughly explained, and comparisons/baselines must be added. For these reasons I recommend that the paper is rejected in its current form. I think that the visualization tool proposed for diagnosing errors could be useful, so I would encourage the authors to improve the manuscript.

Score: 3: reject, not good enough.

============== AFTER REBUTTAL

I have updated my score for this paper, following the authors' clarifications. However, I still think that the paper is incomplete with respect to baselines using alternative image compression methods or other techniques.

Updated Score: 5: marginally below the acceptance threshold."	2
In summary, this paper is mostly well-written except for the theoretical parts which need some work. The idea is really interesting and relevant for resource-intensive applications. The experimental results look very promising. I am leaning towards acceptance, but I might change my rating based on if my concerns have been resolved.	4
"The theory is a compelling motivation for a proposed matrix approximation algorithm that seems to experimentally work well.

They should clean up their experiments a little, but this is overall encouraging as a pragmatic way to reduce the time and space complexity of real kernel algorithms by large constant factors.

I recommend this paper."	3
This paper provided the sparse random features with theoretical guarantees, efficient algorithm and sufficient experimental validations. However, this paper assumed the input data with Gaussian mixture that may limit its applicability. Meanwhile, the influence of the number of random features has not been explored well.	4
My biggest concern is how important the main contribution is (have a constant scale improvement in computation time and storage memory). To be honest I'm not sure how to measure that, and thus will lower my confidence score.	2
I recommend marginally below acceptance threshold. I think the paper is well-written, but it should be much more precise mathematically, explaining the estimation procedure for nonlinear setting, and address some fundamental questions about the optimization problem.  	2
The paper is not written well and does not have a flow. The rationale and objective of the paper is not clear. Therefore, I can not recommend it for publication. 	2
Please refer to the weaknesses in the above section.	2
While the paper highlights and addresses an important problem, and proposes a simple yet effective solution, I believe that without a comparison to *online Kalman Filtering* approaches that minimize regret, it is impossible to determine if this truly represents an improvement over the state of the art in terms of empirical performance.  From a theoretical perspective, I see little to no novelty in the paper's contributions.	1
"The paper is overall well-written and established the preliminary studies of MPG problem and multi-agent learning algorithm for convergences, though there are still many unsolved questions in MPG such as Price of Anarchy. Therefore, I recommend a weak-accept of this paper.
"	3
"In general, the paper is well-written, and I enjoy reading it. It has made valid contribution on multi-agent RL in potential games. With the minor comments above being addressed, I believe it can be a good paper deserving publication. 

"	3
Although the theoretical analysis of this paper is well-organized and seems to be rigorous, the so-defined MPGs seem unrealistic in the MARL setting and are restrictive in practical applications. 	2
"This paper provides interesting, new, and important theoretical results for policy gradient in markov potential games.
"	3
"Overall, this paper is well written and works on an important problem. The assumption is simple and intuitive, the proposed method is also feasible and sound.

Even though, there are still quite a few concerns. I am on the borderline of this paper and could be easily flipped to the other side based on the authors' response."	3
"Although there are some weaknesses of the paper, based on their experiments, I still believe the proposed method could be useful for generating strong adversarial examples and open the door for future research on defense. I also appreciate their theoretical analysis, although I haven't check the correctness. In general, I am feeling positive on this paper and would like to recommend for acceptance.

==============================
Post rebuttal:

I appreciate the author's comments on my questions. My score will keep the same since it's already reflect my recommendation for acceptance."	3
"This paper provides interesting theortical insights on how to find highly transferable adversarial examples. However, from the practical side, the proposed training method seems to be inspired from other previous works rather than the observed insight, and whether the discovered AAI metric is practically useful for the optimization process is at doubt. Thus I would suggest a weak reject for now given the potentially low practical significance of the method.

## After rebuttal

After discussing with the author I have a better understanding on the significance of this work. The analysis of AAI motivates the idea of modifying structural hyperparameters of the source model to improve attack transferability, and AAI serves as a good indicator of the transferability towards all model. Although further experiments show it may not be the best objective for choosing the structural hyperparameters, I think it serves as a good theortical strating point for futher analysis of adversarial transferability. Thus I would like to suggest acceptance.  "	3
The idea is almost the same as the existing works, especially [1].	1
The idea is somewhat novel, but the result is not good enough.	2
"It's a methodology paper working on an important problem (CTR prediction). The proposed method is not extremely innovative but the idea is natural and makes sense, so are the results positive and promising. The novelty is a subjective matter but personally I see enough contributions to clarify and materialize some of the vague commonsense in the industry. One minor concern is about its relevance to ""ICLR"" which focuses on representation learning."	3
This paper provides a study of the role of multiplicity of activation functions in the condensation mechanism of neural network training, claiming that the maximal number of condensed orientations in the initial training stage is twice the multiplicity. Some experimental demonstrations are provided to support the claim but the theory is somehow limited. I believe that the paper could be further improved and will go with a weak reject for now.	2
"The quality of the paper is hampered by
- not fully convincing and unclear experimental analysis
- low readability due to several typos in the manuscript
- unclear improvement wrt the work in Luo et al, 2021"	2
This intriguing paper demonstrates, through experiment and analysis, a connection between the implicit regularization of certain networks and the multiplicity of their activation function.	3
This paper presents a possible perspective to explain why small initialization can serve as implicit regularization during the initial training stage and provides insights to investigate the nonlinear dynamics of neural networks. While it can be improved by conducting more experiments as well as analysis of more complicated cases, it is above the acceptance threshold.	3
Despite strong empirical performance, I am incline towards rejection due to the limited novelty and unclear motivation. The presentation can be improved as well.	2
While the idea describes in the paper is interesting, and the evaluation quite comprehensive, it seems like the essence of the authors' contribution itself is quite minor, and at the moment I am not convinced that it passes the acceptance threshold.	2
"Overall, I was impressed that simple contrastive learning can improve disentanglement learning. However, the paper did not go much deeper into why and how it improves the performance, and lack some technical novelty. The presentation should be improved in many ways too. 
"	1
I recommend an accept since the method seems sound, well-motivated and reasonable and the results seem promising in difficult datasets. However, the novelty of this work is slightly lacking since it is a straightforward integration of a contrastive learning method (which have been a common theme recently) to OverLORD, the cited paper that performs latent optimization with disentanglement.	2
"Overall, I vote for accepting. I think the approach is very novel and the results surprisingly good – especially for the tabular data with BNN priors. This method has a lot of potential for bridging the gap between deep learning models and small data regimes, all the while adopting a Bayesian perspective. It also opens the door to very interesting future research, for instance on the question of design of priors for a particular problem. 
My main concern is with regards to the clarity of the paper, as there is a lot of prior knowledge assumed from the reader and sometimes a lack of details and explanation. Hopefully my comments will be addressed during the rebuttal period.
"	3
In summary, the paper is clearly written and proposes an interesting method for approximation of posterior predictive distribution. The author has validated their approach in the classification task with small datasets. But I would like to see the generalization on other tasks and data/model scalability. In addition, I have concerns or unclear intuition for adopting transformer encoder architecture.	3
Interesting paper and strong results. Probably mainly useful for fast inference in small datasets. The claim of doing Bayesian inference is not fully substantiated beyond 1D GPs.	2
The introduced spatiotemporal model of STE, which consists of a grid-cell spatial encoder and a visiting time encoder, is capable of mining the POI-specific spatiotemporal characteristics. Finally, the authors implement successive POI recommendation systems based on the STEP using simple recurrent neural networks as recommenders. Experimental results on Instagram Check-in and Gowalla datasets demonstrate that STEP achieves the state-of-the-art successive POI recommendation performance. However, the novelty of this paper seems to be over-claimed. 	2
"The inspiration is interesting. But some parts need further clarification. I suggest ""weak reject""."	3
The paper needs to clarify the model definition and how each sub-model is integrated.	2
This paper is technically sound, and the experiments are sufficient. Thus, my rating for this paper is weak accept. 	3
Overall, the authors present an interesting method for optimizing discrete VAEs with compelling theoretical and experimental results. The paper is well written and the story is coherent. My recommendation is to accept.	3
It would be great to provide more details on how to get Equation (2). Especially, why an argmax operation is added. 	3
The paper is well written, the authors clearly describe the problem and the proposed solution that is computationally appealing because does not require propagating gradients. It is theoretical work, the experiments only show the effectiveness of the ELBO function optimization. The authors consider only optimization of the mean distribution of parameters and do not show other experiments that confirm the effectiveness of such optimization in generative models (see the section of the weaknesses of the paper). Hense I rate the work good but not sufficient for the ICLR conference.	3
This is in my view a good paper that deserves a place in ICLR.	4
A good paper. I definitely suggest acceptance.	4
This paper shows a simple yet effective finding and validate this with extensive experiments. However, it lacks some insightful explanation and novelty.	3
"Overall, the empirical results are quite interesting, showing that max-pooling can be used in place of convs or depth-wise convs more extensively than I might have expected, and offers a meaningful datapoint in the wider context parameter-free operations.  While the most specific claims are supported by the experiments, I think the overall framing as a demonstration of parameter-free layers is over-generalized for what has been demonstrated around max-pooling.  There are also many details that I was unclear on, and feel could be more clearly described.
"	2
"Overall, I think this paper proposed an inspiring idea but the author could explore further on this. The current results are not surprisingly good but promising. It would be good If the author could further improve the method.

=============== Post rebuttal ================

Thank the authors for the detailed response. I am satisfied with most of it. I still have some concerns about the proposed deform_max and its performance. The idea of replacing conv with pooling (or general parameter-free operations) seems promising. But the results in the paper are not suprising yet, even with the deform_max. The accuracy is about 1% lower than baseline, with 1.2 GFLOPs less computation but even more inference time. It would be good if the author could further improve and optimize their method. I would like to keep my score as weak accept."	3
"In general, the paper is well written, it has a clear and logical structure, it is easy to read. 
In the introduction, only clearly highlighted contributions are missing.

The proposed criteria can be useful when training models for conditional generation.
The authors did rather detailed discussion of different variants of the score matching procedure.

============

I thank the authors for their responses. The updated version of the paper, with additional information here and there, improves the readability and the overall narrative. Given the authors responses now I am convinced in the concept proposed by the authors, so I can raise my score.
"	3
In general, the paper's contributions are clear; the proposed methods are well-motivated and well-discussed. In addition, I also found that the paper has a well-organized structure so that it is clear to understand the proposed method and other practical techniques to improve training. Thus, I'm inclined to accept the paper. However, I found that several aspects of the submission can be improved. I hope that the aforementioned weak points are well addressed.	4
Niche paper to introduce a conditional generative model by the diffusion process	2
Overall, I think this is a strong paper that would make a positive contribution to the conference and the current discussion surrounding SBMs.  The paper is very well presented and explained throughout, with appropriate figures and tables, as well as accurate derivations. Class conditional generation does appear to improve based on this method, and the complementary performances trade-offs (distribution precision) are broken down and discussed. Besides one line of research that could be included in a future paper, weaknesses are minor and/or aesthetic.	4
The paper introduces a simple method of preserving coherence in language modeling it builds on previous work that tried to implicitly model planning dynamics. The introduced solution is effective and general enough to not need domain-specific planning information. It is a good paper to accept overall. I advise the authors to clarify the information about the used baselines in a more clear manner. 	3
Nice attempt for random generation from neural language models using the idea of Brownian bridge. This work will pave the way for more princpled random generation from language models.	4
"Overall, I think this paper is well motivated and proposes a reasonable solution to improve coherence of model generated text. This is supported by ample experiments but I have serious concerns about some of the crucial experiments and baselines that I have detailed in my main review. Also, I think that the paper could be clearer about its contributions and implementation details.
-----
Post rebuttal: thanks to the authors for the detailed response addressing many of my concerns. My biggest concern about the prior in the VAE baseline is somewhat alleviated given that the the authors used different fixed priors for the two settings. While this could be improved by having learnable priors/better priors, I think the current setting makes the experiments reasonably sound. I have raised my score."	3
Interesting modelling contribution to ensure global coherence of generated text. The proposed modelling approach could have wide applicability, which is why I recommend acceptance.	3
This paper is very well written and it provides empirical evidences to support the intuition that face obfuscation does not decrease the utility of the ImageNet dataset. However, my main concern is that the paper has no technical novelty and it also does not bring sufficiently new insights to the community.	1
"Although the paper mainly focuses on providing plenty of empirical results to evaluate the influence of using the face obfuscated ImageNet dataset and is of limited novelty, it provides a lot of insights to show the effectiveness and feasibility of privacy preserving ImageNet.

I have few concerns of the selection of transferring tasks. For example, the resolution of CIFAR-10 is only 32x32 and only for 10 classes. Similarly, Pascal VOC is also relatively small and easy dataset as compared with COCO or other recently released object detection dataset.
Most of the images in the CelebA dataset are in frontal pose and have much fewer variations than other unconstrained face dataset, like IJB-C, etc. Since these datasets are relatively simpler than others which are more close to real-world scenarios, I wonder if the same experimental results and findings are still valid for harder datasets with more variations."	2
This paper has some contributions in exploring the ethicality of datasets, especially in the current very popular ImageNet database, but it exists some flaws (see weakness). The solutions and results in this paper are open sources and feasible, and this work will inspire subsequent exploration of privacy protection in publicly available datasets. 	2
In short, while I acknowledge this work is novel, its setting is not very practical.  This paper would benefit from presenting a potential real-world application, or else some theoretical generalization results on how well their systems can learn.	2
The paper proposes an interesting solution to an important problem, but as-is the experimental settings and results are not compelling. 	3
"The paper proposes a new concept called quasi-language which allows representing rules in an new informal format that still allows to perform forward or backward reasoning while it is assumed to be mined easily from texts. The experimental results with some datasets with synthetically generated texts show that the methods work very well and advance state-of-the-art results. 
However, I doubt the application of the work with natural language input, I explicitly request to perform more experiments with paraphrased datasets in RuleTaker (MAJOR CONCERN 1). I also have a concern about reproducibility as the presentation lacks details of the core components of the proposed algorithm (MAJOR CONCERN 2). I also requested an additional experiment regarding the training data with low depth queries to check the ability of generalization of the proposed approaches. "	4
See my comments in the main review.	3
I think this is a solid paper. It is generally clear, well-written and insightful, even if its contributions are not surprising or technically challenging. I support acceptance.	3
 The novelty of this paper is incremental but the experiment of this paper is sufficient.	2
"1. The proposed idea is not novel. It is somewhat trivial that whenever we have storage limitations, data compression is going to be helpful. This on its own is not an idea at the level of a top-tier venue.


2. I find subsection 4 to be an unnecessary section to make the idea look non-trivial but I think it only is complicating the method unnecessarily.  A naive grid search can do the job. The justification that the authors have provided is that grid search is of ""huge computational cost"". But first of all, it is not clear how large this computational cost is. Second, even if it is large, it is not going to be a huge burden for continual learning. Because you can determine q one time before starting model execution. This is what we do in most cases when we want to tune a hyperparameter. There is no theoretical contribution in that section either and along with the appendices, only known results are rewritten. As a result, the idea contribution of this manuscript is highly limited.


3. In experiments, it is mentioned that 20 samples per class are stored. However, this is going to lead to a memory buffer that is going to grow as more classes are used. The appropriate way to do this is to consider a memory buffer with a fixed size and then discard samples as new tasks are learned to replace a portion of old samples with new samples.

4. In the results, standard deviation should be added to make the comparison more informative.

5. It is not very clear how much advantageous data compression would be for continual learning. An additional analytic experiment can be to show that how much memory can be saved by storing less compressed samples while getting no considerable performance degradation.
"	1
In this paper, the authors propose to compress the data for memory-based continual learning. Since there is a tradeoff between quantity and quality of the compressed data, the authors propose a novel method to decide the quality of the data. The idea is well-motivated and novel for continual learning. 	4
"Several concerns are listed above. I would be happy to re-consider my recommendation if the responses are reasonable.
"	3
To summarize, I think the SCL view proposed in the paper would be interesting for researchers of few-shot learning. While the result is restricted to few-shot learning, meanwhile no exploration is done to analyse the connection between MAML and metric-based approaches. These drawbacks make the paper less insightful as I expected.	2
I found the proposed zeroing trick (and the supporting theory/experiments) to be interesting. I would, however, encourage the authors to work to better motivate their study in the intro of the paper.	3
My current understanding is, the paper lacks the general applicability and sufficient experimental validation that are essential for acceptance in ICLR. 	2
The proposed neural network architecture and training scheme improve denoising over baselines in relatively subtle ways that are hard to measure with traditional metrics. To me, in the current version it slightly misses the bar for acceptance due to some missing baselines (noise2noise, noise2stack) and insufficient experimental data motivating the architecture choices.	2
"It is a good work overall with a clear presentation. However, the novelty is limited and more experiments should be performed to show the utility and advantage of the proposed method.
"	2
`NRRN` is a novel approach to address the challenging application of denoising *FIB-SEM* images. The authors back this up with state of the art results on a real and a noisy-simulated dataset. I rate this work slightly below acceptance threshold currently because even though the solution to this difficult biomedical problem  is very welcome, it is not evident to me how much of the performance of the method owes itself to a new architecture and how much benefit is gained by the employed loss formulation.  Furthermore, because of the zeroth-order approximation to the Taylor series along the z (third) axis implies to me that as soon as the voxel size along this dimension increases, the results would become sub-par. While this is okay and not a problem perhaps in the *FIB-SEM* context (?) I think it would be important to mention this explicitly or comment on how this can be fixed/handled if one were to extend this method and the proposed architecture to other domains. The writing in general is good but at times lacks some motivating sentences leading to the argument. Since the work concerns this specific biomedical application, perhaps the authors can consider submitting their work to a more domain-relevant conference or journal, in order to accrue a higher impact and relevant target audience.	2
"In summary, I vote for rejecting this paper. 

The paper misses comparisons against many important and recent unsupervised denoising works which are directly applicable for the presented application domain without needing any modifications (see my comment in weaknesses). A motivation for choosing the proposed architecture is missing (see my comment in weaknesses) when in my opinion, simpler architectures can arguably perform similar or better. Moreover, the paper is rather limited in its application (is specifically designed for FIB-SEM images) and is unclear if it can be extended for any other domains. Hence, my rating of reject."	1
The paper appears to be limited in novelty and contribution.	2
This is a good paper in terms of ideas, writeup and experiments. My suggestions and critics would require just minor adjustments to the paper that can be done in the rebuttal phase. Unless major issues arise in the discussion with other reviewers, e.g. the method is not as novel as I thought due to related works that I am not aware of, my recommendation is to accept.	3
Overall, this is an interesting work to dig into the performance of ViT on visual relation reasoning and semantic concept learning. Some non-trivial contributions are made to reveal the effect of contrastive learning designs. And the results on the two benchmarks look promising. However, though studying a very different problem, the method contribution seems weak compared to the previous works. Thus, I think the rating of 6 is suitable for this version. I will adjust my rating according to the response and the discussions from the authors and the other reviewers.	2
Though the idea of using generative model to augment GNN training data sounds interesting, I would not recommend to accept this paper given its current presentation quality. 	2
The idea is interesting but the difference and theoretical contributions are unclear. It may be the presentation issue but seems to borrow several parts from other papers. Moreover, the improvement is minor (0.47% on Cora dataset).	2
This paper presents an interesting local augmentation method in a way of generation to enhance the graph representations and show the results of classification with different backbones on different datasets and we can see the improvement of local augmentations, but can be further improved by providing more ablation studies. 	4
This paper proposes a new graph augmentation method that enhances the performance of various GNN models by augmenting input features. However, the technical quality of this paper is insufficient, and the ideas, writings, and experiments are unclear and overwrapping. The idea of running variational inference for graph augmentation is intriguing, so I suggest the authors further develop their approach.	3
The DP results are solid, but the MLC aspects of the work has serious shortcomings.	3
I think the paper considers an important problem as the design of differentially private aggregation mechanism for multi-label outcomes is applicable in a lot of domains. The proposed mechanisms are also simple and build on top of the existing DP aggregation mechanisms. However, I felt that the experiments were not quite exhaustive. For example, it's not clear if it is beneficial to apply powerset voting on real datasets.	3
The problem is interesting and the results seem to be quite comprehensive. My main concern is that the voting mechanisms proposed are relatively naive (separate into independent elections or run one big single-winner election). However, the technical results seem robust.	3
It's good but may be not enough	3
Although this is a good practical problem, I think this paper needs some revision.	2
The authors should make the privacy scoring module more clear and add more discussion about identifying malicious nodes in continuous requests	3
Novel active strategy to defend against model extraction, good experimental results.	3
This paper is in general well written, and the experiment was done to show the strengths and the weaknesses of the proposed defense. The defense against model stealing is inherently difficult problem where the attacker in the end can steal the model using the queried data and returned label pairs unless architecture or resource is no object (e.g., if not GPT-3). The main idea proposed in the paper against such a strong attack looks interesting, but the key technique used here is the proof of work. I am not convinced this is necessary at all since the original model is hosted and controlled centrally, and response time can be fully controlled without using the proof of work. Just delaying the response would have much better control of information leakage per time unit than the proof of work that a powerful machine can do faster, or simultaneously processing multiple queries with multiple machines reaching up to the speed of a regular user. Rather the important piece is the information leakage estimator which is somewhat overlooked in the paper, especially in terms of why PATE has to be the right choice here. I think focusing on this can lead to a better paper.	3
see main review	3
The experimental results of the proposed method have largely outperformed baseline methods.	3
This paper introduces a simple but effective and efficient method for hyperspectral image reconstruction. However, the novelty and significance of the contribution of the paper are not high. 	2
This work may be a nice contribution to the field of hyperspectral imaging, but it is not easy to see this work as a good contribution for ICLR; using dense skip connections is not new even though the way of connecting them is different from others, using pixelshuffling is common in super resolution field, and narrowing down the RF does not seem to be theoretically solid since it is then possible to miss some information.	1
I have concerns about the motivation of network design, together with the position of the proposed network.	3
This paper has limitations in the problem formulation, the motivation of the method, and the experiments. In my opinion, it needs major improvement.	2
"Overall, the paper addresses the problem of noisy labels next to cross-modal semi-supervised few-shot learning. This problem formulation makes the proposed work different from the existing approaches. Moreover, the proposed formulation could be considered novel regardless of combining existing approaches. The main issue of the paper is the difficulty to follow the proposed approach. One needs to read several times the paper for understanding the structure of the proposed method. This is a major point that needs improvement. Finally, considering semantic label noise would be the whole idea more plausible for real-world setups.

Post-rebuttal comment:

The rebuttal has helped to improve the paper. My concerns have been addressed. "	2
Overall, this paper provides a new perspective on the cross-modal semi-supervised few-shot learning.	2
Due to the concerns on the novelty and the weak comparisons on Imagenet. I temporarily think this paper is marginally below the acceptance threshold. 	2
I think this work has some merit, but it is not ready for publication at this time. I can be swayed at rebuttal time if there is convincing new experimental evidence.	2
"Although the proposed method is simple yet effective on ResNets, the contribution is limited and experimental evaluation is not comprehensive. I tend to reject this paper in this version.


-------------------POST-REBUTTAL COMMENTS-------------------

I thank the authors for the response and the efforts in the updated draft. After this rebuttal, the authors well answer my comments about the comparison to SOTA methods. However, I still believe that the proposed measurement of filter importance is a limited novelty, as it is a simple revision of attention score from [Sergey Zagoruyko, ICLR 16]. Moreover, the rationale of threshold T is based on the assumption that a layer is more likely to have redundant filters to prune if it contains more remaining parameters. I think it is not a correct assumption, as the entire filters of a layer with a smaller number of parameters may be redundant to be removed safely, especially for ResNets. Thus, I still keep my original rate to reject it."	2
The paper proposes an interesting and novel algorithm for curriculum RL. The paper is overall well written. However, the conceptual and empirical comparison to knowledge transfer via reward shaping methods is missing. There were some clarity issues in the experiments. If these two issues are addressed, I am marginally inclined towards acceptance.	3
Overall I liked the ideas presented in the paper and think it provides a good starting point for analyzing theoretically, the non-trivial task of curriculum learning in RL. The idea of using the sum of residuals to model the state-action function approximation was interesting. I am inclined to accept the paper because of its promise and utility in inspiring similar approaches to curriculum learning (which provide a framework for theoretical analysis), though the experimental set-up could have been made clearer and limitations of the approach be made explicit.	3
Paper investigating a relevant research topic. The manuscript contributes interesting ideas and the proposed method seems to have good performance. However, the lack of comparison against state-of-the-art methods makes it hard to assess how good the method is compared to other related works.	3
I recommend this is a borderline paper due to the issues I raised on the theoretical part and the notations.	3
"I like the idea to embed an stochastic LWTA approach into meta-learning. It makes sense and the motivations are well presented. However, the technical novelty of the paper is lacking and does not offer enough methodological advancement to be a strong candidate for acceptance. 

**** Post-rebuttal ****
I have decided to increase my score from 3 to 5. My reasoning is in my response to the author rebuttal."	2
"While this paper does have some strengths (strong empirical results, great set of ablation studies), it is let down by many issues of clarity, a lack of novelty, and a somewhat narrow set of main experiments. 

++++ Post-revision update ++++

The authors have addressed many of my concerns, including correcting some of my misunderstandings, improving the clarity of the text in many ways, and adding additional experimental results in the form of ablations, baselines, and a new task. With this in mind, I have increased my score from 3 to 5.

I have not increased my score further as I still find some parts of the presentation slightly confusing (e.g. I don't think the word ""principals"" is much better than the word ""argument"") and more importantly, I still believe that having experiments only with image classification tasks is slightly too narrow."	3
I think the paper is interesting, but the presentation needs to be greatly improved. Many notations are used without introduction, and the description of the methodology is very dense that I am afraid that unless the reader is very familiar with the field, they could not reproduce the methods based on the description	3
"This paper improves standard MAML by replacing the nonlinearity with stochastic LWTA. The idea has limited novelty and limited insight.
The pros are the experiments are well performed, and the results are significant. I appreciate the ablation study and the overhead complexity discussion."	2
"The paper is generally well written, however one of the main contribution, the reconfigurable attention module, should be more precisely and formally described, at least an appendix. Otherwise it is a little hard to get with Fig3 largely.

The contextualization of the paper wrt YOLOS is also something that could be improved. It seems that the model is more closely related to YOLOS than seems at first read.

The experiments should be made more complete with fair comparison for baseline backbones etc. And the results with conv backbones should also be given for completeness and benefit of the reader.

"	2
I appreciate the effective and efficient of the proposed detector, but I prefer to reject this paper for the above weaknesses. If the authors solve my concerns, I would like to raise my rate.	2
"Overall, I believe this is a meaningful paper from the perspective of both object detection performance and research. It will also inspire the design of [DET] token based framework of other object- and region- level downstream tasks, *e.g.*, instance segmentation, video instance segmentation as well as panoptic segmentation.
"	2
This paper works on the embedding compression problem for GNNs and graph representation models. However, the major limitations on technical novelty and experimental studies make it unlike to be a quality publication.	2
In this work, the authors develop a hashing-based node embedding compression method, which significantly reduces the memory cost in the embedding learning procedure. The proposed method also outperforms the prior random coding based embedding compression method in the experiments. However, the experiments in this paper only utilized GraphSAGE as the graph representation learning model and considered the node classification task. It would be great if the authors could discuss and include more graph representation learning models and more graph related tasks in the experimental section. In addition, I wonder if the authors could discuss about the selection of input auxiliary information?	3
"The method displays clear advantages over ALONE, and has the potential to be applicable in a wide range of problems, especially, large-scale ones.
Although the methodological contribution is little, the method appears simple and effective. 
However, given the mentioned unclear part, I tend to recommend a weak accept. I am confident the authors can clarify the raised doubts and I am happy to increase my score, if appropriate.
"	3
Overall, this paper is well-written, but the technical contribution is not sufficient, and the empirical evaluation should be done on a more practically-relevant task of link prediction.	2
The paper is well-motivated, novel, and well supported by the experiments.	3
It is a well-motivated paper presenting an alternative understanding of the challenges of training binarized BERT. The empirical contribution is a little bit marginal since there is a large gap between the proposed approach and the best result in [1].	3
"This paper is overall structured clearly and easy to follow. However, the motivation, theory part and experiments require some further clarifications.
"	2
"This paper portrays the related performance issues of straightforward fully binarized BERT and introduces the idea of BiBERT as a solution. The paper has proper experimental proofs and analysis to justify the framework. It might be a positive inclusion towards an accurate fully binarized BERT architecture. The proposed method obtained impressive savings on FLOPs and model size. However, it seems that the related SOTA approaches are missing in the comparative performance analysis section, and I would really be looking to see this in a conference paper that meets ICLR's high standards.
"	3
The paper tries to tackle a challenging problem of binarizing BERT and find effective ways to improve the performance. However, the model performances on some tasks are still far from good and comparison in the experiments are not convincing. Therefore, my current recommendation is weak reject unless the author can clarify the concerns.	3
I like the idea of FKD that matches the student network's feature kernel with that of the teacher network in KD. This can apply to situations where the teacher network and student network have different output dimensions. However, the theoretical novelty is limited, and also the empirical evaluations are not thorough (only on some very small datasets). In summary, I vote for a weak acceptance. 	2
The paper examines an interesting problem. The Feature Kernel Distillation is an interesting algorithm. The experiments seem to indicate that it is competitive in a variety of settings. The theory is reasonable interesting as well. The paper is well written as well. Hence I recommend acceptance. 	3
This is an interesting paper. The paper is clearly written. So my recommendation is to accept this paper. 	3
"Overall, this is a well written paper, easy to follow and the proposed method is - mostly - sound and backed up with some theoretical evidence. However, given the amount of literature ignored, together with the high similarity of the proposed method with many existing ones, I am very skeptical of this paper at this current form. Also, authors should provide more in depth experiments to evaluate how the employed kernel behaves when more classes are used, as well as when there are architecture changes. In other works, the scope and application area of the method should be clearly defined and possible limitation clearly discussed. As a minor point, I feel that some claims are quite bold, e.g., ""This enables us to *prove* that KD using only pairwise feature kernel comparisons can improve NN performance in such settings"". "	2
I think this paper proposes a simple yet effective framework for pertaining embedding for 3D deep learning tasks. Despite some minor questions and issues in exposition, I think it would be a nice addition to ICLR.	3
This paper presents a conceptually simple approach that seems to boost performance for most baseline models for 3D deep learning. I like the exposition of the idea and the extensive experimental results. However, I'm concerned that the baselines chosen for comparison might be outdated. If the authors can provide results with updated baselines (specially those that use transformers as backbones) I would be happy to upgrade my current score.	3
"The idea of introducing an embedding layer to the 3D shape analysis just as the NLP community is interesting and novel. 
The proposed framework is general to many mainstream 3D representations and does not require many additional changes to existing 3D learning frameworks in order to be deployed. The experimental results are positive and have shown the effectiveness of the proposed method in a number of applications. Though the performance boost has been verified, the magnitude of the increase seems to be a bit marginal. 

"	3
"Overall, I am lukewarm for this paper and slightly more on the negative side.


--------------------------------------


Other issues:

I am not sure how sensitive the method is to K and the density of the point samples in the inputs.


P.3:

descriptors( -> descriptors (

(Sun et al., 2009) , -> (Sun et al., 2009),


P.6:

a accuracy -> an accuracy

ShapenetPart -> ShapeNetPart

representation, We -> representation, we 

classfication -> classification


P.7:

Adam(Kingma -> Adam (Kingma"	2
Overall, the results in this paper can be better presented, and the connections between certain statements are not clear. Therefore, I recommend a rejection.	2
Overall, I enjoyed reading the paper, and I'll raise my score if my concerns are properly addressed.	2
I appreciate the simplicity of the idea and the performance boost it brings about. From what I understand, it can be applied to any FL algorithms that involve client sampling. Therefore, I think this paper deserves 6: marginally above the acceptance threshold.	2
"In summary, the authors propose to more favorably select FL clients whose representation distribution is more similar to that of the global model. While the current evaluation results are encouraging, data heterogeneity in FL and model robustness need to be carefully considered. 


"	3
To my knowledge, the proposed method is novel and contains some methodologically new ideas, and the performance seems to be on par with previous methods that learn free-form dynamics, and shows an improvement for models that contain a convection component when such prior knowledge is utilised in the model training. 	4
The weaknesses reported above are relatively minor and far outweighed by the strengths of the paper. Therefore, I recommend the paper for acceptance. 	4
"The paper is well written, the model (to my best knowledge) is novel, with the method building on existing work. The experiments rigorously test the model against the necessary baselines and information is given in the appendices on reproducing the results. Therefore I recommend acceptance, with a few clarifications to be made.

**EDIT** I have increased my confidence score from 3 to 4 after my initial questions have been answered."	3
The paper give valuable contribution, the method expected to be practical, robust, and in some cases interpretable. I find the statement on raw prediction error overly strong.	3
While the work is interesting, I find the motivation a little strained at not well aligned with the problem statement or algorithmic instantiation. The empirical study is fairly clear but limited by the simplicity of the domains and the restriction largely to assessing feasibility rather than giving clearer indications of how the method could be used in practice.	3
"
This is a good paper. It contains excellent writing, good research, and makes for a great example of what an ICLR paper should look like. The research problem and proposed methods are interesting and well placed in the literature, and the experimental section is exhaustive."	3
While this work proposes an interesting generalization of prior work on meta-learning MDP dynamics, the case made for these methods is weak, and the experimental results—especially for SEs—are uncompelling. Given the additional compute required by these methods, it seems that various forms of policy distillation or even training from scratch seem more efficient. Further, it is not clear why only certain formulations (SEs and RNs) of meta-learning neural transition dynamics were investigated, while others (like a state-only neural transition network) were not. Given these points, I recommend this paper in its current form for rejection.	2
"A very interesting paper with early results in a potentially interesting direction. It is currently unclear what this method allows to do that was previously impossible from a practical point of view, but there seems to be promise. 
I'd be open to raising my score if my concerns are addressed by the authors. 



Update: 
Based on the discussion phase and the rebuttal I have updated the score to a 6. I believe this paper has enough merit to be published at ICLR, if there is space. It is an intriguing piece of work, even though the practical utility is very unclear at this point. "	3
This paper has a strong motivation as I mentioned above. I like the idea of using a heuristic mixup technique for reducing bias in PU learning. I think the idea that adds heuristic to mixup could be extended to other weakly supervised machine learning tasks. 	3
The paper proposes a new positive-unlabeled learning method which achieves data augmentation and supervision correction simultaneously by refining mixup strategy. The method is well-motivated by empirical findings. Extensive experiments and ablation study demonstrate the effectiveness of the proposed method.	3
This paper proposes a mixup method specialized for PU learning. The idea of marginal pseudo-negative instance estimation is interesting. In the current manuscript, there are however several unclear points and it lacks a simple baseline in the experiments. Having such a simple baseline, the advantage of the proposed method will become more clear.	2
The problem is interesting and the proposed method is promising. I vote for accepting this paper.	3
"First of all, this is not my area of expertise, and the review period was too short to go into all the mathematical details. Therefore, it might as well be possible that this is quite genius and I was just not able to see it. However, the paper does not explain the impact of its contribution or its applications well, does not have a proper related work section, and the results are half hidden in the appendix and their positive aspects not explained. 
I have rated reject because putting contributions into context through the related work (while often annoying and seemingly just taking away space) is an essential part of a paper and skipping over this leaves the paper incomprehensible for many people.
My ratings on correctness and novelty are only approximations, and I would not oppose acceptance if the theory has strong supporters in the other reviewers. "	3
Unfortunately, while I very much appreciate the topic and the underlying mathematics, I find the paper and presentation in its present form to be too unclear that I can recommend acceptance.	2
Interesting new theoretical results, but the connection to gradient flow is lacking	3
This paper introduces some interesting insights for contrastive learning. However, its presentation and experiment design could be improved. I will raise my rating if most of my concerns have been well addressed.	2
The paper proposes a solid yet somewhat incremental modification from the prior work, PCL. Overall, I think the paper is on the boarderline. I'm willing to raise my score if my concerns: (a) improper credit for PCL, (b) ablation study over PCL, amd (c) comparison with SwAV/BYOL are addressed.	2
I like the idea used in the paper and analysis shown in the paper. However lack of results on object detection and semantic segmentation coupled with lack of results using BYOL method, I'm leaning towards borderline rejection. Adding these results would make the paper stronger.	3
This paper proposed a simple method for multi-task problems. The experimental results are convincing, but the usage of this method is limited. Because this method can only be used for environments with multiple reward functions, and their approach can not generate multi-style behaviors for the same task. I think it would be more reasonable for the author to reformat their paper as a multi-task paper.	2
The paper proposes a simple and yet effective algorithm for multi-objective reinforcement learning. The paper is well written and the results are convincing. However, it misses an important prior work that has similar high-level ideas. The examples in the Evaluation Section of this paper show its great potential in real-world applications, especially for designing AI in video games.	2
Following the rebuttal and the paper revisions, I vote to accept. The paper demonstrates that several distinct behaviours can be successfully learned with a novel single-actor, multi-critic setup. While the technique is simple and the main idea shares some motivation with previous work, I believe this will be a valuable contribution to the multi-task RL literature.	3
"The paper presents a technique with promise for training agents to perform tasks in a variety of ways. The current empirical results are ambiguous when comparing the proposed method to baseline methods, leading me to recommend rejection.

My score would increase if the authors can provide clear (statistically) conclusive results of superior performance of the proposed method.

**Update**
I have increased my score in light of the additional experiment seeds added and clearer differences in performance from the resulting reductions in variance."	3
The paper is well-written, easy to follow, and well-position regarding related work (with a note on the Actor-Mimic). The proposed method is simple, yet unique and performs well. An extra experiment that compares MultiCriticAL with policy-based multi-task approaches would have been nice, but I overall still recommend accepting this paper.	3
"Given the issues with the experiments and presentation above, I don't think the paper is ready for publication.

### Post-rebuttal

Increasing score from reject to borderline/marginal  reject. I think the paper still needs a bit more work, but would not object to it being accepted. Please see detailed comment in response to rebuttal below."	3
The paper introduces an interesting observation and proposes novel modules based on the observation where the development is very logical. Overall, the paper is easy to read, however more proofreading is required. Experimental results improve the SOTA but there should be justification for some results. 	3
This paper introduces an interesting idea for improving image deblurring models: the deblurred image should not contain information/traces about the original blur.  This is a bold statement. The paper crystalizes this idea using two networks (deblurring and re-blurring network) that are co-trained with different loss terms. The implementation seems (a little) too complex with many terms that try to avoid different types of artifacts/degenerative cases. But in the end this seems like a valid implementation of the initial idea. The current analysis didn't convince me that the method is doing much better or better than other methods that just compare the restored and sharp images on a pre-computed feature space (is this a matter of tuning the contributions of each loss term?). I think the paper needs to better discuss and analyze this. Additionally, the idea seems to be specific for dealing with blur. Can the same idea apply to other (restoration) tasks?	3
"Overall, the proposed method is outperforming previous approaches in terms of disentanglement scores. My main concern is the general complexity of the model. 

UPD: I keep my score the same."	3
"1) Novelty:
- The authors propose a novel and interesting idea of learning disentangled representations by reusing the decoder/generator of pretrained generative models and only learning the directions that lead to disentanglement. 

2) Correctness of the method:
- Is there an absolute operator in Eq. 1?
- $\mathcal{V} \in \mathbb{R}^{J}$ is not mathematically correct, it should be $\mathcal{V} \subset \mathbb{R}^J_+$ since $v$ is always >= 0.
- Why are the sizes of the query key set and the positive key set different? They should be the same in contrastive learning. If the authors use multiple positive keys per query key, the size of the positive key set should be $B \times N$ instead of $N$.
- I think the formula of NCE in Eq. 2 is incorrect. The sum in the nominator should be outside the $\log$ and should be treated as the mean over $N$. Which equation in (van de Oord et al., 2018) did the authors refer to? 
- I would like to see mathematical proof of why the BCELoss $\mathcal{L}\_{logit}$ (Eq. 3) is a lower bound of the NCELoss $\mathcal{L}\_{NCE}$ (Eq. 2). I have never seen such result in (van de Oord et al., 2018) or anywhere.
- The loss Eq. 3 is indeed an upper bound of the negative mutual information (if the two sums in Eq. 4 is replaced by mean) although it has a different form from InfoNCE (van de Oord et al., 2018). I think the authors simply use the loss without really understanding what it is.
- I do not really understand how contrastive learning help disentangle the right factors (or directions) if the latent code z and the shift $\epsilon$ are sampled randomly for both query and positive samples. Could the authors elaborate more on this?

3) Clarity in presentation:
- It seems that the flipping of negative samples into positive samples in Eq. 7 plays an important role in the model’s performance but is not carefully analyzed in the paper. I am curious about the main cause of flipping. Is it due to the poor selection of positive and negative samples at the first step? How does the flipping rate change during learning? Could the authors show a curve of this? In Table 4, I see that negative flipping does not improve the performance if one-hot regularization is not enforced. Could the authors explain this?
- How MIG and DCI are computed for discovering-based methods that only use GAN? It seems that these discovering-based methods use no encoder which is required to compute MIG and DCI.

4) Choosing metrics:
- I think the authors should use more up-to-date disentanglement metrics (e.g. JEMMIG [1]) when evaluating their method. The problem with MIG and DCI is that MIG was shown to capture only modularity, DCI consists of 3 separate sub-metrics and each of them only captures one aspect of disentanglement [2]. Could the authors tell which sub-metric of DCI you are using for evaluation? 

[1] Theory and Evaluation Metrics for Learning Disentangled Representations, Do & Tran, ICLR 2020

[2] Measuring Disentanglement - A Review of Metrics, Zaidi et al., 2020
"	3
"I think the paper contains good practical ideas and a very extensive experimental evaluation.

The main weakness of the paper is the lack of deep insights. The reader learns a simple idea that is easy to implement and works well in practice, but does not get an answer to the ""why"" question.

Overall the good outweighs the bad.

--------- UPDATE ---------

I have read the other reviews and the rebuttals. The authors have clarified some details in the experiments. I think the experiments are strong and give valuable data for future research. I raise my score to accept.
"	3
The introduced method is simple, effective, and general for disentanglement learning. I recommend accept for this paper.	3
This is an overall well written manuscript describing a novel method backed by in-depth theoretical analysis. The experiments were well designed and evaluations relatively thorough. There are some questions that can be addressed and improvements that can be made to the current manuscript, but overall I consider this to be an interesting work that will have good discussion potential for the community.	4
"I believe the theoretical contribution of this paper is valuable to the group-based disentanglement learning (or general disentanglement learning) community, but I still have concerns listed in the main review section. I currently rate this paper as weak reject, but I will be glad to improve my score if my concerns are properly solved.

======

After rebuttal:
Thanks for the authors' response to my comments. I think a large part of my concerns has been addressed. I will increase the score to 6 weak accept."	3
I find this paper to be overselling the importance of its theoretical contribution, and will therefore recommend rejection. I am looking forward to discussion with the authors, and will reconsider my rating if the authors respond to the two points listed as actionnable feedback in my main review. In particular I'd like to hear more about the isomorphism loss. 	2
This paper proposed a new theoretical framework for achieving unsupervised representation disentanglement. My concern is that this work only compares it with the baselines before 2018, so it is hard to say whether it outperforms the recent baselines. 	3
This paper is clear and well-written. However, the novelty of the paper is somehow limited. Also, there are limited insightful investigations about the idea and intrinsic motivations. Finally, some experiments are missed.	2
"The idea is simple yet efficient for both lightweight and large image SR networks. The ablation study, like the visualization of pruning process, demonstrates the effect of the method. The main comparison results with others are impressive.

--------------------------------
The authors addressed my concerns in the reply. After considering other reviews and response, I decide to keep my initial score and vote for acceptance."	4
"However, there are some ambiguity explanations, and the model design and hyperparameter were arbitrarily selected.
"	3
This paper is well written, containing extensive experiments and impressive results.	3
"Imitation learning can be applied in tasks where a reward function is hard to specify or too sparse to be used in practice. The methods introduced in this work is useful for imitation learning from images directly. However, the learning from pixels can be ambiguous in certain cases since the actions cannot be fully described in a video. 

A paper to appear in the Proceeding of Deep Reinforced Learning Workshop in NeurIPS 2021 has the same tile with this submission: Imitation Learning from Pixel Observations for Continuous Control (https://nips.cc/Conferences/2021/Schedule?showEvent=21848). "	2
I agree wholeheartedly with the goal that the authors have stated in the early portions of the paper, but I think the work that has been done--or at the very least, the way it has been presented--fails to live up to that goal. I'd encourage the authors to focus more on the design decisions and tradeoffs in the existing space of algorithms rather than trying to show that their proposed algorithms dominate others.	3
"I am towards rejection of this paper. Sound and interesting approach which achieves good results, but there is little novelty. Furthermore, paper is quite hard to read and to understand what are the main contributions, which makes an approach that seems easy quite complex.
"	2
My main concern for this paper is the setting. In my view, it is unreasonable to use an encoder from the RL-based expert for learning from videos without actions. 	2
"The authors' response and Appendix F & G address my concern regarding real-world experiments, so I am raising my ratings to ""8: accept, good paper"".

> [Original Review]
My current recommendation is ""6: marginally above the acceptance threshold"", given the simplicity of the method and its results on synthetic dataset. I am concerned about its actual performance on real-world data, and I am happy to increase my rating if the method demonstrates similar improvement over prior work on real-world datasets. "	3
"Summary of review: The paper is very well written with clear mathematics and clear delineation of contributions. The authors present a modification that makes older work scalable and show good results on multiple datasets, competing against the baseline method as well as other methods.

"	3
The paper presents a method which is able to produce multiple plausible shape completions of a sparse input. The main novelty is adding a shape code to the voxel which allows for decoding continuous surfaces. The training procedure of the state transition was adjusted accordingly. The results look promising quantitatively and qualitatively. The submission has a few weaknesses but I think the approach is novel enough and shows quantitative improvement.	3
From my own perspective, this paper is interesting and strong. Especially, the scene local feature autoencoder presented in this paper is inspiring. This paper presents a way towards large-scale high-resolution scene generation/completion. Although I think the structuring of the paper could be improved.	3
"In general, this paper introduces an interesting way of combining neural implicit representations and generative models. The mathematical proofs seem to be sound and correct, but in the experimental part, there are some important experiments and explanations missing. I will change my score accordingly based on the reply from the authors.

Moreover, the previous work GCA has not made the code public. It would be great if the authors of cGCA can open source the code to benefit the community. This is another factor for me when making the decision.
"	3
As aformentioned, I think this is an impactful work. The authors understand both FR and FL in depth and proposed a convincing solution. I think this work is inspiring for both industry and academia. 	4
"The motivation and threat model of the proposed work are not clear. The proposed approach appears to have some practical limitations, which have not been considered. The privacy analysis is not very robust and the experimental validation is also weak. 

During the rebuttal process, concerns 2 and 4 have been addressed to a great extent. Concern 1 still remains a theoretical possibility, but given that I do not have strong evidence to disprove the claims in the paper, it is fair to give the benefit of doubt to the authors. Concern 3 is still a very valid concern, but has been dismissed lightly in the rebuttal. Overall, I would like to upgrade my rating by one level based on all the discussion.  "	2
"I think the idea is novel and there are many solid theoretical proofs for differentially private guarantee to secure the information exchange among clients for model training. However, some important parts for federated learning or face recognition are still not well explained in the current paper. Since the federated learning is to train the model distributedly, some clients may drop for certain round of communication due to network issues. However, from algorithm 2, the experimental results are mainly done in the perfect setting that each round, all clients participate in the training and share their clusters to others through the server. It can be imagined that if some clusters do not participate in the training for a certain round, the consensus-aware recognition may fail and still result in the same situation as shown in Fig. 1(a). Thus, It is worth a study how it  would influence the final performance.

Does the author try different initial models to start training? Does the proposed framework require a strong face model to begin with (trained with a larger dataset and finetuned onto a smaller or similiar-size dataset)? Is it possible to start from scratch or a pretrained model with smaller dataset (CASIA-WebFace) and then to finetune onto a larger dataset (MS1M)?

Can the model scale to train with MS1M or deepGlinit or recent released WebFace260M which contains much more subjects (class centers) than the CASIA-WebFace (10575) and BUPTBalancedFace? 

I will adjust my rating according to the feedback from the authors."	3
Based on the primary opinion above, I will temporarily give the ranking of “marginally below the acceptance threshold”. The final rating will depend on the authors’ feedback.	4
